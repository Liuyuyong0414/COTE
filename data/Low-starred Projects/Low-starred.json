[
    {
        "repo_name": "mixpanel___mixpanel-java",
        "commit": "3e0b4d82219e7ff1d9bc88bc1e351eeffa9914b8",
        "commit_message": "fixed string equality check in ClientDelivery.addMessage()\n",
        "p_path": "src/main/java/com/mixpanel/mixpanelapi/ClientDelivery.java",
        "t_path": "src/test/java/com/mixpanel/mixpanelapi/MixpanelAPITest.java",
        "p_name": "addMessage",
        "t_name": "testValidate",
        "lpfc": "/**\n * Adds an individual message to this delivery. Messages to Mixpanel are often more efficient when sent in batches.\n *\n * @param message a JSONObject produced by #{@link MessageBuilder}. Arguments not from MessageBuilder will throw an exception.\n * @throws MixpanelMessageException if the given JSONObject is not formatted appropriately.\n * @see MessageBuilder\n */\npublic void addMessage(JSONObject message) {\n    if (!isValidMessage(message)) {\n        throw new MixpanelMessageException(\"Given JSONObject was not a valid Mixpanel message\", message);\n    }\n    // ELSE message is valid\n    try {\n        String messageType = message.getString(\"message_type\");\n        JSONObject messageContent = message.getJSONObject(\"message\");\n        if (messageType == \"event\") {\n            mEventsMessages.add(messageContent);\n        } else if (messageType == \"people\") {\n            mPeopleMessages.add(messageContent);\n        }\n    } catch (JSONException e) {\n        throw new RuntimeException(\"Apparently valid mixpanel message could not be interpreted.\", e);\n    }\n}",
        "rpfc": "/**\n * Adds an individual message to this delivery. Messages to Mixpanel are often more efficient when sent in batches.\n *\n * @param message a JSONObject produced by #{@link MessageBuilder}. Arguments not from MessageBuilder will throw an exception.\n * @throws MixpanelMessageException if the given JSONObject is not formatted appropriately.\n * @see MessageBuilder\n */\npublic void addMessage(JSONObject message) {\n    if (!isValidMessage(message)) {\n        throw new MixpanelMessageException(\"Given JSONObject was not a valid Mixpanel message\", message);\n    }\n    // ELSE message is valid\n    try {\n        String messageType = message.getString(\"message_type\");\n        JSONObject messageContent = message.getJSONObject(\"message\");\n        if (messageType.equals(\"event\")) {\n            mEventsMessages.add(messageContent);\n        } else if (messageType.equals(\"people\")) {\n            mPeopleMessages.add(messageContent);\n        }\n    } catch (JSONException e) {\n        throw new RuntimeException(\"Apparently valid mixpanel message could not be interpreted.\", e);\n    }\n}",
        "tuc": "public void testValidate() {\n    ClientDelivery c = new ClientDelivery();\n    JSONObject event = mBuilder.event(\"a distinct id\", \"login\", mSampleProps);\n    assertTrue(c.isValidMessage(event));\n    try {\n        JSONObject rebuitMessage = new JSONObject(event.toString());\n        assertTrue(c.isValidMessage(rebuitMessage));\n    } catch (JSONException e) {\n        fail(\"Failed to build JSONObject\");\n    }\n}",
        "label": 1
    },
    {
        "repo_name": "pkiraly___metadata-qa-api",
        "commit": "253313772d2c3635e231a323c78d01be9ed26d0d",
        "commit_message": "Swaps language detection package\n",
        "p_path": "src/main/java/de/gwdg/metadataqa/api/util/QALanguageDetector.java",
        "t_path": "src/test/java/de/gwdg/metadataqa/api/util/QALanguageDetectorTest.java",
        "p_name": "detect",
        "t_name": "constructionTest",
        "lpfc": "public Optional<LdLocale> detect(String text) {\n    var textObject = textObjectFactory.forText(text);\n    return languageDetector.detect(textObject);\n}",
        "rpfc": "public Language detect(String text) {\n    final Language detectedLanguage = languageDetector.detectLanguageOf(text);\n    return detectedLanguage;\n}",
        "tuc": "@Test\npublic void constructionTest() throws IOException {\n    QALanguageDetector languageDetector = new QALanguageDetector();\n    Optional<LdLocale> langs = languageDetector.detect(\"There are no plans to deprecate this class in the foreseeable future.\");\n    assertNotNull(langs);\n    assertTrue(langs.isPresent());\n    assertEquals(\"en\", langs.get().getLanguage());\n    langs = languageDetector.detect(\"Der Literaturnobelpreis 2016 ging an den Musiker Bob Dylan. Mit dieser Entscheidung erkannte die Jury zum ersten Mal die literarische Qualit\u00e4t von Songtexten an. Nicht jeder fand das gut.\");\n    assertNotNull(langs);\n    assertTrue(langs.isPresent());\n    assertEquals(\"de\", langs.get().getLanguage());\n    langs = languageDetector.detect(\"\u00c9g a napmelegt\u0151l a kop\u00e1r sz\u00edk sarja.\");\n    assertNotNull(langs);\n    assertTrue(langs.isPresent());\n    assertEquals(\"hu\", langs.get().getLanguage());\n    langs = languageDetector.detect(\"1984.\");\n    assertNotNull(langs);\n    assertFalse(langs.isPresent());\n}",
        "label": 1
    },
    {
        "repo_name": "apache___maven-plugin-tools",
        "commit": "0d658ca66ecfb468b7e16019aae92aefe9061546",
        "commit_message": "[MPLUGIN-98] Remove javadoc inline tags from output of help mojo\n\ngit-svn-id: https://svn.apache.org/repos/asf/maven/plugin-tools/trunk@643559 13f79535-47bb-0310-9956-ffa450edef68\n",
        "p_path": "maven-plugin-tools-api/src/main/java/org/apache/maven/tools/plugin/generator/PluginHelpGenerator.java",
        "t_path": "maven-plugin-tools-api/src/test/java/org/apache/maven/tools/plugin/generator/PluginHelpGeneratorTest.java",
        "p_name": "toText",
        "t_name": "testToText",
        "lpfc": "protected static String toText(String str) throws IOException {\n    if (StringUtils.isEmpty(str)) {\n        return \"\";\n    }\n    final StringBuffer sb = new StringBuffer();\n    HTMLEditorKit.Parser parser = new ParserDelegator();\n    HTMLEditorKit.ParserCallback htmlCallback = new HTMLEditorKit.ParserCallback() {\n        public void handleText(char[] data, int pos) {\n            if (data[0] == '>') {\n                for (int i = 1; i < data.length; i++) {\n                    if (data[i] == '\\n') {\n                        sb.append(' ');\n                    } else {\n                        sb.append(data[i]);\n                    }\n                }\n            } else {\n                for (int i = 0; i < data.length; i++) {\n                    if (data[i] == '\\n') {\n                        sb.append(' ');\n                    } else {\n                        sb.append(data[i]);\n                    }\n                }\n            }\n        }\n    };\n    parser.parse(new StringReader(str), htmlCallback, true);\n    return StringUtils.replace(sb.toString(), \"\\\"\", \"'\");\n}",
        "rpfc": "protected static String toText(String str) throws IOException {\n    if (StringUtils.isEmpty(str)) {\n        return \"\";\n    }\n    final StringBuffer sb = new StringBuffer();\n    HTMLEditorKit.Parser parser = new ParserDelegator();\n    HTMLEditorKit.ParserCallback htmlCallback = new HTMLEditorKit.ParserCallback() {\n        public void handleText(char[] data, int pos) {\n            if (data[0] == '>') {\n                for (int i = 1; i < data.length; i++) {\n                    if (data[i] == '\\n') {\n                        sb.append(' ');\n                    } else {\n                        sb.append(data[i]);\n                    }\n                }\n            } else {\n                for (int i = 0; i < data.length; i++) {\n                    if (data[i] == '\\n') {\n                        sb.append(' ');\n                    } else {\n                        sb.append(data[i]);\n                    }\n                }\n            }\n        }\n    };\n    parser.parse(new StringReader(PluginUtils.makeHtmlValid(str)), htmlCallback, true);\n    return StringUtils.replace(sb.toString(), \"\\\"\", \"'\");\n}",
        "tuc": "public void testToText() throws Exception {\n    String javadoc = \"\";\n    assertEquals(\"\", PluginHelpGenerator.toText(javadoc));\n    // true HTML\n    javadoc = \"Generates <i>something</i> for the project.\";\n    assertEquals(\"Generates something for the project.\", PluginHelpGenerator.toText(javadoc));\n    // wrong HTML\n    javadoc = \"Generates <i>something</i> <b> for the project.\";\n    assertEquals(\"Generates something for the project.\", PluginHelpGenerator.toText(javadoc));\n}",
        "label": 1
    },
    {
        "repo_name": "apache___maven-plugin-tools",
        "commit": "cee8797a4490b7e94d300535a370d9ee5888c447",
        "commit_message": "MPLUGIN-102: restore previous functionality and add a deprecation warning\n\ngit-svn-id: https://svn.apache.org/repos/asf/maven/plugin-tools/trunk@639782 13f79535-47bb-0310-9956-ffa450edef68\n",
        "p_path": "maven-plugin-tools-api/src/main/java/org/apache/maven/tools/plugin/scanner/DefaultMojoScanner.java",
        "t_path": "maven-plugin-tools-api/src/test/java/org/apache/maven/tools/plugin/scanner/DefaultMojoScannerTest.java",
        "p_name": "populatePluginDescriptor",
        "t_name": "testNoExtractorsThroughEmptySet",
        "lpfc": "/**\n * {@inheritDoc}\n */\npublic void populatePluginDescriptor(MavenProject project, PluginDescriptor pluginDescriptor) throws ExtractionException, InvalidPluginDescriptorException {\n    Logger logger = getLogger();\n    Set activeExtractors = getActiveExtractors();\n    logger.info(\"Using \" + activeExtractors.size() + \" extractors.\");\n    int numMojoDescriptors = 0;\n    for (Iterator it = activeExtractors.iterator(); it.hasNext(); ) {\n        String language = (String) it.next();\n        MojoDescriptorExtractor extractor = (MojoDescriptorExtractor) mojoDescriptorExtractors.get(language);\n        if (extractor == null) {\n            throw new ExtractionException(\"No extractor for language: \" + language);\n        }\n        logger.info(\"Applying extractor for language: \" + language);\n        List extractorDescriptors = extractor.execute(project, pluginDescriptor);\n        logger.info(\"Extractor for language: \" + language + \" found \" + extractorDescriptors.size() + \" mojo descriptors.\");\n        numMojoDescriptors += extractorDescriptors.size();\n        for (Iterator descriptorIt = extractorDescriptors.iterator(); descriptorIt.hasNext(); ) {\n            MojoDescriptor descriptor = (MojoDescriptor) descriptorIt.next();\n            logger.debug(\"Adding mojo: \" + descriptor + \" to plugin descriptor.\");\n            descriptor.setPluginDescriptor(pluginDescriptor);\n            pluginDescriptor.addMojo(descriptor);\n        }\n    }\n    if (numMojoDescriptors == 0) {\n        throw new InvalidPluginDescriptorException(\"No mojo descriptors found in plugin.\");\n    }\n}",
        "rpfc": "/**\n * {@inheritDoc}\n */\npublic void populatePluginDescriptor(MavenProject project, PluginDescriptor pluginDescriptor) throws ExtractionException, InvalidPluginDescriptorException {\n    Logger logger = getLogger();\n    Set activeExtractors = getActiveExtractors();\n    logger.info(\"Using \" + activeExtractors.size() + \" extractors.\");\n    int numMojoDescriptors = 0;\n    for (Iterator it = activeExtractors.iterator(); it.hasNext(); ) {\n        String language = (String) it.next();\n        MojoDescriptorExtractor extractor = (MojoDescriptorExtractor) mojoDescriptorExtractors.get(language);\n        if (extractor == null) {\n            throw new ExtractionException(\"No extractor for language: \" + language);\n        }\n        logger.info(\"Applying extractor for language: \" + language);\n        List extractorDescriptors = extractor.execute(project, pluginDescriptor);\n        logger.info(\"Extractor for language: \" + language + \" found \" + extractorDescriptors.size() + \" mojo descriptors.\");\n        numMojoDescriptors += extractorDescriptors.size();\n        for (Iterator descriptorIt = extractorDescriptors.iterator(); descriptorIt.hasNext(); ) {\n            MojoDescriptor descriptor = (MojoDescriptor) descriptorIt.next();\n            logger.debug(\"Adding mojo: \" + descriptor + \" to plugin descriptor.\");\n            descriptor.setPluginDescriptor(pluginDescriptor);\n            pluginDescriptor.addMojo(descriptor);\n        }\n    }\n    if (numMojoDescriptors == 0) {\n        //MPLUGIN-102. Restore the old functionality and allow a deprecation period\n        //throw new InvalidPluginDescriptorException( \"No mojo descriptors were found in this project.\" );\n        for (int i = 0; i < 10; i++) {\n            logger.warn(\"\");\n        }\n        logger.warn(\"*******************************************************\");\n        logger.warn(\"Deprecation Alert:\");\n        logger.warn(\"No mojo descriptors were found in this project, which has a packaging type of maven-plugin.\");\n        logger.warn(\"In future versions of the plugin tools, this will fail the build.\");\n        logger.warn(\"If this project is an archetype, change the packaging type from maven-plugin to maven-archetype.\");\n        logger.warn(\"********************************************************\");\n        for (int i = 0; i < 10; i++) {\n            logger.warn(\"\");\n        }\n    }\n}",
        "tuc": "public void testNoExtractorsThroughEmptySet() throws Exception {\n    PluginDescriptor pluginDescriptor = createPluginDescriptor();\n    scanner.setActiveExtractors(Collections.EMPTY_SET);\n    try {\n        scanner.populatePluginDescriptor(project, pluginDescriptor);\n        fail(\"Expected exception\");\n    } catch (InvalidPluginDescriptorException e) {\n    }\n    checkResult(pluginDescriptor, Collections.EMPTY_SET);\n}",
        "label": 1
    },
    {
        "repo_name": "unchartedsoftware___aperture-tiles",
        "commit": "7c635b087217b0b1c8a501f0f43340713eb717b4",
        "commit_message": "Fix return of pyramided slices, and test it.\n",
        "p_path": "binning-utilities/src/main/java/com/oculusinfo/binning/io/impl/HBaseSlicedPyramidIO.java",
        "t_path": "binning-utilities/src/test/java/com/oculusinfo/binning/io/impl/HBaseSlicedPyramidIOTest.java",
        "p_name": "readTiles",
        "t_name": "testSliceDecompositionVsSingleSlices",
        "lpfc": "@Override\npublic <T> List<TileData<T>> readTiles(String tableName, TileSerializer<T> serializer, Iterable<TileIndex> tiles) throws IOException {\n    Matcher m = SLICE_PATTERN.matcher(tableName);\n    TypeDescriptor binType = serializer.getBinTypeDescription();\n    if (List.class == binType.getMainType() && m.matches()) {\n        String realName = m.group(\"table\");\n        HBaseColumn[] columns;\n        int min = Integer.parseInt(m.group(\"min\"));\n        if (null == m.group(\"max\")) {\n            columns = new HBaseColumn[] { getSliceColumn(min, min) };\n        } else {\n            int max = Integer.parseInt(m.group(\"max\"));\n            List<Pair<Integer, Integer>> sliceRanges;\n            if (_doPyramidding) {\n                sliceRanges = decomposeRange(min, max);\n            } else {\n                sliceRanges = new ArrayList<>();\n                for (int n = min; n <= max; ++n) {\n                    sliceRanges.add(new Pair<>(n, n));\n                }\n            }\n            columns = new HBaseColumn[sliceRanges.size()];\n            for (int i = 0; i < sliceRanges.size(); ++i) {\n                Pair<Integer, Integer> sliceRange = sliceRanges.get(i);\n                columns[i] = getSliceColumn(sliceRange.getFirst(), sliceRange.getSecond());\n            }\n        }\n        List<TileData<T>> rawResults = super.readTiles(realName, serializer, tiles, columns);\n        if (1 == columns.length)\n            return rawResults;\n        else {\n            int numRaw = rawResults.size();\n            int numReal = numRaw / columns.length;\n            List<TileData<T>> realResults = new ArrayList<>(numReal);\n            for (int i = 0; i < numReal; ++i) {\n                realResults.add(compose((List) rawResults, i, numReal, columns.length));\n            }\n        }\n        return rawResults;\n    } else {\n        return super.readTiles(tableName, serializer, tiles);\n    }\n}",
        "rpfc": "@Override\npublic <T> List<TileData<T>> readTiles(String tableName, TileSerializer<T> serializer, Iterable<TileIndex> tiles) throws IOException {\n    Matcher m = SLICE_PATTERN.matcher(tableName);\n    TypeDescriptor binType = serializer.getBinTypeDescription();\n    if (List.class == binType.getMainType() && m.matches()) {\n        String realName = m.group(\"table\");\n        HBaseColumn[] columns;\n        int min = Integer.parseInt(m.group(\"min\"));\n        if (null == m.group(\"max\")) {\n            columns = new HBaseColumn[] { getSliceColumn(min, min) };\n        } else {\n            int max = Integer.parseInt(m.group(\"max\"));\n            List<Pair<Integer, Integer>> sliceRanges;\n            if (_doPyramidding) {\n                sliceRanges = decomposeRange(min, max);\n            } else {\n                sliceRanges = new ArrayList<>();\n                for (int n = min; n <= max; ++n) {\n                    sliceRanges.add(new Pair<>(n, n));\n                }\n            }\n            columns = new HBaseColumn[sliceRanges.size()];\n            for (int i = 0; i < sliceRanges.size(); ++i) {\n                Pair<Integer, Integer> sliceRange = sliceRanges.get(i);\n                columns[i] = getSliceColumn(sliceRange.getFirst(), sliceRange.getSecond());\n            }\n        }\n        List<TileData<T>> rawResults = super.readTiles(realName, serializer, tiles, columns);\n        if (1 == columns.length)\n            return rawResults;\n        else {\n            int numRaw = rawResults.size();\n            int numReal = numRaw / columns.length;\n            List<TileData<T>> realResults = new ArrayList<>(numReal);\n            for (int i = 0; i < numReal; ++i) {\n                realResults.add(compose((List) rawResults, i, columns.length, numReal));\n            }\n            return realResults;\n        }\n    } else {\n        return super.readTiles(tableName, serializer, tiles);\n    }\n}",
        "tuc": "@Test\npublic void testSliceDecompositionVsSingleSlices() throws Exception {\n    String table = \"hbsioTest\";\n    HBaseSlicedPyramidIO io = new HBaseSlicedPyramidIO(\"hadoop-s1\", \"2181\", \"hadoop-s1:60000\");\n    TileSerializer<List<Integer>> serializer = new KryoSerializer<>(new TypeDescriptor(List.class, new TypeDescriptor(Integer.class)));\n}",
        "label": 1
    },
    {
        "repo_name": "myzhan___locust4j",
        "commit": "e184868006fceec27145a8c2be8dce4f9fcb974d",
        "commit_message": "Merge pull request #39 from crichardson32/fix-rps\n\nFix requests counts on error events.",
        "p_path": "src/main/java/com/github/myzhan/locust4j/stats/Stats.java",
        "t_path": "src/test/java/com/github/myzhan/locust4j/stats/TestStats.java",
        "p_name": "run",
        "t_name": "TestLogError",
        "lpfc": "@Override\npublic void run() {\n    String name = Thread.currentThread().getName();\n    Thread.currentThread().setName(name + \"stats\");\n    while (true) {\n        boolean allEmpty = true;\n        RequestSuccess successMessage = reportSuccessQueue.poll();\n        if (successMessage != null) {\n            this.logRequest(successMessage.getRequestType(), successMessage.getName(), successMessage.getResponseTime(), successMessage.getContentLength());\n            allEmpty = false;\n        }\n        RequestFailure failureMessage = reportFailureQueue.poll();\n        if (null != failureMessage) {\n            this.logError(failureMessage.getRequestType(), failureMessage.getName(), failureMessage.getError());\n            allEmpty = false;\n        }\n        Boolean needToClearStats = clearStatsQueue.poll();\n        if (null != needToClearStats && needToClearStats) {\n            this.clearAll();\n            allEmpty = false;\n        }\n        Boolean timeToReport = timeToReportQueue.poll();\n        if (null != timeToReport) {\n            Map<String, Object> data = this.collectReportData();\n            messageToRunnerQueue.add(data);\n            allEmpty = false;\n        }\n        if (allEmpty) {\n            this.sleep();\n        }\n    }\n}",
        "rpfc": "@Override\npublic void run() {\n    String name = Thread.currentThread().getName();\n    Thread.currentThread().setName(name + \"stats\");\n    while (true) {\n        boolean allEmpty = true;\n        RequestSuccess successMessage = reportSuccessQueue.poll();\n        if (successMessage != null) {\n            this.logRequest(successMessage.getRequestType(), successMessage.getName(), successMessage.getResponseTime(), successMessage.getContentLength());\n            allEmpty = false;\n        }\n        RequestFailure failureMessage = reportFailureQueue.poll();\n        if (null != failureMessage) {\n            this.logRequest(failureMessage.getRequestType(), failureMessage.getName(), failureMessage.getResponseTime(), 0);\n            this.logError(failureMessage.getRequestType(), failureMessage.getName(), failureMessage.getError());\n            allEmpty = false;\n        }\n        Boolean needToClearStats = clearStatsQueue.poll();\n        if (null != needToClearStats && needToClearStats) {\n            this.clearAll();\n            allEmpty = false;\n        }\n        Boolean timeToReport = timeToReportQueue.poll();\n        if (null != timeToReport) {\n            Map<String, Object> data = this.collectReportData();\n            messageToRunnerQueue.add(data);\n            allEmpty = false;\n        }\n        if (allEmpty) {\n            this.sleep();\n        }\n    }\n}",
        "tuc": "@Test\npublic void TestLogError() {\n    stats.logError(\"http\", \"test\", \"Test Error\");\n    stats.logError(\"udp\", \"test\", \"Unknown Error\");\n    StatsEntry entry = stats.get(\"test\", \"http\");\n    assertEquals(1, entry.getNumFailures());\n    StatsEntry total = stats.getTotal();\n    assertEquals(2, total.getNumFailures());\n    Map<String, Map<String, Object>> errors = stats.serializeErrors();\n    String httpKey = Utils.md5(\"http\" + \"test\" + \"Test Error\");\n    Map<String, Object> httpError = errors.get(httpKey);\n    assertEquals(\"test\", httpError.get(\"name\"));\n    assertEquals(1L, httpError.get(\"occurrences\"));\n    assertEquals(\"http\", httpError.get(\"method\"));\n    assertEquals(\"Test Error\", httpError.get(\"error\"));\n    String udpKey = Utils.md5(\"udp\" + \"test\" + \"Unknown Error\");\n    Map<String, Object> udpError = errors.get(udpKey);\n    assertEquals(\"test\", udpError.get(\"name\"));\n    assertEquals(1L, udpError.get(\"occurrences\"));\n    assertEquals(\"udp\", udpError.get(\"method\"));\n    assertEquals(\"Unknown Error\", udpError.get(\"error\"));\n}",
        "label": 1
    },
    {
        "repo_name": "myzhan___locust4j",
        "commit": "d1b3c8b05880641a07965ea362d80eb58397d806",
        "commit_message": "Fix requests counts on error events.\n\nWhen errors happen requests are not being counted which throws off\nsuccess failure ratios, along with rps.\n",
        "p_path": "src/main/java/com/github/myzhan/locust4j/stats/Stats.java",
        "t_path": "src/test/java/com/github/myzhan/locust4j/stats/TestStats.java",
        "p_name": "run",
        "t_name": "TestLogError",
        "lpfc": "@Override\npublic void run() {\n    String name = Thread.currentThread().getName();\n    Thread.currentThread().setName(name + \"stats\");\n    while (true) {\n        boolean allEmpty = true;\n        RequestSuccess successMessage = reportSuccessQueue.poll();\n        if (successMessage != null) {\n            this.logRequest(successMessage.getRequestType(), successMessage.getName(), successMessage.getResponseTime(), successMessage.getContentLength());\n            allEmpty = false;\n        }\n        RequestFailure failureMessage = reportFailureQueue.poll();\n        if (null != failureMessage) {\n            this.logError(failureMessage.getRequestType(), failureMessage.getName(), failureMessage.getError());\n            allEmpty = false;\n        }\n        Boolean needToClearStats = clearStatsQueue.poll();\n        if (null != needToClearStats && needToClearStats) {\n            this.clearAll();\n            allEmpty = false;\n        }\n        Boolean timeToReport = timeToReportQueue.poll();\n        if (null != timeToReport) {\n            Map<String, Object> data = this.collectReportData();\n            messageToRunnerQueue.add(data);\n            allEmpty = false;\n        }\n        if (allEmpty) {\n            this.sleep();\n        }\n    }\n}",
        "rpfc": "@Override\npublic void run() {\n    String name = Thread.currentThread().getName();\n    Thread.currentThread().setName(name + \"stats\");\n    while (true) {\n        boolean allEmpty = true;\n        RequestSuccess successMessage = reportSuccessQueue.poll();\n        if (successMessage != null) {\n            this.logRequest(successMessage.getRequestType(), successMessage.getName(), successMessage.getResponseTime(), successMessage.getContentLength());\n            allEmpty = false;\n        }\n        RequestFailure failureMessage = reportFailureQueue.poll();\n        if (null != failureMessage) {\n            this.logRequest(failureMessage.getRequestType(), failureMessage.getName(), failureMessage.getResponseTime(), 0);\n            this.logError(failureMessage.getRequestType(), failureMessage.getName(), failureMessage.getError());\n            allEmpty = false;\n        }\n        Boolean needToClearStats = clearStatsQueue.poll();\n        if (null != needToClearStats && needToClearStats) {\n            this.clearAll();\n            allEmpty = false;\n        }\n        Boolean timeToReport = timeToReportQueue.poll();\n        if (null != timeToReport) {\n            Map<String, Object> data = this.collectReportData();\n            messageToRunnerQueue.add(data);\n            allEmpty = false;\n        }\n        if (allEmpty) {\n            this.sleep();\n        }\n    }\n}",
        "tuc": "@Test\npublic void TestLogError() {\n    stats.logError(\"http\", \"test\", \"Test Error\");\n    stats.logError(\"udp\", \"test\", \"Unknown Error\");\n    StatsEntry entry = stats.get(\"test\", \"http\");\n    assertEquals(1, entry.getNumFailures());\n    StatsEntry total = stats.getTotal();\n    assertEquals(2, total.getNumFailures());\n    Map<String, Map<String, Object>> errors = stats.serializeErrors();\n    String httpKey = Utils.md5(\"http\" + \"test\" + \"Test Error\");\n    Map<String, Object> httpError = errors.get(httpKey);\n    assertEquals(\"test\", httpError.get(\"name\"));\n    assertEquals(1L, httpError.get(\"occurrences\"));\n    assertEquals(\"http\", httpError.get(\"method\"));\n    assertEquals(\"Test Error\", httpError.get(\"error\"));\n    String udpKey = Utils.md5(\"udp\" + \"test\" + \"Unknown Error\");\n    Map<String, Object> udpError = errors.get(udpKey);\n    assertEquals(\"test\", udpError.get(\"name\"));\n    assertEquals(1L, udpError.get(\"occurrences\"));\n    assertEquals(\"udp\", udpError.get(\"method\"));\n    assertEquals(\"Unknown Error\", udpError.get(\"error\"));\n}",
        "label": 1
    },
    {
        "repo_name": "myzhan___locust4j",
        "commit": "66a71293f6e5fda086007a5476171735767c4927",
        "commit_message": "Merge pull request #36 from stephen-harris/fix/zmq-array\n\nAdd support for array in messages",
        "p_path": "src/main/java/com/github/myzhan/locust4j/message/Message.java",
        "t_path": "src/test/java/com/github/myzhan/locust4j/message/TestMessage.java",
        "p_name": "unpackMap",
        "t_name": "TestEncodeAndDecodeWithData",
        "lpfc": "public static Map<String, Object> unpackMap(MessageUnpacker unpacker) throws IOException {\n    int mapSize = unpacker.unpackMapHeader();\n    Map<String, Object> result = new HashMap<>(6);\n    while (mapSize > 0) {\n        String key = null;\n        // unpack key\n        if (unpacker.getNextFormat() == MessageFormat.NIL) {\n            unpacker.unpackNil();\n        } else {\n            key = unpacker.unpackString();\n        }\n        // unpack value\n        MessageFormat messageFormat = unpacker.getNextFormat();\n        Object value;\n        switch(messageFormat.getValueType()) {\n            case BOOLEAN:\n                value = unpacker.unpackBoolean();\n                break;\n            case FLOAT:\n                value = unpacker.unpackFloat();\n                break;\n            case INTEGER:\n                value = unpacker.unpackInt();\n                break;\n            case NIL:\n                value = null;\n                unpacker.unpackNil();\n                break;\n            case STRING:\n                value = unpacker.unpackString();\n                break;\n            case MAP:\n                value = unpackMap(unpacker);\n                break;\n            default:\n                throw new IOException(\"Message received unsupported type: \" + messageFormat.getValueType());\n        }\n        if (null != key) {\n            result.put(key, value);\n        }\n        mapSize--;\n    }\n    return result;\n}",
        "rpfc": "public static Map<String, Object> unpackMap(MessageUnpacker unpacker) throws IOException {\n    int mapSize = unpacker.unpackMapHeader();\n    Map<String, Object> result = new HashMap<>(6);\n    while (mapSize > 0) {\n        String key = null;\n        // unpack key\n        if (unpacker.getNextFormat() == MessageFormat.NIL) {\n            unpacker.unpackNil();\n        } else {\n            key = unpacker.unpackString();\n        }\n        // unpack value\n        MessageFormat messageFormat = unpacker.getNextFormat();\n        Object value;\n        switch(messageFormat.getValueType()) {\n            case BOOLEAN:\n                value = unpacker.unpackBoolean();\n                break;\n            case FLOAT:\n                value = unpacker.unpackFloat();\n                break;\n            case INTEGER:\n                value = unpacker.unpackInt();\n                break;\n            case NIL:\n                value = null;\n                unpacker.unpackNil();\n                break;\n            case STRING:\n                value = unpacker.unpackString();\n                break;\n            case MAP:\n                value = unpackMap(unpacker);\n                break;\n            case ARRAY:\n                int size = unpacker.unpackArrayHeader();\n                value = new ArrayList(size);\n                for (int index = 0; index < size; ++index) {\n                    ((ArrayList) value).add(unpacker.unpackString());\n                }\n                break;\n            default:\n                throw new IOException(\"Message received unsupported type: \" + messageFormat.getValueType());\n        }\n        if (null != key) {\n            result.put(key, value);\n        }\n        mapSize--;\n    }\n    return result;\n}",
        "tuc": "@Test\npublic void TestEncodeAndDecodeWithData() throws Exception {\n    Map<String, Object> data = new HashMap<>();\n    data.put(\"string\", \"world\");\n    data.put(\"int\", 1);\n    data.put(\"float\", 0.5f);\n    data.put(\"boolean\", true);\n    data.put(\"null\", null);\n    Message message = new Message(\"test\", data, null, \"nodeId\");\n    Message message2 = new Message(message.getBytes());\n    assertEquals(\"test\", message2.getType());\n    assertEquals(\"world\", message2.getData().get(\"string\"));\n    assertEquals(1, message2.getData().get(\"int\"));\n    assertEquals(0.5f, message2.getData().get(\"float\"));\n    assertEquals(true, message2.getData().get(\"boolean\"));\n    assertNull(message2.getData().get(\"null\"));\n    assertEquals(\"nodeId\", message2.getNodeID());\n}",
        "label": 1
    },
    {
        "repo_name": "myzhan___locust4j",
        "commit": "92e34b399865e0cc3a1ffce9a58e4781af2f0e38",
        "commit_message": "Add support for array in messages\n\nFixes #35.\n",
        "p_path": "src/main/java/com/github/myzhan/locust4j/message/Message.java",
        "t_path": "src/test/java/com/github/myzhan/locust4j/message/TestMessage.java",
        "p_name": "unpackMap",
        "t_name": "TestEncodeAndDecodeWithData",
        "lpfc": "public static Map<String, Object> unpackMap(MessageUnpacker unpacker) throws IOException {\n    int mapSize = unpacker.unpackMapHeader();\n    Map<String, Object> result = new HashMap<>(6);\n    while (mapSize > 0) {\n        String key = null;\n        // unpack key\n        if (unpacker.getNextFormat() == MessageFormat.NIL) {\n            unpacker.unpackNil();\n        } else {\n            key = unpacker.unpackString();\n        }\n        // unpack value\n        MessageFormat messageFormat = unpacker.getNextFormat();\n        Object value;\n        switch(messageFormat.getValueType()) {\n            case BOOLEAN:\n                value = unpacker.unpackBoolean();\n                break;\n            case FLOAT:\n                value = unpacker.unpackFloat();\n                break;\n            case INTEGER:\n                value = unpacker.unpackInt();\n                break;\n            case NIL:\n                value = null;\n                unpacker.unpackNil();\n                break;\n            case STRING:\n                value = unpacker.unpackString();\n                break;\n            case MAP:\n                value = unpackMap(unpacker);\n                break;\n            default:\n                throw new IOException(\"Message received unsupported type: \" + messageFormat.getValueType());\n        }\n        if (null != key) {\n            result.put(key, value);\n        }\n        mapSize--;\n    }\n    return result;\n}",
        "rpfc": "public static Map<String, Object> unpackMap(MessageUnpacker unpacker) throws IOException {\n    int mapSize = unpacker.unpackMapHeader();\n    Map<String, Object> result = new HashMap<>(6);\n    while (mapSize > 0) {\n        String key = null;\n        // unpack key\n        if (unpacker.getNextFormat() == MessageFormat.NIL) {\n            unpacker.unpackNil();\n        } else {\n            key = unpacker.unpackString();\n        }\n        // unpack value\n        MessageFormat messageFormat = unpacker.getNextFormat();\n        Object value;\n        switch(messageFormat.getValueType()) {\n            case BOOLEAN:\n                value = unpacker.unpackBoolean();\n                break;\n            case FLOAT:\n                value = unpacker.unpackFloat();\n                break;\n            case INTEGER:\n                value = unpacker.unpackInt();\n                break;\n            case NIL:\n                value = null;\n                unpacker.unpackNil();\n                break;\n            case STRING:\n                value = unpacker.unpackString();\n                break;\n            case MAP:\n                value = unpackMap(unpacker);\n                break;\n            case ARRAY:\n                int size = unpacker.unpackArrayHeader();\n                value = new ArrayList(size);\n                for (int index = 0; index < size; ++index) {\n                    ((ArrayList) value).add(unpacker.unpackString());\n                }\n                break;\n            default:\n                throw new IOException(\"Message received unsupported type: \" + messageFormat.getValueType());\n        }\n        if (null != key) {\n            result.put(key, value);\n        }\n        mapSize--;\n    }\n    return result;\n}",
        "tuc": "@Test\npublic void TestEncodeAndDecodeWithData() throws Exception {\n    Map<String, Object> data = new HashMap<>();\n    data.put(\"string\", \"world\");\n    data.put(\"int\", 1);\n    data.put(\"float\", 0.5f);\n    data.put(\"boolean\", true);\n    data.put(\"null\", null);\n    Message message = new Message(\"test\", data, null, \"nodeId\");\n    Message message2 = new Message(message.getBytes());\n    assertEquals(\"test\", message2.getType());\n    assertEquals(\"world\", message2.getData().get(\"string\"));\n    assertEquals(1, message2.getData().get(\"int\"));\n    assertEquals(0.5f, message2.getData().get(\"float\"));\n    assertEquals(true, message2.getData().get(\"boolean\"));\n    assertNull(message2.getData().get(\"null\"));\n    assertEquals(\"nodeId\", message2.getNodeID());\n}",
        "label": 1
    },
    {
        "repo_name": "easymodeling___easy-modeling",
        "commit": "34d0cc58685c844fa084d0b2e919792803304bfe",
        "commit_message": "introduce easy-random to generate model\n",
        "p_path": "core/src/main/java/xyz/v2my/easymodeling/EasyModeling.java",
        "t_path": "core/src/test/java/xyz/v2my/easymodeling/EasyModelingTest.java",
        "p_name": "generate",
        "t_name": "should_do_test",
        "lpfc": "public static <T> T generate(Class<T> clazz) {\n    return null;\n}",
        "rpfc": "public static <T> T generate(Class<T> clazz) {\n    return new EasyRandom().nextObject(clazz);\n}",
        "tuc": "@Test\nvoid should_do_test() {\n    final String generated = EasyModeling.generate(String.class);\n    assertThat(generated).isNull();\n}",
        "label": 1
    },
    {
        "repo_name": "SvenEwald___xmlbeam",
        "commit": "57f53700e0f47f7420d57a2758c766202cee7359",
        "commit_message": "minor bugfixes",
        "p_path": "src/main/java/org/xmlbeam/AutoValue.java",
        "t_path": "src/test/java/org/xmlbeam/tests/autovalues/TestAutoValues.java",
        "p_name": "iterator",
        "t_name": "testIterator",
        "lpfc": "@Override\npublic Iterator<E> iterator() {\n    return new Iterator<E>() {\n        boolean read = false;\n        @Override\n        public boolean hasNext() {\n            return (!read) && (isPresent());\n        }\n        @Override\n        public E next() {\n            if (read) {\n                throw new IllegalStateException();\n            }\n            read = true;\n            return get();\n        }\n        @Override\n        public void remove() {\n            AutoValue.this.remove();\n        }\n    };\n}",
        "rpfc": "@Override\npublic Iterator<E> iterator() {\n    return new Iterator<E>() {\n        boolean read = !(isPresent());\n        @Override\n        public boolean hasNext() {\n            return (!read) && (isPresent());\n        }\n        @Override\n        public E next() {\n            if (read) {\n                throw new IllegalStateException();\n            }\n            read = true;\n            return get();\n        }\n        @Override\n        public void remove() {\n            if ((!read) || (!isPresent())) {\n                throw new IllegalStateException();\n            }\n            AutoValue.this.remove();\n        }\n    };\n}",
        "tuc": "@Test\npublic void testIterator() {\n    EntryWithSubelements entry = projector.projectEmptyElement(\"entry\", EntryWithSubelements.class);\n    for (String s : entry.key()) {\n        s.toString();\n        assertTrue(false);\n    }\n    Iterator<String> iterator = entry.key().iterator();\n    iterator.next();\n    assertFalse(iterator.hasNext());\n}",
        "label": 1
    },
    {
        "repo_name": "SvenEwald___xmlbeam",
        "commit": "5e0baf3f34eb5868350c759b4d6264e2a436c947",
        "commit_message": "added artifact signing\n",
        "p_path": "src/main/java/org/xmlbeam/util/intern/DOMHelper.java",
        "t_path": "src/test/java/org/xmlbeam/tests/TestObjectInvoker.java",
        "p_name": "getDocumentFromURL",
        "t_name": "testToString",
        "lpfc": "@SuppressWarnings(\"unchecked\")\npublic static Document getDocumentFromURL(DocumentBuilder documentBuilder, final String url, Map<String, String> requestProperties, final Class<?> resourceAwareClass) throws IOException {\n    try {\n        if (url.startsWith(\"resource://\")) {\n            return documentBuilder.parse(resourceAwareClass.getResourceAsStream(url.substring(\"resource://\".length())));\n        }\n        if (url.startsWith(\"http:\") || url.startsWith(\"https:\")) {\n            return documentBuilder.parse(IOHelper.httpGet(url, requestProperties), url);\n        }\n        Document document = documentBuilder.parse(url);\n        if (document == null) {\n            throw new IOException(\"Document could not be created form uri \" + url);\n        }\n        return document;\n    } catch (SAXException e) {\n        throw new RuntimeException(e);\n    }\n}",
        "rpfc": "@SuppressWarnings(\"unchecked\")\npublic static Document getDocumentFromURL(DocumentBuilder documentBuilder, final String url, Map<String, String> requestProperties, final Class<?> resourceAwareClass) throws IOException {\n    try {\n        if (url.startsWith(\"resource://\")) {\n            InputStream is = resourceAwareClass.getResourceAsStream(url.substring(\"resource://\".length()));\n            InputSource source = new InputSource(is);\n            return documentBuilder.parse(source);\n        }\n        if (url.startsWith(\"http:\") || url.startsWith(\"https:\")) {\n            return documentBuilder.parse(IOHelper.httpGet(url, requestProperties), url);\n        }\n        Document document = documentBuilder.parse(url);\n        if (document == null) {\n            throw new IOException(\"Document could not be created form uri \" + url);\n        }\n        return document;\n    } catch (SAXException e) {\n        throw new RuntimeException(e);\n    }\n}",
        "tuc": "@Test\npublic void testToString() throws IOException {\n    DefaultXMLFactoriesConfig config = new DefaultXMLFactoriesConfig().setOmitXMLDeclaration(false);\n    XMLBeamTestSuite testSuite = new XBProjector(config).io().fromURLAnnotation(XMLBeamTestSuite.class);\n    String orig = IOHelper.inputStreamToString(TestObjectInvoker.class.getResourceAsStream(XMLBeamTestSuite.class.getAnnotation(XBDocURL.class).value().substring(\"resource://\".length())));\n    assertEquals(orig.replaceAll(\"\\\\s\", \"\"), testSuite.toString().replaceAll(\"\\\\s\", \"\"));\n}",
        "label": 1
    },
    {
        "repo_name": "SvenEwald___xmlbeam",
        "commit": "9bbf4e5635aa637021cf40023c84a13a6e884713",
        "commit_message": "added artifact signing",
        "p_path": "src/main/java/org/xmlbeam/util/intern/DOMHelper.java",
        "t_path": "src/test/java/org/xmlbeam/tests/TestObjectInvoker.java",
        "p_name": "getDocumentFromURL",
        "t_name": "testToString",
        "lpfc": "@SuppressWarnings(\"unchecked\")\npublic static Document getDocumentFromURL(DocumentBuilder documentBuilder, final String url, Map<String, String> requestProperties, final Class<?> resourceAwareClass) throws IOException {\n    try {\n        if (url.startsWith(\"resource://\")) {\n            return documentBuilder.parse(resourceAwareClass.getResourceAsStream(url.substring(\"resource://\".length())));\n        }\n        if (url.startsWith(\"http:\") || url.startsWith(\"https:\")) {\n            return documentBuilder.parse(IOHelper.httpGet(url, requestProperties), url);\n        }\n        Document document = documentBuilder.parse(url);\n        if (document == null) {\n            throw new IOException(\"Document could not be created form uri \" + url);\n        }\n        return document;\n    } catch (SAXException e) {\n        throw new RuntimeException(e);\n    }\n}",
        "rpfc": "@SuppressWarnings(\"unchecked\")\npublic static Document getDocumentFromURL(DocumentBuilder documentBuilder, final String url, Map<String, String> requestProperties, final Class<?> resourceAwareClass) throws IOException {\n    try {\n        if (url.startsWith(\"resource://\")) {\n            InputStream is = resourceAwareClass.getResourceAsStream(url.substring(\"resource://\".length()));\n            InputSource source = new InputSource(is);\n            return documentBuilder.parse(source);\n        }\n        if (url.startsWith(\"http:\") || url.startsWith(\"https:\")) {\n            return documentBuilder.parse(IOHelper.httpGet(url, requestProperties), url);\n        }\n        Document document = documentBuilder.parse(url);\n        if (document == null) {\n            throw new IOException(\"Document could not be created form uri \" + url);\n        }\n        return document;\n    } catch (SAXException e) {\n        throw new RuntimeException(e);\n    }\n}",
        "tuc": "@Test\npublic void testToString() throws IOException {\n    DefaultXMLFactoriesConfig config = new DefaultXMLFactoriesConfig().setOmitXMLDeclaration(false);\n    XMLBeamTestSuite testSuite = new XBProjector(config).io().fromURLAnnotation(XMLBeamTestSuite.class);\n    String orig = IOHelper.inputStreamToString(TestObjectInvoker.class.getResourceAsStream(XMLBeamTestSuite.class.getAnnotation(XBDocURL.class).value().substring(\"resource://\".length())));\n    assertEquals(orig.replaceAll(\"\\\\s\", \"\"), testSuite.toString().replaceAll(\"\\\\s\", \"\"));\n}",
        "label": 1
    },
    {
        "repo_name": "zwwhnly___mybatis-action",
        "commit": "5e2fa5e3683784de0db43eab81d26f203e43bcb7",
        "commit_message": "association\u6807\u7b7e\u4ee3\u7801\u4fee\u6539\n",
        "p_path": "src/main/java/com/zwwhnly/mybatisaction/mapper/SysUserMapper.java",
        "t_path": "src/test/java/com/zwwhnly/mybatisaction/mapper/SysUserMapperTest.java",
        "p_name": "selectUserAndRoleByIdSelect",
        "t_name": "testSelectUserAndRoleByIdSelect",
        "lpfc": "SysUser selectUserAndRoleByIdSelect(Long id);",
        "rpfc": "SysUserExtend selectUserAndRoleByIdSelect(Long id);",
        "tuc": "@Test\npublic void testSelectUserAndRoleByIdSelect() {\n    SqlSession sqlSession = getSqlSession();\n    try {\n        SysUserMapper sysUserMapper = sqlSession.getMapper(SysUserMapper.class);\n        SysUser sysUser = sysUserMapper.selectUserAndRoleByIdSelect(1001L);\n        Assert.assertNotNull(sysUser);\n        System.out.println(\"\u8c03\u7528sysUser.equals(null)\");\n        sysUser.equals(null);\n        System.out.println(\"\u8c03\u7528sysUser.getSysRole()\");\n    } finally {\n        sqlSession.close();\n    }\n}",
        "label": 1
    },
    {
        "repo_name": "karussell___Jetwick",
        "commit": "6599f88f9b3efe2dd5ea5641e5028cd5f31912e5",
        "commit_message": "advertisment note on homepage\n",
        "p_path": "src/main/java/de/jetwick/solr/SolrTweetSearch.java",
        "t_path": "src/test/java/de/jetwick/solr/SolrTweetSearchTest.java",
        "p_name": "searchAds",
        "t_name": "testAdSearch",
        "lpfc": "public Collection<SolrTweet> searchAds(String query) throws SolrServerException {\n    query = query.trim();\n    if (query.isEmpty())\n        return Collections.EMPTY_LIST;\n    Collection<SolrUser> users = new LinkedHashSet<SolrUser>();\n    search(users, new SolrQuery(query).addFilterQuery(\"tw:#jetwick\").addFilterQuery(RT_COUNT + \":[1 TO *]\").addFilterQuery(QUALITY + \":[90 TO 100]\").addFilterQuery(IS_RT + \":false\").addFilterQuery(DATE + \":[NOW/DAY-3DAYS TO NOW]\").setSortField(RT_COUNT, SolrQuery.ORDER.desc));\n    Set<SolrTweet> res = new LinkedHashSet<SolrTweet>();\n    for (SolrUser u : users) {\n        if (u.getOwnTweets().size() > 0)\n            res.add(u.getOwnTweets().iterator().next());\n    }\n    return res;\n}",
        "rpfc": "public Collection<SolrTweet> searchAds(String query) throws SolrServerException {\n    query = query.trim();\n    if (query.isEmpty())\n        return Collections.EMPTY_LIST;\n    Collection<SolrUser> users = new LinkedHashSet<SolrUser>();\n    search(users, new SolrQuery(query).addFilterQuery(\"tw:#jetwick\").addFilterQuery(QUALITY + \":[90 TO 100]\").addFilterQuery(IS_RT + \":false\").addFilterQuery(DATE + \":[NOW/DAY-3DAYS TO NOW]\").setSortField(RT_COUNT, SolrQuery.ORDER.desc));\n    Set<SolrTweet> res = new LinkedHashSet<SolrTweet>();\n    for (SolrUser u : users) {\n        if (u.getOwnTweets().size() > 0)\n            res.add(u.getOwnTweets().iterator().next());\n    }\n    return res;\n}",
        "tuc": "@Test\npublic void testAdSearch() throws Exception {\n    SolrTweet tw = createNowTweet(1L, \"text jetwick @jetwick\", \"peter\");\n    tw.setRt(1);\n    tw.setQuality(100);\n    twSearch.update(Arrays.asList(tw));\n    twSearch.commit();\n    assertEquals(1, twSearch.search(\"text\").size());\n    assertEquals(0, twSearch.searchAds(\"text\").size());\n    tw = createNowTweet(1L, \"text #jetwick\", \"peter\");\n    tw.setQuality(100);\n    twSearch.update(Arrays.asList(tw));\n    twSearch.commit();\n    assertEquals(0, twSearch.searchAds(\"text\").size());\n    tw = createNowTweet(1L, \"RT @karsten: text #jetwick\", \"peter\");\n    tw.setRt(1);\n    tw.setQuality(100);\n    twSearch.update(Arrays.asList(tw));\n    twSearch.commit();\n    assertEquals(0, twSearch.searchAds(\"text\").size());\n    tw = createNowTweet(1L, \"text #jetwick\", \"peter\");\n    tw.setRt(1);\n    tw.setQuality(90);\n    twSearch.update(Arrays.asList(tw));\n    twSearch.commit();\n    assertEquals(1, twSearch.searchAds(\"text\").size());\n    assertEquals(0, twSearch.searchAds(\" \").size());\n    tw = createNowTweet(1L, \"text #jetwick\", \"peter\");\n    tw.setQuality(89);\n    tw.setRt(1);\n    twSearch.update(Arrays.asList(tw));\n    twSearch.commit();\n    assertEquals(0, twSearch.searchAds(\"text\").size());\n}",
        "label": 1
    },
    {
        "repo_name": "jetty-project___jetty-load-generator",
        "commit": "9fa9345436db092d0e3e95957c46d71bb2a8bb63",
        "commit_message": "Tracking a strange test failure.\n\nSigned-off-by: Simone Bordet <simone.bordet@gmail.com>\n",
        "p_path": "jetty-load-generator-client/src/main/java/org/mortbay/jetty/load/generator/LoadGenerator.java",
        "t_path": "jetty-load-generator-client/src/test/java/org/mortbay/jetty/load/generator/LoadGeneratorTest.java",
        "p_name": "interrupt",
        "t_name": "testJMX",
        "lpfc": "@ManagedOperation(value = \"Interrupts this LoadGenerator\", impact = \"ACTION\")\npublic void interrupt() {\n    interrupt = true;\n}",
        "rpfc": "@ManagedOperation(value = \"Interrupts this LoadGenerator\", impact = \"ACTION\")\npublic void interrupt() {\n    if (LOGGER.isDebugEnabled()) {\n        LOGGER.debug(\"interrupting {}\", this);\n    }\n    interrupt = true;\n}",
        "tuc": "@Test\npublic void testJMX() throws Exception {\n    startServer(new TestHandler());\n    LoadGenerator loadGenerator = new LoadGenerator.Builder().port(connector.getLocalPort()).httpClientTransportBuilder(clientTransportBuilder).iterationsPerThread(0).resourceRate(5).build();\n    MBeanContainer mbeanContainer = new MBeanContainer(ManagementFactory.getPlatformMBeanServer());\n    loadGenerator.addBean(mbeanContainer);\n    ObjectName pattern = new ObjectName(LoadGenerator.class.getPackage().getName() + \":*\");\n    Set<ObjectName> objectNames = mbeanContainer.getMBeanServer().queryNames(pattern, null);\n    Assert.assertTrue(objectNames.size() > 0);\n    Optional<ObjectName> objectNameOpt = objectNames.stream().filter(o -> o.getKeyProperty(\"type\").equalsIgnoreCase(LoadGenerator.class.getSimpleName())).findAny();\n    Assert.assertTrue(objectNameOpt.isPresent());\n    ObjectName objectName = objectNameOpt.get();\n    CompletableFuture<Void> cf = loadGenerator.begin();\n    Thread.sleep(1000);\n    mbeanContainer.getMBeanServer().invoke(objectName, \"interrupt\", null, new String[] {});\n    cf.handle((r, x) -> {\n        Throwable cause = x.getCause();\n        if (cause instanceof InterruptedException) {\n            return null;\n        } else {\n            throw new CompletionException(cause);\n        }\n    }).get(5, TimeUnit.SECONDS);\n}",
        "label": 1
    },
    {
        "repo_name": "jetty-project___jetty-load-generator",
        "commit": "6da4c80c9641bd098c29b87ca5e4acd1b12b33e1",
        "commit_message": "Fixed JMX tests.\n",
        "p_path": "jetty-load-generator-client/src/main/java/org/mortbay/jetty/load/generator/LoadGenerator.java",
        "t_path": "jetty-load-generator-client/src/test/java/org/mortbay/jetty/load/generator/LoadGeneratorTest.java",
        "p_name": "process",
        "t_name": "testJMX",
        "lpfc": "private CompletableFuture<Void> process() {\n    CompletableFuture<Void> process = new CompletableFuture<>();\n    CompletableFuture<Void> result = process;\n    try {\n        barrier.await();\n        String threadName = Thread.currentThread().getName();\n        if (logger.isDebugEnabled()) {\n            logger.debug(\"sender thread {} running\", threadName);\n        }\n        HttpClient[] clients = new HttpClient[config.getUsersPerThread()];\n        result = process.whenCompleteAsync((r, x) -> {\n            if (logger.isDebugEnabled()) {\n                logger.debug(\"stopping http clients\");\n            }\n            Arrays.stream(clients).forEach(this::stopHttpClient);\n        }, executorService);\n        for (int i = 0; i < clients.length; ++i) {\n            HttpClient client = clients[i] = newHttpClient(getConfig());\n            client.start();\n            addBean(client, false);\n        }\n        Callback processCallback = new Callback() {\n            @Override\n            public void succeeded() {\n                if (logger.isDebugEnabled()) {\n                    logger.debug(\"sender thread {} completed\", threadName);\n                }\n                process.complete(null);\n            }\n            @Override\n            public void failed(Throwable x) {\n                if (logger.isDebugEnabled()) {\n                    logger.debug(\"sender thread {} failed\", threadName);\n                }\n                logger.info(\"sender thread \" + threadName + \" failed\", x);\n                process.completeExceptionally(x);\n            }\n        };\n        Callback callback = new Callback.Nested(processCallback) {\n            @Override\n            public void succeeded() {\n            }\n        };\n        int rate = config.getResourceRate();\n        long period = rate > 0 ? TimeUnit.SECONDS.toNanos(config.getThreads()) / rate : 0;\n        long runFor = config.getRunFor();\n        int warmupIterations = config.getWarmupIterationsPerThread();\n        int iterations = runFor > 0 ? 0 : config.getIterationsPerThread();\n        long begin = System.nanoTime();\n        long next = begin + period;\n        int clientIndex = 0;\n        send: while (true) {\n            int batch = 1;\n            if (period > 0) {\n                long pause = next - System.nanoTime();\n                if (pause > 0) {\n                    TimeUnit.NANOSECONDS.sleep(pause);\n                    long elapsed = System.nanoTime() - next;\n                    batch += (int) (elapsed / period);\n                }\n            }\n            while (batch > 0) {\n                HttpClient client = clients[clientIndex];\n                boolean warmup = false;\n                boolean lastIteration = false;\n                if (warmupIterations > 0) {\n                    warmup = --warmupIterations >= 0;\n                } else if (iterations > 0) {\n                    lastIteration = --iterations == 0;\n                }\n                boolean ranEnough = runFor > 0 && TimeUnit.NANOSECONDS.toSeconds(System.nanoTime() - begin) >= runFor;\n                Callback c = lastIteration || ranEnough ? processCallback : callback;\n                sendResourceTree(client, config.getResource(), warmup, c);\n                --batch;\n                next += period;\n                if (lastIteration || ranEnough || process.isCompletedExceptionally()) {\n                    break send;\n                }\n                if (interrupt) {\n                    callback.failed(new InterruptedException());\n                    break send;\n                }\n                if (++clientIndex == clients.length) {\n                    clientIndex = 0;\n                }\n            }\n        }\n    } catch (Throwable x) {\n        if (logger.isDebugEnabled()) {\n            logger.debug(x);\n        }\n        logger.info(\"process failed\", x);\n        process.completeExceptionally(x);\n    }\n    return result;\n}",
        "rpfc": "private CompletableFuture<Void> process() {\n    CompletableFuture<Void> process = new CompletableFuture<>();\n    CompletableFuture<Void> result = process;\n    try {\n        barrier.await();\n        String threadName = Thread.currentThread().getName();\n        if (logger.isDebugEnabled()) {\n            logger.debug(\"sender thread {} running\", threadName);\n        }\n        HttpClient[] clients = new HttpClient[config.getUsersPerThread()];\n        result = process.whenCompleteAsync((r, x) -> {\n            if (logger.isDebugEnabled()) {\n                logger.debug(\"stopping http clients\");\n            }\n            Arrays.stream(clients).forEach(this::stopHttpClient);\n        }, executorService);\n        for (int i = 0; i < clients.length; ++i) {\n            HttpClient client = clients[i] = newHttpClient(getConfig());\n            client.start();\n            addBean(client, false);\n        }\n        Callback processCallback = new Callback() {\n            @Override\n            public void succeeded() {\n                if (logger.isDebugEnabled()) {\n                    logger.debug(\"sender thread {} completed\", threadName);\n                }\n                process.complete(null);\n            }\n            @Override\n            public void failed(Throwable x) {\n                if (logger.isDebugEnabled()) {\n                    logger.debug(\"sender thread \" + threadName + \" failed\", x);\n                }\n                process.completeExceptionally(x);\n            }\n        };\n        Callback callback = new Callback.Nested(processCallback) {\n            @Override\n            public void succeeded() {\n            }\n        };\n        int rate = config.getResourceRate();\n        long period = rate > 0 ? TimeUnit.SECONDS.toNanos(config.getThreads()) / rate : 0;\n        long runFor = config.getRunFor();\n        int warmupIterations = config.getWarmupIterationsPerThread();\n        int iterations = runFor > 0 ? 0 : config.getIterationsPerThread();\n        long begin = System.nanoTime();\n        long next = begin + period;\n        int clientIndex = 0;\n        send: while (true) {\n            int batch = 1;\n            if (period > 0) {\n                long pause = next - System.nanoTime();\n                if (pause > 0) {\n                    TimeUnit.NANOSECONDS.sleep(pause);\n                    long elapsed = System.nanoTime() - next;\n                    batch += (int) (elapsed / period);\n                }\n            }\n            while (batch > 0) {\n                HttpClient client = clients[clientIndex];\n                boolean warmup = false;\n                boolean lastIteration = false;\n                if (warmupIterations > 0) {\n                    warmup = --warmupIterations >= 0;\n                } else if (iterations > 0) {\n                    lastIteration = --iterations == 0;\n                }\n                boolean ranEnough = runFor > 0 && TimeUnit.NANOSECONDS.toSeconds(System.nanoTime() - begin) >= runFor;\n                Callback c = lastIteration || ranEnough ? processCallback : callback;\n                sendResourceTree(client, config.getResource(), warmup, c);\n                --batch;\n                next += period;\n                if (lastIteration || ranEnough || process.isCompletedExceptionally()) {\n                    break send;\n                }\n                if (interrupt) {\n                    callback.failed(new InterruptedException());\n                    break send;\n                }\n                if (++clientIndex == clients.length) {\n                    clientIndex = 0;\n                }\n            }\n        }\n    } catch (Throwable x) {\n        if (logger.isDebugEnabled()) {\n            logger.debug(x);\n        }\n        logger.info(\"process failed\", x);\n        process.completeExceptionally(x);\n    }\n    return result;\n}",
        "tuc": "@Test\npublic void testJMX() throws Exception {\n    prepare(new TestHandler());\n    LoadGenerator loadGenerator = new LoadGenerator.Builder().port(connector.getLocalPort()).httpClientTransportBuilder(clientTransportBuilder).iterationsPerThread(0).resourceRate(5).build();\n    MBeanContainer mbeanContainer = new MBeanContainer(ManagementFactory.getPlatformMBeanServer());\n    loadGenerator.addBean(mbeanContainer);\n    ObjectName pattern = new ObjectName(LoadGenerator.class.getPackage().getName() + \":*\");\n    Set<ObjectName> objectNames = mbeanContainer.getMBeanServer().queryNames(pattern, null);\n    Assert.assertEquals(1, objectNames.size());\n    ObjectName objectName = objectNames.iterator().next();\n    CompletableFuture<Void> cf = loadGenerator.begin();\n    Thread.sleep(1000);\n    mbeanContainer.getMBeanServer().invoke(objectName, \"interrupt\", null, null);\n    mbeanContainer.beanRemoved(null, loadGenerator);\n    cf.handle((r, x) -> {\n        Throwable cause = x.getCause();\n        if (cause instanceof InterruptedException) {\n            return null;\n        } else {\n            throw new CompletionException(cause);\n        }\n    }).get(5, TimeUnit.SECONDS);\n}",
        "label": 1
    },
    {
        "repo_name": "jetty-project___jetty-load-generator",
        "commit": "c799ae6feba3cc1e91bdd8dbb74ea5cfb8f6d27a",
        "commit_message": "fix unit test\n\nSigned-off-by: olivier lamy <olamy@webtide.com>\n",
        "p_path": "jetty-load-generator-starter/src/main/java/org/mortbay/jetty/load/generator/starter/LoadGeneratorStarter.java",
        "t_path": "jetty-load-generator-starter/src/test/java/org/mortbay/jetty/load/generator/starter/LoadGeneratorStarterTest.java",
        "p_name": "main",
        "t_name": "simpletest",
        "lpfc": "public static void main(String[] args) throws Exception {\n    LoadGeneratorStarterArgs runnerArgs = new LoadGeneratorStarterArgs();\n    try {\n        JCommander jCommander = new JCommander(runnerArgs, args);\n        if (runnerArgs.isHelp()) {\n            jCommander.usage();\n            return;\n        }\n    } catch (Exception e) {\n        e.printStackTrace();\n        new JCommander(runnerArgs).usage();\n        return;\n    }\n    try {\n        LoadGeneratorStarter runner = new LoadGeneratorStarter(runnerArgs);\n        if (runnerArgs.getRunIteration() > 0) {\n            runner.run(runnerArgs.getRunIteration());\n        } else {\n            runner.run();\n        }\n    } catch (Exception e) {\n        e.printStackTrace();\n        new JCommander(runnerArgs).usage();\n        System.exit(1);\n    }\n}",
        "rpfc": "public static void main(String[] args) throws Exception {\n    LoadGeneratorStarterArgs runnerArgs = new LoadGeneratorStarterArgs();\n    try {\n        JCommander jCommander = new JCommander(runnerArgs, args);\n        if (runnerArgs.isHelp()) {\n            jCommander.usage();\n            return;\n        }\n    } catch (Exception e) {\n        e.printStackTrace();\n        new JCommander(runnerArgs).usage();\n        return;\n    }\n    try {\n        LoadGeneratorStarter runner = new LoadGeneratorStarter(runnerArgs);\n        if (runnerArgs.getRunIteration() > 0) {\n            runner.run(runnerArgs.getRunIteration());\n        } else if (runnerArgs.getRunningTime() > 0 && runnerArgs.getRunningTimeUnit() != null) {\n            runner.run(runnerArgs.getRunningTime(), runnerArgs.getRunningTimeUnit(), false);\n        } else {\n            runner.run();\n        }\n    } catch (Exception e) {\n        e.printStackTrace();\n        new JCommander(runnerArgs).usage();\n        System.exit(1);\n    }\n}",
        "tuc": "@Test\npublic void simpletest() throws Exception {\n    List<String> args = new ArrayList<>();\n    args.add(\"-Dwarmup.number=10\");\n    args.add(\"-h\");\n    args.add(\"localhost\");\n    args.add(\"--port\");\n    args.add(Integer.toString(connector.getLocalPort()));\n    args.add(\"--running-time\");\n    args.add(\"10\");\n    args.add(\"--running-time-unit\");\n    args.add(\"s\");\n    args.add(\"--transaction-rate\");\n    args.add(\"3\");\n    args.add(\"--transport\");\n    args.add(\"http\");\n    args.add(\"--users\");\n    args.add(\"3\");\n    args.add(\"--profile-groovy-path\");\n    args.add(\"src/main/resources/loadgenerator_profile.groovy\");\n    LoadGeneratorStarter.main(args.toArray(new String[args.size()]));\n    int getNumber = testHandler.getNumber.get();\n    Assert.assertTrue(getNumber > 10);\n}",
        "label": 1
    },
    {
        "repo_name": "google___robotstxt-java",
        "commit": "e5608923324774e2918d6d5ad56eba26f4e3cea2",
        "commit_message": "Fixed whitespace separator choice strategy and added test.\n",
        "p_path": "src/main/java/com/google/search/robotstxt/RobotsParser.java",
        "t_path": "src/test/java/com/google/search/robotstxt/RobotsParserTest.java",
        "p_name": "parseLine",
        "t_name": "testMissingSeparator",
        "lpfc": "private void parseLine(final String robotsTxtBody, final int lineBegin, final int lineEnd, final int lineNumber) {\n    int limit = lineEnd;\n    int separator = lineEnd;\n    int whitespaceSeparator = lineEnd;\n    boolean hasContents = false;\n    for (int i = lineBegin; i < lineEnd; i++) {\n        final char ch = robotsTxtBody.charAt(i);\n        if (ch == '#') {\n            limit = i;\n            break;\n        }\n        if (!isWhitespace(ch)) {\n            hasContents = true;\n        }\n        if (isWhitespace(ch) && hasContents) {\n            whitespaceSeparator = i;\n        }\n        if (separator == lineEnd && ch == ':') {\n            separator = i;\n        }\n    }\n    if (separator == lineEnd) {\n        if (whitespaceSeparator != lineEnd) {\n            log(Level.INFO, \"Assuming whitespace as a separator.\", robotsTxtBody, lineBegin, lineEnd, lineNumber);\n            separator = whitespaceSeparator;\n        } else {\n            if (hasContents) {\n                log(Level.WARNING, \"No separator found.\", robotsTxtBody, lineBegin, lineEnd, lineNumber);\n            }\n            return;\n        }\n    }\n    final String key;\n    try {\n        key = trimBounded(robotsTxtBody, lineBegin, separator);\n    } catch (ParseException e) {\n        log(Level.WARNING, \"No key found.\", robotsTxtBody, lineBegin, lineEnd, lineNumber);\n        return;\n    }\n    final String value;\n    try {\n        final byte[] untrimmedValueBytes = trimBounded(robotsTxtBody, separator + 1, limit).getBytes(StandardCharsets.UTF_8);\n        final int maxLengthBytes = 2083 - 2;\n        if (untrimmedValueBytes.length > maxLengthBytes) {\n            log(Level.INFO, \"Value truncated to 2083 bytes.\", robotsTxtBody, lineBegin, lineEnd, lineNumber);\n        }\n        value = new String(untrimmedValueBytes, 0, Math.min(untrimmedValueBytes.length, maxLengthBytes), StandardCharsets.UTF_8);\n    } catch (ParseException e) {\n        log(Level.WARNING, \"No value found.\", robotsTxtBody, lineBegin, lineEnd, lineNumber);\n        return;\n    }\n    final DirectiveType directiveType = parseDirective(key);\n    if (directiveType == DirectiveType.UNKNOWN) {\n        log(Level.WARNING, \"Unknown key.\", robotsTxtBody, lineBegin, lineEnd, lineNumber);\n    }\n    parseHandler.handleDirective(directiveType, value);\n}",
        "rpfc": "private void parseLine(final String robotsTxtBody, final int lineBegin, final int lineEnd, final int lineNumber) {\n    int limit = lineEnd;\n    int separator = lineEnd;\n    int whitespaceSeparator = lineEnd;\n    boolean hasContents = false;\n    for (int i = lineBegin; i < lineEnd; i++) {\n        final char ch = robotsTxtBody.charAt(i);\n        if (ch == '#') {\n            limit = i;\n            break;\n        }\n        if (!isWhitespace(ch)) {\n            hasContents = true;\n        }\n        if (isWhitespace(ch) && hasContents && whitespaceSeparator == lineEnd) {\n            whitespaceSeparator = i;\n        }\n        if (separator == lineEnd && ch == ':') {\n            separator = i;\n        }\n    }\n    if (separator == lineEnd) {\n        if (whitespaceSeparator != lineEnd) {\n            log(Level.INFO, \"Assuming whitespace as a separator.\", robotsTxtBody, lineBegin, lineEnd, lineNumber);\n            separator = whitespaceSeparator;\n        } else {\n            if (hasContents) {\n                log(Level.WARNING, \"No separator found.\", robotsTxtBody, lineBegin, lineEnd, lineNumber);\n            }\n            return;\n        }\n    }\n    final String key;\n    try {\n        key = trimBounded(robotsTxtBody, lineBegin, separator);\n    } catch (ParseException e) {\n        log(Level.WARNING, \"No key found.\", robotsTxtBody, lineBegin, lineEnd, lineNumber);\n        return;\n    }\n    final String value;\n    try {\n        final byte[] untrimmedValueBytes = trimBounded(robotsTxtBody, separator + 1, limit).getBytes(StandardCharsets.UTF_8);\n        final int maxLengthBytes = 2083 - 2;\n        if (untrimmedValueBytes.length > maxLengthBytes) {\n            log(Level.INFO, \"Value truncated to 2083 bytes.\", robotsTxtBody, lineBegin, lineEnd, lineNumber);\n        }\n        value = new String(untrimmedValueBytes, 0, Math.min(untrimmedValueBytes.length, maxLengthBytes), StandardCharsets.UTF_8);\n    } catch (ParseException e) {\n        log(Level.WARNING, \"No value found.\", robotsTxtBody, lineBegin, lineEnd, lineNumber);\n        return;\n    }\n    final DirectiveType directiveType = parseDirective(key);\n    if (directiveType == DirectiveType.UNKNOWN) {\n        log(Level.WARNING, \"Unknown key.\", robotsTxtBody, lineBegin, lineEnd, lineNumber);\n    }\n    parseHandler.handleDirective(directiveType, value);\n}",
        "tuc": "@Test\npublic void testMissingSeparator() {\n    final String robotsTxtBody = \"user-agent FooBot\\n\" + \"disallow /\\n\";\n    final RobotsContents expectedContents = new RobotsContents(Collections.singletonList(new RobotsContents.Group(Collections.singletonList(\"FooBot\"), Collections.singletonList(new RobotsContents.Group.Rule(Parser.DirectiveType.DISALLOW, \"/\")))));\n    parseAndValidate(robotsTxtBody, expectedContents);\n}",
        "label": 1
    },
    {
        "repo_name": "jMotif___SAX",
        "commit": "af2087ee38d12f92655c17663d7293a123c6b2d8",
        "commit_message": "fixing the chunking",
        "p_path": "src/main/java/net/seninp/jmotif/sax/SAXProcessor.java",
        "t_path": "src/test/java/net/seninp/jmotif/sax/TestSAXProcessor.java",
        "p_name": "ts2saxByChunking",
        "t_name": "testTs2SAXByChunks",
        "lpfc": "public SAXRecords ts2saxByChunking(double[] ts, int paaSize, double[] cuts, double nThreshold) throws SAXException {\n    SAXRecords saxFrequencyData = new SAXRecords();\n    double[] normalizedTS = tsProcessor.znorm(ts, nThreshold);\n    double[] paa = tsProcessor.paa(normalizedTS, paaSize);\n    char[] currentString = tsProcessor.ts2String(paa, cuts);\n    for (int i = 0; i < currentString.length; i++) {\n        char c = currentString[i];\n        saxFrequencyData.add(String.valueOf(c).toCharArray(), i);\n    }\n    return saxFrequencyData;\n}",
        "rpfc": "public SAXRecords ts2saxByChunking(double[] ts, int paaSize, double[] cuts, double nThreshold) throws SAXException {\n    SAXRecords saxFrequencyData = new SAXRecords();\n    double[] normalizedTS = tsProcessor.znorm(ts, nThreshold);\n    double[] paa = tsProcessor.paa(normalizedTS, paaSize);\n    char[] currentString = tsProcessor.ts2String(paa, cuts);\n    for (int i = 0; i < currentString.length; i++) {\n        char c = currentString[i];\n        int pos = (int) Math.floor(i * ts.length / currentString.length);\n        saxFrequencyData.add(String.valueOf(c).toCharArray(), pos);\n    }\n    return saxFrequencyData;\n}",
        "tuc": "@Test\npublic void testTs2SAXByChunks() throws Exception {\n    final SAXProcessor sp = new SAXProcessor();\n    final TSProcessor tp = new TSProcessor();\n    final double[] ts1 = TSProcessor.readFileColumn(ts1File, 0, length);\n    final double[] ts2 = TSProcessor.readFileColumn(ts2File, 0, length);\n    final double[] ser = { -1.0, -2.0, -1.0, 0.0, 2.0, 1.0, 1.0, 0.0 };\n    LOGGER.debug(\" ** \" + Arrays.toString(tp.paa(ser, 3)));\n    String ts1sax = sp.ts2saxByChunking(ts1, 10, normalA.getCuts(11), delta).getSAXString(\"\");\n    assertEquals(\"testing SAX\", strLength, ts1sax.length());\n    assertTrue(\"testing SAX\", ts1StrRep10.equalsIgnoreCase(ts1sax));\n    ts1sax = sp.ts2saxByChunking(ts1, 14, normalA.getCuts(10), delta).getSAXString(\"\");\n    assertEquals(\"testing SAX\", 14, ts1sax.length());\n    assertTrue(\"testing SAX\", ts1StrRep14.equalsIgnoreCase(ts1sax));\n    ts1sax = sp.ts2saxByChunking(ts1, 9, normalA.getCuts(7), delta).getSAXString(\"\");\n    assertEquals(\"testing SAX\", 9, ts1sax.length());\n    assertTrue(\"testing SAX\", ts1StrRep7.equalsIgnoreCase(ts1sax));\n    String ts2sax = sp.ts2saxByChunking(ts2, 10, normalA.getCuts(11), delta).getSAXString(\"\");\n    assertEquals(\"testing SAX\", strLength, ts2sax.length());\n    assertTrue(\"testing SAX\", ts2StrRep10.equalsIgnoreCase(ts2sax));\n    ts2sax = sp.ts2saxByChunking(ts2, 14, normalA.getCuts(10), delta).getSAXString(\"\");\n    assertEquals(\"testing SAX\", 14, ts2sax.length());\n    assertTrue(\"testing SAX\", ts2StrRep14.equalsIgnoreCase(ts2sax));\n    ts2sax = sp.ts2saxByChunking(ts2, 9, normalA.getCuts(7), delta).getSAXString(\"\");\n    assertEquals(\"testing SAX\", 9, ts2sax.length());\n    assertTrue(\"testing SAX\", ts2StrRep7.equalsIgnoreCase(ts2sax));\n}",
        "label": 1
    },
    {
        "repo_name": "jMotif___SAX",
        "commit": "29245528838c176f288173a439b1d4f6724baf79",
        "commit_message": "updating the issue #12 1nd #13 algorithms",
        "p_path": "src/main/java/net/seninp/jmotif/sax/SAXProcessor.java",
        "t_path": "src/test/java/net/seninp/jmotif/sax/TestApproxDistance.java",
        "p_name": "approximationDistance",
        "t_name": "test",
        "lpfc": "public double approximationDistance(double[] ts, int winSize, int paaSize, int alphabetSize, double normThreshold) throws Exception {\n    double resDistance = 0d;\n    int windowCounter = 0;\n    double pointsPerWindow = (double) winSize / (double) paaSize;\n    double[] centralLines = na.getCentralCuts(alphabetSize);\n    for (int i = 0; i < ts.length - winSize + 1; i++) {\n        double[] subseries = Arrays.copyOfRange(ts, i, i + winSize);\n        if (tsProcessor.stDev(subseries) > normThreshold) {\n            subseries = tsProcessor.znorm(subseries, normThreshold);\n        }\n        double[] paa = tsProcessor.paa(subseries, paaSize);\n        int[] leterIndexes = tsProcessor.ts2Index(paa, na, alphabetSize);\n        windowCounter++;\n        for (int j = 0; j < subseries.length; j++) {\n            int paaIdx = (int) Math.floor(((double) j + 0.5) / (double) pointsPerWindow);\n            if (paaIdx < 0) {\n                paaIdx = 0;\n            }\n            if (paaIdx > paa.length) {\n                paaIdx = paa.length - 1;\n            }\n            int letterIdx = leterIndexes[paaIdx];\n            double cLine = centralLines[letterIdx];\n            resDistance = resDistance + ed.distance(cLine, subseries[j]);\n        }\n    }\n    return resDistance / (double) windowCounter;\n}",
        "rpfc": "public double approximationDistance(double[] ts, int winSize, int paaSize, int alphabetSize, double normThreshold) throws Exception {\n    double resDistance = 0d;\n    int windowCounter = 0;\n    double pointsPerWindow = (double) winSize / (double) paaSize;\n    double[] centralLines = na.getCentralCuts(alphabetSize);\n    for (int i = 0; i < ts.length - winSize + 1; i++) {\n        double[] subseries = Arrays.copyOfRange(ts, i, i + winSize);\n        if (tsProcessor.stDev(subseries) > normThreshold) {\n            subseries = tsProcessor.znorm(subseries, normThreshold);\n        }\n        double[] paa = tsProcessor.paa(subseries, paaSize);\n        int[] leterIndexes = tsProcessor.ts2Index(paa, na, alphabetSize);\n        windowCounter++;\n        for (int j = 0; j < paa.length; j++) {\n            int letterIdx = leterIndexes[j];\n            double cLine = centralLines[letterIdx];\n            resDistance = resDistance + ed.distance(cLine, paa[j]);\n        }\n        resDistance = resDistance / paa.length;\n    }\n    return resDistance / (double) windowCounter;\n}",
        "tuc": "@Test\npublic void test() {\n    double dist, distZnorm;\n    try {\n        dist = sp.approximationDistance(series, 15, 7, 5.0);\n        assertEquals(\"testing approx distance\", 8.0, dist, 0.000001);\n        distZnorm = sp.approximationDistance(series, 15, 7, 0.01);\n        assertEquals(\"testing approx distance\", 1.788854, distZnorm, 0.000001);\n        double newApproximationDistance = sp.approximationDistance(series, 15, 7, 3, 0.01);\n        assertEquals(\"testing approx distance\", 4.47213595499958, newApproximationDistance, 0.01);\n    } catch (Exception e) {\n        fail(\"exception shall not be thrown!\");\n    }\n}",
        "label": 1
    },
    {
        "repo_name": "jMotif___SAX",
        "commit": "a713a14206152749122df1eba92af6d0a911048b",
        "commit_message": "fixing the approx distance and the build",
        "p_path": "src/main/java/net/seninp/jmotif/sax/SAXProcessor.java",
        "t_path": "src/test/java/net/seninp/jmotif/sax/TestApproxDistance.java",
        "p_name": "approximationDistance",
        "t_name": "test",
        "lpfc": "public double approximationDistance(double[] ts, int winSize, int paaSize, double normThreshold) throws Exception {\n    double resDistance = 0d;\n    int windowCounter = 0;\n    double pointsPerWindow = (double) winSize / (double) paaSize;\n    for (int i = 0; i < ts.length - winSize + 1; i++) {\n        double[] subseries = Arrays.copyOfRange(ts, i, i + winSize);\n        if (tsProcessor.stDev(subseries) > normThreshold) {\n            subseries = tsProcessor.znorm(subseries, normThreshold);\n        }\n        double[] paa = tsProcessor.paa(subseries, paaSize);\n        windowCounter++;\n        for (int j = 0; j < subseries.length; j++) {\n            int paaIdx = (int) Math.round((double) j / (double) pointsPerWindow) - 1;\n            if (paaIdx < 0) {\n                paaIdx = 0;\n            }\n            if (paaIdx > paa.length) {\n                paaIdx = paa.length - 1;\n            }\n            resDistance = resDistance + ed.distance(paa[paaIdx], subseries[j]);\n            System.out.println(paaIdx + \" == \" + resDistance);\n        }\n    }\n    return resDistance / (double) windowCounter;\n}",
        "rpfc": "public double approximationDistance(double[] ts, int winSize, int paaSize, double normThreshold) throws Exception {\n    double resDistance = 0d;\n    int windowCounter = 0;\n    double pointsPerWindow = (double) winSize / (double) paaSize;\n    for (int i = 0; i < ts.length - winSize + 1; i++) {\n        double[] subseries = Arrays.copyOfRange(ts, i, i + winSize);\n        if (tsProcessor.stDev(subseries) > normThreshold) {\n            subseries = tsProcessor.znorm(subseries, normThreshold);\n        }\n        double[] paa = tsProcessor.paa(subseries, paaSize);\n        windowCounter++;\n        for (int j = 0; j < subseries.length; j++) {\n            int paaIdx = (int) Math.floor(((double) j + 0.5) / (double) pointsPerWindow);\n            if (paaIdx < 0) {\n                paaIdx = 0;\n            }\n            if (paaIdx > paa.length) {\n                paaIdx = paa.length - 1;\n            }\n            resDistance = resDistance + ed.distance(paa[paaIdx], subseries[j]);\n        }\n    }\n    return resDistance / (double) windowCounter;\n}",
        "tuc": "@Test\npublic void test() {\n    double dist;\n    try {\n        dist = sp.approximationDistance(series, 15, 7, 5.0);\n        assertEquals(\"testing approx distance\", 4.472136, dist, 0.000001);\n    } catch (Exception e) {\n        fail(\"exception shall not be thrown!\");\n    }\n}",
        "label": 1
    },
    {
        "repo_name": "jMotif___SAX",
        "commit": "4eabc8d9a4c7ac936f29ceb05e5f6e8e505890dc",
        "commit_message": "working on PAA and approx. distance",
        "p_path": "src/main/java/net/seninp/jmotif/sax/SAXProcessor.java",
        "t_path": "src/test/java/net/seninp/jmotif/sax/TestApproxDistance.java",
        "p_name": "approximationDistance",
        "t_name": "test",
        "lpfc": "public double approximationDistance(double[] ts, int winSize, int paaSize, double normThreshold) throws Exception {\n    double resDistance = 0d;\n    int windowCounter = 0;\n    double pointsPerWindow = (double) winSize / (double) paaSize;\n    for (int i = 0; i < ts.length - winSize + 1; i++) {\n        double[] subseries = Arrays.copyOfRange(ts, i, i + winSize);\n        if (tsProcessor.stDev(subseries) > normThreshold) {\n            subseries = tsProcessor.znorm(subseries, normThreshold);\n        }\n        double[] paa = tsProcessor.paa(subseries, paaSize);\n        windowCounter++;\n        for (int j = 0; j < subseries.length; j++) {\n            int paaIdx = (int) Math.round((double) j / pointsPerWindow) - 1;\n            if (paaIdx < 0) {\n                paaIdx = 0;\n            }\n            if (paaIdx > paa.length) {\n                paaIdx = paa.length - 1;\n            }\n            resDistance = resDistance + ed.distance(paa[paaIdx], subseries[j]);\n        }\n    }\n    return resDistance / (double) windowCounter;\n}",
        "rpfc": "public double approximationDistance(double[] ts, int winSize, int paaSize, double normThreshold) throws Exception {\n    double resDistance = 0d;\n    int windowCounter = 0;\n    double pointsPerWindow = (double) winSize / (double) paaSize;\n    for (int i = 0; i < ts.length - winSize + 1; i++) {\n        double[] subseries = Arrays.copyOfRange(ts, i, i + winSize);\n        if (tsProcessor.stDev(subseries) > normThreshold) {\n            subseries = tsProcessor.znorm(subseries, normThreshold);\n        }\n        double[] paa = tsProcessor.paa(subseries, paaSize);\n        windowCounter++;\n        for (int j = 0; j < subseries.length; j++) {\n            int paaIdx = (int) Math.round((double) j / (double) pointsPerWindow) - 1;\n            if (paaIdx < 0) {\n                paaIdx = 0;\n            }\n            if (paaIdx > paa.length) {\n                paaIdx = paa.length - 1;\n            }\n            resDistance = resDistance + ed.distance(paa[paaIdx], subseries[j]);\n            System.out.println(paaIdx + \" == \" + resDistance);\n        }\n    }\n    return resDistance / (double) windowCounter;\n}",
        "tuc": "@Test\npublic void test() {\n    double dist;\n    try {\n        dist = sp.approximationDistance(series, 14, 7, 5.0);\n        assertEquals(\"testing approx distance\", 4.472136, dist, 0.000001);\n    } catch (Exception e) {\n        fail(\"exception shall not be thrown!\");\n    }\n}",
        "label": 1
    },
    {
        "repo_name": "jMotif___SAX",
        "commit": "24e31aab3177635d4439472fcf3f2a88df426141",
        "commit_message": "fixing the build",
        "p_path": "src/main/java/net/seninp/jmotif/sax/parallel/ParallelSAXImplementation.java",
        "t_path": "src/test/java/net/seninp/jmotif/sax/TestSAXProcessor.java",
        "p_name": "process",
        "t_name": "testTs2sax",
        "lpfc": "/**\n * Discretizes a time series using N threads. If interrupted returns null.\n *\n * @param timeseries the input time series.\n * @param threadsNum the number of threads to allocate for conversion.\n * @param slidingWindowSize the SAX sliding window size.\n * @param paaSize the SAX PAA size.\n * @param alphabetSize the SAX alphabet size.\n * @param numRedStrategy the SAX numerosity reduction strategy.\n * @param normalizationThreshold the normalization threshold.\n * @return a SAX representation of the input time series.\n *\n * @throws SAXException if error occurs.\n */\npublic SAXRecords process(double[] timeseries, int threadsNum, int slidingWindowSize, int paaSize, int alphabetSize, NumerosityReductionStrategy numRedStrategy, double normalizationThreshold) throws SAXException {\n    consoleLogger.debug(\"Starting the parallel SAX\");\n    NormalAlphabet na = new NormalAlphabet();\n    SAXProcessor sp = new SAXProcessor();\n    SAXRecords res = new SAXRecords(0);\n    executorService = Executors.newFixedThreadPool(threadsNum);\n    consoleLogger.debug(\"Created thread pool of \" + threadsNum + \" threads\");\n    NumerosityReductionStrategy nrStrategy = NumerosityReductionStrategy.fromValue(numRedStrategy.index());\n    //\n    // *** I can't figure out how to process MINDIST in parallel for now, rolling back onto failsafe\n    // implementation\n    //\n    if (NumerosityReductionStrategy.MINDIST.equals(nrStrategy)) {\n        nrStrategy = NumerosityReductionStrategy.NONE;\n    }\n    completionService = new ExecutorCompletionService<HashMap<Integer, char[]>>(executorService);\n    int totalTaskCounter = 0;\n    // this value used as a job id in future\n    //\n    final long tstamp = System.currentTimeMillis();\n    // first chunk takes on the uneven division\n    //\n    int evenIncrement = timeseries.length / threadsNum;\n    if (evenIncrement <= slidingWindowSize) {\n        consoleLogger.warn(\"Unable to run with \" + threadsNum + \" threads. Rolling back to single-threaded implementation.\");\n        return sp.ts2saxViaWindow(timeseries, slidingWindowSize, paaSize, na.getCuts(alphabetSize), nrStrategy, normalizationThreshold);\n    }\n    int reminder = timeseries.length % threadsNum;\n    int firstChunkSize = evenIncrement + reminder;\n    consoleLogger.debug(\"data size \" + timeseries.length + \", evenIncrement \" + evenIncrement + \", reminder \" + reminder + \", firstChunkSize \" + firstChunkSize);\n    // execute chunks processing\n    //\n    // the first chunk\n    {\n        int firstChunkStart = 0;\n        int firstChunkEnd = (firstChunkSize - 1) + slidingWindowSize;\n        final SAXWorker job0 = new SAXWorker(tstamp + totalTaskCounter, timeseries, firstChunkStart, firstChunkEnd, slidingWindowSize, paaSize, alphabetSize, nrStrategy, normalizationThreshold);\n        completionService.submit(job0);\n        consoleLogger.debug(\"submitted first chunk job \" + tstamp);\n        totalTaskCounter++;\n    }\n    // intermediate chunks\n    while (totalTaskCounter < threadsNum - 1) {\n        int intermediateChunkStart = (firstChunkSize - 1) + (totalTaskCounter - 1) * evenIncrement + 1;\n        int intermediateChunkEnd = (firstChunkSize - 1) + (totalTaskCounter * evenIncrement) + slidingWindowSize;\n        final SAXWorker job = new SAXWorker(tstamp + totalTaskCounter, timeseries, intermediateChunkStart, intermediateChunkEnd, slidingWindowSize, paaSize, alphabetSize, nrStrategy, normalizationThreshold);\n        completionService.submit(job);\n        consoleLogger.debug(\"submitted intermediate chunk job \" + String.valueOf(tstamp + totalTaskCounter));\n        totalTaskCounter++;\n    }\n    // the last chunk\n    {\n        int lastChunkStart = timeseries.length - evenIncrement;\n        int lastChunkEnd = timeseries.length - 1;\n        final SAXWorker jobN = new SAXWorker(tstamp + totalTaskCounter, timeseries, lastChunkStart, lastChunkEnd, slidingWindowSize, paaSize, alphabetSize, nrStrategy, normalizationThreshold);\n        completionService.submit(jobN);\n        consoleLogger.debug(\"submitted last chunk job \" + String.valueOf(tstamp + totalTaskCounter));\n        totalTaskCounter++;\n    }\n    executorService.shutdown();\n    // the array of completed tasks\n    int[] completedChunks = new int[threadsNum];\n    try {\n        while (totalTaskCounter > 0) {\n            if (Thread.currentThread().isInterrupted()) {\n                System.err.println(\"Parallel SAX being interrupted, returning NULL!\");\n                return null;\n            }\n            Future<HashMap<Integer, char[]>> finished = completionService.poll(24, TimeUnit.HOURS);\n            if (null == finished) {\n                // something went wrong - break from here\n                System.err.println(\"Breaking POLL loop after 24 HOURS of waiting...\");\n                break;\n            } else {\n                // get the result out\n                //\n                HashMap<Integer, char[]> chunkRes = finished.get();\n                // ArrayList<Integer> keys = new ArrayList<Integer>();\n                // for (int i : chunkRes.keySet()) {\n                // keys.add(i);\n                // }\n                // Collections.sort(keys);\n                // for (int i : keys) {\n                // System.out.println(i + \",\" + String.valueOf(chunkRes.get(i)));\n                // }\n                // get the real job index out\n                //\n                int idx = (int) (Long.parseLong(String.valueOf(chunkRes.get(-1))) - tstamp);\n                consoleLogger.debug(\"job with stamp \" + String.valueOf(chunkRes.get(-1)) + \" of chunk \" + idx + \" has finished\");\n                consoleLogger.debug(\"current completion status: \" + Arrays.toString(completedChunks) + \" completion flag: \" + COMPLETED_FLAG);\n                chunkRes.remove(-1);\n                if (0 == res.size() || nrStrategy.equals(NumerosityReductionStrategy.NONE)) {\n                    res.addAll(chunkRes);\n                    completedChunks[idx] = COMPLETED_FLAG;\n                    if (nrStrategy.equals(NumerosityReductionStrategy.NONE)) {\n                        consoleLogger.debug(\"merged in as is because the NR strategy is NONE\");\n                    } else {\n                        consoleLogger.debug(\"merged in as is because the result id empty\");\n                    }\n                } else {\n                    consoleLogger.debug(\"processing chunk \" + idx + \"; res has results already...\");\n                    // the very first chunk has ID=0\n                    //\n                    if (0 == idx) {\n                        completedChunks[0] = COMPLETED_FLAG;\n                        if (completedChunks[1] == COMPLETED_FLAG) {\n                            consoleLogger.debug(\"this is the very first chunk, merging the tail only\");\n                            // chunk tail\n                            int chunkTailIndex = Collections.max(chunkRes.keySet());\n                            String tailStr = String.valueOf(chunkRes.get(chunkTailIndex));\n                            // res head\n                            int resultHeadIndex = res.getMinIndex();\n                            SAXRecord resultHead = res.getByIndex(resultHeadIndex);\n                            String headStr = String.valueOf(resultHead.getPayload());\n                            // print the log\n                            consoleLogger.debug(\"first index in the res \" + resultHeadIndex + \" for \" + headStr + \", last index in head \" + chunkTailIndex + \" for \" + headStr);\n                            // if the last entry equals the first, drop the first\n                            if (nrStrategy.equals(NumerosityReductionStrategy.EXACT) && headStr.equalsIgnoreCase(tailStr)) {\n                                consoleLogger.debug(\"res head \" + headStr + \" at \" + resultHeadIndex + \" is dropped in favor of head tail \" + tailStr + \" at \" + chunkTailIndex);\n                                res.dropByIndex(resultHeadIndex);\n                            }\n                            // else if (nrStrategy.equals(NumerosityReductionStrategy.MINDIST)\n                            // && (sp.checkMinDistIsZero(tailStr.toCharArray(), headStr.toCharArray()))) {\n                            // consoleLogger.debug(\"res head \" + headStr + \" at \" + resultHeadIndex\n                            // + \" is dropped in favor of head tail \" + tailStr + \" at \" + chunkTailIndex);\n                            // res.dropByIndex(resultHeadIndex);\n                            //\n                            // }\n                        } else {\n                            consoleLogger.debug(\"this is the very first chunk, but second is not yet in the results, merging all in\");\n                        }\n                        res.addAll(chunkRes);\n                    } else if (threadsNum - 1 == idx) {\n                        completedChunks[idx] = COMPLETED_FLAG;\n                        if (completedChunks[idx - 1] == COMPLETED_FLAG) {\n                            consoleLogger.debug(\"this is the very last chunk, merging the head only\");\n                            int chunkHeadIndex = Collections.min(chunkRes.keySet());\n                            String headStr = String.valueOf(chunkRes.get(chunkHeadIndex));\n                            // find the RES last index\n                            int resultTailIndex = res.getMaxIndex();\n                            SAXRecord resTail = res.getByIndex(resultTailIndex);\n                            String resStr = String.valueOf(resTail.getPayload());\n                            consoleLogger.debug(\"last index in the res \" + resultTailIndex + \" for \" + resStr + \", first index in the tail \" + chunkHeadIndex + \" for \" + headStr);\n                            // if the last entry equals the first, drop the first\n                            if (nrStrategy.equals(NumerosityReductionStrategy.EXACT) && resStr.equalsIgnoreCase(headStr)) {\n                                consoleLogger.debug(\"chunk head \" + headStr + \" at \" + chunkHeadIndex + \" is dropped in favor of res tail \" + resStr + \" at \" + resultTailIndex);\n                                chunkRes.remove(chunkHeadIndex);\n                            }\n                            // else if (nrStrategy.equals(NumerosityReductionStrategy.MINDIST)\n                            // && (sp.checkMinDistIsZero(headStr.toCharArray(), resStr.toCharArray()))) {\n                            // consoleLogger.debug(\"chunk head \" + headStr + \" at \" + chunkHeadIndex\n                            // + \" is dropped in favor of res tail \" + resStr + \" at \" + resultTailIndex);\n                            // chunkRes.remove(chunkHeadIndex);\n                            // }\n                        } else {\n                            consoleLogger.debug(\"this is the very last chunk, but previous is not yet in the results, merging all in\");\n                        }\n                        res.addAll(chunkRes);\n                    } else {\n                        // the other chunks\n                        //\n                        completedChunks[idx] = COMPLETED_FLAG;\n                        consoleLogger.debug(\"processing chunk \" + idx);\n                        if (completedChunks[idx - 1] == COMPLETED_FLAG) {\n                            consoleLogger.debug(\"previous chunk was completed, merging in\");\n                            int chunkHeadIndex = Collections.min(chunkRes.keySet());\n                            String headStr = String.valueOf(chunkRes.get(chunkHeadIndex));\n                            // find the RES last index\n                            int tmpIdx = chunkHeadIndex;\n                            while (null == res.getByIndex(tmpIdx)) {\n                                tmpIdx--;\n                            }\n                            int resultTailIndex = tmpIdx;\n                            SAXRecord resTail = res.getByIndex(resultTailIndex);\n                            String resStr = String.valueOf(resTail.getPayload());\n                            consoleLogger.debug(\"last index in the res \" + resultTailIndex + \" for \" + resStr + \", first index in the chunk \" + chunkHeadIndex + \" for \" + headStr);\n                            // if the last entry equals the first, drop the first\n                            if (nrStrategy.equals(NumerosityReductionStrategy.EXACT) && resStr.equalsIgnoreCase(headStr)) {\n                                consoleLogger.debug(\"chunk head \" + headStr + \" at \" + chunkHeadIndex + \" is dropped in favor of res tail \" + resStr + \" at \" + resultTailIndex);\n                                chunkRes.remove(chunkHeadIndex);\n                            }\n                            // else if (nrStrategy.equals(NumerosityReductionStrategy.MINDIST)\n                            // && (sp.checkMinDistIsZero(headStr.toCharArray(), resStr.toCharArray()))) {\n                            // consoleLogger.debug(\"chunk head \" + headStr + \" at \" + chunkHeadIndex\n                            // + \" is dropped in favor of res tail \" + resStr + \" at \" + resultTailIndex);\n                            // chunkRes.remove(chunkHeadIndex);\n                            // }\n                        }\n                        if (completedChunks[idx + 1] == COMPLETED_FLAG) {\n                            consoleLogger.debug(\"next chunk was completed, merging the tail\");\n                            // chunk tail\n                            int chunkTailIdx = Collections.max(chunkRes.keySet());\n                            String tailStr = String.valueOf(chunkRes.get(chunkTailIdx));\n                            // res head\n                            int tmpIdx = chunkTailIdx;\n                            while (null == res.getByIndex(tmpIdx)) {\n                                tmpIdx++;\n                            }\n                            int resultHeadIndex = tmpIdx;\n                            SAXRecord resultHead = res.getByIndex(resultHeadIndex);\n                            String headStr = String.valueOf(resultHead.getPayload());\n                            // print the log\n                            consoleLogger.debug(\"first index in the res \" + resultHeadIndex + \" for \" + headStr + \", last index in chunk \" + chunkTailIdx + \" for \" + headStr);\n                            // if the last entry equals the first, drop the first\n                            if (nrStrategy.equals(NumerosityReductionStrategy.EXACT) && headStr.equalsIgnoreCase(tailStr)) {\n                                consoleLogger.debug(\"res head \" + headStr + \" at \" + resultHeadIndex + \" is dropped in favor of chunk tail \" + tailStr + \" at \" + chunkTailIdx);\n                                res.dropByIndex(resultHeadIndex);\n                            }\n                            // else if (nrStrategy.equals(NumerosityReductionStrategy.MINDIST)\n                            // && (sp.checkMinDistIsZero(tailStr.toCharArray(), headStr.toCharArray()))) {\n                            // consoleLogger.debug(\"res head \" + headStr + \" at \" + resultHeadIndex\n                            // + \" is dropped in favor of chunk tail \" + tailStr + \" at \" + chunkTailIdx);\n                            // res.dropByIndex(resultHeadIndex);\n                            // }\n                        }\n                        res.addAll(chunkRes);\n                    }\n                }\n            }\n            totalTaskCounter--;\n        }\n    } catch (InterruptedException e) {\n        System.err.println(\"Error while waiting results: \" + StackTrace.toString(e));\n        this.cancel();\n    } catch (Exception e) {\n        System.err.println(\"Error while waiting results: \" + StackTrace.toString(e));\n    } finally {\n        // wait at least 1 more hour before terminate and fail\n        try {\n            if (!executorService.awaitTermination(1, TimeUnit.HOURS)) {\n                // Cancel currently executing tasks\n                executorService.shutdownNow();\n                if (!executorService.awaitTermination(30, TimeUnit.MINUTES)) {\n                    System.err.println(\"Pool did not terminate... FATAL ERROR\");\n                    throw new RuntimeException(\"Parallel SAX pool did not terminate... FATAL ERROR\");\n                }\n            }\n        } catch (InterruptedException ie) {\n            System.err.println(\"Error while waiting interrupting: \" + StackTrace.toString(ie));\n            // (Re-)Cancel if current thread also interrupted\n            executorService.shutdownNow();\n            // Preserve interrupt status\n            Thread.currentThread().interrupt();\n        }\n    }\n    if (NumerosityReductionStrategy.MINDIST.equals(numRedStrategy)) {\n        // need to prune the result according to MINDIST strategy\n        SAXRecords newRes = new SAXRecords();\n        ArrayList<Integer> keys = res.getAllIndices();\n        char[] oldStr = null;\n        for (int i : keys) {\n            SAXRecord entry = res.getByIndex(i);\n            if (null != oldStr && sp.checkMinDistIsZero(entry.getPayload(), oldStr)) {\n                continue;\n            }\n            newRes.add(entry.getPayload(), i);\n            oldStr = entry.getPayload();\n        }\n        res = newRes;\n    }\n    return res;\n}",
        "rpfc": "/**\n * Discretizes a time series using N threads. If interrupted returns null.\n *\n * @param timeseries the input time series.\n * @param threadsNum the number of threads to allocate for conversion.\n * @param slidingWindowSize the SAX sliding window size.\n * @param paaSize the SAX PAA size.\n * @param alphabetSize the SAX alphabet size.\n * @param numRedStrategy the SAX numerosity reduction strategy.\n * @param normalizationThreshold the normalization threshold.\n * @return a SAX representation of the input time series.\n *\n * @throws SAXException if error occurs.\n */\npublic SAXRecords process(double[] timeseries, int threadsNum, int slidingWindowSize, int paaSize, int alphabetSize, NumerosityReductionStrategy numRedStrategy, double normalizationThreshold) throws SAXException {\n    consoleLogger.debug(\"Starting the parallel SAX\");\n    NormalAlphabet na = new NormalAlphabet();\n    SAXProcessor sp = new SAXProcessor();\n    SAXRecords res = new SAXRecords(0);\n    executorService = Executors.newFixedThreadPool(threadsNum);\n    consoleLogger.debug(\"Created thread pool of \" + threadsNum + \" threads\");\n    NumerosityReductionStrategy nrStrategy = NumerosityReductionStrategy.fromValue(numRedStrategy.index());\n    //\n    // *** I can't figure out how to process MINDIST in parallel for now, rolling back onto failsafe\n    // implementation\n    //\n    if (NumerosityReductionStrategy.MINDIST.equals(nrStrategy)) {\n        nrStrategy = NumerosityReductionStrategy.NONE;\n    }\n    completionService = new ExecutorCompletionService<HashMap<Integer, char[]>>(executorService);\n    int totalTaskCounter = 0;\n    // this value used as a job id in future\n    //\n    final long tstamp = System.currentTimeMillis();\n    // first chunk takes on the uneven division\n    //\n    int evenIncrement = timeseries.length / threadsNum;\n    if (evenIncrement <= slidingWindowSize) {\n        consoleLogger.warn(\"Unable to run with \" + threadsNum + \" threads. Rolling back to single-threaded implementation.\");\n        return sp.ts2saxViaWindow(timeseries, slidingWindowSize, paaSize, na.getCuts(alphabetSize), nrStrategy, normalizationThreshold);\n    }\n    int reminder = timeseries.length % threadsNum;\n    int firstChunkSize = evenIncrement + reminder;\n    consoleLogger.debug(\"data size \" + timeseries.length + \", evenIncrement \" + evenIncrement + \", reminder \" + reminder + \", firstChunkSize \" + firstChunkSize);\n    // execute chunks processing\n    //\n    // the first chunk\n    {\n        int firstChunkStart = 0;\n        int firstChunkEnd = (firstChunkSize - 1) + slidingWindowSize;\n        final SAXWorker job0 = new SAXWorker(tstamp + totalTaskCounter, timeseries, firstChunkStart, firstChunkEnd, slidingWindowSize, paaSize, alphabetSize, nrStrategy, normalizationThreshold);\n        completionService.submit(job0);\n        consoleLogger.debug(\"submitted first chunk job \" + tstamp);\n        totalTaskCounter++;\n    }\n    // intermediate chunks\n    while (totalTaskCounter < threadsNum - 1) {\n        int intermediateChunkStart = (firstChunkSize - 1) + (totalTaskCounter - 1) * evenIncrement + 1;\n        int intermediateChunkEnd = (firstChunkSize - 1) + (totalTaskCounter * evenIncrement) + slidingWindowSize;\n        final SAXWorker job = new SAXWorker(tstamp + totalTaskCounter, timeseries, intermediateChunkStart, intermediateChunkEnd, slidingWindowSize, paaSize, alphabetSize, nrStrategy, normalizationThreshold);\n        completionService.submit(job);\n        consoleLogger.debug(\"submitted intermediate chunk job \" + String.valueOf(tstamp + totalTaskCounter));\n        totalTaskCounter++;\n    }\n    // the last chunk\n    {\n        int lastChunkStart = timeseries.length - evenIncrement;\n        int lastChunkEnd = timeseries.length;\n        final SAXWorker jobN = new SAXWorker(tstamp + totalTaskCounter, timeseries, lastChunkStart, lastChunkEnd, slidingWindowSize, paaSize, alphabetSize, nrStrategy, normalizationThreshold);\n        completionService.submit(jobN);\n        consoleLogger.debug(\"submitted last chunk job \" + String.valueOf(tstamp + totalTaskCounter));\n        totalTaskCounter++;\n    }\n    executorService.shutdown();\n    // the array of completed tasks\n    int[] completedChunks = new int[threadsNum];\n    try {\n        while (totalTaskCounter > 0) {\n            if (Thread.currentThread().isInterrupted()) {\n                System.err.println(\"Parallel SAX being interrupted, returning NULL!\");\n                return null;\n            }\n            Future<HashMap<Integer, char[]>> finished = completionService.poll(24, TimeUnit.HOURS);\n            if (null == finished) {\n                // something went wrong - break from here\n                System.err.println(\"Breaking POLL loop after 24 HOURS of waiting...\");\n                break;\n            } else {\n                // get the result out\n                //\n                HashMap<Integer, char[]> chunkRes = finished.get();\n                // ArrayList<Integer> keys = new ArrayList<Integer>();\n                // for (int i : chunkRes.keySet()) {\n                // keys.add(i);\n                // }\n                // Collections.sort(keys);\n                // for (int i : keys) {\n                // System.out.println(i + \",\" + String.valueOf(chunkRes.get(i)));\n                // }\n                // get the real job index out\n                //\n                int idx = (int) (Long.parseLong(String.valueOf(chunkRes.get(-1))) - tstamp);\n                consoleLogger.debug(\"job with stamp \" + String.valueOf(chunkRes.get(-1)) + \" of chunk \" + idx + \" has finished\");\n                consoleLogger.debug(\"current completion status: \" + Arrays.toString(completedChunks) + \" completion flag: \" + COMPLETED_FLAG);\n                chunkRes.remove(-1);\n                if (0 == res.size() || nrStrategy.equals(NumerosityReductionStrategy.NONE)) {\n                    res.addAll(chunkRes);\n                    completedChunks[idx] = COMPLETED_FLAG;\n                    if (nrStrategy.equals(NumerosityReductionStrategy.NONE)) {\n                        consoleLogger.debug(\"merged in as is because the NR strategy is NONE\");\n                    } else {\n                        consoleLogger.debug(\"merged in as is because the result id empty\");\n                    }\n                } else {\n                    consoleLogger.debug(\"processing chunk \" + idx + \"; res has results already...\");\n                    // the very first chunk has ID=0\n                    //\n                    if (0 == idx) {\n                        completedChunks[0] = COMPLETED_FLAG;\n                        if (completedChunks[1] == COMPLETED_FLAG) {\n                            consoleLogger.debug(\"this is the very first chunk, merging the tail only\");\n                            // chunk tail\n                            int chunkTailIndex = Collections.max(chunkRes.keySet());\n                            String tailStr = String.valueOf(chunkRes.get(chunkTailIndex));\n                            // res head\n                            int resultHeadIndex = res.getMinIndex();\n                            SAXRecord resultHead = res.getByIndex(resultHeadIndex);\n                            String headStr = String.valueOf(resultHead.getPayload());\n                            // print the log\n                            consoleLogger.debug(\"first index in the res \" + resultHeadIndex + \" for \" + headStr + \", last index in head \" + chunkTailIndex + \" for \" + headStr);\n                            // if the last entry equals the first, drop the first\n                            if (nrStrategy.equals(NumerosityReductionStrategy.EXACT) && headStr.equalsIgnoreCase(tailStr)) {\n                                consoleLogger.debug(\"res head \" + headStr + \" at \" + resultHeadIndex + \" is dropped in favor of head tail \" + tailStr + \" at \" + chunkTailIndex);\n                                res.dropByIndex(resultHeadIndex);\n                            }\n                            // else if (nrStrategy.equals(NumerosityReductionStrategy.MINDIST)\n                            // && (sp.checkMinDistIsZero(tailStr.toCharArray(), headStr.toCharArray()))) {\n                            // consoleLogger.debug(\"res head \" + headStr + \" at \" + resultHeadIndex\n                            // + \" is dropped in favor of head tail \" + tailStr + \" at \" + chunkTailIndex);\n                            // res.dropByIndex(resultHeadIndex);\n                            //\n                            // }\n                        } else {\n                            consoleLogger.debug(\"this is the very first chunk, but second is not yet in the results, merging all in\");\n                        }\n                        res.addAll(chunkRes);\n                    } else if (threadsNum - 1 == idx) {\n                        completedChunks[idx] = COMPLETED_FLAG;\n                        if (completedChunks[idx - 1] == COMPLETED_FLAG) {\n                            consoleLogger.debug(\"this is the very last chunk, merging the head only\");\n                            int chunkHeadIndex = Collections.min(chunkRes.keySet());\n                            String headStr = String.valueOf(chunkRes.get(chunkHeadIndex));\n                            // find the RES last index\n                            int resultTailIndex = res.getMaxIndex();\n                            SAXRecord resTail = res.getByIndex(resultTailIndex);\n                            String resStr = String.valueOf(resTail.getPayload());\n                            consoleLogger.debug(\"last index in the res \" + resultTailIndex + \" for \" + resStr + \", first index in the tail \" + chunkHeadIndex + \" for \" + headStr);\n                            // if the last entry equals the first, drop the first\n                            if (nrStrategy.equals(NumerosityReductionStrategy.EXACT) && resStr.equalsIgnoreCase(headStr)) {\n                                consoleLogger.debug(\"chunk head \" + headStr + \" at \" + chunkHeadIndex + \" is dropped in favor of res tail \" + resStr + \" at \" + resultTailIndex);\n                                chunkRes.remove(chunkHeadIndex);\n                            }\n                            // else if (nrStrategy.equals(NumerosityReductionStrategy.MINDIST)\n                            // && (sp.checkMinDistIsZero(headStr.toCharArray(), resStr.toCharArray()))) {\n                            // consoleLogger.debug(\"chunk head \" + headStr + \" at \" + chunkHeadIndex\n                            // + \" is dropped in favor of res tail \" + resStr + \" at \" + resultTailIndex);\n                            // chunkRes.remove(chunkHeadIndex);\n                            // }\n                        } else {\n                            consoleLogger.debug(\"this is the very last chunk, but previous is not yet in the results, merging all in\");\n                        }\n                        res.addAll(chunkRes);\n                    } else {\n                        // the other chunks\n                        //\n                        completedChunks[idx] = COMPLETED_FLAG;\n                        consoleLogger.debug(\"processing chunk \" + idx);\n                        if (completedChunks[idx - 1] == COMPLETED_FLAG) {\n                            consoleLogger.debug(\"previous chunk was completed, merging in\");\n                            int chunkHeadIndex = Collections.min(chunkRes.keySet());\n                            String headStr = String.valueOf(chunkRes.get(chunkHeadIndex));\n                            // find the RES last index\n                            int tmpIdx = chunkHeadIndex;\n                            while (null == res.getByIndex(tmpIdx)) {\n                                tmpIdx--;\n                            }\n                            int resultTailIndex = tmpIdx;\n                            SAXRecord resTail = res.getByIndex(resultTailIndex);\n                            String resStr = String.valueOf(resTail.getPayload());\n                            consoleLogger.debug(\"last index in the res \" + resultTailIndex + \" for \" + resStr + \", first index in the chunk \" + chunkHeadIndex + \" for \" + headStr);\n                            // if the last entry equals the first, drop the first\n                            if (nrStrategy.equals(NumerosityReductionStrategy.EXACT) && resStr.equalsIgnoreCase(headStr)) {\n                                consoleLogger.debug(\"chunk head \" + headStr + \" at \" + chunkHeadIndex + \" is dropped in favor of res tail \" + resStr + \" at \" + resultTailIndex);\n                                chunkRes.remove(chunkHeadIndex);\n                            }\n                            // else if (nrStrategy.equals(NumerosityReductionStrategy.MINDIST)\n                            // && (sp.checkMinDistIsZero(headStr.toCharArray(), resStr.toCharArray()))) {\n                            // consoleLogger.debug(\"chunk head \" + headStr + \" at \" + chunkHeadIndex\n                            // + \" is dropped in favor of res tail \" + resStr + \" at \" + resultTailIndex);\n                            // chunkRes.remove(chunkHeadIndex);\n                            // }\n                        }\n                        if (completedChunks[idx + 1] == COMPLETED_FLAG) {\n                            consoleLogger.debug(\"next chunk was completed, merging the tail\");\n                            // chunk tail\n                            int chunkTailIdx = Collections.max(chunkRes.keySet());\n                            String tailStr = String.valueOf(chunkRes.get(chunkTailIdx));\n                            // res head\n                            int tmpIdx = chunkTailIdx;\n                            while (null == res.getByIndex(tmpIdx)) {\n                                tmpIdx++;\n                            }\n                            int resultHeadIndex = tmpIdx;\n                            SAXRecord resultHead = res.getByIndex(resultHeadIndex);\n                            String headStr = String.valueOf(resultHead.getPayload());\n                            // print the log\n                            consoleLogger.debug(\"first index in the res \" + resultHeadIndex + \" for \" + headStr + \", last index in chunk \" + chunkTailIdx + \" for \" + headStr);\n                            // if the last entry equals the first, drop the first\n                            if (nrStrategy.equals(NumerosityReductionStrategy.EXACT) && headStr.equalsIgnoreCase(tailStr)) {\n                                consoleLogger.debug(\"res head \" + headStr + \" at \" + resultHeadIndex + \" is dropped in favor of chunk tail \" + tailStr + \" at \" + chunkTailIdx);\n                                res.dropByIndex(resultHeadIndex);\n                            }\n                            // else if (nrStrategy.equals(NumerosityReductionStrategy.MINDIST)\n                            // && (sp.checkMinDistIsZero(tailStr.toCharArray(), headStr.toCharArray()))) {\n                            // consoleLogger.debug(\"res head \" + headStr + \" at \" + resultHeadIndex\n                            // + \" is dropped in favor of chunk tail \" + tailStr + \" at \" + chunkTailIdx);\n                            // res.dropByIndex(resultHeadIndex);\n                            // }\n                        }\n                        res.addAll(chunkRes);\n                    }\n                }\n            }\n            totalTaskCounter--;\n        }\n    } catch (InterruptedException e) {\n        System.err.println(\"Error while waiting results: \" + StackTrace.toString(e));\n        this.cancel();\n    } catch (Exception e) {\n        System.err.println(\"Error while waiting results: \" + StackTrace.toString(e));\n    } finally {\n        // wait at least 1 more hour before terminate and fail\n        try {\n            if (!executorService.awaitTermination(1, TimeUnit.HOURS)) {\n                // Cancel currently executing tasks\n                executorService.shutdownNow();\n                if (!executorService.awaitTermination(30, TimeUnit.MINUTES)) {\n                    System.err.println(\"Pool did not terminate... FATAL ERROR\");\n                    throw new RuntimeException(\"Parallel SAX pool did not terminate... FATAL ERROR\");\n                }\n            }\n        } catch (InterruptedException ie) {\n            System.err.println(\"Error while waiting interrupting: \" + StackTrace.toString(ie));\n            // (Re-)Cancel if current thread also interrupted\n            executorService.shutdownNow();\n            // Preserve interrupt status\n            Thread.currentThread().interrupt();\n        }\n    }\n    if (NumerosityReductionStrategy.MINDIST.equals(numRedStrategy)) {\n        // need to prune the result according to MINDIST strategy\n        SAXRecords newRes = new SAXRecords();\n        ArrayList<Integer> keys = res.getAllIndices();\n        char[] oldStr = null;\n        for (int i : keys) {\n            SAXRecord entry = res.getByIndex(i);\n            if (null != oldStr && sp.checkMinDistIsZero(entry.getPayload(), oldStr)) {\n                continue;\n            }\n            newRes.add(entry.getPayload(), i);\n            oldStr = entry.getPayload();\n        }\n        res = newRes;\n    }\n    return res;\n}",
        "tuc": "@Test\npublic void testTs2sax() throws Exception {\n    final TSProcessor tp = new TSProcessor();\n    final SAXProcessor sp = new SAXProcessor();\n    double[] ts2 = TSProcessor.readFileColumn(ts2File, 0, length);\n    String ts2str_0 = sp.ts2saxByChunking(tp.subseriesByCopy(ts2, 0, 5), 5, normalA.getCuts(10), delta).getSAXString(\"\");\n    String ts2str_3 = sp.ts2saxByChunking(tp.subseriesByCopy(ts2, 3, 8), 5, normalA.getCuts(10), delta).getSAXString(\"\");\n    String ts2str_7 = sp.ts2saxByChunking(tp.subseriesByCopy(ts2, 7, 12), 5, normalA.getCuts(10), delta).getSAXString(\"\");\n    SAXRecords ts2SAX = sp.ts2saxViaWindow(ts2, 5, 5, normalA.getCuts(10), NumerosityReductionStrategy.NONE, delta);\n    assertEquals(\"Testing conversion\", ts2.length - 5, ts2SAX.size());\n    assertNotNull(\"Testing ts2sax\", ts2SAX.getByWord(ts2str_0));\n    assertNotNull(\"Testing ts2sax\", ts2SAX.getByWord(ts2str_3));\n    assertNotNull(\"Testing ts2sax\", ts2SAX.getByWord(ts2str_7));\n    assertEquals(\"Testing ts2sax\", ts2SAX.getByWord(ts2str_0).getIndexes().iterator().next(), new Integer(0));\n    assertEquals(\"Testing ts2sax\", ts2SAX.getByWord(ts2str_3).getIndexes().iterator().next(), new Integer(3));\n    assertEquals(\"Testing ts2sax\", ts2SAX.getByWord(ts2str_7).getIndexes().iterator().next(), new Integer(7));\n}",
        "label": 1
    },
    {
        "repo_name": "elaatifi___orika",
        "commit": "fb5acebb5b87b88e4dbfa9613221f4adbd0acc09",
        "commit_message": "Fixes to ScoringClassMapBuilder (broken by previous checkin)",
        "p_path": "core/src/main/java/ma/glasnost/orika/metadata/ScoringClassMapBuilder.java",
        "t_path": "core/src/test/java/ma/glasnost/orika/test/metadata/ScoringClassMapBuilderTest.java",
        "p_name": "splitIntoLowerCaseWords",
        "t_name": "testSplittingWords",
        "lpfc": "private static List<List<String>> splitIntoLowerCaseWords(String s) {\n    List<List<String>> results = new ArrayList<List<String>>();\n    for (String property : s.split(\"[.]\")) {\n        List<String> words = new LinkedList<String>(Arrays.asList(property.toLowerCase().split(WORD_SPLITTER)));\n        results.add(words);\n        for (Iterator<String> iter = words.iterator(); iter.hasNext(); ) {\n            String current = iter.next();\n            if (current == null || current.trim().length() == 0) {\n                iter.remove();\n            }\n        }\n    }\n    return results;\n}",
        "rpfc": "private static List<List<String>> splitIntoLowerCaseWords(String s) {\n    List<List<String>> results = new ArrayList<List<String>>();\n    for (String property : s.split(\"[.]\")) {\n        List<String> words = new LinkedList<String>();\n        results.add(words);\n        for (String word : property.split(WORD_SPLITTER)) {\n            if (word != null && word.trim().length() > 0) {\n                words.add(word.toLowerCase());\n            }\n        }\n    }\n    return results;\n}",
        "tuc": "@SuppressWarnings(\"unchecked\")\n@Test\npublic void testSplittingWords() throws Throwable {\n    Map<String, List<String[]>> tests = new HashMap<String, List<String[]>>() {\n        private static final long serialVersionUID = 1L;\n        {\n            put(\"lowercase\", Arrays.<String[]>asList(new String[] { \"lowercase\" }));\n            put(\"Class\", Arrays.<String[]>asList(new String[] { \"class\" }));\n            put(\"MyClass\", Arrays.<String[]>asList(new String[] { \"my\", \"class\" }));\n            put(\"HTML\", Arrays.<String[]>asList(new String[] { \"html\" }));\n            put(\"PDFLoader\", Arrays.<String[]>asList(new String[] { \"pdf\", \"loader\" }));\n            put(\"AString\", Arrays.<String[]>asList(new String[] { \"a\", \"string\" }));\n            put(\"SimpleXMLParser\", Arrays.<String[]>asList(new String[] { \"Simple\", \"xml\", \"parser\" }));\n            put(\"GL11Version\", Arrays.<String[]>asList(new String[] { \"gl\", \"11\", \"version\" }));\n            put(\"99Bottles\", Arrays.<String[]>asList(new String[] { \"99\", \"bottles\" }));\n            put(\"May5\", Arrays.<String[]>asList(new String[] { \"may\", \"5\" }));\n            put(\"BFG9000\", Arrays.<String[]>asList(new String[] { \"bfg\", \"9000\" }));\n            put(\"SimpleXMLParser\", Arrays.<String[]>asList(new String[] { \"simple\", \"xml\", \"parser\" }));\n            put(\"postalAddress.country\", Arrays.<String[]>asList(new String[] { \"postal\", \"address\" }, new String[] { \"country\" }));\n            put(\"aVeryLongWord.name.first\", Arrays.<String[]>asList(new String[] { \"a\", \"very\", \"long\", \"word\" }, new String[] { \"name\" }, new String[] { \"first\" }));\n        }\n    };\n    Method splitIntoWords = ScoringClassMapBuilder.FieldMatchScore.class.getDeclaredMethod(\"splitIntoLowerCaseWords\", String.class);\n    splitIntoWords.setAccessible(true);\n    for (Entry<String, List<String[]>> test : tests.entrySet()) {\n        List<String[]> testValue = test.getValue();\n        List<String[]> result = (List<String[]>) splitIntoWords.invoke(null, test.getKey());\n        Assert.assertEquals(testValue.size(), result.size());\n        for (int i = 0, len = testValue.size(); i < len; ++i) {\n            Assert.assertTrue(\"Expected <\" + Arrays.toString(testValue.get(i)) + \">, found <\" + Arrays.toString(result.get(i)) + \">\", Arrays.deepEquals(testValue.get(i), result.get(i)));\n        }\n    }\n}",
        "label": 1
    },
    {
        "repo_name": "SAP___fosstars-rating-core",
        "commit": "2219af487f189a51680b56d6a25e10aa1695970a",
        "commit_message": "Add organization name to rules of play report findings (#825)\n\n* Add organization name to rules of play report findings\r\n\r\n* Checkstyle fixes for RoP markdown changes",
        "p_path": "src/main/java/com/sap/oss/phosphor/fosstars/tool/report/OssRulesOfPlayMarkdownReporter.java",
        "t_path": "src/test/java/com/sap/oss/phosphor/fosstars/tool/report/OssRulesOfPlayMarkdownReporterTest.java",
        "p_name": "runFor",
        "t_name": "testReport",
        "lpfc": "@Override\npublic void runFor(List<GitHubProject> projects) throws IOException {\n    Map<Feature<Boolean>, Set<String>> featureResults = new HashMap<Feature<Boolean>, Set<String>>();\n    OssRulesOfPlayScore.EXPECTED_TRUE.stream().forEach(feature -> featureResults.put(feature, new HashSet<String>()));\n    OssRulesOfPlayScore.EXPECTED_FALSE.stream().forEach(feature -> featureResults.put(feature, new HashSet<String>()));\n    int total = 0;\n    long numberOfPassed = 0;\n    long numberOfWarnings = 0;\n    long numberOfFailed = 0;\n    long numberOfUnclear = 0;\n    for (GitHubProject project : projects) {\n        Optional<RatingValue> something = project.ratingValue();\n        if (!something.isPresent()) {\n            continue;\n        }\n        RatingValue ratingValue = something.get();\n        switch((OssRulesOfPlayLabel) ratingValue.label()) {\n            case PASSED:\n                numberOfPassed++;\n                break;\n            case FAILED:\n                numberOfFailed++;\n                break;\n            case PASSED_WITH_WARNING:\n                numberOfWarnings++;\n                break;\n            case UNCLEAR:\n                numberOfUnclear++;\n                break;\n            default:\n                throw new IllegalStateException(String.format(\"Oh no! Unexpected label: %s\", ratingValue.label()));\n        }\n        total++;\n        Path organizationDirectory = outputDirectory.resolve(project.organization().name());\n        if (!Files.isDirectory(organizationDirectory)) {\n            Files.createDirectories(organizationDirectory);\n        }\n        String projectReportFilename = String.format(\"%s.md\", project.name());\n        Files.write(organizationDirectory.resolve(projectReportFilename), formatter.print(project).getBytes());\n        OssRulesOfPlayScore.findViolatedRulesIn(ratingValue.scoreValue().usedValues()).stream().forEach(value -> featureResults.get(value.feature()).add(project.name()));\n    }\n    double percentOfPass = (double) numberOfPassed / total * 100;\n    double percentOfWarnings = (double) numberOfWarnings / total * 100;\n    double percentOfFail = (double) numberOfFailed / total * 100;\n    double percentOfUnclear = (double) numberOfUnclear / total * 100;\n    String content = REPORT_TEMPLATE.replace(\"%NUMBER_OF_PROJECTS%\", String.valueOf(total)).replace(\"%NUMBER_FAILED_PROJECTS%\", String.valueOf(numberOfFailed)).replace(\"%NUMBER_PASSED_PROJECTS%\", String.valueOf(numberOfPassed)).replace(\"%NUMBER_PROJECTS_WITH_WARNINGS%\", String.valueOf(numberOfWarnings)).replace(\"%NUMBER_UNCLEAR_PROJECTS%\", String.valueOf(numberOfUnclear)).replace(\"%PERCENT_FAILED_PROJECTS%\", String.format(\"%2.1f\", percentOfFail)).replace(\"%PERCENT_PASSED_PROJECTS%\", String.format(\"%2.1f\", percentOfPass)).replace(\"%PERCENT_PROJECTS_WITH_WARNINGS%\", String.format(\"%2.1f\", percentOfWarnings)).replace(\"%PERCENT_UNCLEAR_PROJECTS%\", String.format(\"%2.1f\", percentOfUnclear)).replace(\"%PER_RULE_STATISTICS%\", perRuleStatistics(featureResults, projects.size())).replace(\"%PROJECT_TABLE%\", tableOf(projects));\n    Path path = outputDirectory.resolve(REPORT_FILENAME);\n    logger.info(\"Storing a report to {}\", path);\n    Files.write(path, content.getBytes());\n}",
        "rpfc": "@Override\npublic void runFor(List<GitHubProject> projects) throws IOException {\n    Map<Feature<Boolean>, Set<String>> featureResults = new HashMap<Feature<Boolean>, Set<String>>();\n    OssRulesOfPlayScore.EXPECTED_TRUE.stream().forEach(feature -> featureResults.put(feature, new HashSet<String>()));\n    OssRulesOfPlayScore.EXPECTED_FALSE.stream().forEach(feature -> featureResults.put(feature, new HashSet<String>()));\n    int total = 0;\n    long numberOfPassed = 0;\n    long numberOfWarnings = 0;\n    long numberOfFailed = 0;\n    long numberOfUnclear = 0;\n    for (GitHubProject project : projects) {\n        Optional<RatingValue> something = project.ratingValue();\n        if (!something.isPresent()) {\n            continue;\n        }\n        RatingValue ratingValue = something.get();\n        switch((OssRulesOfPlayLabel) ratingValue.label()) {\n            case PASSED:\n                numberOfPassed++;\n                break;\n            case FAILED:\n                numberOfFailed++;\n                break;\n            case PASSED_WITH_WARNING:\n                numberOfWarnings++;\n                break;\n            case UNCLEAR:\n                numberOfUnclear++;\n                break;\n            default:\n                throw new IllegalStateException(String.format(\"Oh no! Unexpected label: %s\", ratingValue.label()));\n        }\n        total++;\n        Path organizationDirectory = outputDirectory.resolve(project.organization().name());\n        if (!Files.isDirectory(organizationDirectory)) {\n            Files.createDirectories(organizationDirectory);\n        }\n        String projectReportFilename = String.format(\"%s.md\", project.name());\n        Files.write(organizationDirectory.resolve(projectReportFilename), formatter.print(project).getBytes());\n        OssRulesOfPlayScore.findViolatedRulesIn(ratingValue.scoreValue().usedValues()).stream().forEach(value -> featureResults.get(value.feature()).add(String.format(\"%s/%s\", project.organization().name(), project.name())));\n    }\n    double percentOfPass = (double) numberOfPassed / total * 100;\n    double percentOfWarnings = (double) numberOfWarnings / total * 100;\n    double percentOfFail = (double) numberOfFailed / total * 100;\n    double percentOfUnclear = (double) numberOfUnclear / total * 100;\n    String content = REPORT_TEMPLATE.replace(\"%NUMBER_OF_PROJECTS%\", String.valueOf(total)).replace(\"%NUMBER_FAILED_PROJECTS%\", String.valueOf(numberOfFailed)).replace(\"%NUMBER_PASSED_PROJECTS%\", String.valueOf(numberOfPassed)).replace(\"%NUMBER_PROJECTS_WITH_WARNINGS%\", String.valueOf(numberOfWarnings)).replace(\"%NUMBER_UNCLEAR_PROJECTS%\", String.valueOf(numberOfUnclear)).replace(\"%PERCENT_FAILED_PROJECTS%\", String.format(\"%2.1f\", percentOfFail)).replace(\"%PERCENT_PASSED_PROJECTS%\", String.format(\"%2.1f\", percentOfPass)).replace(\"%PERCENT_PROJECTS_WITH_WARNINGS%\", String.format(\"%2.1f\", percentOfWarnings)).replace(\"%PERCENT_UNCLEAR_PROJECTS%\", String.format(\"%2.1f\", percentOfUnclear)).replace(\"%PER_RULE_STATISTICS%\", perRuleStatistics(featureResults, projects.size())).replace(\"%PROJECT_TABLE%\", tableOf(projects));\n    Path path = outputDirectory.resolve(REPORT_FILENAME);\n    logger.info(\"Storing a report to {}\", path);\n    Files.write(path, content.getBytes());\n}",
        "tuc": "@Test\npublic void testReport() throws IOException {\n    Path outputDirectory = Files.createTempDirectory(OssRulesOfPlayMarkdownReporterTest.class.getName());\n    try {\n        OssRulesOfPlayRating rating = RatingRepository.INSTANCE.rating(OssRulesOfPlayRating.class);\n        GitHubProject passedProject = new GitHubProject(\"org\", \"passed\");\n        passedProject.set(new RatingValue(new ScoreValue(rating.score()).set(Score.MAX).confidence(10.0), PASSED));\n        GitHubProject projectWithWarnings = new GitHubProject(\"org\", \"warnings\");\n        projectWithWarnings.set(new RatingValue(new ScoreValue(rating.score()).set(SCORE_WITH_WARNING).confidence(9.0), PASSED_WITH_WARNING));\n        BooleanValue failedReadme = new BooleanValue(OssFeatures.HAS_LICENSE, false);\n        BooleanValue failedReuse = new BooleanValue(OssFeatures.README_HAS_REUSE_INFO, false);\n        GitHubProject failedProject1 = new GitHubProject(\"org\", \"failed1\");\n        failedProject1.set(new RatingValue(new ScoreValue(rating.score(), Score.MIN, Weight.MAX, Confidence.MIN, Arrays.asList(failedReadme, failedReuse)).set(Score.MIN).confidence(8.0), FAILED));\n        GitHubProject failedProject2 = new GitHubProject(\"org\", \"failed2\");\n        failedProject2.set(new RatingValue(new ScoreValue(rating.score(), Score.MIN, Weight.MAX, Confidence.MIN, Arrays.asList(failedReadme)).set(Score.MIN).confidence(8.0), FAILED));\n        GitHubProject unclearProject = new GitHubProject(\"org\", \"unclear\");\n        unclearProject.set(new RatingValue(new ScoreValue(rating.score()).set(Score.MIN).confidence(Confidence.MIN), UNCLEAR));\n        List<GitHubProject> projects = Arrays.asList(passedProject, projectWithWarnings, failedProject1, failedProject2, unclearProject);\n        OssRulesOfPlayMarkdownReporter reporter = new OssRulesOfPlayMarkdownReporter(outputDirectory.toString(), new OssRulesOfPlayAdvisor());\n        reporter.runFor(projects);\n        Path reportFileName = outputDirectory.resolve(OssRulesOfPlayMarkdownReporter.REPORT_FILENAME);\n        assertTrue(Files.exists(reportFileName));\n        String report = new String(Files.readAllBytes(reportFileName));\n        System.out.println(report);\n        assertFalse(report.isEmpty());\n        assertTrue(report.contains(\"Total\"));\n        assertTrue(report.contains(\"Passed\"));\n        assertTrue(report.contains(\"Failed\"));\n        assertTrue(report.contains(\"Not clear\"));\n        assertTrue(report.contains(\"Passed with warnings\"));\n        assertTrue(report.contains(\"org/passed\"));\n        assertTrue(report.contains(\"org/warnings\"));\n        assertTrue(report.contains(\"org/failed\"));\n        assertTrue(report.contains(\"org/unclear\"));\n        assertTrue(report.contains(\"Does README mention REUSE?\"));\n        assertTrue(report.contains(\"Is it registered in REUSE?\"));\n        assertTrue(report.contains(\"[Failed](org/failed1.md) | 2 violated rules |\"));\n        assertTrue(report.contains(\"[Failed](org/failed2.md) | 1 violated rule |\"));\n        assertEquals(1, linesWith(\"100%\", report));\n        assertEquals(4, linesWith(\"20.0%\", report));\n        assertEquals(2, linesWith(\"40.0%\", report));\n        assertEquals(1, linesWith(\"60.0%\", report));\n        assertEquals(1, linesWith(\"80.0%\", report));\n        assertEquals(2, linesWith(\"- failed1\", report));\n        assertEquals(1, linesWith(\"- failed2\", report));\n        assertEquals(15, linesWith(\"### Project statistics for \", report));\n    } finally {\n        FileUtils.forceDeleteOnExit(outputDirectory.toFile());\n    }\n}",
        "label": 1
    },
    {
        "repo_name": "SAP___fosstars-rating-core",
        "commit": "d4944cca8aa3aaefdbed96cca146e11502b2aab5",
        "commit_message": "GitHub issue creation: Use identifer only for existence check (#676)\n\n* GitHub issue existence check: Use identifier only\r\n\r\n* GitHub issue creation: Reuse issueHeader variable\r\n\r\n* Follow checkstyle guidelines properly\r\n\r\n* Avoid IndexOutOfBoundsException\r\n\r\nCo-authored-by: Artem Smotrakov <artem.smotrakov@gmail.com>\r\n\r\nCo-authored-by: Artem Smotrakov <artem.smotrakov@gmail.com>",
        "p_path": "src/main/java/com/sap/oss/phosphor/fosstars/tool/report/OssRulesOfPlayGitHubIssuesReporter.java",
        "t_path": "src/test/java/com/sap/oss/phosphor/fosstars/tool/report/OssRulesOfPlayGitHubIssuesReporterTest.java",
        "p_name": "createIssuesFor",
        "t_name": "testIssuesEnabled",
        "lpfc": "public void createIssuesFor(GitHubProject project) throws IOException {\n    if (!project.ratingValue().isPresent()) {\n        LOGGER.warn(\"Oops! No rating calculated for {}\", project.scm());\n        return;\n    }\n    if (!fetcher.repositoryFor(project).hasIssues()) {\n        LOGGER.warn(\"Creating issues is disabled for project {}\", project.toString());\n        return;\n    }\n    LOGGER.info(\"Creating issues for violations on {}\", project.toString());\n    List<Value<Boolean>> violations = OssRulesOfPlayScore.findViolatedRulesIn(project.ratingValue().get().scoreValue().usedValues());\n    for (Value<Boolean> violation : violations) {\n        String issueHeader = printTitle(violation);\n        List<GHIssue> existingGitHubIssues = fetcher.gitHubIssuesFor(project, issueHeader);\n        if (existingGitHubIssues.isEmpty()) {\n            fetcher.createGitHubIssue(project, printTitle(violation), printBody(violation));\n            LOGGER.info(\"New issue: \" + issueHeader);\n        } else {\n            LOGGER.info(\"Issue already exists: \" + issueHeader);\n        }\n    }\n}",
        "rpfc": "public void createIssuesFor(GitHubProject project) throws IOException {\n    if (!project.ratingValue().isPresent()) {\n        LOGGER.warn(\"Oops! No rating calculated for {}\", project.scm());\n        return;\n    }\n    if (!fetcher.repositoryFor(project).hasIssues()) {\n        LOGGER.warn(\"Creating issues is disabled for project {}\", project.toString());\n        return;\n    }\n    LOGGER.info(\"Creating issues for violations on {}\", project.toString());\n    List<Value<Boolean>> violations = OssRulesOfPlayScore.findViolatedRulesIn(project.ratingValue().get().scoreValue().usedValues());\n    for (Value<Boolean> violation : violations) {\n        String issueHeader = printTitle(violation);\n        String withIssueTitle = formatter.identifierOf(violation.feature()).orElse(issueHeader);\n        List<GHIssue> existingGitHubIssues = fetcher.gitHubIssuesFor(project, withIssueTitle);\n        if (existingGitHubIssues.isEmpty()) {\n            fetcher.createGitHubIssue(project, issueHeader, printBody(violation));\n            LOGGER.info(\"New issue: \" + issueHeader);\n        } else {\n            LOGGER.info(\"Issue already exists: \" + issueHeader);\n        }\n    }\n}",
        "tuc": "@Test\npublic void testIssuesEnabled() throws IOException {\n    OssRulesOfPlayRating rating = RatingRepository.INSTANCE.rating(OssRulesOfPlayRating.class);\n    GitHubProject project = new GitHubProject(\"test\", \"project\");\n    project.set(new RatingValue(new ScoreValue(rating.score()).set(Score.MIN).confidence(8.0).usedValues(new BooleanValue(OssFeatures.HAS_LICENSE, false), new BooleanValue(OssFeatures.HAS_README, false)), OssRulesOfPlayRating.OssRulesOfPlayLabel.FAILED));\n    GitHubDataFetcher ghDataFetcher = mock(GitHubDataFetcher.class);\n    when(ghDataFetcher.gitHubIssuesFor(any(), any())).thenReturn(Lists.newArrayList());\n    when(ghDataFetcher.createGitHubIssue(any(), any(), any())).thenReturn(new GHIssue());\n    GHRepository repository = mock(GHRepository.class);\n    when(repository.hasIssues()).thenReturn(true);\n    when(ghDataFetcher.repositoryFor(any())).thenReturn(repository);\n    OssRulesOfPlayGitHubIssuesReporter reporter = new OssRulesOfPlayGitHubIssuesReporter(ghDataFetcher, new OssRulesOfPlayRatingMarkdownFormatter(new OssRulesOfPlayAdvisor()));\n    reporter.createIssuesFor(project);\n    verify(repository, times(1)).hasIssues();\n    verify(ghDataFetcher, times(2)).createGitHubIssue(any(), any(), any());\n}",
        "label": 1
    },
    {
        "repo_name": "SAP___fosstars-rating-core",
        "commit": "3225801309a26ea4b5339312f8129b74ceb8396f",
        "commit_message": "Use Precision.equals() in CommonFormatter\n\nPart of #501\n",
        "p_path": "src/main/java/com/sap/oss/phosphor/fosstars/tool/format/CommonFormatter.java",
        "t_path": "src/test/java/com/sap/oss/phosphor/fosstars/tool/format/OssSecurityRatingMarkdownFormatterTest.java",
        "p_name": "confidenceLabelFor",
        "t_name": "testWithCustomTemplate",
        "lpfc": "static String confidenceLabelFor(double confidence) {\n    if (Double.compare(confidence, Confidence.MAX) == 0) {\n        return \"Max\";\n    }\n    if (Double.compare(confidence, Confidence.MIN) == 0) {\n        return \"Min\";\n    }\n    return confidence >= 9.0 ? \"High\" : \"Low\";\n}",
        "rpfc": "static String confidenceLabelFor(double confidence) {\n    if (Precision.equals(confidence, Confidence.MAX)) {\n        return \"Max\";\n    }\n    if (Precision.equals(confidence, Confidence.MIN)) {\n        return \"Min\";\n    }\n    return confidence >= 9.0 ? \"High\" : \"Low\";\n}",
        "tuc": "@Test\npublic void testWithCustomTemplate() {\n    RatingValue ratingValue = RATING.calculate(TEST_VALUES);\n    GitHubProject project = new GitHubProject(\"org\", \"test\");\n    project.set(ratingValue);\n    String template = \"%RATING_LABEL%|%SCORE_VALUE%|%MAX_SCORE%|%CONFIDENCE_LABEL%\" + \"|%CONFIDENCE_VALUE%|%MAX_CONFIDENCE%|%MAIN_SCORE_NAME%\" + \"|%MAIN_SCORE_DESCRIPTION%|%MAIN_SCORE_EXPLANATION%\";\n    OssSecurityRatingMarkdownFormatter formatter = new OssSecurityRatingMarkdownFormatter(new OssSecurityGithubAdvisor(), template);\n    String text = formatter.print(project);\n    assertNotNull(text);\n    assertEquals(\"BAD|4.09|10.0|High|10.0|10.0|security score for open-source projects||\", text);\n}",
        "label": 1
    },
    {
        "repo_name": "SAP___fosstars-rating-core",
        "commit": "5bc95acdabc1f10827d390dfe26b761caf2c344c",
        "commit_message": "UsesGithubForDevelopment should not divide integers\n\nThis fixes #184\n",
        "p_path": "src/main/java/com/sap/sgs/phosphor/fosstars/data/github/UsesGithubForDevelopment.java",
        "t_path": "src/test/java/com/sap/sgs/phosphor/fosstars/data/github/UsesGithubForDevelopmentTest.java",
        "p_name": "usesGitHubForDevelopment",
        "t_name": "testVariousChecks",
        "lpfc": "static boolean usesGitHubForDevelopment(GHRepository repository, double threshold) {\n    int points = CHECKS.stream().map(check -> check.test(repository) ? 1 : 0).reduce(0, Integer::sum);\n    return points / CHECKS.size() >= threshold;\n}",
        "rpfc": "static boolean usesGitHubForDevelopment(GHRepository repository, double threshold) {\n    int points = CHECKS.stream().map(check -> check.test(repository) ? 1 : 0).reduce(0, Integer::sum);\n    return (double) points / CHECKS.size() >= threshold;\n}",
        "tuc": "@Test\npublic void testVariousChecks() {\n    Random random = new Random();\n    random.setSeed(0);\n    RepositoryMockBuilder builder = new RepositoryMockBuilder();\n    int i = 0;\n    while (i < builder.allChecks()) {\n        builder.init();\n        random.ints(i, 0, builder.allChecks()).forEach(builder::passCheck);\n        boolean expected = builder.passedChecks() / builder.allChecks() >= CONFIDENCE_THRESHOLD;\n        assertEquals(expected, UsesGithubForDevelopment.usesGitHubForDevelopment(builder.repository(), CONFIDENCE_THRESHOLD));\n        i++;\n    }\n}",
        "label": 1
    },
    {
        "repo_name": "jbock-java___jbock",
        "commit": "30407126777732409127124198fb0f96e29e662b",
        "commit_message": "prevent problems with single underscore\n",
        "p_path": "compiler/src/main/java/net/jbock/common/EnumName.java",
        "t_path": "compiler/src/test/java/net/jbock/common/EnumNameTest.java",
        "p_name": "create",
        "t_name": "testEnumConstantUpperCase",
        "lpfc": "public static EnumName create(String input) {\n    return new EnumName(input, input.toUpperCase(Locale.US));\n}",
        "rpfc": "public static EnumName create(String input) {\n    if (\"_\".equals(input)) {\n        // prevent potential problems\n        return new EnumName(\"__\", \"__\");\n    }\n    return new EnumName(input, input.toUpperCase(Locale.US));\n}",
        "tuc": "@Test\nvoid testEnumConstantUpperCase() {\n    assertEquals(\"MYCOMMAND\", EnumName.create(\"MyCommand\").enumConstant());\n}",
        "label": 1
    },
    {
        "repo_name": "jbock-java___jbock",
        "commit": "d329fda17cddec242934c84baf648ec25b2295a8",
        "commit_message": "refactoring\n",
        "p_path": "core/src/main/java/net/jbock/coerce/CollectorClassValidator.java",
        "t_path": "core/src/test/java/net/jbock/coerce/CollectorClassValidatorTest.java",
        "p_name": "getCollectorInfo",
        "t_name": "simpleTest",
        "lpfc": "CollectorInfo getCollectorInfo() {\n    commonChecks(collectorClass);\n    checkNotAbstract(collectorClass);\n    ReferencedType<Collector> collectorType = new ReferenceTool<>(COLLECTOR, errorHandler, tool, collectorClass).getReferencedType();\n    TypeMirror inputType = collectorType.typeArguments().get(0);\n    TypeMirror outputType = collectorType.typeArguments().get(2);\n    TypevarMapping rightSolution = tool.unify(originalReturnType, outputType).orElseThrow(this::boom);\n    TypevarMapping leftSolution = new TypevarMapping(Collections.emptyMap(), tool);\n    FlattenerResult result = new Flattener(tool, collectorClass).mergeSolutions(leftSolution, rightSolution).orElseThrow(this::boom);\n    return CollectorInfo.create(tool, result.substitute(inputType).orElseThrow(f -> boom(f.getMessage())), collectorClass, collectorType.isSupplier(), result.getTypeParameters());\n}",
        "rpfc": "CollectorInfo getCollectorInfo() {\n    commonChecks(collectorClass);\n    checkNotAbstract(collectorClass);\n    ReferencedType<Collector> collectorType = new ReferenceTool<>(COLLECTOR, errorHandler, tool, collectorClass).getReferencedType();\n    TypeMirror inputType = collectorType.typeArguments().get(0);\n    TypeMirror outputType = collectorType.typeArguments().get(2);\n    TypevarMapping rightSolution = tool.unify(returnType, outputType).orElseThrow(this::boom);\n    TypevarMapping leftSolution = new TypevarMapping(Collections.emptyMap(), tool);\n    FlattenerResult result = new Flattener(tool, collectorClass).mergeSolutions(leftSolution, rightSolution).orElseThrow(this::boom);\n    return CollectorInfo.create(tool, result.substitute(inputType).orElseThrow(f -> boom(f.getMessage())), collectorClass, collectorType.isSupplier(), result.getTypeParameters());\n}",
        "tuc": "@Test\nvoid simpleTest() {\n    EvaluatingProcessor.source(\"import java.util.Set;\", \"import java.util.function.Supplier;\", \"import java.util.stream.Collector;\", \"import java.util.stream.Collectors;\", \"\", \"class ToSetCollector<E> implements Supplier<Collector<E, ?, Set<E>>> {\", \"  public Collector<E, ?, Set<E>> get() {\", \"    return Collectors.toSet();\", \"  }\", \"}\").run(\"ToSetCollector\", (elements, types) -> {\n        TypeTool tool = new TypeTool(elements, types);\n        DeclaredType originalReturnType = tool.getDeclaredType(Set.class, Collections.singletonList(tool.asType(String.class)));\n        TypeElement collectorClass = elements.getTypeElement(\"ToSetCollector\");\n        CollectorInfo collectorInfo = new CollectorClassValidator(s -> null, tool, collectorClass, originalReturnType).getCollectorInfo();\n        CodeBlock expected = CodeBlock.of(\".collect(new $T<$T>().get())\", types.erasure(collectorClass.asType()), String.class);\n        assertEquals(expected, collectorInfo.collectExpr());\n    });\n}",
        "label": 1
    },
    {
        "repo_name": "jbock-java___jbock",
        "commit": "774acd497a924209e046a67d8b4a4356b88bb6be",
        "commit_message": "update curl example\n",
        "p_path": "examples/src/main/java/net/zerobuilder/examples/gradle/CurlArguments.java",
        "t_path": "examples/src/test/java/net/zerobuilder/examples/gradle/CurlArgumentsTest.java",
        "p_name": "urls",
        "t_name": "testPrintUsage",
        "lpfc": "@OtherTokens\n@Description({ \"@OtherTokens to capture any 'other' tokens in the input.\", \"In this case, that's any token which is not one of\", \"'-v', '--verbose', '-X', '--method', '-H', '--header',\", \"or follows immediately after one of the latter 4.\", \"If there were no method with the @OtherTokens annotation,\", \"such a token would cause an IllegalArgumentException to be\", \"thrown from the CurlArguments_Parser.parse method.\" })\nabstract List<String> urls();",
        "rpfc": "@OtherTokens\n@Description({ \"@OtherTokens to capture any 'other' tokens in the input.\", \"In this case, that's any token which doesn't match one of\", \"-v, --verbose, -X(=.*)?, --method(=.*)?, -H(=.*)?, --header(=.*)?\", \"or follows immediately after the equality-less version\", \"of one of the latter 4.\", \"If there were no method with the @OtherTokens annotation,\", \"such a token would cause an IllegalArgumentException to be\", \"thrown from the CurlArguments_Parser.parse method.\" })\nabstract List<String> urls();",
        "tuc": "@Test\npublic void testPrintUsage() throws Exception {\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\n    CurlArguments_Parser.printUsage(new PrintStream(out), 2);\n    String[] lines = new String(out.toByteArray()).split(\"\\n\", -1);\n    assertThat(lines.length).isEqualTo(15);\n    assertThat(lines[0]).isEqualTo(\"-X, --method VAL\");\n    assertThat(lines[1]).isEqualTo(\"  Optional<String> for regular arguments\");\n    assertThat(lines[2]).isEqualTo(\"-H, --header VAL\");\n    assertThat(lines[3]).isEqualTo(\"  List<String> for repeatable arguments\");\n    assertThat(lines[4]).isEqualTo(\"-v, --verbose\");\n    assertThat(lines[5]).isEqualTo(\"  boolean for flags\");\n    assertThat(lines[6]).isEqualTo(\"Other tokens\");\n    assertThat(lines[7]).isEqualTo(\"  @OtherTokens to capture any 'other' tokens in the input.\");\n    assertThat(lines[8]).isEqualTo(\"  In this case, that's any token which is not one of\");\n    assertThat(lines[9]).isEqualTo(\"  '-v', '--verbose', '-X', '--method', '-H', '--header',\");\n    assertThat(lines[10]).isEqualTo(\"  or follows immediately after one of the latter 4.\");\n    assertThat(lines[11]).isEqualTo(\"  If there were no method with the @OtherTokens annotation,\");\n    assertThat(lines[12]).isEqualTo(\"  such a token would cause an IllegalArgumentException to be\");\n    assertThat(lines[13]).isEqualTo(\"  thrown from the CurlArguments_Parser.parse method.\");\n    assertThat(lines[14]).isEqualTo(\"\");\n}",
        "label": 1
    },
    {
        "repo_name": "jbock-java___jbock",
        "commit": "eab2a452efdeb3a0f83f9c5a90a7c1ab110c9fbb",
        "commit_message": "bump version\n",
        "p_path": "examples/src/main/java/net/zerobuilder/examples/gradle/CurlArguments.java",
        "t_path": "examples/src/test/java/net/zerobuilder/examples/gradle/CurlArgumentsTest.java",
        "p_name": "urls",
        "t_name": "testPrintUsage",
        "lpfc": "@OtherTokens\n@Description({ \"@OtherTokens to capture everything else.\", \"In this case, everything that isn't '-v' or follows '-H' or '-X'\" })\nabstract List<String> urls();",
        "rpfc": "@OtherTokens\n@Description({ \"@OtherTokens to capture any 'other' tokens in the input.\", \"In this case, that's any token which is not one of\", \"'-v', '--verbose', '-X', '--method', '-H', '--header',\", \"or follows immediately after one of the latter 4.\", \"If there were no method with the @OtherTokens annotation,\", \"such a token would cause an IllegalArgumentException to be\", \"thrown from the CurlArguments_Parser.parse method.\" })\nabstract List<String> urls();",
        "tuc": "@Test\npublic void testPrintUsage() throws Exception {\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\n    CurlArguments_Parser.printUsage(new PrintStream(out), 2);\n    String[] lines = new String(out.toByteArray()).split(\"\\n\", -1);\n    assertThat(lines.length).isEqualTo(10);\n    assertThat(lines[0]).isEqualTo(\"-X, --method VAL\");\n    assertThat(lines[1]).isEqualTo(\"  Optional<String> for regular arguments\");\n    assertThat(lines[2]).isEqualTo(\"-H, --header VAL\");\n    assertThat(lines[3]).isEqualTo(\"  List<String> for repeatable arguments\");\n    assertThat(lines[4]).isEqualTo(\"-v, --verbose\");\n    assertThat(lines[5]).isEqualTo(\"  boolean for flags\");\n    assertThat(lines[6]).isEqualTo(\"Other tokens\");\n    assertThat(lines[7]).isEqualTo(\"  @OtherTokens to capture everything else.\");\n    assertThat(lines[8]).isEqualTo(\"  In this case, everything that isn't '-v' or follows '-H' or '-X'\");\n    assertThat(lines[9]).isEqualTo(\"\");\n}",
        "label": 1
    },
    {
        "repo_name": "strimzi___strimzi-mqtt-bridge",
        "commit": "5c8ff6c03be8f5c060d296c02d7f23583bb5e094",
        "commit_message": "Make the --config-file mandatory and fix config related test (#27)\n\nSigned-off-by: Paolo Patierno <ppatierno@live.com>",
        "p_path": "src/main/java/io/strimzi/kafka/bridge/mqtt/Main.java",
        "t_path": "src/test/java/io/strimzi/kafka/bridge/mqtt/core/ConfigRetrieverTest.java",
        "p_name": "generateCommandLineOptions",
        "t_name": "testApplicationPropertiesFile",
        "lpfc": "private static Options generateCommandLineOptions() {\n    Options options = new Options();\n    Option optionConfigFile = Option.builder().longOpt(Main.CONFIG_FILE_OPTION).hasArg(true).desc(\"The path to the configuration file\").build();\n    options.addOption(optionConfigFile);\n    Option optionMappingRulesFile = Option.builder().longOpt(Main.MAPPING_RULES_FILE_OPTION).hasArg(true).required().desc(\"The path to the topic mapping rules file\").build();\n    options.addOption(optionMappingRulesFile);\n    return options;\n}",
        "rpfc": "private static Options generateCommandLineOptions() {\n    Options options = new Options();\n    Option optionConfigFile = Option.builder().longOpt(Main.CONFIG_FILE_OPTION).hasArg(true).required().desc(\"The path to the configuration file\").build();\n    options.addOption(optionConfigFile);\n    Option optionMappingRulesFile = Option.builder().longOpt(Main.MAPPING_RULES_FILE_OPTION).hasArg(true).required().desc(\"The path to the topic mapping rules file\").build();\n    options.addOption(optionMappingRulesFile);\n    return options;\n}",
        "tuc": "/**\n * Test if the application.properties file has all configuration parameters.\n */\n@Test\npublic void testApplicationPropertiesFile() throws IOException {\n    String filePath = Objects.requireNonNull(getClass().getClassLoader().getResource(\"application.properties\")).getPath();\n    Map<String, Object> config = ConfigRetriever.getConfig(filePath);\n    BridgeConfig bridgeConfig = BridgeConfig.fromMap(config);\n    // bridge config related tests\n    assertThat(\"Bridge-ID should be 'my-bridge'\", bridgeConfig.getBridgeID(), is(\"my-bridge\"));\n    // Mqtt server config related tests\n    assertThat(\"There should be 2 related mqtt server config parameters\", bridgeConfig.getMqttConfig().getConfig().size(), is(2));\n    assertThat(\"Mqtt server host should be 'localhost'\", bridgeConfig.getMqttConfig().getHost(), is(\"localhost\"));\n    assertThat(\"Mqtt server port should be '1883'\", bridgeConfig.getMqttConfig().getPort(), is(1883));\n    // Kafka server config related tests\n    assertThat(\"There should be 1 related kafka config parameters\", bridgeConfig.getKafkaConfig().getConfig().size(), is(1));\n    assertThat(\"The address of the kafka bootstrap server should be 'localhost:9092'\", bridgeConfig.getKafkaConfig().getConfig().get(KafkaConfig.BOOTSTRAP_SERVERS_CONFIG), is(\"localhost:9092\"));\n}",
        "label": 1
    },
    {
        "repo_name": "strimzi___strimzi-mqtt-bridge",
        "commit": "89d53b8a8af345511e509e1472abaeae8d8d8dc6",
        "commit_message": "MQTT pattern matching for multi level wildcard (#21)\n\nSigned-off-by: Antonio Pedro <tonio.pedro99@gmail.com>",
        "p_path": "src/main/java/io/strimzi/kafka/bridge/mqtt/mapper/MqttKafkaMapper.java",
        "t_path": "src/test/java/io/strimzi/kafka/bridge/mqtt/mapper/MqttKafkaMapperTest.java",
        "p_name": "buildRegex",
        "t_name": "testMultiLevel",
        "lpfc": "private void buildRegex() {\n    String[] mqttTopicPatternParts;\n    StringBuilder ruleRegex;\n    for (MappingRule rule : this.rules) {\n        mqttTopicPatternParts = rule.getMqttTopicPattern().split(\"/\");\n        ruleRegex = new StringBuilder();\n        for (String part : mqttTopicPatternParts) {\n            if (part.matches(MQTT_TOPIC_PLACEHOLDER_REGEX)) {\n                ruleRegex.append(buildNamedRegexExpression(part));\n            } else if (part.equals(MQTT_TOPIC_SINGLE_LEVEL_WILDCARD_CHARACTER)) {\n                ruleRegex.append(SINGLE_LEVEL_WILDCARD_REGEX);\n            } else if (part.equals(MQTT_TOPIC_MULTI_LEVEL_WILDCARD_CHARACTER)) {\n                ruleRegex.append(MULTIPLE_LEVEL_WILDCARD_REGEX);\n            } else {\n                ruleRegex.append(part);\n            }\n            ruleRegex.append(\"/\");\n        }\n        ruleRegex.deleteCharAt(ruleRegex.length() - 1);\n        patterns.add(Pattern.compile(ruleRegex.toString()));\n    }\n}",
        "rpfc": "private void buildRegex() {\n    String[] mqttTopicPatternParts;\n    StringBuilder ruleRegex;\n    for (MappingRule rule : this.rules) {\n        mqttTopicPatternParts = rule.getMqttTopicPattern().split(\"/\");\n        ruleRegex = new StringBuilder();\n        for (String part : mqttTopicPatternParts) {\n            if (part.matches(MQTT_TOPIC_PLACEHOLDER_REGEX)) {\n                ruleRegex.append(buildNamedRegexExpression(part));\n            } else if (part.equals(MQTT_TOPIC_SINGLE_LEVEL_WILDCARD_CHARACTER)) {\n                ruleRegex.append(SINGLE_LEVEL_WILDCARD_REGEX);\n            } else if (part.equals(MQTT_TOPIC_MULTI_LEVEL_WILDCARD_CHARACTER)) {\n                if (ruleRegex.length() > 1) {\n                    ruleRegex.deleteCharAt(ruleRegex.length() - 1);\n                }\n                ruleRegex.append(MULTIPLE_LEVEL_WILDCARD_REGEX);\n            } else {\n                ruleRegex.append(part);\n            }\n            ruleRegex.append(\"/\");\n        }\n        ruleRegex.deleteCharAt(ruleRegex.length() - 1);\n        patterns.add(Pattern.compile(ruleRegex.toString()));\n    }\n}",
        "tuc": "@Test\npublic void testMultiLevel() {\n    List<MappingRule> rules = new ArrayList<>();\n    rules.add(new MappingRule(\"building/{building}/room/{room}/#\", \"building_{building}_room_{room}\"));\n    rules.add(new MappingRule(\"building/{building}/#\", \"building_{building}_others\"));\n    rules.add(new MappingRule(\"building/#\", \"building_others\"));\n    rules.add(new MappingRule(\"fleet/{fleet}/vehicle/{vehicle}/#\", \"fleet_{vehicle}\"));\n    MqttKafkaMapper mapper = new MqttKafkaMapper(rules);\n    assertThat(\"Mqtt topic pattern fleet/{fleet}/vehicle/{vehicle}/# should be mapped to fleet_{vehicle}\", mapper.map(\"fleet/4/vehicle/23/velocity\"), is(\"fleet_23\"));\n    assertThat(\"Mqtt pattern building/{building}/room/{room}/# should be mapped to building_{building}_room_{room}\", mapper.map(\"building/4/room/23/temperature\"), is(\"building_4_room_23\"));\n    assertThat(\"Mqtt pattern building/{building}/# should be mapped to building_{building}_others\", mapper.map(\"building/405/room\"), is(\"building_405_others\"));\n    assertThat(\"Mqtt pattern building/# should be mapped to building_others\", mapper.map(\"building/101\"), is(\"building_others\"));\n}",
        "label": 1
    },
    {
        "repo_name": "KonduitAI___konduit-serving",
        "commit": "224ee759c4eaf76f6635e136fcafbea2c1469790",
        "commit_message": "Speedup Heatmap Drawing (#412)\n\nSigned-off-by: Paul Dubs <paul.dubs@gmail.com>",
        "p_path": "konduit-serving-data/konduit-serving-image/src/main/java/ai/konduit/serving/data/image/step/point/heatmap/DrawHeatmapStepRunner.java",
        "t_path": "konduit-serving-data/konduit-serving-image/src/test/java/ai/konduit/serving/data/image/TestDrawHeatmapStep.java",
        "p_name": "exec",
        "t_name": "testSingleStep",
        "lpfc": "@Override\npublic Data exec(Context ctx, Data data) {\n    Data out = Data.empty();\n    for (String key : data.keys()) {\n        out.copyFrom(key, data);\n    }\n    int width;\n    int height;\n    Mat targetImage = null;\n    if (step.image() != null) {\n        ValueType type = data.type(step.image());\n        if (type == ValueType.IMAGE) {\n            Image image = data.getImage(step.image());\n            width = image.width();\n            height = image.height();\n            targetImage = image.getAs(Mat.class);\n        } else {\n            throw new IllegalArgumentException(\"The configured reference image input \" + step.image() + \" is not an Image!\");\n        }\n    } else if (step.width() != null && step.height() != null) {\n        width = step.width();\n        height = step.height();\n    } else {\n        throw new IllegalArgumentException(\"You have to provide either a reference image or width AND height!\");\n    }\n    if (prev == null) {\n        prev = new Mat();\n        prev.put(Mat.zeros(height, width, CvType.CV_64FC1));\n    }\n    List<Point> points = new LinkedList<>();\n    for (String pointName : step.points()) {\n        ValueType type = data.type(pointName);\n        if (type == ValueType.POINT) {\n            Point point = data.getPoint(pointName);\n            if (point.dimensions() != 2) {\n                throw new IllegalArgumentException(\"Point in input \" + pointName + \" has \" + point.dimensions() + \" dimensions, but only 2 dimensional points are supported for drawing!\");\n            }\n            points.add(accountForCrop(point, width, height, step.imageToNDArrayConfig()));\n        } else if (type == ValueType.LIST) {\n            List<Point> pointList = data.getListPoint(pointName);\n            for (Point point : pointList) {\n                if (point.dimensions() != 2) {\n                    throw new IllegalArgumentException(\"Point in input \" + pointName + \" has \" + point.dimensions() + \" dimensions, but only 2 dimensional points are supported for drawing!\");\n                }\n                points.add(accountForCrop(point, width, height, step.imageToNDArrayConfig()));\n            }\n        } else {\n            throw new IllegalArgumentException(\"The configured input \" + pointName + \" is neither a point nor a list of points!\");\n        }\n    }\n    Mat mat = new Mat();\n    mat.put(Mat.zeros(height, width, CvType.CV_64FC1));\n    DoubleIndexer idx = mat.createIndexer();\n    for (Point point : points) {\n        int row = (int) point.y();\n        int col = (int) point.x();\n        if (row > height || col > width) {\n            log.warn(\"{} is out of bounds ({}, {})\", point, width, height);\n        } else {\n            idx.put(row, col, idx.get(row, col) + 255);\n        }\n    }\n    int radius = step.radius() == null ? 15 : step.radius();\n    int kSize = radius * 8 + 1;\n    Size kernelSize = new Size(kSize, kSize);\n    opencv_imgproc.GaussianBlur(mat, mat, kernelSize, radius, radius, opencv_core.BORDER_ISOLATED);\n    opencv_core.addWeighted(prev, step.fadingFactor() == null ? 0.9 : step.fadingFactor(), mat, 1.0, 0, mat);\n    prev.close();\n    prev = mat;\n    DoublePointer maxVal = new DoublePointer(1);\n    opencv_core.minMaxLoc(mat, null, maxVal, null, null, null);\n    Mat scaledOut = new Mat();\n    mat.convertTo(scaledOut, CvType.CV_8UC1, 255 / maxVal.get(), 0);\n    maxVal.close();\n    Mat image = new Mat();\n    opencv_imgproc.applyColorMap(scaledOut, image, opencv_imgproc.COLORMAP_TURBO);\n    Image outputImage;\n    if (targetImage == null) {\n        outputImage = Image.create(image);\n    } else {\n        Mat composed = new Mat();\n        opencv_core.addWeighted(targetImage, 1.0, image, step.opacity() == null ? 0.5 : step.opacity(), 0, composed);\n        outputImage = Image.create(composed);\n    }\n    out.put(step.outputName() == null ? DrawHeatmapStep.DEFAULT_OUTPUT_NAME : step.outputName(), outputImage);\n    return out;\n}",
        "rpfc": "@Override\npublic Data exec(Context ctx, Data data) {\n    Data out = Data.empty();\n    for (String key : data.keys()) {\n        out.copyFrom(key, data);\n    }\n    int width;\n    int height;\n    Mat targetImage = null;\n    if (step.image() != null) {\n        ValueType type = data.type(step.image());\n        if (type == ValueType.IMAGE) {\n            Image image = data.getImage(step.image());\n            width = image.width();\n            height = image.height();\n            targetImage = image.getAs(Mat.class);\n        } else {\n            throw new IllegalArgumentException(\"The configured reference image input \" + step.image() + \" is not an Image!\");\n        }\n    } else if (step.width() != null && step.height() != null) {\n        width = step.width();\n        height = step.height();\n    } else {\n        throw new IllegalArgumentException(\"You have to provide either a reference image or width AND height!\");\n    }\n    if (prev == null) {\n        prev = new Mat();\n        prev.put(Mat.zeros(height, width, CvType.CV_64FC1));\n    }\n    List<Point> points = new LinkedList<>();\n    for (String pointName : step.points()) {\n        ValueType type = data.type(pointName);\n        if (type == ValueType.POINT) {\n            Point point = data.getPoint(pointName);\n            if (point.dimensions() != 2) {\n                throw new IllegalArgumentException(\"Point in input \" + pointName + \" has \" + point.dimensions() + \" dimensions, but only 2 dimensional points are supported for drawing!\");\n            }\n            points.add(accountForCrop(point, width, height, step.imageToNDArrayConfig()));\n        } else if (type == ValueType.LIST) {\n            List<Point> pointList = data.getListPoint(pointName);\n            for (Point point : pointList) {\n                if (point.dimensions() != 2) {\n                    throw new IllegalArgumentException(\"Point in input \" + pointName + \" has \" + point.dimensions() + \" dimensions, but only 2 dimensional points are supported for drawing!\");\n                }\n                points.add(accountForCrop(point, width, height, step.imageToNDArrayConfig()));\n            }\n        } else {\n            throw new IllegalArgumentException(\"The configured input \" + pointName + \" is neither a point nor a list of points!\");\n        }\n    }\n    int radius = step.radius() == null ? 15 : step.radius();\n    int kSize = radius * 8 + 1;\n    if (brush == null) {\n        Size kernelSize = new Size(kSize, kSize);\n        brush = new Mat();\n        brush.put(Mat.zeros(kSize, kSize, CvType.CV_64FC1));\n        brush.createIndexer().putDouble(new long[] { kSize / 2, kSize / 2 }, 255);\n        opencv_imgproc.GaussianBlur(brush, brush, kernelSize, radius, radius, opencv_core.BORDER_ISOLATED);\n    }\n    Mat mat = new Mat();\n    mat.put(Mat.zeros(height, width, CvType.CV_64FC1));\n    for (Point point : points) {\n        int row = (int) point.y();\n        int col = (int) point.x();\n        if (row > height || col > width) {\n            log.warn(\"{} is out of bounds ({}, {})\", point, width, height);\n        } else {\n            int offsetRow = row - kSize / 2;\n            int offsetCol = col - kSize / 2;\n            int brushWidth = kSize;\n            int brushHeight = kSize;\n            int brushOffsetRow = 0;\n            int brushOffsetCol = 0;\n            if (offsetRow < 0) {\n                brushHeight += offsetRow;\n                brushOffsetRow -= offsetRow;\n                offsetRow = 0;\n            }\n            if (offsetCol < 0) {\n                brushWidth += offsetCol;\n                brushOffsetCol -= offsetCol;\n                offsetCol = 0;\n            }\n            if (offsetRow + brushHeight > mat.arrayHeight()) {\n                brushHeight = mat.arrayHeight() - offsetRow;\n            }\n            if (offsetCol + brushWidth > mat.arrayWidth()) {\n                brushWidth = mat.arrayWidth() - offsetCol;\n            }\n            Mat region = mat.apply(new Rect(offsetCol, offsetRow, brushWidth, brushHeight));\n            Mat brushRegion = brush.apply(new Rect(brushOffsetCol, brushOffsetRow, brushWidth, brushHeight));\n            opencv_core.add(region, brushRegion, region);\n        }\n    }\n    opencv_core.addWeighted(prev, step.fadingFactor() == null ? 0.9 : step.fadingFactor(), mat, 1.0, 0, mat);\n    prev.close();\n    prev = mat;\n    DoublePointer maxVal = new DoublePointer(1);\n    opencv_core.minMaxLoc(mat, null, maxVal, null, null, null);\n    Mat scaledOut = new Mat();\n    mat.convertTo(scaledOut, CvType.CV_8UC1, 255 / maxVal.get(), 0);\n    maxVal.close();\n    Mat image = new Mat();\n    opencv_imgproc.applyColorMap(scaledOut, image, opencv_imgproc.COLORMAP_TURBO);\n    Image outputImage;\n    if (targetImage == null) {\n        outputImage = Image.create(image);\n    } else {\n        opencv_core.addWeighted(targetImage, 1.0, image, step.opacity() == null ? 0.5 : step.opacity(), 0, image);\n        outputImage = Image.create(image);\n    }\n    out.put(step.outputName() == null ? DrawHeatmapStep.DEFAULT_OUTPUT_NAME : step.outputName(), outputImage);\n    return out;\n}",
        "tuc": "@Ignore\n@Test\npublic void testSingleStep() throws Exception {\n    Pipeline p = SequencePipeline.builder().add(new DrawHeatmapStep().points(Collections.singletonList(\"points\")).width(100).height(200).outputName(\"image\")).add(new ShowImagePipelineStep()).build();\n    Data data = Data.empty();\n    data.putListPoint(\"points\", Arrays.asList(Point.create(2, 2, \"bar\", 0.1), Point.create(0.1, 0.1, \"foo\", 0.2), Point.create(0.5, 0.8, \"bar\", 0.3), Point.create(30, 59, \"foo\", 0.4)));\n    Data out = p.executor().exec(data);\n    Thread.sleep(Long.MAX_VALUE);\n}",
        "label": 1
    },
    {
        "repo_name": "KonduitAI___konduit-serving",
        "commit": "515b04e527ef5879b0265923cccd8fbc9b742f14",
        "commit_message": "GraphPipeline support for build tool (#366)\n\nSigned-off-by: Alex Black <blacka101@gmail.com>",
        "p_path": "konduit-serving-build/src/main/java/ai/konduit/serving/build/util/ModuleUtils.java",
        "t_path": "konduit-serving-build/src/test/java/ai/konduit/serving/build/TestResolving.java",
        "p_name": "runnersForJson",
        "t_name": "testResolvingDL4JSameDiff",
        "lpfc": "public static Map<StepId, List<RunnerInfo>> runnersForJson(String json) {\n    Map<StepId, List<RunnerInfo>> out = new HashMap<>();\n    Map<String, Object> map;\n    try {\n        map = ObjectMappers.json().readValue(json, Map.class);\n    } catch (IOException e) {\n        throw new RuntimeException(e);\n    }\n    Object stepsObj = map.get(\"steps\");\n    int stepCount = 0;\n    if (stepsObj instanceof List) {\n        List<Object> l = (List<Object>) stepsObj;\n        for (Object o : l) {\n            if (o instanceof Map) {\n                Map<String, Object> m = (Map<String, Object>) o;\n                String jsonType = (String) m.get(\"@type\");\n                Module mod = moduleForJsonType(jsonType);\n                if (mod == null)\n                    continue;\n                String runnerClass = null;\n                String name = \"\";\n                StepId id = new StepId(stepCount, name, jsonType);\n                RunnerInfo ri = new RunnerInfo(runnerClass, mod);\n                out.put(id, Collections.singletonList(ri));\n            }\n        }\n    } else {\n        throw new UnsupportedOperationException(\"GraphPipeline handling not yet implemented\");\n    }\n    return out;\n}",
        "rpfc": "public static Map<StepId, List<RunnerInfo>> runnersForJson(String json) {\n    Map<StepId, List<RunnerInfo>> out = new HashMap<>();\n    Map<String, Object> map;\n    try {\n        map = ObjectMappers.json().readValue(json, Map.class);\n    } catch (IOException e) {\n        throw new RuntimeException(e);\n    }\n    Object stepsObj = map.get(\"steps\");\n    int stepCount = 0;\n    if (stepsObj instanceof List) {\n        List<Object> l = (List<Object>) stepsObj;\n        for (Object o : l) {\n            if (o instanceof Map) {\n                Map<String, Object> m = (Map<String, Object>) o;\n                String jsonType = (String) m.get(\"@type\");\n                Module mod = moduleForJsonType(jsonType);\n                if (mod == null)\n                    continue;\n                String runnerClass = null;\n                String name = \"\";\n                StepId id = new StepId(stepCount, name, jsonType);\n                RunnerInfo ri = new RunnerInfo(runnerClass, mod);\n                out.put(id, Collections.singletonList(ri));\n            }\n        }\n    } else if (stepsObj instanceof Map) {\n        Map<String, Object> m = (Map<String, Object>) stepsObj;\n        for (Map.Entry<String, Object> e : m.entrySet()) {\n            if (e.getValue() instanceof Map) {\n                Map<String, Object> step = (Map<String, Object>) e.getValue();\n                if (step.containsKey(\"@type\")) {\n                    String jsonType = (String) step.get(\"@type\");\n                    Module mod = moduleForJsonType(jsonType);\n                    if (mod == null)\n                        continue;\n                    String runnerClass = null;\n                    String name = \"\";\n                    StepId id = new StepId(stepCount, name, jsonType);\n                    RunnerInfo ri = new RunnerInfo(runnerClass, mod);\n                    out.put(id, Collections.singletonList(ri));\n                }\n            }\n        }\n    }\n    return out;\n}",
        "tuc": "@Test\npublic void testResolvingDL4JSameDiff() throws Exception {\n    for (int testNum = 0; testNum <= 1; testNum++) {\n        for (Target t : new Target[] { Target.LINUX_X86, Target.LINUX_X86_AVX2, Target.LINUX_X86_AVX512, Target.WINDOWS_X86, Target.WINDOWS_X86_AVX2, Target.LINUX_CUDA_10_2, Target.LINUX_CUDA_10_1, Target.LINUX_CUDA_10_0, Target.WINDOWS_CUDA_10_2, Target.WINDOWS_CUDA_10_1, Target.WINDOWS_CUDA_10_0 }) {\n            PipelineStep step;\n            if (testNum == 0) {\n                step = new DL4JModelPipelineStep(\"file:///some/model/path.zip\", null, null);\n                System.out.println(\"----- DL4J - \" + t + \" -----\");\n            } else {\n                step = new SameDiffModelPipelineStep(\"file://some/model/path.fb\", null);\n            }\n            Pipeline p = SequencePipeline.builder().add(step).build();\n            File dir = testDir.newFolder();\n            File jsonF = new File(dir, \"pipeline.json\");\n            FileUtils.writeStringToFile(jsonF, p.toJson(), StandardCharsets.UTF_8);\n            Config c = new Config().pipelinePath(jsonF.getAbsolutePath()).ksVersion(\"LATEST\").metadata(new Metadata().author(\"User Name\").buildVersion(\"1.0.0\").timestamp(\"2020/05/26 12:00:00\")).target(t).serving(Serving.HTTP, Serving.GRPC).deployments(new UberJarDeployment().outputDir(\"/my/output/dir\").jarName(\"my.jar\"));\n            List<Dependency> resolvedDeps = c.resolveDependencies();\n            if (testNum == 0) {\n                for (Dependency d : resolvedDeps) {\n                    System.out.println(d);\n                }\n            }\n            String ksVersion = \"0.1.0-SNAPSHOT\";\n            List<Dependency> expectedDeps = new ArrayList<>();\n            expectedDeps.add(new Dependency(\"ai.konduit.serving\", \"konduit-serving-pipeline\", ksVersion));\n            expectedDeps.add(new Dependency(\"ai.konduit.serving\", \"konduit-serving-vertx\", ksVersion));\n            expectedDeps.add(new Dependency(\"ai.konduit.serving\", \"konduit-serving-cli\", ksVersion));\n            expectedDeps.add(new Dependency(\"ai.konduit.serving\", \"konduit-serving-http\", ksVersion));\n            expectedDeps.add(new Dependency(\"ai.konduit.serving\", \"konduit-serving-grpc\", ksVersion));\n            if (testNum == 0) {\n                expectedDeps.add(new Dependency(\"ai.konduit.serving\", \"konduit-serving-deeplearning4j\", ksVersion));\n            } else {\n                expectedDeps.add(new Dependency(\"ai.konduit.serving\", \"konduit-serving-samediff\", ksVersion));\n            }\n            String ossVer = \"1.0.0-beta7\";\n            if (t.equals(Target.LINUX_X86)) {\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-native\", ossVer));\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-native\", ossVer, \"linux-x86_64\"));\n            } else if (t.equals(Target.LINUX_X86_AVX2)) {\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-native\", ossVer));\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-native\", ossVer, \"linux-x86_64-avx2\"));\n            } else if (t.equals(Target.LINUX_X86_AVX512)) {\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-native\", ossVer));\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-native\", ossVer, \"linux-x86_64-avx512\"));\n            } else if (t.equals(Target.WINDOWS_X86)) {\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-native\", ossVer));\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-native\", ossVer, \"windows-x86_64\"));\n            } else if (t.equals(Target.WINDOWS_X86_AVX2)) {\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-native\", ossVer));\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-native\", ossVer, \"windows-x86_64-avx2\"));\n            } else if (t.equals(Target.LINUX_CUDA_10_2)) {\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-cuda-10.2\", ossVer));\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-cuda-10.2\", ossVer, \"linux-x86_64\"));\n            } else if (t.equals(Target.LINUX_CUDA_10_1)) {\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-cuda-10.1\", ossVer));\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-cuda-10.1\", ossVer, \"linux-x86_64\"));\n            } else if (t.equals(Target.LINUX_CUDA_10_0)) {\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-cuda-10.0\", ossVer));\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-cuda-10.0\", ossVer, \"linux-x86_64\"));\n            } else if (t.equals(Target.WINDOWS_CUDA_10_2)) {\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-cuda-10.2\", ossVer));\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-cuda-10.2\", ossVer, \"windows-x86_64\"));\n            } else if (t.equals(Target.WINDOWS_CUDA_10_1)) {\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-cuda-10.1\", ossVer));\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-cuda-10.1\", ossVer, \"windows-x86_64\"));\n            } else if (t.equals(Target.WINDOWS_CUDA_10_0)) {\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-cuda-10.0\", ossVer));\n                expectedDeps.add(new Dependency(\"org.nd4j\", \"nd4j-cuda-10.0\", ossVer, \"windows-x86_64\"));\n            } else {\n                throw new UnsupportedOperationException(t.toString());\n            }\n            assertEquals(expectedDeps, resolvedDeps);\n            if (testNum == 0) {\n                System.out.println();\n            }\n        }\n    }\n}",
        "label": 1
    },
    {
        "repo_name": "KonduitAI___konduit-serving",
        "commit": "5b0329ee719ac6f78fe3e6abb41920924b15ca29",
        "commit_message": "Merge remote-tracking branch 'origin/sa/consolidating-dl4j-model-types' into sa/consolidating-dl4j-model-types\n",
        "p_path": "konduit-serving-core/src/main/java/ai/konduit/serving/util/image/NativeImageLoader.java",
        "t_path": "konduit-serving-test/src/test/java/ai/konduit/serving/pipeline/steps/TransformProcessStepTest.java",
        "p_name": "convert",
        "t_name": "testTransformProcessPipe",
        "lpfc": "static Mat convert(PIX pix) {\n    PIX tempPix = null;\n    int dtype = -1;\n    int height = pix.h();\n    int width = pix.w();\n    Mat mat2;\n    if (pix.colormap() != null) {\n        PIX pix2 = pixRemoveColormap(pix, REMOVE_CMAP_TO_FULL_COLOR);\n        tempPix = pix = pix2;\n        dtype = CV_8UC4;\n    } else if (pix.d() <= 8 || pix.d() == 24) {\n        PIX pix2 = null;\n        switch(pix.d()) {\n            case 1:\n                pix2 = pixConvert1To8(null, pix, (byte) 0, (byte) 255);\n                break;\n            case 2:\n                pix2 = pixConvert2To8(pix, (byte) 0, (byte) 85, (byte) 170, (byte) 255, 0);\n                break;\n            case 4:\n                pix2 = pixConvert4To8(pix, 0);\n                break;\n            case 8:\n                pix2 = pix;\n                break;\n            case 24:\n                pix2 = pix;\n                break;\n            default:\n                assert false;\n        }\n        tempPix = pix = pix2;\n        int channels = pix.d() / 8;\n        dtype = CV_8UC(channels);\n        Mat mat = new Mat(height, width, dtype, pix.data(), 4 * pix.wpl());\n        mat2 = new Mat(height, width, CV_8UC(channels));\n        // swap bytes if needed\n        int[] swap = { 0, channels - 1, 1, channels - 2, 2, channels - 3, 3, channels - 4 }, copy = { 0, 0, 1, 1, 2, 2, 3, 3 }, fromTo = channels > 1 && ByteOrder.nativeOrder().equals(ByteOrder.LITTLE_ENDIAN) ? swap : copy;\n        mixChannels(mat, 1, mat2, 1, fromTo, Math.min(channels, fromTo.length / 2));\n    } else if (pix.d() == 16) {\n        dtype = CV_16UC(pix.d() / 16);\n    } else if (pix.d() == 32) {\n        dtype = CV_32FC(pix.d() / 32);\n    }\n    mat2 = new Mat(height, width, dtype, pix.data());\n    if (tempPix != null) {\n        pixDestroy(tempPix);\n    }\n    return mat2;\n}",
        "rpfc": "static Mat convert(PIX pix) {\n    PIX tempPix = null;\n    int dtype = -1;\n    int height = pix.h();\n    int width = pix.w();\n    Mat mat2;\n    if (pix.colormap() != null) {\n        PIX pix2 = pixRemoveColormap(pix, REMOVE_CMAP_TO_FULL_COLOR);\n        tempPix = pix = pix2;\n        dtype = CV_8UC4;\n    } else if (pix.d() <= 8 || pix.d() == 24) {\n        PIX pix2 = pix;\n        switch(pix.d()) {\n            case 1:\n                pix2 = pixConvert1To8(null, pix, (byte) 0, (byte) 255);\n                break;\n            case 2:\n                pix2 = pixConvert2To8(pix, (byte) 0, (byte) 85, (byte) 170, (byte) 255, 0);\n                break;\n            case 4:\n                pix2 = pixConvert4To8(pix, 0);\n                break;\n            case 8:\n            case 24:\n                pix2 = pix;\n                break;\n            default:\n                throw new IllegalStateException(\"Unrecognized pixel depth of \" + pix.d());\n        }\n        tempPix = pix = pix2;\n        int channels = pix.d() / 8;\n        dtype = CV_8UC(channels);\n        Mat mat = new Mat(height, width, dtype, pix.data(), 4 * pix.wpl());\n        mat2 = new Mat(height, width, CV_8UC(channels));\n        // swap bytes if needed\n        int[] swap = { 0, channels - 1, 1, channels - 2, 2, channels - 3, 3, channels - 4 }, copy = { 0, 0, 1, 1, 2, 2, 3, 3 }, fromTo = channels > 1 && ByteOrder.nativeOrder().equals(ByteOrder.LITTLE_ENDIAN) ? swap : copy;\n        mixChannels(mat, 1, mat2, 1, fromTo, Math.min(channels, fromTo.length / 2));\n    } else if (pix.d() == 16) {\n        dtype = CV_16UC(pix.d() / 16);\n    } else if (pix.d() == 32) {\n        dtype = CV_32FC(pix.d() / 32);\n    }\n    mat2 = new Mat(height, width, dtype, pix.data());\n    if (tempPix != null) {\n        pixDestroy(tempPix);\n    }\n    return mat2;\n}",
        "tuc": "@Test\npublic void testTransformProcessPipe() throws Exception {\n    Schema inputSchema = TrainUtils.getIrisInputSchema();\n    Schema outputSchema = getIrisOutputSchema();\n    TransformProcess.Builder transformProcessBuilder = new TransformProcess.Builder(inputSchema);\n    for (int i = 0; i < inputSchema.numColumns(); i++) {\n        transformProcessBuilder.convertToDouble(inputSchema.getName(i));\n    }\n    TransformProcess tp = transformProcessBuilder.build();\n    TransformProcessStep tpps = new TransformProcessStep().step(\"foo\", tp, outputSchema).step(\"bar\", tp, outputSchema);\n    assert tpps.getInputNames().contains(\"foo\");\n    assert tpps.getInputNames().contains(\"bar\");\n    assert tpps.getOutputNames().contains(\"foo\");\n    assert tpps.getOutputNames().contains(\"bar\");\n    TransformProcessStep tpps2 = new TransformProcessStep(tp, outputSchema);\n    assert tpps2.getTransformProcesses().get(\"default\") == tp;\n    assert Arrays.equals(tpps2.getInputSchemas().get(\"default\"), SchemaTypeUtils.typesForSchema(inputSchema));\n}",
        "label": 1
    },
    {
        "repo_name": "KonduitAI___konduit-serving",
        "commit": "90ddab3c2c5d77930809454989a0e50869889438",
        "commit_message": "Merge branch 'master' into sa/consolidating-dl4j-model-types",
        "p_path": "konduit-serving-core/src/main/java/ai/konduit/serving/util/image/NativeImageLoader.java",
        "t_path": "konduit-serving-test/src/test/java/ai/konduit/serving/pipeline/steps/TransformProcessStepTest.java",
        "p_name": "convert",
        "t_name": "testTransformProcessPipe",
        "lpfc": "static Mat convert(PIX pix) {\n    PIX tempPix = null;\n    int dtype = -1;\n    int height = pix.h();\n    int width = pix.w();\n    Mat mat2;\n    if (pix.colormap() != null) {\n        PIX pix2 = pixRemoveColormap(pix, REMOVE_CMAP_TO_FULL_COLOR);\n        tempPix = pix = pix2;\n        dtype = CV_8UC4;\n    } else if (pix.d() <= 8 || pix.d() == 24) {\n        PIX pix2 = null;\n        switch(pix.d()) {\n            case 1:\n                pix2 = pixConvert1To8(null, pix, (byte) 0, (byte) 255);\n                break;\n            case 2:\n                pix2 = pixConvert2To8(pix, (byte) 0, (byte) 85, (byte) 170, (byte) 255, 0);\n                break;\n            case 4:\n                pix2 = pixConvert4To8(pix, 0);\n                break;\n            case 8:\n                pix2 = pix;\n                break;\n            case 24:\n                pix2 = pix;\n                break;\n            default:\n                assert false;\n        }\n        tempPix = pix = pix2;\n        int channels = pix.d() / 8;\n        dtype = CV_8UC(channels);\n        Mat mat = new Mat(height, width, dtype, pix.data(), 4 * pix.wpl());\n        mat2 = new Mat(height, width, CV_8UC(channels));\n        // swap bytes if needed\n        int[] swap = { 0, channels - 1, 1, channels - 2, 2, channels - 3, 3, channels - 4 }, copy = { 0, 0, 1, 1, 2, 2, 3, 3 }, fromTo = channels > 1 && ByteOrder.nativeOrder().equals(ByteOrder.LITTLE_ENDIAN) ? swap : copy;\n        mixChannels(mat, 1, mat2, 1, fromTo, Math.min(channels, fromTo.length / 2));\n    } else if (pix.d() == 16) {\n        dtype = CV_16UC(pix.d() / 16);\n    } else if (pix.d() == 32) {\n        dtype = CV_32FC(pix.d() / 32);\n    }\n    mat2 = new Mat(height, width, dtype, pix.data());\n    if (tempPix != null) {\n        pixDestroy(tempPix);\n    }\n    return mat2;\n}",
        "rpfc": "static Mat convert(PIX pix) {\n    PIX tempPix = null;\n    int dtype = -1;\n    int height = pix.h();\n    int width = pix.w();\n    Mat mat2;\n    if (pix.colormap() != null) {\n        PIX pix2 = pixRemoveColormap(pix, REMOVE_CMAP_TO_FULL_COLOR);\n        tempPix = pix = pix2;\n        dtype = CV_8UC4;\n    } else if (pix.d() <= 8 || pix.d() == 24) {\n        PIX pix2 = pix;\n        switch(pix.d()) {\n            case 1:\n                pix2 = pixConvert1To8(null, pix, (byte) 0, (byte) 255);\n                break;\n            case 2:\n                pix2 = pixConvert2To8(pix, (byte) 0, (byte) 85, (byte) 170, (byte) 255, 0);\n                break;\n            case 4:\n                pix2 = pixConvert4To8(pix, 0);\n                break;\n            case 8:\n            case 24:\n                pix2 = pix;\n                break;\n            default:\n                throw new IllegalStateException(\"Unrecognized pixel depth of \" + pix.d());\n        }\n        tempPix = pix = pix2;\n        int channels = pix.d() / 8;\n        dtype = CV_8UC(channels);\n        Mat mat = new Mat(height, width, dtype, pix.data(), 4 * pix.wpl());\n        mat2 = new Mat(height, width, CV_8UC(channels));\n        // swap bytes if needed\n        int[] swap = { 0, channels - 1, 1, channels - 2, 2, channels - 3, 3, channels - 4 }, copy = { 0, 0, 1, 1, 2, 2, 3, 3 }, fromTo = channels > 1 && ByteOrder.nativeOrder().equals(ByteOrder.LITTLE_ENDIAN) ? swap : copy;\n        mixChannels(mat, 1, mat2, 1, fromTo, Math.min(channels, fromTo.length / 2));\n    } else if (pix.d() == 16) {\n        dtype = CV_16UC(pix.d() / 16);\n    } else if (pix.d() == 32) {\n        dtype = CV_32FC(pix.d() / 32);\n    }\n    mat2 = new Mat(height, width, dtype, pix.data());\n    if (tempPix != null) {\n        pixDestroy(tempPix);\n    }\n    return mat2;\n}",
        "tuc": "@Test\npublic void testTransformProcessPipe() throws Exception {\n    Schema inputSchema = TrainUtils.getIrisInputSchema();\n    Schema outputSchema = getIrisOutputSchema();\n    TransformProcess.Builder transformProcessBuilder = new TransformProcess.Builder(inputSchema);\n    for (int i = 0; i < inputSchema.numColumns(); i++) {\n        transformProcessBuilder.convertToDouble(inputSchema.getName(i));\n    }\n    TransformProcess tp = transformProcessBuilder.build();\n    TransformProcessStep tpps = new TransformProcessStep().step(\"foo\", tp, outputSchema).step(\"bar\", tp, outputSchema);\n    assert tpps.getInputNames().contains(\"foo\");\n    assert tpps.getInputNames().contains(\"bar\");\n    assert tpps.getOutputNames().contains(\"foo\");\n    assert tpps.getOutputNames().contains(\"bar\");\n    TransformProcessStep tpps2 = new TransformProcessStep(tp, outputSchema);\n    assert tpps2.getTransformProcesses().get(\"default\") == tp;\n    assert Arrays.equals(tpps2.getInputSchemas().get(\"default\"), SchemaTypeUtils.typesForSchema(inputSchema));\n}",
        "label": 1
    },
    {
        "repo_name": "KonduitAI___konduit-serving",
        "commit": "ba7fd4d81a3a736ed35dc210fd6f647f5a3e433c",
        "commit_message": "Merge branch 'master' into sa/switching-to-dl4j-snapshots",
        "p_path": "konduit-serving-core/src/main/java/ai/konduit/serving/util/image/NativeImageLoader.java",
        "t_path": "konduit-serving-test/src/test/java/ai/konduit/serving/pipeline/steps/TransformProcessStepTest.java",
        "p_name": "convert",
        "t_name": "testTransformProcessPipe",
        "lpfc": "static Mat convert(PIX pix) {\n    PIX tempPix = null;\n    int dtype = -1;\n    int height = pix.h();\n    int width = pix.w();\n    Mat mat2;\n    if (pix.colormap() != null) {\n        PIX pix2 = pixRemoveColormap(pix, REMOVE_CMAP_TO_FULL_COLOR);\n        tempPix = pix = pix2;\n        dtype = CV_8UC4;\n    } else if (pix.d() <= 8 || pix.d() == 24) {\n        PIX pix2 = null;\n        switch(pix.d()) {\n            case 1:\n                pix2 = pixConvert1To8(null, pix, (byte) 0, (byte) 255);\n                break;\n            case 2:\n                pix2 = pixConvert2To8(pix, (byte) 0, (byte) 85, (byte) 170, (byte) 255, 0);\n                break;\n            case 4:\n                pix2 = pixConvert4To8(pix, 0);\n                break;\n            case 8:\n                pix2 = pix;\n                break;\n            case 24:\n                pix2 = pix;\n                break;\n            default:\n                assert false;\n        }\n        tempPix = pix = pix2;\n        int channels = pix.d() / 8;\n        dtype = CV_8UC(channels);\n        Mat mat = new Mat(height, width, dtype, pix.data(), 4 * pix.wpl());\n        mat2 = new Mat(height, width, CV_8UC(channels));\n        // swap bytes if needed\n        int[] swap = { 0, channels - 1, 1, channels - 2, 2, channels - 3, 3, channels - 4 }, copy = { 0, 0, 1, 1, 2, 2, 3, 3 }, fromTo = channels > 1 && ByteOrder.nativeOrder().equals(ByteOrder.LITTLE_ENDIAN) ? swap : copy;\n        mixChannels(mat, 1, mat2, 1, fromTo, Math.min(channels, fromTo.length / 2));\n    } else if (pix.d() == 16) {\n        dtype = CV_16UC(pix.d() / 16);\n    } else if (pix.d() == 32) {\n        dtype = CV_32FC(pix.d() / 32);\n    }\n    mat2 = new Mat(height, width, dtype, pix.data());\n    if (tempPix != null) {\n        pixDestroy(tempPix);\n    }\n    return mat2;\n}",
        "rpfc": "static Mat convert(PIX pix) {\n    PIX tempPix = null;\n    int dtype = -1;\n    int height = pix.h();\n    int width = pix.w();\n    Mat mat2;\n    if (pix.colormap() != null) {\n        PIX pix2 = pixRemoveColormap(pix, REMOVE_CMAP_TO_FULL_COLOR);\n        tempPix = pix = pix2;\n        dtype = CV_8UC4;\n    } else if (pix.d() <= 8 || pix.d() == 24) {\n        PIX pix2 = pix;\n        switch(pix.d()) {\n            case 1:\n                pix2 = pixConvert1To8(null, pix, (byte) 0, (byte) 255);\n                break;\n            case 2:\n                pix2 = pixConvert2To8(pix, (byte) 0, (byte) 85, (byte) 170, (byte) 255, 0);\n                break;\n            case 4:\n                pix2 = pixConvert4To8(pix, 0);\n                break;\n            case 8:\n            case 24:\n                pix2 = pix;\n                break;\n            default:\n                throw new IllegalStateException(\"Unrecognized pixel depth of \" + pix.d());\n        }\n        tempPix = pix = pix2;\n        int channels = pix.d() / 8;\n        dtype = CV_8UC(channels);\n        Mat mat = new Mat(height, width, dtype, pix.data(), 4 * pix.wpl());\n        mat2 = new Mat(height, width, CV_8UC(channels));\n        // swap bytes if needed\n        int[] swap = { 0, channels - 1, 1, channels - 2, 2, channels - 3, 3, channels - 4 }, copy = { 0, 0, 1, 1, 2, 2, 3, 3 }, fromTo = channels > 1 && ByteOrder.nativeOrder().equals(ByteOrder.LITTLE_ENDIAN) ? swap : copy;\n        mixChannels(mat, 1, mat2, 1, fromTo, Math.min(channels, fromTo.length / 2));\n    } else if (pix.d() == 16) {\n        dtype = CV_16UC(pix.d() / 16);\n    } else if (pix.d() == 32) {\n        dtype = CV_32FC(pix.d() / 32);\n    }\n    mat2 = new Mat(height, width, dtype, pix.data());\n    if (tempPix != null) {\n        pixDestroy(tempPix);\n    }\n    return mat2;\n}",
        "tuc": "@Test\npublic void testTransformProcessPipe() throws Exception {\n    Schema inputSchema = TrainUtils.getIrisInputSchema();\n    Schema outputSchema = getIrisOutputSchema();\n    TransformProcess.Builder transformProcessBuilder = new TransformProcess.Builder(inputSchema);\n    for (int i = 0; i < inputSchema.numColumns(); i++) {\n        transformProcessBuilder.convertToDouble(inputSchema.getName(i));\n    }\n    TransformProcess tp = transformProcessBuilder.build();\n    TransformProcessStep tpps = new TransformProcessStep().step(\"foo\", tp, outputSchema).step(\"bar\", tp, outputSchema);\n    assert tpps.getInputNames().contains(\"foo\");\n    assert tpps.getInputNames().contains(\"bar\");\n    assert tpps.getOutputNames().contains(\"foo\");\n    assert tpps.getOutputNames().contains(\"bar\");\n    TransformProcessStep tpps2 = new TransformProcessStep(tp, outputSchema);\n    assert tpps2.getTransformProcesses().get(\"default\") == tp;\n    assert Arrays.equals(tpps2.getInputSchemas().get(\"default\"), SchemaTypeUtils.typesForSchema(inputSchema));\n}",
        "label": 1
    },
    {
        "repo_name": "KonduitAI___konduit-serving",
        "commit": "51cb57c3a199934a3d414b94f4684760a01d2d83",
        "commit_message": "Merge pull request #237 from KonduitAI/sa/removing-java-asserts\n\nReplacing java asserts with junit assert methods",
        "p_path": "konduit-serving-core/src/main/java/ai/konduit/serving/util/image/NativeImageLoader.java",
        "t_path": "konduit-serving-test/src/test/java/ai/konduit/serving/pipeline/steps/TransformProcessStepTest.java",
        "p_name": "convert",
        "t_name": "testTransformProcessPipe",
        "lpfc": "static Mat convert(PIX pix) {\n    PIX tempPix = null;\n    int dtype = -1;\n    int height = pix.h();\n    int width = pix.w();\n    Mat mat2;\n    if (pix.colormap() != null) {\n        PIX pix2 = pixRemoveColormap(pix, REMOVE_CMAP_TO_FULL_COLOR);\n        tempPix = pix = pix2;\n        dtype = CV_8UC4;\n    } else if (pix.d() <= 8 || pix.d() == 24) {\n        PIX pix2 = null;\n        switch(pix.d()) {\n            case 1:\n                pix2 = pixConvert1To8(null, pix, (byte) 0, (byte) 255);\n                break;\n            case 2:\n                pix2 = pixConvert2To8(pix, (byte) 0, (byte) 85, (byte) 170, (byte) 255, 0);\n                break;\n            case 4:\n                pix2 = pixConvert4To8(pix, 0);\n                break;\n            case 8:\n                pix2 = pix;\n                break;\n            case 24:\n                pix2 = pix;\n                break;\n            default:\n                assert false;\n        }\n        tempPix = pix = pix2;\n        int channels = pix.d() / 8;\n        dtype = CV_8UC(channels);\n        Mat mat = new Mat(height, width, dtype, pix.data(), 4 * pix.wpl());\n        mat2 = new Mat(height, width, CV_8UC(channels));\n        // swap bytes if needed\n        int[] swap = { 0, channels - 1, 1, channels - 2, 2, channels - 3, 3, channels - 4 }, copy = { 0, 0, 1, 1, 2, 2, 3, 3 }, fromTo = channels > 1 && ByteOrder.nativeOrder().equals(ByteOrder.LITTLE_ENDIAN) ? swap : copy;\n        mixChannels(mat, 1, mat2, 1, fromTo, Math.min(channels, fromTo.length / 2));\n    } else if (pix.d() == 16) {\n        dtype = CV_16UC(pix.d() / 16);\n    } else if (pix.d() == 32) {\n        dtype = CV_32FC(pix.d() / 32);\n    }\n    mat2 = new Mat(height, width, dtype, pix.data());\n    if (tempPix != null) {\n        pixDestroy(tempPix);\n    }\n    return mat2;\n}",
        "rpfc": "static Mat convert(PIX pix) {\n    PIX tempPix = null;\n    int dtype = -1;\n    int height = pix.h();\n    int width = pix.w();\n    Mat mat2;\n    if (pix.colormap() != null) {\n        PIX pix2 = pixRemoveColormap(pix, REMOVE_CMAP_TO_FULL_COLOR);\n        tempPix = pix = pix2;\n        dtype = CV_8UC4;\n    } else if (pix.d() <= 8 || pix.d() == 24) {\n        PIX pix2 = pix;\n        switch(pix.d()) {\n            case 1:\n                pix2 = pixConvert1To8(null, pix, (byte) 0, (byte) 255);\n                break;\n            case 2:\n                pix2 = pixConvert2To8(pix, (byte) 0, (byte) 85, (byte) 170, (byte) 255, 0);\n                break;\n            case 4:\n                pix2 = pixConvert4To8(pix, 0);\n                break;\n            case 8:\n            case 24:\n                pix2 = pix;\n                break;\n            default:\n                throw new IllegalStateException(\"Unrecognized pixel depth of \" + pix.d());\n        }\n        tempPix = pix = pix2;\n        int channels = pix.d() / 8;\n        dtype = CV_8UC(channels);\n        Mat mat = new Mat(height, width, dtype, pix.data(), 4 * pix.wpl());\n        mat2 = new Mat(height, width, CV_8UC(channels));\n        // swap bytes if needed\n        int[] swap = { 0, channels - 1, 1, channels - 2, 2, channels - 3, 3, channels - 4 }, copy = { 0, 0, 1, 1, 2, 2, 3, 3 }, fromTo = channels > 1 && ByteOrder.nativeOrder().equals(ByteOrder.LITTLE_ENDIAN) ? swap : copy;\n        mixChannels(mat, 1, mat2, 1, fromTo, Math.min(channels, fromTo.length / 2));\n    } else if (pix.d() == 16) {\n        dtype = CV_16UC(pix.d() / 16);\n    } else if (pix.d() == 32) {\n        dtype = CV_32FC(pix.d() / 32);\n    }\n    mat2 = new Mat(height, width, dtype, pix.data());\n    if (tempPix != null) {\n        pixDestroy(tempPix);\n    }\n    return mat2;\n}",
        "tuc": "@Test\npublic void testTransformProcessPipe() throws Exception {\n    Schema inputSchema = TrainUtils.getIrisInputSchema();\n    Schema outputSchema = getIrisOutputSchema();\n    TransformProcess.Builder transformProcessBuilder = new TransformProcess.Builder(inputSchema);\n    for (int i = 0; i < inputSchema.numColumns(); i++) {\n        transformProcessBuilder.convertToDouble(inputSchema.getName(i));\n    }\n    TransformProcess tp = transformProcessBuilder.build();\n    TransformProcessStep tpps = new TransformProcessStep().step(\"foo\", tp, outputSchema).step(\"bar\", tp, outputSchema);\n    assert tpps.getInputNames().contains(\"foo\");\n    assert tpps.getInputNames().contains(\"bar\");\n    assert tpps.getOutputNames().contains(\"foo\");\n    assert tpps.getOutputNames().contains(\"bar\");\n    TransformProcessStep tpps2 = new TransformProcessStep(tp, outputSchema);\n    assert tpps2.getTransformProcesses().get(\"default\") == tp;\n    assert Arrays.equals(tpps2.getInputSchemas().get(\"default\"), SchemaTypeUtils.typesForSchema(inputSchema));\n}",
        "label": 1
    },
    {
        "repo_name": "KonduitAI___konduit-serving",
        "commit": "80c16ba847946359e9b7b3aeabb9c8fc2b841b85",
        "commit_message": "assertSame -> assertEquals. and also, preconditions to throw statement\n",
        "p_path": "konduit-serving-core/src/main/java/ai/konduit/serving/util/image/NativeImageLoader.java",
        "t_path": "konduit-serving-test/src/test/java/ai/konduit/serving/pipeline/steps/TransformProcessStepTest.java",
        "p_name": "convert",
        "t_name": "testTransformProcessPipe",
        "lpfc": "static Mat convert(PIX pix) {\n    PIX tempPix = null;\n    int dtype = -1;\n    int height = pix.h();\n    int width = pix.w();\n    Mat mat2;\n    if (pix.colormap() != null) {\n        PIX pix2 = pixRemoveColormap(pix, REMOVE_CMAP_TO_FULL_COLOR);\n        tempPix = pix = pix2;\n        dtype = CV_8UC4;\n    } else if (pix.d() <= 8 || pix.d() == 24) {\n        PIX pix2 = pix;\n        switch(pix.d()) {\n            case 1:\n                pix2 = pixConvert1To8(null, pix, (byte) 0, (byte) 255);\n                break;\n            case 2:\n                pix2 = pixConvert2To8(pix, (byte) 0, (byte) 85, (byte) 170, (byte) 255, 0);\n                break;\n            case 4:\n                pix2 = pixConvert4To8(pix, 0);\n                break;\n            case 8:\n            case 24:\n                pix2 = pix;\n                break;\n            default:\n                Preconditions.checkState(false, String.format(\"Unrecognized pixel depth of %d\", pix.d()));\n        }\n        tempPix = pix = pix2;\n        int channels = pix.d() / 8;\n        dtype = CV_8UC(channels);\n        Mat mat = new Mat(height, width, dtype, pix.data(), 4 * pix.wpl());\n        mat2 = new Mat(height, width, CV_8UC(channels));\n        // swap bytes if needed\n        int[] swap = { 0, channels - 1, 1, channels - 2, 2, channels - 3, 3, channels - 4 }, copy = { 0, 0, 1, 1, 2, 2, 3, 3 }, fromTo = channels > 1 && ByteOrder.nativeOrder().equals(ByteOrder.LITTLE_ENDIAN) ? swap : copy;\n        mixChannels(mat, 1, mat2, 1, fromTo, Math.min(channels, fromTo.length / 2));\n    } else if (pix.d() == 16) {\n        dtype = CV_16UC(pix.d() / 16);\n    } else if (pix.d() == 32) {\n        dtype = CV_32FC(pix.d() / 32);\n    }\n    mat2 = new Mat(height, width, dtype, pix.data());\n    if (tempPix != null) {\n        pixDestroy(tempPix);\n    }\n    return mat2;\n}",
        "rpfc": "static Mat convert(PIX pix) {\n    PIX tempPix = null;\n    int dtype = -1;\n    int height = pix.h();\n    int width = pix.w();\n    Mat mat2;\n    if (pix.colormap() != null) {\n        PIX pix2 = pixRemoveColormap(pix, REMOVE_CMAP_TO_FULL_COLOR);\n        tempPix = pix = pix2;\n        dtype = CV_8UC4;\n    } else if (pix.d() <= 8 || pix.d() == 24) {\n        PIX pix2 = pix;\n        switch(pix.d()) {\n            case 1:\n                pix2 = pixConvert1To8(null, pix, (byte) 0, (byte) 255);\n                break;\n            case 2:\n                pix2 = pixConvert2To8(pix, (byte) 0, (byte) 85, (byte) 170, (byte) 255, 0);\n                break;\n            case 4:\n                pix2 = pixConvert4To8(pix, 0);\n                break;\n            case 8:\n            case 24:\n                pix2 = pix;\n                break;\n            default:\n                throw new IllegalStateException(\"Unrecognized pixel depth of \" + pix.d());\n        }\n        tempPix = pix = pix2;\n        int channels = pix.d() / 8;\n        dtype = CV_8UC(channels);\n        Mat mat = new Mat(height, width, dtype, pix.data(), 4 * pix.wpl());\n        mat2 = new Mat(height, width, CV_8UC(channels));\n        // swap bytes if needed\n        int[] swap = { 0, channels - 1, 1, channels - 2, 2, channels - 3, 3, channels - 4 }, copy = { 0, 0, 1, 1, 2, 2, 3, 3 }, fromTo = channels > 1 && ByteOrder.nativeOrder().equals(ByteOrder.LITTLE_ENDIAN) ? swap : copy;\n        mixChannels(mat, 1, mat2, 1, fromTo, Math.min(channels, fromTo.length / 2));\n    } else if (pix.d() == 16) {\n        dtype = CV_16UC(pix.d() / 16);\n    } else if (pix.d() == 32) {\n        dtype = CV_32FC(pix.d() / 32);\n    }\n    mat2 = new Mat(height, width, dtype, pix.data());\n    if (tempPix != null) {\n        pixDestroy(tempPix);\n    }\n    return mat2;\n}",
        "tuc": "@Test\npublic void testTransformProcessPipe() throws Exception {\n    Schema inputSchema = TrainUtils.getIrisInputSchema();\n    Schema outputSchema = getIrisOutputSchema();\n    TransformProcess.Builder transformProcessBuilder = new TransformProcess.Builder(inputSchema);\n    for (int i = 0; i < inputSchema.numColumns(); i++) {\n        transformProcessBuilder.convertToDouble(inputSchema.getName(i));\n    }\n    TransformProcess tp = transformProcessBuilder.build();\n    TransformProcessStep tpps = new TransformProcessStep().step(\"foo\", tp, outputSchema).step(\"bar\", tp, outputSchema);\n    assertTrue(tpps.getInputNames().contains(\"foo\"));\n    assertTrue(tpps.getInputNames().contains(\"bar\"));\n    assertTrue(tpps.getOutputNames().contains(\"foo\"));\n    assertTrue(tpps.getOutputNames().contains(\"bar\"));\n    TransformProcessStep tpps2 = new TransformProcessStep(tp, outputSchema);\n    assertSame(tpps2.getTransformProcesses().get(\"default\"), tp);\n    assertArrayEquals(tpps2.getInputSchemas().get(\"default\"), SchemaTypeUtils.typesForSchema(inputSchema));\n}",
        "label": 1
    },
    {
        "repo_name": "KonduitAI___konduit-serving",
        "commit": "105b47c05310014e93bd90a2b62b520695d2110a",
        "commit_message": "Removing java asserts\n",
        "p_path": "konduit-serving-core/src/main/java/ai/konduit/serving/util/image/NativeImageLoader.java",
        "t_path": "konduit-serving-test/src/test/java/ai/konduit/serving/pipeline/steps/TransformProcessStepTest.java",
        "p_name": "convert",
        "t_name": "testTransformProcessPipe",
        "lpfc": "static Mat convert(PIX pix) {\n    PIX tempPix = null;\n    int dtype = -1;\n    int height = pix.h();\n    int width = pix.w();\n    Mat mat2;\n    if (pix.colormap() != null) {\n        PIX pix2 = pixRemoveColormap(pix, REMOVE_CMAP_TO_FULL_COLOR);\n        tempPix = pix = pix2;\n        dtype = CV_8UC4;\n    } else if (pix.d() <= 8 || pix.d() == 24) {\n        PIX pix2 = null;\n        switch(pix.d()) {\n            case 1:\n                pix2 = pixConvert1To8(null, pix, (byte) 0, (byte) 255);\n                break;\n            case 2:\n                pix2 = pixConvert2To8(pix, (byte) 0, (byte) 85, (byte) 170, (byte) 255, 0);\n                break;\n            case 4:\n                pix2 = pixConvert4To8(pix, 0);\n                break;\n            case 8:\n                pix2 = pix;\n                break;\n            case 24:\n                pix2 = pix;\n                break;\n            default:\n                assert false;\n        }\n        tempPix = pix = pix2;\n        int channels = pix.d() / 8;\n        dtype = CV_8UC(channels);\n        Mat mat = new Mat(height, width, dtype, pix.data(), 4 * pix.wpl());\n        mat2 = new Mat(height, width, CV_8UC(channels));\n        // swap bytes if needed\n        int[] swap = { 0, channels - 1, 1, channels - 2, 2, channels - 3, 3, channels - 4 }, copy = { 0, 0, 1, 1, 2, 2, 3, 3 }, fromTo = channels > 1 && ByteOrder.nativeOrder().equals(ByteOrder.LITTLE_ENDIAN) ? swap : copy;\n        mixChannels(mat, 1, mat2, 1, fromTo, Math.min(channels, fromTo.length / 2));\n    } else if (pix.d() == 16) {\n        dtype = CV_16UC(pix.d() / 16);\n    } else if (pix.d() == 32) {\n        dtype = CV_32FC(pix.d() / 32);\n    }\n    mat2 = new Mat(height, width, dtype, pix.data());\n    if (tempPix != null) {\n        pixDestroy(tempPix);\n    }\n    return mat2;\n}",
        "rpfc": "static Mat convert(PIX pix) {\n    PIX tempPix = null;\n    int dtype = -1;\n    int height = pix.h();\n    int width = pix.w();\n    Mat mat2;\n    if (pix.colormap() != null) {\n        PIX pix2 = pixRemoveColormap(pix, REMOVE_CMAP_TO_FULL_COLOR);\n        tempPix = pix = pix2;\n        dtype = CV_8UC4;\n    } else if (pix.d() <= 8 || pix.d() == 24) {\n        PIX pix2 = null;\n        switch(pix.d()) {\n            case 1:\n                pix2 = pixConvert1To8(null, pix, (byte) 0, (byte) 255);\n                break;\n            case 2:\n                pix2 = pixConvert2To8(pix, (byte) 0, (byte) 85, (byte) 170, (byte) 255, 0);\n                break;\n            case 4:\n                pix2 = pixConvert4To8(pix, 0);\n                break;\n            case 8:\n            case 24:\n                pix2 = pix;\n                break;\n            default:\n                Preconditions.checkState(false, String.format(\"Unrecognized pixel depth of %d\", pix.d()));\n        }\n        tempPix = pix = pix2;\n        int channels = pix.d() / 8;\n        dtype = CV_8UC(channels);\n        Mat mat = new Mat(height, width, dtype, pix.data(), 4 * pix.wpl());\n        mat2 = new Mat(height, width, CV_8UC(channels));\n        // swap bytes if needed\n        int[] swap = { 0, channels - 1, 1, channels - 2, 2, channels - 3, 3, channels - 4 }, copy = { 0, 0, 1, 1, 2, 2, 3, 3 }, fromTo = channels > 1 && ByteOrder.nativeOrder().equals(ByteOrder.LITTLE_ENDIAN) ? swap : copy;\n        mixChannels(mat, 1, mat2, 1, fromTo, Math.min(channels, fromTo.length / 2));\n    } else if (pix.d() == 16) {\n        dtype = CV_16UC(pix.d() / 16);\n    } else if (pix.d() == 32) {\n        dtype = CV_32FC(pix.d() / 32);\n    }\n    mat2 = new Mat(height, width, dtype, pix.data());\n    if (tempPix != null) {\n        pixDestroy(tempPix);\n    }\n    return mat2;\n}",
        "tuc": "@Test\npublic void testTransformProcessPipe() throws Exception {\n    Schema inputSchema = TrainUtils.getIrisInputSchema();\n    Schema outputSchema = getIrisOutputSchema();\n    TransformProcess.Builder transformProcessBuilder = new TransformProcess.Builder(inputSchema);\n    for (int i = 0; i < inputSchema.numColumns(); i++) {\n        transformProcessBuilder.convertToDouble(inputSchema.getName(i));\n    }\n    TransformProcess tp = transformProcessBuilder.build();\n    TransformProcessStep tpps = new TransformProcessStep().step(\"foo\", tp, outputSchema).step(\"bar\", tp, outputSchema);\n    assert tpps.getInputNames().contains(\"foo\");\n    assert tpps.getInputNames().contains(\"bar\");\n    assert tpps.getOutputNames().contains(\"foo\");\n    assert tpps.getOutputNames().contains(\"bar\");\n    TransformProcessStep tpps2 = new TransformProcessStep(tp, outputSchema);\n    assert tpps2.getTransformProcesses().get(\"default\") == tp;\n    assert Arrays.equals(tpps2.getInputSchemas().get(\"default\"), SchemaTypeUtils.typesForSchema(inputSchema));\n}",
        "label": 1
    },
    {
        "repo_name": "IanWraith___Rivet",
        "commit": "18f3dd6af430ed97eaf5624d5324b8ced40e7ffb",
        "commit_message": "Further work on the GW MMSI encoding.",
        "p_path": "src/org/e2k/GW.java",
        "t_path": "src/test/org/e2k/testGW.java",
        "p_name": "convertMMSI",
        "t_name": "testMMSI",
        "lpfc": "private String convertMMSI(int n) {\n    if (n == 0x0)\n        return \"3\";\n    else if (n == 0x1)\n        return \"7\";\n    else if (n == 0x2)\n        return \"1\";\n    else if (n == 0x3)\n        return \"5\";\n    else if (n == 0x4)\n        return \"2\";\n    else if (n == 0x5)\n        return \"6\";\n    else if (n == 0x6)\n        return \"0\";\n    else if (n == 0x7)\n        return \"4\";\n    else if (n == 0x8)\n        return \"3\";\n    else if (n == 0x9)\n        return \"7\";\n    else if (n == 0xa)\n        return \"A\";\n    else if (n == 0xb)\n        return \"B\";\n    else if (n == 0xc)\n        return \"C\";\n    else if (n == 0xd)\n        return \"D\";\n    else if (n == 0xe)\n        return \"0\";\n    else if (n == 0xf)\n        return \"F\";\n    else\n        return (\"[0x\" + Integer.toHexString(n) + \"]\");\n}",
        "rpfc": "private String convertMMSI(int n) {\n    if (n == 0x0)\n        return \"3\";\n    else if (n == 0x1)\n        return \"7\";\n    else if (n == 0x2)\n        return \"1\";\n    else if (n == 0x3)\n        return \"5\";\n    else if (n == 0x4)\n        return \"2\";\n    else if (n == 0x5)\n        return \"6\";\n    else if (n == 0x6)\n        return \"0\";\n    else if (n == 0x7)\n        return \"4\";\n    else if (n == 0x8)\n        return \"3\";\n    else if (n == 0x9)\n        return \"7\";\n    else if (n == 0xa)\n        return \"1\";\n    else if (n == 0xb)\n        return \"5\";\n    else if (n == 0xc)\n        return \"2\";\n    else if (n == 0xd)\n        return \"6\";\n    else if (n == 0xe)\n        return \"0\";\n    else if (n == 0xf)\n        return \"4\";\n    else\n        return (\"[0x\" + Integer.toHexString(n) + \"]\");\n}",
        "tuc": "public void testMMSI() {\n    int a;\n    GW gw = new GW(null);\n    List<Integer> testData = new ArrayList<Integer>();\n    final int[] loadData1 = { 0x84, 0x63, 0x66, 0x85, 0x60, 0x66 };\n    testData.clear();\n    for (a = 0; a < 6; a++) {\n        testData.add(loadData1[a]);\n    }\n    String rs1 = gw.displayGW_MMSI(testData);\n    final int[] loadData2 = { 0x84, 0x05, 0x02, 0x6E, 0x66, 0x66 };\n    testData.clear();\n    for (a = 0; a < 6; a++) {\n        testData.add(loadData2[a]);\n    }\n    String rs2 = gw.displayGW_MMSI(testData);\n    final int[] loadData3 = { 0x85, 0x65, 0x22, 0xF2, 0xE1, 0x66 };\n    testData.clear();\n    for (a = 0; a < 6; a++) {\n        testData.add(loadData3[a]);\n    }\n    String rs3 = gw.displayGW_MMSI(testData);\n    final int[] loadData4 = { 0xD3, 0x73, 0x72, 0xE6, 0x66, 0x66 };\n    testData.clear();\n    for (a = 0; a < 6; a++) {\n        testData.add(loadData4[a]);\n    }\n    String rs4 = gw.displayGW_MMSI(testData);\n    final int[] loadData5 = { 0xE0, 0x57, 0x03, 0x66, 0x66, 0x66 };\n    testData.clear();\n    for (a = 0; a < 6; a++) {\n        testData.add(loadData5[a]);\n    }\n    String rs5 = gw.displayGW_MMSI(testData);\n    final int[] loadData6 = { 0xb4, 0x2e, 0x83, 0x66, 0x66, 0x66 };\n    testData.clear();\n    for (a = 0; a < 6; a++) {\n        testData.add(loadData6[a]);\n    }\n    String rs6 = gw.displayGW_MMSI(testData);\n    final int[] loadData7 = { 0x84, 0x63, 0x2D, 0x14, 0x62, 0x66 };\n    testData.clear();\n    for (a = 0; a < 6; a++) {\n        testData.add(loadData7[a]);\n    }\n    String rs7 = gw.displayGW_MMSI(testData);\n    final int[] loadData8 = { 0x97, 0x69, 0x74, 0x66, 0x66, 0x66 };\n    testData.clear();\n    for (a = 0; a < 6; a++) {\n        testData.add(loadData8[a]);\n    }\n    String rs8 = gw.displayGW_MMSI(testData);\n    String ent = \"rs1=\" + rs1 + \"\\nrs2=\" + rs2 + \"\\nrs3=\" + rs3 + \"\\nrs4=\" + rs4 + \"\\nrs5=\" + rs5 + \"\\nrs6=\" + rs6 + \"\\nrs7=\" + rs7 + \"\\nrs8=\" + rs8;\n    fail(\"Not yet implemented\");\n}",
        "label": 1
    },
    {
        "repo_name": "project-ncl___gradle-manipulator",
        "commit": "056f1c55c021cf5d09a3f8d363ae4bbd9bc6ad4a",
        "commit_message": "Attempt to modify animalsniffer plugin removal\n",
        "p_path": "common/src/main/java/org/jboss/gm/common/utils/PluginUtils.java",
        "t_path": "common/src/test/java/org/jboss/gm/common/utils/PluginUtilsTest.java",
        "p_name": "pluginRemoval",
        "t_name": "testRemoval18",
        "lpfc": "/**\n * Removes plugins from a target build file.\n * <p>\n * Will automatically examine build.gradle, settings.gradle, build.gradle.kts, settings.gradle.kts\n *\n * @param logger the current logger in use\n * @param target the build target directory\n * @param plugins the plugins to remove\n * @throws ManipulationException if an error occurs\n */\npublic static void pluginRemoval(Logger logger, File target, Set<String> plugins) throws ManipulationException {\n    if (plugins == null || plugins.isEmpty()) {\n        return;\n    } else if (plugins.contains(\"ALL\")) {\n        // Shortcut to represent removing any/all of the supported plugins\n        plugins.clear();\n        plugins.addAll(PLUGINS.keySet());\n    } else if (plugins.stream().anyMatch(s -> s.matches(\"REC.*\"))) {\n        // Shortcut to represent removing the default selection of the supported plugins\n        // This is equivalent to 'ALL' barring removal of the signing plugin\n        plugins.clear();\n        plugins.addAll(PLUGINS.keySet());\n        plugins.remove(\"signing\");\n    }\n    // As the configuration block is named the same as the plugin we differentiate via the\n    // potential quoting mechanism to avoid problems when parsing the file to remove the plugins.\n    // This will likely leave in Kotlin build files items like plugins { signing } but that\n    // is harmless and has been seen to be required for e.g. opentelemetry-java-instrumentation\n    if (plugins.remove(\"signing\")) {\n        plugins.add(\"\\\"signing\\\"\");\n        plugins.add(\"'signing'\");\n    }\n    for (String plugin : plugins) {\n        PluginReference pluginReference = PLUGINS.get(plugin);\n        Set<String> tasks;\n        Set<String> configBlocks;\n        String pluginType;\n        String configureExtension;\n        Set<String> pluginImports;\n        if (plugin.matches(\".signing.\")) {\n            configBlocks = Collections.singleton(\"signing\");\n            tasks = Collections.emptySet();\n            pluginType = \"SigningPlugin\";\n            pluginImports = Collections.emptySet();\n            configureExtension = \"SigningExtension\";\n        } else if (pluginReference == null) {\n            throw new ManipulationException(\"No support for removing plugin {}\", plugin);\n        } else {\n            configBlocks = pluginReference.configBlocks;\n            tasks = pluginReference.tasks;\n            pluginType = pluginReference.type;\n            pluginImports = pluginReference.imports;\n            configureExtension = pluginReference.configureExtension;\n        }\n        final Collection<File> files = FileUtils.listFiles(target, new WildcardFileFilter(\"*.gradle\", \"*.gradle.kts\"), TrueFileFilter.INSTANCE);\n        for (File buildFile : files) {\n            try {\n                List<String> lines = org.apache.commons.io.FileUtils.readLines(buildFile, Charset.defaultCharset());\n                String eol = getEOL(logger, buildFile);\n                for (int i = 0; i < lines.size(); i++) {\n                    // Special case handling - https://github.com/marcphilipp/nexus-publish-plugin implicitly\n                    // applies the maven-publish plugin so apply it manually to avoid breakages\n                    if (plugin.equals(\"de.marcphilipp.nexus-publish\") && lines.get(i).matches(\"\\\\s*apply.*\" + plugin + \".*\")) {\n                        logger.debug(\"Replacing nexus-publish apply plugin with maven-publish\");\n                        if (buildFile.getName().endsWith(\".gradle\")) {\n                            lines.set(i, \"apply plugin: \\\"maven-publish\\\"\");\n                        } else {\n                            lines.set(i, \"apply(plugin = \\\"maven-publish\\\")\");\n                        }\n                        break;\n                    }\n                }\n                // Remove the plugin. Plugins can be applied as below with quote variation of \", ', `\n                // id(\"...\")\n                // id \"...\"\n                // apply plugin: \"...\"\n                boolean removed = lines.removeIf(i -> i.matches(\".*([`\\\"'])\" + plugin + \"([`\\\"']).*\") && !i.contains(\"plugins.withId\"));\n                removed |= lines.removeIf(i -> i.matches(\".*\\\\s+\" + plugin + \"(\\\\s|$)+.*\") && !i.contains(\"{\"));\n                // Remove any task references.\n                for (String t : tasks) {\n                    removed |= lines.removeIf(i -> i.contains(t) && !i.contains(\"{\"));\n                }\n                // Remove any imports.\n                for (String pluginImport : pluginImports) {\n                    removed |= lines.removeIf(i -> i.contains(\"import \" + pluginImport));\n                }\n                String content = String.join(eol, lines);\n                StringBuilder contentBuilder;\n                // Remove any configuration block\n                for (String configBlock : configBlocks) {\n                    contentBuilder = new StringBuilder(content);\n                    removed |= removeBlock(logger, buildFile, eol, contentBuilder, \"(?m)(^|project\\\\.|\\\\s)+\" + configBlock + \"(\\\\s|$)+\");\n                    content = contentBuilder.toString();\n                }\n                // Remove withType blocks\n                if (isNotEmpty(pluginType)) {\n                    contentBuilder = new StringBuilder(content);\n                    removed |= removeBlock(logger, buildFile, eol, contentBuilder, \"plugins.withType<\" + pluginType + \">\");\n                    removed |= removeBlock(logger, buildFile, eol, contentBuilder, \"tasks.withType<\" + pluginType + \">\");\n                    content = contentBuilder.toString();\n                }\n                // Remove withId blocks e.g.\n                // plugins.withId(\"de.marcphilipp.nexus-publish\") { ... }\n                contentBuilder = new StringBuilder(content);\n                removed |= removeBlock(logger, buildFile, eol, contentBuilder, \"plugins.withId\\\\(\\\"\" + plugin + \"\\\"\\\\)\");\n                content = contentBuilder.toString();\n                // Remove configure extension blocks e.g.\n                // configure<NexusPublishExtension> {\n                if (isNotEmpty(configureExtension)) {\n                    contentBuilder = new StringBuilder(content);\n                    removed |= removeBlock(logger, buildFile, eol, contentBuilder, \"configure<\" + configureExtension + \">\");\n                    content = contentBuilder.toString();\n                }\n                // Remove any multi-line task references e.g.\n                // rootProject.tasks.named(\"closeAndReleaseRepository\") {\n                for (String t : tasks) {\n                    contentBuilder = new StringBuilder(content);\n                    removed |= removeBlock(logger, buildFile, eol, contentBuilder, \"(?m)^.*\\\\(\\\"\" + t + \"\\\"\\\\)(\\\\.configure|)\");\n                    // Sometimes tasks can be single quoted e.g.\n                    // tasks.named('closeAndReleaseRepository') {\n                    removed |= removeBlock(logger, buildFile, eol, contentBuilder, \"(?m)^.*\\\\('\" + t + \"'\\\\)(\\\\.configure|)\");\n                    removed |= removeBlock(logger, buildFile, eol, contentBuilder, \"(?m)^.*named<\" + t + \">.*?\\\\s\");\n                    content = contentBuilder.toString();\n                }\n                if (removed) {\n                    logger.info(\"Removed instances of plugin {} with configuration block of {} from {}\", plugin, String.join(\",\", configBlocks), buildFile);\n                    FileUtils.writeStringToFile(buildFile, content, Charset.defaultCharset());\n                }\n            } catch (IOException e) {\n                throw new ManipulationException(\"Unable to read build file {}\", buildFile, e);\n            }\n        }\n    }\n}",
        "rpfc": "/**\n * Removes plugins from a target build file.\n * <p>\n * Will automatically examine build.gradle, settings.gradle, build.gradle.kts, settings.gradle.kts\n *\n * @param logger the current logger in use\n * @param target the build target directory\n * @param plugins the plugins to remove\n * @throws ManipulationException if an error occurs\n */\npublic static void pluginRemoval(Logger logger, File target, Set<String> plugins) throws ManipulationException {\n    if (plugins == null || plugins.isEmpty()) {\n        return;\n    } else if (plugins.contains(\"ALL\")) {\n        // Shortcut to represent removing any/all of the supported plugins\n        plugins.clear();\n        plugins.addAll(PLUGINS.keySet());\n    } else if (plugins.stream().anyMatch(s -> s.matches(\"REC.*\"))) {\n        // Shortcut to represent removing the default selection of the supported plugins\n        // This is equivalent to 'ALL' barring removal of the signing plugin\n        plugins.clear();\n        plugins.addAll(PLUGINS.keySet());\n        plugins.remove(\"signing\");\n    }\n    // As the configuration block is named the same as the plugin we differentiate via the\n    // potential quoting mechanism to avoid problems when parsing the file to remove the plugins.\n    // This will likely leave in Kotlin build files items like plugins { signing } but that\n    // is harmless and has been seen to be required for e.g. opentelemetry-java-instrumentation\n    if (plugins.remove(\"signing\")) {\n        plugins.add(\"\\\"signing\\\"\");\n        plugins.add(\"'signing'\");\n    }\n    for (String plugin : plugins) {\n        PluginReference pluginReference = PLUGINS.get(plugin);\n        Set<String> tasks;\n        Set<String> configBlocks;\n        String pluginType;\n        String configureExtension;\n        Set<String> pluginImports;\n        if (plugin.matches(\".signing.\")) {\n            configBlocks = Collections.singleton(\"signing\");\n            tasks = Collections.emptySet();\n            pluginType = \"SigningPlugin\";\n            pluginImports = Collections.emptySet();\n            configureExtension = \"SigningExtension\";\n        } else if (pluginReference == null) {\n            throw new ManipulationException(\"No support for removing plugin {}\", plugin);\n        } else {\n            configBlocks = pluginReference.configBlocks;\n            tasks = pluginReference.tasks;\n            pluginType = pluginReference.type;\n            pluginImports = pluginReference.imports;\n            configureExtension = pluginReference.configureExtension;\n        }\n        final Collection<File> files = FileUtils.listFiles(target, new WildcardFileFilter(\"*.gradle\", \"*.gradle.kts\"), TrueFileFilter.INSTANCE);\n        for (File buildFile : files) {\n            try {\n                List<String> lines = org.apache.commons.io.FileUtils.readLines(buildFile, Charset.defaultCharset());\n                String eol = getEOL(logger, buildFile);\n                for (int i = 0; i < lines.size(); i++) {\n                    // Special case handling - https://github.com/marcphilipp/nexus-publish-plugin implicitly\n                    // applies the maven-publish plugin so apply it manually to avoid breakages\n                    if (plugin.equals(\"de.marcphilipp.nexus-publish\") && lines.get(i).matches(\"\\\\s*apply.*\" + plugin + \".*\")) {\n                        logger.debug(\"Replacing nexus-publish apply plugin with maven-publish\");\n                        if (buildFile.getName().endsWith(\".gradle\")) {\n                            lines.set(i, \"apply plugin: \\\"maven-publish\\\"\");\n                        } else {\n                            lines.set(i, \"apply(plugin = \\\"maven-publish\\\")\");\n                        }\n                        break;\n                    }\n                }\n                // Remove the plugin. Plugins can be applied as below with quote variation of \", ', `\n                // id(\"...\")\n                // id \"...\"\n                // apply plugin: \"...\"\n                boolean removed = lines.removeIf(i -> i.matches(\".*([`\\\"'])\" + plugin + \"([`\\\"']).*\") && !i.contains(\"plugins.withId\"));\n                removed |= lines.removeIf(i -> i.matches(\".*\\\\s+\" + plugin + \"(\\\\s|$)+.*\") && !i.contains(\"{\"));\n                // Remove any task references.\n                for (String t : tasks) {\n                    // Look for regexp type\n                    if (t.contains(\"*\") || t.contains(\"+\")) {\n                        removed |= lines.removeIf(i -> i.matches(t) && !i.contains(\"{\"));\n                    } else {\n                        removed |= lines.removeIf(i -> i.contains(t) && !i.contains(\"{\"));\n                    }\n                }\n                // Remove any imports.\n                for (String pluginImport : pluginImports) {\n                    removed |= lines.removeIf(i -> i.contains(\"import \" + pluginImport));\n                }\n                String content = String.join(eol, lines);\n                StringBuilder contentBuilder;\n                // Remove any configuration block\n                for (String configBlock : configBlocks) {\n                    contentBuilder = new StringBuilder(content);\n                    removed |= removeBlock(logger, buildFile, eol, contentBuilder, \"(?m)(^|project\\\\.|\\\\s)+\" + configBlock + \"(\\\\s|$)+\");\n                    content = contentBuilder.toString();\n                }\n                // Remove withType blocks\n                if (isNotEmpty(pluginType)) {\n                    contentBuilder = new StringBuilder(content);\n                    removed |= removeBlock(logger, buildFile, eol, contentBuilder, \"plugins.withType<\" + pluginType + \">\");\n                    removed |= removeBlock(logger, buildFile, eol, contentBuilder, \"tasks.withType<\" + pluginType + \">\");\n                    content = contentBuilder.toString();\n                }\n                // Remove withId blocks e.g.\n                // plugins.withId(\"de.marcphilipp.nexus-publish\") { ... }\n                contentBuilder = new StringBuilder(content);\n                removed |= removeBlock(logger, buildFile, eol, contentBuilder, \"plugins.withId\\\\(\\\"\" + plugin + \"\\\"\\\\)\");\n                content = contentBuilder.toString();\n                // Remove configure extension blocks e.g.\n                // configure<NexusPublishExtension> {\n                if (isNotEmpty(configureExtension)) {\n                    contentBuilder = new StringBuilder(content);\n                    removed |= removeBlock(logger, buildFile, eol, contentBuilder, \"configure<\" + configureExtension + \">\");\n                    content = contentBuilder.toString();\n                }\n                // Remove any multi-line task references e.g.\n                // rootProject.tasks.named(\"closeAndReleaseRepository\") {\n                for (String t : tasks) {\n                    contentBuilder = new StringBuilder(content);\n                    removed |= removeBlock(logger, buildFile, eol, contentBuilder, \"(?m)^.*\\\\(\\\"\" + t + \"\\\"\\\\)(\\\\.configure|)\");\n                    // Sometimes tasks can be single quoted e.g.\n                    // tasks.named('closeAndReleaseRepository') {\n                    removed |= removeBlock(logger, buildFile, eol, contentBuilder, \"(?m)^.*\\\\('\" + t + \"'\\\\)(\\\\.configure|)\");\n                    removed |= removeBlock(logger, buildFile, eol, contentBuilder, \"(?m)^.*named<\" + t + \">.*?\\\\s\");\n                    content = contentBuilder.toString();\n                }\n                if (removed) {\n                    logger.info(\"Removed instances of plugin {} with configuration block of {} from {}\", plugin, String.join(\",\", configBlocks), buildFile);\n                    FileUtils.writeStringToFile(buildFile, content, Charset.defaultCharset());\n                }\n            } catch (IOException e) {\n                throw new ManipulationException(\"Unable to read build file {}\", buildFile, e);\n            }\n        }\n    }\n}",
        "tuc": "@Test\npublic void testRemoval18() throws IOException, ManipulationException {\n    File target = folder.newFile(\"build.gradle\");\n    org.apache.commons.io.FileUtils.writeStringToFile(target, \"buildscript {\\n\" + \"  repositories {\\n\" + \"    jcenter()\\n\" + \"    mavenCentral()\\n\" + \"    maven {\\n\" + \"        url \\\"https://plugins.gradle.org/m2/\\\"\\n\" + \"    }\\n\" + \"  }\\n\" + \"  dependencies {\\n\" + \"    classpath \\\"ru.vyarus:gradle-animalsniffer-plugin:1.2.0\\\"\\n\" + \"  }\\n\" + \"}\\n\" + \"\\n\" + \"apply plugin: \\\"ru.vyarus.animalsniffer\\\"\\n\" + \"\\n\" + \"dependencies {\\n\" + \"    signature \\\"org.codehaus.mojo.signature:java16:1.1@signature\\\"\\n\" + \"}\\n\" + \"\\n\" + \"animalsniffer {\\n\" + \"    annotation = \\\"io.reactivex.internal.util.SuppressAnimalSniffer\\\"\\n\" + \"}\", Charset.defaultCharset());\n    HashSet<String> plugins = new LinkedHashSet<>();\n    plugins.add(\"ALL\");\n    PluginUtils.pluginRemoval(logger, target.getParentFile(), plugins);\n    String result = FileUtils.readFileToString(target, Charset.defaultCharset());\n    assertFalse(result.contains(\"signature\"));\n    assertFalse(result.contains(\"animalsniffer\"));\n}",
        "label": 1
    },
    {
        "repo_name": "project-ncl___gradle-manipulator",
        "commit": "e1b7fc2f795ca5b0e3d06b16166d878544b1928e",
        "commit_message": "NCL6074 Use same parse code as PME\n",
        "p_path": "common/src/main/java/org/jboss/gm/common/utils/GroovyUtils.java",
        "t_path": "cli/src/test/java/org/jboss/gm/cli/MainTest.java",
        "p_name": "runCustomGroovyScript",
        "t_name": "testInvokeGradleWithProperties",
        "lpfc": "/**\n * @param logger The logger to use. Note: this is specifically using SLF4J logging to allow interaction with the CLI.\n * @param targetStage The stage we are running\n * @param target The target directory to run against.\n * @param configuration The current configuration object.\n * @param rootProject The current root project ; may be null.\n * @param alignmentModel The current model ; may be null.\n * @throws ManipulationException if an error occurs.\n */\npublic static void runCustomGroovyScript(Logger logger, InvocationStage targetStage, File target, Configuration configuration, Project rootProject, ManipulationModel alignmentModel) throws ManipulationException {\n    final List<File> groovyFiles = new ArrayList<>();\n    final String[] scripts = configuration.groovyScripts();\n    if (scripts != null) {\n        int i = 0;\n        for (String script : scripts) {\n            logger.info(\"Attempting to read URL {} \", script);\n            try {\n                File remote = File.createTempFile(\"gme-\" + i, \".groovy\");\n                remote.deleteOnExit();\n                FileUtils.copyURLToFile(new URL(script), remote);\n                groovyFiles.add(remote);\n            } catch (IOException e) {\n                logger.error(\"Ignoring script {} as unable to locate it.\", script);\n                logger.debug(\"Problem with script URL is\", e);\n            }\n        }\n    }\n    for (File scriptFile : groovyFiles) {\n        final Binding binding = new Binding();\n        // We use the current class' classloader so the script has access to this plugin's API and the\n        // groovy API.\n        final GroovyShell groovyShell = new GroovyShell(logger.getClass().getClassLoader(), binding);\n        final Script script;\n        try {\n            script = groovyShell.parse(scriptFile);\n        } catch (IOException e) {\n            throw new ManipulationException(\"Unable to parse script\");\n        }\n        final InvocationStage stage;\n        final InvocationPoint invocationPoint = script.getClass().getAnnotation(InvocationPoint.class);\n        logger.info(\"For target stage {} attempting to invoke groovy script {} \", targetStage, scriptFile);\n        if (invocationPoint != null) {\n            logger.debug(\"InvocationPoint is {}\", invocationPoint.invocationPoint().toString());\n            stage = invocationPoint.invocationPoint();\n        } else {\n            throw new ManipulationException(\"Mandatory annotation '@InvocationPoint(invocationPoint = ' not declared\");\n        }\n        if (targetStage == stage || InvocationStage.BOTH == stage) {\n            // Inject the values via a new BaseScript so user's can have completion.\n            if (script instanceof BaseScript) {\n                ((BaseScript) script).setValues(stage, target, configuration.getProperties(), rootProject, alignmentModel);\n            } else {\n                throw new ManipulationException(\"Cannot cast {} to a BaseScript to set values.\", script);\n            }\n            try {\n                logger.info(\"Executing {} on {} at invocation point {}\", script, rootProject, stage);\n                script.run();\n            } catch (Exception e) {\n                //noinspection ConstantConditions\n                if (e instanceof ManipulationException) {\n                    throw ((ManipulationException) e);\n                } else {\n                    throw new ManipulationException(\"Problem running script\", e);\n                }\n            }\n        } else {\n            logger.debug(\"Ignoring script {} as invocation point {} does not match.\", script, stage);\n        }\n    }\n}",
        "rpfc": "/**\n * @param logger The logger to use. Note: this is specifically using SLF4J logging to allow interaction with the CLI.\n * @param targetStage The stage we are running\n * @param target The target directory to run against.\n * @param configuration The current configuration object.\n * @param rootProject The current root project ; may be null.\n * @param alignmentModel The current model ; may be null.\n * @throws ManipulationException if an error occurs.\n */\npublic static void runCustomGroovyScript(Logger logger, InvocationStage targetStage, File target, Configuration configuration, Project rootProject, ManipulationModel alignmentModel) throws ManipulationException {\n    final List<File> groovyFiles = new ArrayList<>();\n    final String[] scripts = configuration.groovyScripts();\n    final File tmpDir;\n    if (scripts != null) {\n        try {\n            tmpDir = Files.createTempDirectory(\"gme-\" + UUID.randomUUID().toString()).toFile();\n            tmpDir.deleteOnExit();\n        } catch (IOException e) {\n            throw new ManipulationUncheckedException(\"Unable to create temporary directory\", e);\n        }\n        GalleyInfrastructure galleyInfra = new GalleyInfrastructure(null, null).init(null, null, tmpDir);\n        FileIO fileIO = new FileIO(galleyInfra);\n        for (String script : scripts) {\n            logger.info(\"Attempting to read URL {} \", script);\n            try {\n                groovyFiles.add(fileIO.resolveURL(script));\n            } catch (IOException e) {\n                logger.error(\"Ignoring script {} as unable to locate it.\", script);\n                logger.debug(\"Problem with script URL is\", e);\n            }\n        }\n    }\n    for (File scriptFile : groovyFiles) {\n        final Binding binding = new Binding();\n        // We use the current class' classloader so the script has access to this plugin's API and the\n        // groovy API.\n        final GroovyShell groovyShell = new GroovyShell(logger.getClass().getClassLoader(), binding);\n        final Script script;\n        try {\n            script = groovyShell.parse(scriptFile);\n        } catch (IOException e) {\n            throw new ManipulationException(\"Unable to parse script\");\n        }\n        final InvocationStage stage;\n        final InvocationPoint invocationPoint = script.getClass().getAnnotation(InvocationPoint.class);\n        logger.info(\"For target stage {} attempting to invoke groovy script {} \", targetStage, scriptFile);\n        if (invocationPoint != null) {\n            logger.debug(\"InvocationPoint is {}\", invocationPoint.invocationPoint().toString());\n            stage = invocationPoint.invocationPoint();\n        } else {\n            throw new ManipulationException(\"Mandatory annotation '@InvocationPoint(invocationPoint = ' not declared\");\n        }\n        if (targetStage == stage || InvocationStage.BOTH == stage) {\n            // Inject the values via a new BaseScript so user's can have completion.\n            if (script instanceof BaseScript) {\n                ((BaseScript) script).setValues(stage, target, configuration.getProperties(), rootProject, alignmentModel);\n            } else {\n                throw new ManipulationException(\"Cannot cast {} to a BaseScript to set values.\", script);\n            }\n            try {\n                logger.info(\"Executing {} on {} at invocation point {}\", script, rootProject, stage);\n                script.run();\n            } catch (Exception e) {\n                //noinspection ConstantConditions\n                if (e instanceof ManipulationException) {\n                    throw ((ManipulationException) e);\n                } else {\n                    throw new ManipulationException(\"Problem running script\", e);\n                }\n            }\n        } else {\n            logger.debug(\"Ignoring script {} as invocation point {} does not match.\", script, stage);\n        }\n    }\n}",
        "tuc": "@Test\npublic void testInvokeGradleWithProperties() throws Exception {\n    final File projectRoot = new File(MainTest.class.getClassLoader().getResource(\"build.gradle\").getPath());\n    Main m = new Main();\n    String[] args = new String[] { \"-d\", \"-t\", projectRoot.getParentFile().getAbsolutePath(), \"help\", \"--info\", \"-Dfoobar=nothing\", \"-Dfoobar=barfoo\", \"-DdependencyOverride.org.jboss.slf4j:*@*=\", \"-DgroovyScripts=file:///tmp/fake-file\" };\n    m.run(args);\n    assertTrue(systemOutRule.getLog().contains(\"Welcome to Gradle\"));\n}",
        "label": 1
    },
    {
        "repo_name": "powsybl___pypowsybl",
        "commit": "68f616027d469f623d5edb1127c13cfdc4cd6dbe",
        "commit_message": "add load, switch and battery contingencies (#589)\n\nSigned-off-by: Etienne LESOT <etienne.lesot@rte-france.com>",
        "p_path": "java/src/main/java/com/powsybl/python/contingency/ContingencyContainerImpl.java",
        "t_path": "java/src/test/java/com/powsybl/python/contingency/ContingencyContainerTest.java",
        "p_name": "createContingencyElement",
        "t_name": "testContingencyConverter",
        "lpfc": "private static ContingencyElement createContingencyElement(Network network, String elementId) {\n    Identifiable<?> identifiable = network.getIdentifiable(elementId);\n    if (identifiable == null) {\n        throw new PowsyblException(\"Element '\" + elementId + \"' not found\");\n    }\n    if (identifiable instanceof Branch) {\n        return new BranchContingency(elementId);\n    } else if (identifiable instanceof HvdcLine) {\n        return new HvdcLineContingency(elementId);\n    } else if (identifiable instanceof BusbarSection) {\n        return new BusbarSectionContingency(elementId);\n    } else if (identifiable instanceof Generator) {\n        return new GeneratorContingency(elementId);\n    } else if (identifiable instanceof DanglingLine) {\n        return new DanglingLineContingency(elementId);\n    } else if (identifiable instanceof StaticVarCompensator) {\n        return new StaticVarCompensatorContingency(elementId);\n    } else if (identifiable instanceof ShuntCompensator) {\n        return new ShuntCompensatorContingency(elementId);\n    } else if (identifiable instanceof ThreeWindingsTransformer) {\n        return new ThreeWindingsTransformerContingency(elementId);\n    } else {\n        throw new PowsyblException(\"Element type not supported: \" + identifiable.getClass().getSimpleName());\n    }\n}",
        "rpfc": "private static ContingencyElement createContingencyElement(Network network, String elementId) {\n    Identifiable<?> identifiable = network.getIdentifiable(elementId);\n    if (identifiable == null) {\n        throw new PowsyblException(\"Element '\" + elementId + \"' not found\");\n    }\n    if (identifiable instanceof Line) {\n        return new LineContingency(elementId);\n    } else if (identifiable instanceof TwoWindingsTransformer) {\n        return new TwoWindingsTransformerContingency(elementId);\n    } else if (identifiable instanceof HvdcLine) {\n        return new HvdcLineContingency(elementId);\n    } else if (identifiable instanceof BusbarSection) {\n        return new BusbarSectionContingency(elementId);\n    } else if (identifiable instanceof Generator) {\n        return new GeneratorContingency(elementId);\n    } else if (identifiable instanceof DanglingLine) {\n        return new DanglingLineContingency(elementId);\n    } else if (identifiable instanceof StaticVarCompensator) {\n        return new StaticVarCompensatorContingency(elementId);\n    } else if (identifiable instanceof ShuntCompensator) {\n        return new ShuntCompensatorContingency(elementId);\n    } else if (identifiable instanceof ThreeWindingsTransformer) {\n        return new ThreeWindingsTransformerContingency(elementId);\n    } else if (identifiable instanceof Load) {\n        return new LoadContingency(elementId);\n    } else if (identifiable instanceof Battery) {\n        return new BatteryContingency(elementId);\n    } else if (identifiable instanceof Switch) {\n        return new SwitchContingency(elementId);\n    } else {\n        throw new PowsyblException(\"Element type not supported: \" + identifiable.getClass().getSimpleName());\n    }\n}",
        "tuc": "@Test\nvoid testContingencyConverter() {\n    var container1 = new ContingencyContainerImpl();\n    var network = EurostagTutorialExample1Factory.create();\n    container1.addContingency(\"l1\", List.of(\"NHV1_NHV2_1\"));\n    container1.addContingency(\"gen\", List.of(\"GEN\"));\n    List<Contingency> contingencies = container1.createContingencies(network);\n    assertThat(contingencies).hasSize(2);\n    assertThat(contingencies.get(1).getElements().get(0)).isInstanceOf(BranchContingency.class);\n    assertThat(contingencies.get(0).getElements().get(0)).isInstanceOf(GeneratorContingency.class);\n    network = HvdcTestNetwork.createVsc();\n    var container2 = new ContingencyContainerImpl();\n    container2.addContingency(\"bbs\", List.of(\"BBS1\", \"L\"));\n    assertThat(container2.createContingencies(network)).hasOnlyOneElementSatisfying(c -> {\n        assertThat(c.getElements().get(0)).isInstanceOf(BusbarSectionContingency.class);\n        assertThat(c.getElements().get(1)).isInstanceOf(HvdcLineContingency.class);\n    });\n    network = DanglingLineNetworkFactory.create();\n    var container3 = new ContingencyContainerImpl();\n    container3.addContingency(\"ddl\", List.of(\"DL\"));\n    assertThat(container3.createContingencies(network)).hasOnlyOneElementSatisfying(c -> assertThat(c.getElements().get(0)).isInstanceOf(DanglingLineContingency.class));\n    network = SvcTestCaseFactory.create();\n    var container4 = new ContingencyContainerImpl();\n    container4.addContingency(\"svc\", List.of(\"SVC2\"));\n    assertThat(container4.createContingencies(network)).hasOnlyOneElementSatisfying(c -> assertThat(c.getElements().get(0)).isInstanceOf(StaticVarCompensatorContingency.class));\n    network = ShuntTestCaseFactory.create();\n    var container5 = new ContingencyContainerImpl();\n    container5.addContingency(\"shunt\", List.of(\"SHUNT\"));\n    assertThat(container5.createContingencies(network)).hasOnlyOneElementSatisfying(c -> assertThat(c.getElements().get(0)).isInstanceOf(ShuntCompensatorContingency.class));\n    network = ThreeWindingsTransformerNetworkFactory.create();\n    var container6 = new ContingencyContainerImpl();\n    container6.addContingency(\"twt3\", List.of(\"3WT\"));\n    assertThat(container6.createContingencies(network)).hasOnlyOneElementSatisfying(c -> assertThat(c.getElements().get(0)).isInstanceOf(ThreeWindingsTransformerContingency.class));\n    var container7 = new ContingencyContainerImpl();\n    container7.addContingency(\"exception\", List.of(\"not_exists_id\"));\n    assertThatThrownBy(() -> container7.createContingencies(EurostagTutorialExample1Factory.create())).hasMessageContaining(\"Element 'not_exists_id' not found\");\n}",
        "label": 1
    },
    {
        "repo_name": "clulab___reach-banner",
        "commit": "9de937a56ea6282aa930400da07b8eb00d772eff",
        "commit_message": "Improved unit test.\n",
        "p_path": "src/main/java/banner/BannerWrapper.java",
        "t_path": "src/test/java/TestBanner.java",
        "p_name": "tag",
        "t_name": "testBanner",
        "lpfc": "public List<Mention> tag(String sentenceText) {\n    String originalText = new String(sentenceText);\n    Sentence sentence = new Sentence(sentenceText);\n    tokenizer.tokenize(sentence);\n    tagger.tag(sentence);\n    if (postProcessor != null)\n        postProcessor.postProcess(sentence);\n    // make sure the text of the sentence did not change!\n    //   if it did, it is impossible to align the mentions with the original text...\n    if (!originalText.equals(sentence.getText())) {\n        throw new RuntimeException(\"ERROR: input sentence [\" + originalText + \"] is different from sentence output by Banner [\" + sentence.getText() + \"]!\");\n    }\n    return sentence.getMentions();\n}",
        "rpfc": "public List<Mention> tag(String sentenceText) {\n    String originalText = new String(sentenceText);\n    Sentence sentence = new Sentence(sentenceText);\n    tokenizer.tokenize(sentence);\n    //System.out.println(\"original text: \" + sentenceText);\n    //System.out.println(\"Sentence text: \" + sentence.getText());\n    //for(Token t: sentence.getTokens()) {\n    //  System.out.println(\"\\t\" + t.getText() + \" \" + t.getStart());\n    //}\n    tagger.tag(sentence);\n    if (postProcessor != null)\n        postProcessor.postProcess(sentence);\n    // make sure the text of the sentence did not change!\n    //   if it did, it is impossible to align the mentions with the original text...\n    if (!originalText.equals(sentence.getText())) {\n        throw new RuntimeException(\"ERROR: input sentence [\" + originalText + \"] is different from sentence output by Banner [\" + sentence.getText() + \"]!\");\n    }\n    return sentence.getMentions();\n}",
        "tuc": "@Test\npublic void testBanner() {\n    BannerWrapper banner = new BannerWrapper();\n    List<Mention> mentions = banner.tag(\"Co-immunoprecipitation analysis confirmed that Bis interacted with Bcl-2 in vivo.\");\n    for (Mention m : mentions) {\n        System.out.println(\"\\tMENTION: \" + m.getText() + \" \" + m.getType() + \" \" + m.getStartChar() + \" \" + m.getEndChar());\n        assertEquals(m.getText(), \"Bcl-2\");\n        assertEquals(m.getStartChar(), 67);\n        assertEquals(m.getEndChar(), 72);\n    }\n}",
        "label": 1
    },
    {
        "repo_name": "genepi___imputationserver",
        "commit": "07b37a48aa38c40a60fffcf1eff9dceb677c2c9b",
        "commit_message": "Merge pull request #80 from genepi/features/pgs-html-report\n\nFeatures/pgs html report",
        "p_path": "src/main/java/genepi/imputationserver/steps/CompressionEncryption.java",
        "t_path": "src/test/java/genepi/imputationserver/steps/ImputationTest.java",
        "p_name": "run",
        "t_name": "testPipelineWithEagleAndScores",
        "lpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    String workingDirectory = getFolder(CompressionEncryption.class);\n    String output = context.get(\"outputimputation\");\n    String outputScores = context.get(\"outputScores\");\n    String localOutput = context.get(\"local\");\n    String aesEncryptionValue = context.get(\"aesEncryption\");\n    String meta = context.get(\"meta\");\n    String mode = context.get(\"mode\");\n    String password = context.get(\"password\");\n    PgsPanel pgsPanel = PgsPanel.loadFromProperties(context.getData(\"pgsPanel\"));\n    boolean phasingOnly = false;\n    if (mode != null && mode.equals(\"phasing\")) {\n        phasingOnly = true;\n    }\n    boolean mergeMetaFiles = !phasingOnly && (meta != null && meta.equals(\"yes\"));\n    boolean aesEncryption = (aesEncryptionValue != null && aesEncryptionValue.equals(\"yes\"));\n    // read config if mails should be sent\n    String folderConfig = getFolder(CompressionEncryption.class);\n    File jobConfig = new File(FileUtil.path(folderConfig, \"job.config\"));\n    DefaultPreferenceStore store = new DefaultPreferenceStore();\n    if (jobConfig.exists()) {\n        store.load(jobConfig);\n    } else {\n        context.log(\"Configuration file '\" + jobConfig.getAbsolutePath() + \"' not available. Use default values.\");\n    }\n    String notification = \"no\";\n    if (store.getString(\"minimac.sendmail\") != null && !store.getString(\"minimac.sendmail\").equals(\"\")) {\n        notification = store.getString(\"minimac.sendmail\");\n    }\n    String serverUrl = \"https://imputationserver.sph.umich.edu\";\n    if (store.getString(\"server.url\") != null && !store.getString(\"server.url\").isEmpty()) {\n        serverUrl = store.getString(\"server.url\");\n    }\n    String sanityCheck = \"yes\";\n    if (store.getString(\"sanitycheck\") != null && !store.getString(\"sanitycheck\").equals(\"\")) {\n        sanityCheck = store.getString(\"sanitycheck\");\n    }\n    if (password == null || (password != null && password.equals(\"auto\"))) {\n        password = PasswordCreator.createPassword();\n    }\n    try {\n        context.beginTask(\"Export data...\");\n        // get sorted directories\n        List<String> folders = HdfsUtil.getDirectories(output);\n        ImputationResults imputationResults = new ImputationResults(folders, phasingOnly);\n        Map<String, ImputedChromosome> imputedChromosomes = imputationResults.getChromosomes();\n        Set<String> chromosomes = imputedChromosomes.keySet();\n        boolean lastChromosome = false;\n        int index = 0;\n        String checksumFilename = FileUtil.path(localOutput, \"results.md5\");\n        LineWriter writer = new LineWriter(checksumFilename);\n        for (String name : chromosomes) {\n            index++;\n            if (index == chromosomes.size()) {\n                lastChromosome = true;\n            }\n            ImputedChromosome imputedChromosome = imputedChromosomes.get(name);\n            context.println(\"Export and merge chromosome \" + name);\n            // create temp dir\n            String temp = FileUtil.path(localOutput, \"temp\");\n            FileUtil.createDirectory(temp);\n            // output files\n            ArrayList<File> files = new ArrayList<File>();\n            // merge info files\n            if (!phasingOnly) {\n                String infoOutput = FileUtil.path(temp, \"chr\" + name + \".info.gz\");\n                FileMerger.mergeAndGzInfo(imputedChromosome.getInfoFiles(), infoOutput);\n                files.add(new File(infoOutput));\n            }\n            // merge all dosage files\n            String dosageOutput;\n            if (phasingOnly) {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".phased.vcf.gz\");\n            } else {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".dose.vcf.gz\");\n            }\n            files.add(new File(dosageOutput));\n            MergedVcfFile vcfFile = new MergedVcfFile(dosageOutput);\n            vcfFile.addHeader(context, imputedChromosome.getHeaderFiles());\n            for (String file : imputedChromosome.getDataFiles()) {\n                context.println(\"Read file \" + file);\n                vcfFile.addFile(HdfsUtil.open(file));\n                HdfsUtil.delete(file);\n            }\n            vcfFile.close();\n            // merge all meta files\n            if (mergeMetaFiles) {\n                context.println(\"Merging meta files...\");\n                String dosageMetaOutput = FileUtil.path(temp, \"chr\" + name + \".empiricalDose.vcf.gz\");\n                MergedVcfFile vcfFileMeta = new MergedVcfFile(dosageMetaOutput);\n                String headerMetaFile = imputedChromosome.getHeaderMetaFiles().get(0);\n                context.println(\"Use header from file \" + headerMetaFile);\n                vcfFileMeta.addFile(HdfsUtil.open(headerMetaFile));\n                for (String file : imputedChromosome.getDataMetaFiles()) {\n                    context.println(\"Read file \" + file);\n                    vcfFileMeta.addFile(HdfsUtil.open(file));\n                    HdfsUtil.delete(file);\n                }\n                vcfFileMeta.close();\n                context.println(\"Meta files merged.\");\n                files.add(new File(dosageMetaOutput));\n            }\n            if (sanityCheck.equals(\"yes\") && lastChromosome) {\n                context.log(\"Run tabix on chromosome \" + name + \"...\");\n                Command tabix = new Command(FileUtil.path(workingDirectory, \"bin\", \"tabix\"));\n                tabix.setSilent(false);\n                tabix.setParams(\"-f\", dosageOutput);\n                if (tabix.execute() != 0) {\n                    context.endTask(\"Error during index creation: \" + tabix.getStdOut(), WorkflowContext.ERROR);\n                    return false;\n                }\n                context.log(\"Tabix done.\");\n            }\n            // create zip file\n            String fileName = \"chr_\" + name + \".zip\";\n            String filePath = FileUtil.path(localOutput, fileName);\n            File file = new File(filePath);\n            createEncryptedZipFile(file, files, password, aesEncryption);\n            // add checksum to hash file\n            context.log(\"Creating file checksum for \" + filePath);\n            long checksumStart = System.currentTimeMillis();\n            String checksum = FileChecksum.HashFile(new File(filePath), FileChecksum.Algorithm.MD5);\n            writer.write(checksum + \" \" + fileName);\n            long checksumEnd = (System.currentTimeMillis() - checksumStart) / 1000;\n            context.log(\"File checksum for \" + filePath + \" created in \" + checksumEnd + \" seconds.\");\n            // delete temp dir\n            FileUtil.deleteDirectory(temp);\n            IExternalWorkspace externalWorkspace = context.getExternalWorkspace();\n            if (externalWorkspace != null) {\n                long start = System.currentTimeMillis();\n                context.log(\"External Workspace '\" + externalWorkspace.getName() + \"' found\");\n                context.log(\"Start file upload: \" + filePath);\n                String url = externalWorkspace.upload(\"local\", file);\n                long end = (System.currentTimeMillis() - start) / 1000;\n                context.log(\"Upload finished in  \" + end + \" sec. File Location: \" + url);\n                context.log(\"Add \" + localOutput + \" to custom download\");\n                String size = FileUtils.byteCountToDisplaySize(file.length());\n                context.addDownload(\"local\", fileName, size, url);\n                FileUtil.deleteFile(filePath);\n                context.log(\"File deleted: \" + filePath);\n            } else {\n                context.log(\"No external Workspace set.\");\n            }\n        }\n        writer.close();\n        // delete temporary files\n        HdfsUtil.delete(output);\n        // Export calculated risk scores\n        if (pgsPanel != null) {\n            context.println(\"Exporting PGS scores...\");\n            String temp2 = FileUtil.path(localOutput, \"temp2\");\n            FileUtil.createDirectory(temp2);\n            List<String> scoreList = HdfsUtil.getFiles(outputScores);\n            String[] chunksScores = new String[scoreList.size() / 2];\n            String[] chunksReports = new String[scoreList.size() / 2];\n            int chunksScoresCount = 0;\n            int chunksReportsCount = 0;\n            for (String score : scoreList) {\n                String filename = FileUtil.getFilename(score);\n                String localPath = FileUtil.path(temp2, filename);\n                HdfsUtil.get(score, localPath);\n                if (score.endsWith(\".json\")) {\n                    chunksReports[chunksReportsCount] = localPath;\n                    chunksReportsCount++;\n                } else {\n                    chunksScores[chunksScoresCount] = localPath;\n                    chunksScoresCount++;\n                }\n            }\n            String outputFileScores = FileUtil.path(temp2, \"scores.txt\");\n            String outputFileReports = FileUtil.path(temp2, \"report.json\");\n            String outputFileHtml = FileUtil.path(localOutput, \"report.html\");\n            // disable ansi\n            TaskService.setAnsiSupport(false);\n            MergeScoreTask mergeScore = new MergeScoreTask();\n            mergeScore.setInputs(chunksScores);\n            mergeScore.setOutput(outputFileScores);\n            TaskService.run(mergeScore);\n            MergeReportTask mergeReport = new MergeReportTask();\n            mergeReport.setInputs(chunksReports);\n            mergeReport.setOutput(outputFileReports);\n            TaskService.run(mergeReport);\n            ReportFile report = mergeReport.getResult();\n            String folder = getFolder(CompressionEncryption.class);\n            MetaFile metaFile = MetaFile.load(FileUtil.path(folder, \"pgs-catalog.json\"));\n            report.mergeWithMeta(metaFile);\n            String fileName = \"scores.zip\";\n            String filePath = FileUtil.path(localOutput, fileName);\n            File file = new File(filePath);\n            createEncryptedZipFile(file, new File(outputFileScores), password, aesEncryption);\n            context.println(\"Exported PGS scores to \" + fileName + \".\");\n            FileUtil.deleteDirectory(temp2);\n        }\n        context.endTask(\"Exported data.\", WorkflowContext.OK);\n    } catch (Exception e) {\n        e.printStackTrace();\n        context.endTask(\"Data export failed: \" + e.getMessage(), WorkflowContext.ERROR);\n        return false;\n    }\n    // submit counters!\n    context.submitCounter(\"samples\");\n    context.submitCounter(\"genotypes\");\n    context.submitCounter(\"chromosomes\");\n    context.submitCounter(\"runs\");\n    // submit panel and phasing method counters\n    String reference = context.get(\"refpanel\");\n    String phasing = context.get(\"phasing\");\n    context.submitCounter(\"refpanel_\" + reference);\n    context.submitCounter(\"phasing_\" + phasing);\n    context.submitCounter(\"23andme-input\");\n    // send email\n    if (notification.equals(\"yes\")) {\n        Object mail = context.getData(\"cloudgene.user.mail\");\n        Object name = context.getData(\"cloudgene.user.name\");\n        if (mail != null) {\n            String subject = \"Job \" + context.getJobId() + \" is complete.\";\n            String message = \"Dear \" + name + \",\\nthe password for the imputation results is: \" + password + \"\\n\\nThe results can be downloaded from \" + serverUrl + \"/start.html#!jobs/\" + context.getJobId() + \"/results\";\n            try {\n                context.sendMail(subject, message);\n                context.ok(\"We have sent an email to <b>\" + mail + \"</b> with the password.\");\n                return true;\n            } catch (Exception e) {\n                context.error(\"Data compression failed: \" + e.getMessage());\n                return false;\n            }\n        } else {\n            context.error(\"No email address found. Please enter your email address (Account -> Profile).\");\n            return false;\n        }\n    } else {\n        context.ok(\"Email notification is disabled. All results are encrypted with password <b>\" + password + \"</b>\");\n        return true;\n    }\n}",
        "rpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    String workingDirectory = getFolder(CompressionEncryption.class);\n    String output = context.get(\"outputimputation\");\n    String outputScores = context.get(\"outputScores\");\n    String localOutput = context.get(\"local\");\n    String aesEncryptionValue = context.get(\"aesEncryption\");\n    String meta = context.get(\"meta\");\n    String mode = context.get(\"mode\");\n    String password = context.get(\"password\");\n    PgsPanel pgsPanel = PgsPanel.loadFromProperties(context.getData(\"pgsPanel\"));\n    boolean phasingOnly = false;\n    if (mode != null && mode.equals(\"phasing\")) {\n        phasingOnly = true;\n    }\n    boolean mergeMetaFiles = !phasingOnly && (meta != null && meta.equals(\"yes\"));\n    boolean aesEncryption = (aesEncryptionValue != null && aesEncryptionValue.equals(\"yes\"));\n    // read config if mails should be sent\n    String folderConfig = getFolder(CompressionEncryption.class);\n    File jobConfig = new File(FileUtil.path(folderConfig, \"job.config\"));\n    DefaultPreferenceStore store = new DefaultPreferenceStore();\n    if (jobConfig.exists()) {\n        store.load(jobConfig);\n    } else {\n        context.log(\"Configuration file '\" + jobConfig.getAbsolutePath() + \"' not available. Use default values.\");\n    }\n    String notification = \"no\";\n    if (store.getString(\"minimac.sendmail\") != null && !store.getString(\"minimac.sendmail\").equals(\"\")) {\n        notification = store.getString(\"minimac.sendmail\");\n    }\n    String serverUrl = \"https://imputationserver.sph.umich.edu\";\n    if (store.getString(\"server.url\") != null && !store.getString(\"server.url\").isEmpty()) {\n        serverUrl = store.getString(\"server.url\");\n    }\n    String sanityCheck = \"yes\";\n    if (store.getString(\"sanitycheck\") != null && !store.getString(\"sanitycheck\").equals(\"\")) {\n        sanityCheck = store.getString(\"sanitycheck\");\n    }\n    if (password == null || (password != null && password.equals(\"auto\"))) {\n        password = PasswordCreator.createPassword();\n    }\n    try {\n        context.beginTask(\"Export data...\");\n        // get sorted directories\n        List<String> folders = HdfsUtil.getDirectories(output);\n        ImputationResults imputationResults = new ImputationResults(folders, phasingOnly);\n        Map<String, ImputedChromosome> imputedChromosomes = imputationResults.getChromosomes();\n        Set<String> chromosomes = imputedChromosomes.keySet();\n        boolean lastChromosome = false;\n        int index = 0;\n        String checksumFilename = FileUtil.path(localOutput, \"results.md5\");\n        LineWriter writer = new LineWriter(checksumFilename);\n        for (String name : chromosomes) {\n            index++;\n            if (index == chromosomes.size()) {\n                lastChromosome = true;\n            }\n            ImputedChromosome imputedChromosome = imputedChromosomes.get(name);\n            context.println(\"Export and merge chromosome \" + name);\n            // create temp dir\n            String temp = FileUtil.path(localOutput, \"temp\");\n            FileUtil.createDirectory(temp);\n            // output files\n            ArrayList<File> files = new ArrayList<File>();\n            // merge info files\n            if (!phasingOnly) {\n                String infoOutput = FileUtil.path(temp, \"chr\" + name + \".info.gz\");\n                FileMerger.mergeAndGzInfo(imputedChromosome.getInfoFiles(), infoOutput);\n                files.add(new File(infoOutput));\n            }\n            // merge all dosage files\n            String dosageOutput;\n            if (phasingOnly) {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".phased.vcf.gz\");\n            } else {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".dose.vcf.gz\");\n            }\n            files.add(new File(dosageOutput));\n            MergedVcfFile vcfFile = new MergedVcfFile(dosageOutput);\n            vcfFile.addHeader(context, imputedChromosome.getHeaderFiles());\n            for (String file : imputedChromosome.getDataFiles()) {\n                context.println(\"Read file \" + file);\n                vcfFile.addFile(HdfsUtil.open(file));\n                HdfsUtil.delete(file);\n            }\n            vcfFile.close();\n            // merge all meta files\n            if (mergeMetaFiles) {\n                context.println(\"Merging meta files...\");\n                String dosageMetaOutput = FileUtil.path(temp, \"chr\" + name + \".empiricalDose.vcf.gz\");\n                MergedVcfFile vcfFileMeta = new MergedVcfFile(dosageMetaOutput);\n                String headerMetaFile = imputedChromosome.getHeaderMetaFiles().get(0);\n                context.println(\"Use header from file \" + headerMetaFile);\n                vcfFileMeta.addFile(HdfsUtil.open(headerMetaFile));\n                for (String file : imputedChromosome.getDataMetaFiles()) {\n                    context.println(\"Read file \" + file);\n                    vcfFileMeta.addFile(HdfsUtil.open(file));\n                    HdfsUtil.delete(file);\n                }\n                vcfFileMeta.close();\n                context.println(\"Meta files merged.\");\n                files.add(new File(dosageMetaOutput));\n            }\n            if (sanityCheck.equals(\"yes\") && lastChromosome) {\n                context.log(\"Run tabix on chromosome \" + name + \"...\");\n                Command tabix = new Command(FileUtil.path(workingDirectory, \"bin\", \"tabix\"));\n                tabix.setSilent(false);\n                tabix.setParams(\"-f\", dosageOutput);\n                if (tabix.execute() != 0) {\n                    context.endTask(\"Error during index creation: \" + tabix.getStdOut(), WorkflowContext.ERROR);\n                    return false;\n                }\n                context.log(\"Tabix done.\");\n            }\n            // create zip file\n            String fileName = \"chr_\" + name + \".zip\";\n            String filePath = FileUtil.path(localOutput, fileName);\n            File file = new File(filePath);\n            createEncryptedZipFile(file, files, password, aesEncryption);\n            // add checksum to hash file\n            context.log(\"Creating file checksum for \" + filePath);\n            long checksumStart = System.currentTimeMillis();\n            String checksum = FileChecksum.HashFile(new File(filePath), FileChecksum.Algorithm.MD5);\n            writer.write(checksum + \" \" + fileName);\n            long checksumEnd = (System.currentTimeMillis() - checksumStart) / 1000;\n            context.log(\"File checksum for \" + filePath + \" created in \" + checksumEnd + \" seconds.\");\n            // delete temp dir\n            FileUtil.deleteDirectory(temp);\n            IExternalWorkspace externalWorkspace = context.getExternalWorkspace();\n            if (externalWorkspace != null) {\n                long start = System.currentTimeMillis();\n                context.log(\"External Workspace '\" + externalWorkspace.getName() + \"' found\");\n                context.log(\"Start file upload: \" + filePath);\n                String url = externalWorkspace.upload(\"local\", file);\n                long end = (System.currentTimeMillis() - start) / 1000;\n                context.log(\"Upload finished in  \" + end + \" sec. File Location: \" + url);\n                context.log(\"Add \" + localOutput + \" to custom download\");\n                String size = FileUtils.byteCountToDisplaySize(file.length());\n                context.addDownload(\"local\", fileName, size, url);\n                FileUtil.deleteFile(filePath);\n                context.log(\"File deleted: \" + filePath);\n            } else {\n                context.log(\"No external Workspace set.\");\n            }\n        }\n        writer.close();\n        // delete temporary files\n        HdfsUtil.delete(output);\n        // Export calculated risk scores\n        if (pgsPanel != null) {\n            context.println(\"Exporting PGS scores...\");\n            String temp2 = FileUtil.path(localOutput, \"temp2\");\n            FileUtil.createDirectory(temp2);\n            List<String> scoreList = HdfsUtil.getFiles(outputScores);\n            String[] chunksScores = new String[scoreList.size() / 2];\n            String[] chunksReports = new String[scoreList.size() / 2];\n            int chunksScoresCount = 0;\n            int chunksReportsCount = 0;\n            for (String score : scoreList) {\n                String filename = FileUtil.getFilename(score);\n                String localPath = FileUtil.path(temp2, filename);\n                HdfsUtil.get(score, localPath);\n                if (score.endsWith(\".json\")) {\n                    chunksReports[chunksReportsCount] = localPath;\n                    chunksReportsCount++;\n                } else {\n                    chunksScores[chunksScoresCount] = localPath;\n                    chunksScoresCount++;\n                }\n            }\n            String outputFileScores = FileUtil.path(temp2, \"scores.txt\");\n            String outputFileReports = FileUtil.path(temp2, \"report.json\");\n            String outputFileHtml = FileUtil.path(localOutput, \"scores.html\");\n            // disable ansi\n            TaskService.setAnsiSupport(false);\n            MergeScoreTask mergeScore = new MergeScoreTask();\n            mergeScore.setInputs(chunksScores);\n            mergeScore.setOutput(outputFileScores);\n            TaskService.run(mergeScore);\n            MergeReportTask mergeReport = new MergeReportTask();\n            mergeReport.setInputs(chunksReports);\n            mergeReport.setOutput(outputFileReports);\n            TaskService.run(mergeReport);\n            ReportFile report = mergeReport.getResult();\n            String folder = getFolder(CompressionEncryption.class);\n            MetaFile metaFile = MetaFile.load(FileUtil.path(folder, \"pgs-catalog.json\"));\n            report.mergeWithMeta(metaFile);\n            CreateHtmlReportTask htmlReportTask = new CreateHtmlReportTask();\n            htmlReportTask.setApplicationName(\"PGS Server\");\n            htmlReportTask.setVersion(ImputationPipeline.PIPELINE_VERSION + \" - BETA\");\n            htmlReportTask.setShowCommand(false);\n            htmlReportTask.setReport(report);\n            htmlReportTask.setOutput(outputFileHtml);\n            TaskService.run(htmlReportTask);\n            context.println(\"Created html report \" + outputFileHtml + \".\");\n            String fileName = \"scores.zip\";\n            String filePath = FileUtil.path(localOutput, fileName);\n            File file = new File(filePath);\n            createEncryptedZipFile(file, new File(outputFileScores), password, aesEncryption);\n            context.println(\"Exported PGS scores to \" + fileName + \".\");\n            FileUtil.deleteDirectory(temp2);\n        }\n        context.endTask(\"Exported data.\", WorkflowContext.OK);\n    } catch (Exception e) {\n        e.printStackTrace();\n        context.endTask(\"Data export failed: \" + e.getMessage(), WorkflowContext.ERROR);\n        return false;\n    }\n    // submit counters!\n    context.submitCounter(\"samples\");\n    context.submitCounter(\"genotypes\");\n    context.submitCounter(\"chromosomes\");\n    context.submitCounter(\"runs\");\n    // submit panel and phasing method counters\n    String reference = context.get(\"refpanel\");\n    String phasing = context.get(\"phasing\");\n    context.submitCounter(\"refpanel_\" + reference);\n    context.submitCounter(\"phasing_\" + phasing);\n    context.submitCounter(\"23andme-input\");\n    // send email\n    if (notification.equals(\"yes\")) {\n        Object mail = context.getData(\"cloudgene.user.mail\");\n        Object name = context.getData(\"cloudgene.user.name\");\n        if (mail != null) {\n            String subject = \"Job \" + context.getJobId() + \" is complete.\";\n            String message = \"Dear \" + name + \",\\nthe password for the imputation results is: \" + password + \"\\n\\nThe results can be downloaded from \" + serverUrl + \"/start.html#!jobs/\" + context.getJobId() + \"/results\";\n            try {\n                context.sendMail(subject, message);\n                context.ok(\"We have sent an email to <b>\" + mail + \"</b> with the password.\");\n                return true;\n            } catch (Exception e) {\n                context.error(\"Data compression failed: \" + e.getMessage());\n                return false;\n            }\n        } else {\n            context.error(\"No email address found. Please enter your email address (Account -> Profile).\");\n            return false;\n        }\n    } else {\n        context.ok(\"Email notification is disabled. All results are encrypted with password <b>\" + password + \"</b>\");\n        return true;\n    }\n}",
        "tuc": "@Test\npublic void testPipelineWithEagleAndScores() throws IOException, ZipException {\n    String configFolder = \"test-data/configs/hapmap-chr20\";\n    String inputFolder = \"test-data/data/chr20-unphased\";\n    String score1 = PGSCatalog.getFilenameById(\"PGS000018\");\n    String score2 = PGSCatalog.getFilenameById(\"PGS000027\");\n    String targetScore1 = HdfsUtil.path(\"scores-hdfs\", \"PGS000018.txt.gz\");\n    HdfsUtil.put(score1, targetScore1);\n    String targetScore2 = HdfsUtil.path(\"scores-hdfs\", \"PGS000027.txt.gz\");\n    HdfsUtil.put(score2, targetScore2);\n    WorkflowTestContext context = buildContext(inputFolder, \"hapmap2\");\n    context.setOutput(\"outputScores\", \"cloudgene2-hdfs\");\n    Map<String, Object> pgsPanel = new HashMap<String, Object>();\n    List<String> scores = new Vector<String>();\n    scores.add(\"PGS000018.txt.gz\");\n    scores.add(\"PGS000027.txt.gz\");\n    pgsPanel.put(\"location\", \"scores-hdfs\");\n    pgsPanel.put(\"scores\", scores);\n    pgsPanel.put(\"build\", \"hg19\");\n    context.setData(\"pgsPanel\", pgsPanel);\n    InputValidation inputValidation = new InputValidationMock(configFolder);\n    boolean result = run(context, inputValidation);\n    assertTrue(result);\n    QcStatisticsMock qcStats = new QcStatisticsMock(configFolder);\n    result = run(context, qcStats);\n    assertTrue(result);\n    assertTrue(context.hasInMemory(\"Remaining sites in total: 7,735\"));\n    importRefPanel(FileUtil.path(configFolder, \"ref-panels\"));\n    importBinaries(\"files/bin\");\n    ImputationMinimac3Mock imputation = new ImputationMinimac3Mock(configFolder);\n    result = run(context, imputation);\n    assertTrue(result);\n    CompressionEncryptionMock export = new CompressionEncryptionMock(\"files\");\n    result = run(context, export);\n    assertTrue(result);\n    ZipFile zipFile = new ZipFile(\"test-data/tmp/local/chr_20.zip\", PASSWORD.toCharArray());\n    zipFile.extractAll(\"test-data/tmp\");\n    VcfFile file = VcfFileUtil.load(\"test-data/tmp/chr20.dose.vcf.gz\", 100000000, false);\n    assertEquals(\"20\", file.getChromosome());\n    assertEquals(51, file.getNoSamples());\n    assertEquals(true, file.isPhased());\n    assertEquals(TOTAL_REFPANEL_CHR20_B37, file.getNoSnps());\n    int snpInInfo = getLineCount(\"test-data/tmp/chr20.info.gz\") - 1;\n    assertEquals(snpInInfo, file.getNoSnps());\n    String[] args = { \"test-data/tmp/chr20.dose.vcf.gz\", \"--ref\", \"PGS000018,PGS000027\", \"--out\", \"test-data/tmp/expected.txt\" };\n    int resultScore = new CommandLine(new ApplyScoreCommand()).execute(args);\n    assertEquals(0, resultScore);\n    zipFile = new ZipFile(\"test-data/tmp/local/scores.zip\", PASSWORD.toCharArray());\n    zipFile.extractAll(\"test-data/tmp\");\n    CsvTableReader readerExpected = new CsvTableReader(\"test-data/tmp/expected.txt\", ',');\n    CsvTableReader readerActual = new CsvTableReader(\"test-data/tmp/scores.txt\", ',');\n    while (readerExpected.next() && readerActual.next()) {\n        assertEquals(readerExpected.getDouble(\"PGS000018\"), readerActual.getDouble(\"PGS000018\"), 0.00001);\n        assertEquals(readerExpected.getDouble(\"PGS000027\"), readerActual.getDouble(\"PGS000027\"), 0.00001);\n    }\n    readerExpected.close();\n    readerActual.close();\n    FileUtil.deleteDirectory(\"test-data/tmp\");\n}",
        "label": 1
    },
    {
        "repo_name": "genepi___imputationserver",
        "commit": "ae6245a2f6a2bf17a48028acfa77dc2cf810ff18",
        "commit_message": "Update pgs-calc to 0.9.17 and add html report",
        "p_path": "src/main/java/genepi/imputationserver/steps/CompressionEncryption.java",
        "t_path": "src/test/java/genepi/imputationserver/steps/ImputationTest.java",
        "p_name": "run",
        "t_name": "testPipelineWithEagleAndScores",
        "lpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    String workingDirectory = getFolder(CompressionEncryption.class);\n    String output = context.get(\"outputimputation\");\n    String outputScores = context.get(\"outputScores\");\n    String localOutput = context.get(\"local\");\n    String aesEncryptionValue = context.get(\"aesEncryption\");\n    String meta = context.get(\"meta\");\n    String mode = context.get(\"mode\");\n    String password = context.get(\"password\");\n    PgsPanel pgsPanel = PgsPanel.loadFromProperties(context.getData(\"pgsPanel\"));\n    boolean phasingOnly = false;\n    if (mode != null && mode.equals(\"phasing\")) {\n        phasingOnly = true;\n    }\n    boolean mergeMetaFiles = !phasingOnly && (meta != null && meta.equals(\"yes\"));\n    boolean aesEncryption = (aesEncryptionValue != null && aesEncryptionValue.equals(\"yes\"));\n    // read config if mails should be sent\n    String folderConfig = getFolder(CompressionEncryption.class);\n    File jobConfig = new File(FileUtil.path(folderConfig, \"job.config\"));\n    DefaultPreferenceStore store = new DefaultPreferenceStore();\n    if (jobConfig.exists()) {\n        store.load(jobConfig);\n    } else {\n        context.log(\"Configuration file '\" + jobConfig.getAbsolutePath() + \"' not available. Use default values.\");\n    }\n    String notification = \"no\";\n    if (store.getString(\"minimac.sendmail\") != null && !store.getString(\"minimac.sendmail\").equals(\"\")) {\n        notification = store.getString(\"minimac.sendmail\");\n    }\n    String serverUrl = \"https://imputationserver.sph.umich.edu\";\n    if (store.getString(\"server.url\") != null && !store.getString(\"server.url\").isEmpty()) {\n        serverUrl = store.getString(\"server.url\");\n    }\n    String sanityCheck = \"yes\";\n    if (store.getString(\"sanitycheck\") != null && !store.getString(\"sanitycheck\").equals(\"\")) {\n        sanityCheck = store.getString(\"sanitycheck\");\n    }\n    if (password == null || (password != null && password.equals(\"auto\"))) {\n        password = PasswordCreator.createPassword();\n    }\n    try {\n        context.beginTask(\"Export data...\");\n        // get sorted directories\n        List<String> folders = HdfsUtil.getDirectories(output);\n        ImputationResults imputationResults = new ImputationResults(folders, phasingOnly);\n        Map<String, ImputedChromosome> imputedChromosomes = imputationResults.getChromosomes();\n        Set<String> chromosomes = imputedChromosomes.keySet();\n        boolean lastChromosome = false;\n        int index = 0;\n        String checksumFilename = FileUtil.path(localOutput, \"results.md5\");\n        LineWriter writer = new LineWriter(checksumFilename);\n        for (String name : chromosomes) {\n            index++;\n            if (index == chromosomes.size()) {\n                lastChromosome = true;\n            }\n            ImputedChromosome imputedChromosome = imputedChromosomes.get(name);\n            context.println(\"Export and merge chromosome \" + name);\n            // create temp dir\n            String temp = FileUtil.path(localOutput, \"temp\");\n            FileUtil.createDirectory(temp);\n            // output files\n            ArrayList<File> files = new ArrayList<File>();\n            // merge info files\n            if (!phasingOnly) {\n                String infoOutput = FileUtil.path(temp, \"chr\" + name + \".info.gz\");\n                FileMerger.mergeAndGzInfo(imputedChromosome.getInfoFiles(), infoOutput);\n                files.add(new File(infoOutput));\n            }\n            // merge all dosage files\n            String dosageOutput;\n            if (phasingOnly) {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".phased.vcf.gz\");\n            } else {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".dose.vcf.gz\");\n            }\n            files.add(new File(dosageOutput));\n            MergedVcfFile vcfFile = new MergedVcfFile(dosageOutput);\n            vcfFile.addHeader(context, imputedChromosome.getHeaderFiles());\n            for (String file : imputedChromosome.getDataFiles()) {\n                context.println(\"Read file \" + file);\n                vcfFile.addFile(HdfsUtil.open(file));\n                HdfsUtil.delete(file);\n            }\n            vcfFile.close();\n            // merge all meta files\n            if (mergeMetaFiles) {\n                context.println(\"Merging meta files...\");\n                String dosageMetaOutput = FileUtil.path(temp, \"chr\" + name + \".empiricalDose.vcf.gz\");\n                MergedVcfFile vcfFileMeta = new MergedVcfFile(dosageMetaOutput);\n                String headerMetaFile = imputedChromosome.getHeaderMetaFiles().get(0);\n                context.println(\"Use header from file \" + headerMetaFile);\n                vcfFileMeta.addFile(HdfsUtil.open(headerMetaFile));\n                for (String file : imputedChromosome.getDataMetaFiles()) {\n                    context.println(\"Read file \" + file);\n                    vcfFileMeta.addFile(HdfsUtil.open(file));\n                    HdfsUtil.delete(file);\n                }\n                vcfFileMeta.close();\n                context.println(\"Meta files merged.\");\n                files.add(new File(dosageMetaOutput));\n            }\n            if (sanityCheck.equals(\"yes\") && lastChromosome) {\n                context.log(\"Run tabix on chromosome \" + name + \"...\");\n                Command tabix = new Command(FileUtil.path(workingDirectory, \"bin\", \"tabix\"));\n                tabix.setSilent(false);\n                tabix.setParams(\"-f\", dosageOutput);\n                if (tabix.execute() != 0) {\n                    context.endTask(\"Error during index creation: \" + tabix.getStdOut(), WorkflowContext.ERROR);\n                    return false;\n                }\n                context.log(\"Tabix done.\");\n            }\n            // create zip file\n            String fileName = \"chr_\" + name + \".zip\";\n            String filePath = FileUtil.path(localOutput, fileName);\n            File file = new File(filePath);\n            createEncryptedZipFile(file, files, password, aesEncryption);\n            // add checksum to hash file\n            context.log(\"Creating file checksum for \" + filePath);\n            long checksumStart = System.currentTimeMillis();\n            String checksum = FileChecksum.HashFile(new File(filePath), FileChecksum.Algorithm.MD5);\n            writer.write(checksum + \" \" + fileName);\n            long checksumEnd = (System.currentTimeMillis() - checksumStart) / 1000;\n            context.log(\"File checksum for \" + filePath + \" created in \" + checksumEnd + \" seconds.\");\n            // delete temp dir\n            FileUtil.deleteDirectory(temp);\n            IExternalWorkspace externalWorkspace = context.getExternalWorkspace();\n            if (externalWorkspace != null) {\n                long start = System.currentTimeMillis();\n                context.log(\"External Workspace '\" + externalWorkspace.getName() + \"' found\");\n                context.log(\"Start file upload: \" + filePath);\n                String url = externalWorkspace.upload(\"local\", file);\n                long end = (System.currentTimeMillis() - start) / 1000;\n                context.log(\"Upload finished in  \" + end + \" sec. File Location: \" + url);\n                context.log(\"Add \" + localOutput + \" to custom download\");\n                String size = FileUtils.byteCountToDisplaySize(file.length());\n                context.addDownload(\"local\", fileName, size, url);\n                FileUtil.deleteFile(filePath);\n                context.log(\"File deleted: \" + filePath);\n            } else {\n                context.log(\"No external Workspace set.\");\n            }\n        }\n        writer.close();\n        // delete temporary files\n        HdfsUtil.delete(output);\n        // Export calculated risk scores\n        if (pgsPanel != null) {\n            context.println(\"Exporting PGS scores...\");\n            String temp2 = FileUtil.path(localOutput, \"temp2\");\n            FileUtil.createDirectory(temp2);\n            List<String> scoreList = HdfsUtil.getFiles(outputScores);\n            String[] chunksScores = new String[scoreList.size() / 2];\n            String[] chunksReports = new String[scoreList.size() / 2];\n            int chunksScoresCount = 0;\n            int chunksReportsCount = 0;\n            for (String score : scoreList) {\n                String filename = FileUtil.getFilename(score);\n                String localPath = FileUtil.path(temp2, filename);\n                HdfsUtil.get(score, localPath);\n                if (score.endsWith(\".json\")) {\n                    chunksReports[chunksReportsCount] = localPath;\n                    chunksReportsCount++;\n                } else {\n                    chunksScores[chunksScoresCount] = localPath;\n                    chunksScoresCount++;\n                }\n            }\n            String outputFileScores = FileUtil.path(temp2, \"scores.txt\");\n            String outputFileReports = FileUtil.path(temp2, \"report.json\");\n            String outputFileHtml = FileUtil.path(localOutput, \"report.html\");\n            // disable ansi\n            TaskService.setAnsiSupport(false);\n            MergeScoreTask mergeScore = new MergeScoreTask();\n            mergeScore.setInputs(chunksScores);\n            mergeScore.setOutput(outputFileScores);\n            TaskService.run(mergeScore);\n            MergeReportTask mergeReport = new MergeReportTask();\n            mergeReport.setInputs(chunksReports);\n            mergeReport.setOutput(outputFileReports);\n            TaskService.run(mergeReport);\n            ReportFile report = mergeReport.getResult();\n            String folder = getFolder(CompressionEncryption.class);\n            MetaFile metaFile = MetaFile.load(FileUtil.path(folder, \"pgs-catalog.json\"));\n            report.mergeWithMeta(metaFile);\n            String fileName = \"scores.zip\";\n            String filePath = FileUtil.path(localOutput, fileName);\n            File file = new File(filePath);\n            createEncryptedZipFile(file, new File(outputFileScores), password, aesEncryption);\n            context.println(\"Exported PGS scores to \" + fileName + \".\");\n            FileUtil.deleteDirectory(temp2);\n        }\n        context.endTask(\"Exported data.\", WorkflowContext.OK);\n    } catch (Exception e) {\n        e.printStackTrace();\n        context.endTask(\"Data export failed: \" + e.getMessage(), WorkflowContext.ERROR);\n        return false;\n    }\n    // submit counters!\n    context.submitCounter(\"samples\");\n    context.submitCounter(\"genotypes\");\n    context.submitCounter(\"chromosomes\");\n    context.submitCounter(\"runs\");\n    // submit panel and phasing method counters\n    String reference = context.get(\"refpanel\");\n    String phasing = context.get(\"phasing\");\n    context.submitCounter(\"refpanel_\" + reference);\n    context.submitCounter(\"phasing_\" + phasing);\n    context.submitCounter(\"23andme-input\");\n    // send email\n    if (notification.equals(\"yes\")) {\n        Object mail = context.getData(\"cloudgene.user.mail\");\n        Object name = context.getData(\"cloudgene.user.name\");\n        if (mail != null) {\n            String subject = \"Job \" + context.getJobId() + \" is complete.\";\n            String message = \"Dear \" + name + \",\\nthe password for the imputation results is: \" + password + \"\\n\\nThe results can be downloaded from \" + serverUrl + \"/start.html#!jobs/\" + context.getJobId() + \"/results\";\n            try {\n                context.sendMail(subject, message);\n                context.ok(\"We have sent an email to <b>\" + mail + \"</b> with the password.\");\n                return true;\n            } catch (Exception e) {\n                context.error(\"Data compression failed: \" + e.getMessage());\n                return false;\n            }\n        } else {\n            context.error(\"No email address found. Please enter your email address (Account -> Profile).\");\n            return false;\n        }\n    } else {\n        context.ok(\"Email notification is disabled. All results are encrypted with password <b>\" + password + \"</b>\");\n        return true;\n    }\n}",
        "rpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    String workingDirectory = getFolder(CompressionEncryption.class);\n    String output = context.get(\"outputimputation\");\n    String outputScores = context.get(\"outputScores\");\n    String localOutput = context.get(\"local\");\n    String aesEncryptionValue = context.get(\"aesEncryption\");\n    String meta = context.get(\"meta\");\n    String mode = context.get(\"mode\");\n    String password = context.get(\"password\");\n    PgsPanel pgsPanel = PgsPanel.loadFromProperties(context.getData(\"pgsPanel\"));\n    boolean phasingOnly = false;\n    if (mode != null && mode.equals(\"phasing\")) {\n        phasingOnly = true;\n    }\n    boolean mergeMetaFiles = !phasingOnly && (meta != null && meta.equals(\"yes\"));\n    boolean aesEncryption = (aesEncryptionValue != null && aesEncryptionValue.equals(\"yes\"));\n    // read config if mails should be sent\n    String folderConfig = getFolder(CompressionEncryption.class);\n    File jobConfig = new File(FileUtil.path(folderConfig, \"job.config\"));\n    DefaultPreferenceStore store = new DefaultPreferenceStore();\n    if (jobConfig.exists()) {\n        store.load(jobConfig);\n    } else {\n        context.log(\"Configuration file '\" + jobConfig.getAbsolutePath() + \"' not available. Use default values.\");\n    }\n    String notification = \"no\";\n    if (store.getString(\"minimac.sendmail\") != null && !store.getString(\"minimac.sendmail\").equals(\"\")) {\n        notification = store.getString(\"minimac.sendmail\");\n    }\n    String serverUrl = \"https://imputationserver.sph.umich.edu\";\n    if (store.getString(\"server.url\") != null && !store.getString(\"server.url\").isEmpty()) {\n        serverUrl = store.getString(\"server.url\");\n    }\n    String sanityCheck = \"yes\";\n    if (store.getString(\"sanitycheck\") != null && !store.getString(\"sanitycheck\").equals(\"\")) {\n        sanityCheck = store.getString(\"sanitycheck\");\n    }\n    if (password == null || (password != null && password.equals(\"auto\"))) {\n        password = PasswordCreator.createPassword();\n    }\n    try {\n        context.beginTask(\"Export data...\");\n        // get sorted directories\n        List<String> folders = HdfsUtil.getDirectories(output);\n        ImputationResults imputationResults = new ImputationResults(folders, phasingOnly);\n        Map<String, ImputedChromosome> imputedChromosomes = imputationResults.getChromosomes();\n        Set<String> chromosomes = imputedChromosomes.keySet();\n        boolean lastChromosome = false;\n        int index = 0;\n        String checksumFilename = FileUtil.path(localOutput, \"results.md5\");\n        LineWriter writer = new LineWriter(checksumFilename);\n        for (String name : chromosomes) {\n            index++;\n            if (index == chromosomes.size()) {\n                lastChromosome = true;\n            }\n            ImputedChromosome imputedChromosome = imputedChromosomes.get(name);\n            context.println(\"Export and merge chromosome \" + name);\n            // create temp dir\n            String temp = FileUtil.path(localOutput, \"temp\");\n            FileUtil.createDirectory(temp);\n            // output files\n            ArrayList<File> files = new ArrayList<File>();\n            // merge info files\n            if (!phasingOnly) {\n                String infoOutput = FileUtil.path(temp, \"chr\" + name + \".info.gz\");\n                FileMerger.mergeAndGzInfo(imputedChromosome.getInfoFiles(), infoOutput);\n                files.add(new File(infoOutput));\n            }\n            // merge all dosage files\n            String dosageOutput;\n            if (phasingOnly) {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".phased.vcf.gz\");\n            } else {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".dose.vcf.gz\");\n            }\n            files.add(new File(dosageOutput));\n            MergedVcfFile vcfFile = new MergedVcfFile(dosageOutput);\n            vcfFile.addHeader(context, imputedChromosome.getHeaderFiles());\n            for (String file : imputedChromosome.getDataFiles()) {\n                context.println(\"Read file \" + file);\n                vcfFile.addFile(HdfsUtil.open(file));\n                HdfsUtil.delete(file);\n            }\n            vcfFile.close();\n            // merge all meta files\n            if (mergeMetaFiles) {\n                context.println(\"Merging meta files...\");\n                String dosageMetaOutput = FileUtil.path(temp, \"chr\" + name + \".empiricalDose.vcf.gz\");\n                MergedVcfFile vcfFileMeta = new MergedVcfFile(dosageMetaOutput);\n                String headerMetaFile = imputedChromosome.getHeaderMetaFiles().get(0);\n                context.println(\"Use header from file \" + headerMetaFile);\n                vcfFileMeta.addFile(HdfsUtil.open(headerMetaFile));\n                for (String file : imputedChromosome.getDataMetaFiles()) {\n                    context.println(\"Read file \" + file);\n                    vcfFileMeta.addFile(HdfsUtil.open(file));\n                    HdfsUtil.delete(file);\n                }\n                vcfFileMeta.close();\n                context.println(\"Meta files merged.\");\n                files.add(new File(dosageMetaOutput));\n            }\n            if (sanityCheck.equals(\"yes\") && lastChromosome) {\n                context.log(\"Run tabix on chromosome \" + name + \"...\");\n                Command tabix = new Command(FileUtil.path(workingDirectory, \"bin\", \"tabix\"));\n                tabix.setSilent(false);\n                tabix.setParams(\"-f\", dosageOutput);\n                if (tabix.execute() != 0) {\n                    context.endTask(\"Error during index creation: \" + tabix.getStdOut(), WorkflowContext.ERROR);\n                    return false;\n                }\n                context.log(\"Tabix done.\");\n            }\n            // create zip file\n            String fileName = \"chr_\" + name + \".zip\";\n            String filePath = FileUtil.path(localOutput, fileName);\n            File file = new File(filePath);\n            createEncryptedZipFile(file, files, password, aesEncryption);\n            // add checksum to hash file\n            context.log(\"Creating file checksum for \" + filePath);\n            long checksumStart = System.currentTimeMillis();\n            String checksum = FileChecksum.HashFile(new File(filePath), FileChecksum.Algorithm.MD5);\n            writer.write(checksum + \" \" + fileName);\n            long checksumEnd = (System.currentTimeMillis() - checksumStart) / 1000;\n            context.log(\"File checksum for \" + filePath + \" created in \" + checksumEnd + \" seconds.\");\n            // delete temp dir\n            FileUtil.deleteDirectory(temp);\n            IExternalWorkspace externalWorkspace = context.getExternalWorkspace();\n            if (externalWorkspace != null) {\n                long start = System.currentTimeMillis();\n                context.log(\"External Workspace '\" + externalWorkspace.getName() + \"' found\");\n                context.log(\"Start file upload: \" + filePath);\n                String url = externalWorkspace.upload(\"local\", file);\n                long end = (System.currentTimeMillis() - start) / 1000;\n                context.log(\"Upload finished in  \" + end + \" sec. File Location: \" + url);\n                context.log(\"Add \" + localOutput + \" to custom download\");\n                String size = FileUtils.byteCountToDisplaySize(file.length());\n                context.addDownload(\"local\", fileName, size, url);\n                FileUtil.deleteFile(filePath);\n                context.log(\"File deleted: \" + filePath);\n            } else {\n                context.log(\"No external Workspace set.\");\n            }\n        }\n        writer.close();\n        // delete temporary files\n        HdfsUtil.delete(output);\n        // Export calculated risk scores\n        if (pgsPanel != null) {\n            context.println(\"Exporting PGS scores...\");\n            String temp2 = FileUtil.path(localOutput, \"temp2\");\n            FileUtil.createDirectory(temp2);\n            List<String> scoreList = HdfsUtil.getFiles(outputScores);\n            String[] chunksScores = new String[scoreList.size() / 2];\n            String[] chunksReports = new String[scoreList.size() / 2];\n            int chunksScoresCount = 0;\n            int chunksReportsCount = 0;\n            for (String score : scoreList) {\n                String filename = FileUtil.getFilename(score);\n                String localPath = FileUtil.path(temp2, filename);\n                HdfsUtil.get(score, localPath);\n                if (score.endsWith(\".json\")) {\n                    chunksReports[chunksReportsCount] = localPath;\n                    chunksReportsCount++;\n                } else {\n                    chunksScores[chunksScoresCount] = localPath;\n                    chunksScoresCount++;\n                }\n            }\n            String outputFileScores = FileUtil.path(temp2, \"scores.txt\");\n            String outputFileReports = FileUtil.path(temp2, \"report.json\");\n            String outputFileHtml = FileUtil.path(localOutput, \"scores.html\");\n            // disable ansi\n            TaskService.setAnsiSupport(false);\n            MergeScoreTask mergeScore = new MergeScoreTask();\n            mergeScore.setInputs(chunksScores);\n            mergeScore.setOutput(outputFileScores);\n            TaskService.run(mergeScore);\n            MergeReportTask mergeReport = new MergeReportTask();\n            mergeReport.setInputs(chunksReports);\n            mergeReport.setOutput(outputFileReports);\n            TaskService.run(mergeReport);\n            ReportFile report = mergeReport.getResult();\n            String folder = getFolder(CompressionEncryption.class);\n            MetaFile metaFile = MetaFile.load(FileUtil.path(folder, \"pgs-catalog.json\"));\n            report.mergeWithMeta(metaFile);\n            CreateHtmlReportTask htmlReportTask = new CreateHtmlReportTask();\n            htmlReportTask.setApplicationName(\"PGS Server\");\n            htmlReportTask.setVersion(ImputationPipeline.PIPELINE_VERSION + \" - BETA\");\n            htmlReportTask.setShowCommand(false);\n            htmlReportTask.setReport(report);\n            htmlReportTask.setOutput(outputFileHtml);\n            TaskService.run(htmlReportTask);\n            context.println(\"Created html report \" + outputFileHtml + \".\");\n            String fileName = \"scores.zip\";\n            String filePath = FileUtil.path(localOutput, fileName);\n            File file = new File(filePath);\n            createEncryptedZipFile(file, new File(outputFileScores), password, aesEncryption);\n            context.println(\"Exported PGS scores to \" + fileName + \".\");\n            FileUtil.deleteDirectory(temp2);\n        }\n        context.endTask(\"Exported data.\", WorkflowContext.OK);\n    } catch (Exception e) {\n        e.printStackTrace();\n        context.endTask(\"Data export failed: \" + e.getMessage(), WorkflowContext.ERROR);\n        return false;\n    }\n    // submit counters!\n    context.submitCounter(\"samples\");\n    context.submitCounter(\"genotypes\");\n    context.submitCounter(\"chromosomes\");\n    context.submitCounter(\"runs\");\n    // submit panel and phasing method counters\n    String reference = context.get(\"refpanel\");\n    String phasing = context.get(\"phasing\");\n    context.submitCounter(\"refpanel_\" + reference);\n    context.submitCounter(\"phasing_\" + phasing);\n    context.submitCounter(\"23andme-input\");\n    // send email\n    if (notification.equals(\"yes\")) {\n        Object mail = context.getData(\"cloudgene.user.mail\");\n        Object name = context.getData(\"cloudgene.user.name\");\n        if (mail != null) {\n            String subject = \"Job \" + context.getJobId() + \" is complete.\";\n            String message = \"Dear \" + name + \",\\nthe password for the imputation results is: \" + password + \"\\n\\nThe results can be downloaded from \" + serverUrl + \"/start.html#!jobs/\" + context.getJobId() + \"/results\";\n            try {\n                context.sendMail(subject, message);\n                context.ok(\"We have sent an email to <b>\" + mail + \"</b> with the password.\");\n                return true;\n            } catch (Exception e) {\n                context.error(\"Data compression failed: \" + e.getMessage());\n                return false;\n            }\n        } else {\n            context.error(\"No email address found. Please enter your email address (Account -> Profile).\");\n            return false;\n        }\n    } else {\n        context.ok(\"Email notification is disabled. All results are encrypted with password <b>\" + password + \"</b>\");\n        return true;\n    }\n}",
        "tuc": "@Test\npublic void testPipelineWithEagleAndScores() throws IOException, ZipException {\n    String configFolder = \"test-data/configs/hapmap-chr20\";\n    String inputFolder = \"test-data/data/chr20-unphased\";\n    String score1 = PGSCatalog.getFilenameById(\"PGS000018\");\n    String score2 = PGSCatalog.getFilenameById(\"PGS000027\");\n    String targetScore1 = HdfsUtil.path(\"scores-hdfs\", \"PGS000018.txt.gz\");\n    HdfsUtil.put(score1, targetScore1);\n    String targetScore2 = HdfsUtil.path(\"scores-hdfs\", \"PGS000027.txt.gz\");\n    HdfsUtil.put(score2, targetScore2);\n    WorkflowTestContext context = buildContext(inputFolder, \"hapmap2\");\n    context.setOutput(\"outputScores\", \"cloudgene2-hdfs\");\n    Map<String, Object> pgsPanel = new HashMap<String, Object>();\n    List<String> scores = new Vector<String>();\n    scores.add(\"PGS000018.txt.gz\");\n    scores.add(\"PGS000027.txt.gz\");\n    pgsPanel.put(\"location\", \"scores-hdfs\");\n    pgsPanel.put(\"scores\", scores);\n    pgsPanel.put(\"build\", \"hg19\");\n    context.setData(\"pgsPanel\", pgsPanel);\n    InputValidation inputValidation = new InputValidationMock(configFolder);\n    boolean result = run(context, inputValidation);\n    assertTrue(result);\n    QcStatisticsMock qcStats = new QcStatisticsMock(configFolder);\n    result = run(context, qcStats);\n    assertTrue(result);\n    assertTrue(context.hasInMemory(\"Remaining sites in total: 7,735\"));\n    importRefPanel(FileUtil.path(configFolder, \"ref-panels\"));\n    importBinaries(\"files/bin\");\n    ImputationMinimac3Mock imputation = new ImputationMinimac3Mock(configFolder);\n    result = run(context, imputation);\n    assertTrue(result);\n    CompressionEncryptionMock export = new CompressionEncryptionMock(\"files\");\n    result = run(context, export);\n    assertTrue(result);\n    ZipFile zipFile = new ZipFile(\"test-data/tmp/local/chr_20.zip\", PASSWORD.toCharArray());\n    zipFile.extractAll(\"test-data/tmp\");\n    VcfFile file = VcfFileUtil.load(\"test-data/tmp/chr20.dose.vcf.gz\", 100000000, false);\n    assertEquals(\"20\", file.getChromosome());\n    assertEquals(51, file.getNoSamples());\n    assertEquals(true, file.isPhased());\n    assertEquals(TOTAL_REFPANEL_CHR20_B37, file.getNoSnps());\n    int snpInInfo = getLineCount(\"test-data/tmp/chr20.info.gz\") - 1;\n    assertEquals(snpInInfo, file.getNoSnps());\n    String[] args = { \"test-data/tmp/chr20.dose.vcf.gz\", \"--ref\", \"PGS000018,PGS000027\", \"--out\", \"test-data/tmp/expected.txt\" };\n    int resultScore = new CommandLine(new ApplyScoreCommand()).execute(args);\n    assertEquals(0, resultScore);\n    zipFile = new ZipFile(\"test-data/tmp/local/scores.zip\", PASSWORD.toCharArray());\n    zipFile.extractAll(\"test-data/tmp\");\n    CsvTableReader readerExpected = new CsvTableReader(\"test-data/tmp/expected.txt\", ',');\n    CsvTableReader readerActual = new CsvTableReader(\"test-data/tmp/scores.txt\", ',');\n    while (readerExpected.next() && readerActual.next()) {\n        assertEquals(readerExpected.getDouble(\"PGS000018\"), readerActual.getDouble(\"PGS000018\"), 0.00001);\n        assertEquals(readerExpected.getDouble(\"PGS000027\"), readerActual.getDouble(\"PGS000027\"), 0.00001);\n    }\n    readerExpected.close();\n    readerActual.close();\n    FileUtil.deleteDirectory(\"test-data/tmp\");\n}",
        "label": 1
    },
    {
        "repo_name": "genepi___imputationserver",
        "commit": "d854b31d27e80cfd9316fc2ab92e1d7dedadfc81",
        "commit_message": "Check if score parameter is null and adapt test case",
        "p_path": "src/main/java/genepi/imputationserver/steps/Imputation.java",
        "t_path": "src/test/java/genepi/imputationserver/steps/ImputationTest.java",
        "p_name": "run",
        "t_name": "testPipelineWithEagle",
        "lpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    final String folder = getFolder(Imputation.class);\n    String input = context.get(\"chunkFileDir\");\n    String reference = context.get(\"refpanel\");\n    String binariesHDFS = context.getConfig(\"binaries\");\n    String mode = context.get(\"mode\");\n    String phasing = context.get(\"phasing\");\n    PgsPanel pgsPanel = PgsPanel.loadFromProperties(context.getData(\"pgsPanel\"));\n    String r2Filter = context.get(\"r2Filter\");\n    if (r2Filter == null) {\n        r2Filter = \"0\";\n    }\n    output = context.get(\"outputimputation\");\n    outputScores = context.get(\"outputScores\");\n    String log = context.get(\"logfile\");\n    if (!(new File(input)).exists()) {\n        context.error(\"No chunks passed the QC step.\");\n        return false;\n    }\n    RefPanelList panels = RefPanelList.loadFromFile(FileUtil.path(folder, RefPanelList.FILENAME));\n    RefPanel panel = null;\n    try {\n        panel = panels.getById(reference, context.getData(\"refpanel\"));\n        if (panel == null) {\n            context.error(\"reference panel '\" + reference + \"' not found.\");\n            return false;\n        }\n    } catch (Exception e) {\n        context.error(\"Unable to parse reference panel '\" + reference + \"': \" + e.getMessage());\n        return false;\n    }\n    context.println(\"Reference Panel: \");\n    context.println(\"  Name: \" + reference);\n    context.println(\"  ID: \" + panel.getId());\n    context.println(\"  Build: \" + panel.getBuild());\n    context.println(\"  Location: \" + panel.getHdfs());\n    context.println(\"  Legend: \" + panel.getLegend());\n    context.println(\"  Version: \" + panel.getVersion());\n    context.println(\"  Eagle Map: \" + panel.getMapEagle());\n    context.println(\"  Eagle BCFs: \" + panel.getRefEagle());\n    context.println(\"  Beagle Bref3: \" + panel.getRefBeagle());\n    context.println(\"  Beagle Map: \" + panel.getMapBeagle());\n    context.println(\"  Minimac Map: \" + panel.getMapMinimac());\n    context.println(\"  Populations:\");\n    for (Map.Entry<String, String> entry : panel.getPopulations().entrySet()) {\n        context.println(\"    \" + entry.getKey() + \"/\" + entry.getValue());\n    }\n    context.println(\"  Samples:\");\n    for (Map.Entry<String, String> entry : panel.getSamples().entrySet()) {\n        context.println(\"    \" + entry.getKey() + \"/\" + entry.getValue());\n    }\n    if (panel.getQcFilter() != null) {\n        context.println(\"  QC Filters:\");\n        for (Map.Entry<String, String> entry : panel.getQcFilter().entrySet()) {\n            context.println(\"    \" + entry.getKey() + \"/\" + entry.getValue());\n        }\n    }\n    if (pgsPanel != null) {\n        context.println(\"  PGS: \" + pgsPanel.getScores().size() + \" scores\");\n    } else {\n        context.println(\"  PGS: no scores selected\");\n    }\n    try {\n        String[] chunkFiles = FileUtil.getFiles(input, \"*.*\");\n        context.beginTask(\"Start Imputation...\");\n        if (chunkFiles.length == 0) {\n            context.error(\"<br><b>Error:</b> No chunks found. Imputation cannot be started!\");\n            return false;\n        }\n        for (String chunkFile : chunkFiles) {\n            String[] tiles = chunkFile.split(\"/\");\n            String chr = tiles[tiles.length - 1];\n            ChunkFileConverterResult result = convertChunkfile(chunkFile, context.getHdfsTemp());\n            ImputationJob job = new ImputationJob(context.getJobId() + \"-chr-\" + chr, new ContextLog(context)) {\n                @Override\n                protected void readConfigFile() {\n                    File file = new File(folder + \"/\" + CONFIG_FILE);\n                    DefaultPreferenceStore preferenceStore = new DefaultPreferenceStore();\n                    if (file.exists()) {\n                        log.info(\"Loading distributed configuration file '\" + file.getAbsolutePath() + \"'...\");\n                        preferenceStore.load(file);\n                    } else {\n                        log.info(\"Configuration file '\" + file.getAbsolutePath() + \"' not available. Use default values\");\n                    }\n                    preferenceStore.write(getConfiguration());\n                    for (Object key : preferenceStore.getKeys()) {\n                        log.info(\"  \" + key + \": \" + preferenceStore.getString(key.toString()));\n                    }\n                }\n            };\n            job.setBinariesHDFS(binariesHDFS);\n            String hdfsFilenameChromosome = resolvePattern(panel.getHdfs(), chr);\n            job.setRefPanelHdfs(hdfsFilenameChromosome);\n            job.setR2Filter(r2Filter);\n            job.setBuild(panel.getBuild());\n            if (panel.getMapMinimac() != null) {\n                context.println(\"Setting up minimac map file...\");\n                job.setMapMinimac(panel.getMapMinimac());\n            } else {\n                context.println(\"Reference panel has no minimac map file.\");\n            }\n            if (result.needsPhasing) {\n                context.println(\"Input data is unphased.\");\n                if (phasing.equals(\"beagle\")) {\n                    context.println(\"  Setting up beagle reference and map files...\");\n                    String refBeagleFilenameChromosome = resolvePattern(panel.getRefBeagle(), chr);\n                    String mapBeagleFilenameChromosome = resolvePattern(panel.getMapBeagle(), chr);\n                    job.setRefBeagleHdfs(refBeagleFilenameChromosome);\n                    job.setMapBeagleHdfs(mapBeagleFilenameChromosome);\n                } else {\n                    if (!panel.checkEagleMap()) {\n                        context.error(\"Eagle map file not found.\");\n                        return false;\n                    }\n                    context.println(\"  Setting up eagle reference and map files...\");\n                    job.setMapEagleHdfs(panel.getMapEagle());\n                    String refEagleFilenameChromosome = resolvePattern(panel.getRefEagle(), chr);\n                    job.setRefEagleHdfs(refEagleFilenameChromosome);\n                }\n            } else {\n                context.println(\"Input data is phased.\");\n            }\n            if (mode != null && mode.equals(\"phasing\")) {\n                job.setPhasingOnly(\"true\");\n            } else {\n                job.setPhasingOnly(\"false\");\n            }\n            job.setPhasingEngine(phasing);\n            job.setInput(result.filename);\n            job.setOutput(HdfsUtil.path(output, chr));\n            job.setOutputScores(outputScores);\n            if (pgsPanel != null) {\n                job.setScores(pgsPanel.getScores());\n            }\n            job.setRefPanel(reference);\n            job.setLogFilename(FileUtil.path(log, \"chr_\" + chr + \".log\"));\n            job.setJarByClass(ImputationJob.class);\n            executeJarInBackground(chr, context, job);\n            jobs.put(chr, job);\n        }\n        waitForAll();\n        running = false;\n        context.println(\"All jobs terminated.\");\n        if (error) {\n            context.println(\"Imputation on chromosome \" + errorChr + \" failed. Imputation was stopped.\");\n            updateProgress();\n            String text = updateMessage();\n            context.endTask(text, WorkflowContext.ERROR);\n            printSummary();\n            context.error(\"Imputation on chromosome \" + errorChr + \" failed. Imputation was stopped.\");\n            return false;\n        }\n        if (isCanceled()) {\n            context.println(\"Canceled by user.\");\n            updateProgress();\n            String text = updateMessage();\n            context.endTask(text, WorkflowContext.ERROR);\n            printSummary();\n            context.error(\"Canceled by user.\");\n            return false;\n        }\n        updateProgress();\n        printSummary();\n        String text = updateMessage();\n        context.endTask(text, ok ? WorkflowContext.OK : WorkflowContext.ERROR);\n        return ok;\n    } catch (Exception e) {\n        updateProgress();\n        printSummary();\n        e.printStackTrace();\n        context.updateTask(e.getMessage(), WorkflowContext.ERROR);\n        return false;\n    }\n}",
        "rpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    final String folder = getFolder(Imputation.class);\n    String input = context.get(\"chunkFileDir\");\n    String reference = context.get(\"refpanel\");\n    String binariesHDFS = context.getConfig(\"binaries\");\n    String mode = context.get(\"mode\");\n    String phasing = context.get(\"phasing\");\n    PgsPanel pgsPanel = PgsPanel.loadFromProperties(context.getData(\"pgsPanel\"));\n    String r2Filter = context.get(\"r2Filter\");\n    if (r2Filter == null) {\n        r2Filter = \"0\";\n    }\n    output = context.get(\"outputimputation\");\n    outputScores = context.get(\"outputScores\");\n    String log = context.get(\"logfile\");\n    if (!(new File(input)).exists()) {\n        context.error(\"No chunks passed the QC step.\");\n        return false;\n    }\n    RefPanelList panels = RefPanelList.loadFromFile(FileUtil.path(folder, RefPanelList.FILENAME));\n    RefPanel panel = null;\n    try {\n        panel = panels.getById(reference, context.getData(\"refpanel\"));\n        if (panel == null) {\n            context.error(\"reference panel '\" + reference + \"' not found.\");\n            return false;\n        }\n    } catch (Exception e) {\n        context.error(\"Unable to parse reference panel '\" + reference + \"': \" + e.getMessage());\n        return false;\n    }\n    context.println(\"Reference Panel: \");\n    context.println(\"  Name: \" + reference);\n    context.println(\"  ID: \" + panel.getId());\n    context.println(\"  Build: \" + panel.getBuild());\n    context.println(\"  Location: \" + panel.getHdfs());\n    context.println(\"  Legend: \" + panel.getLegend());\n    context.println(\"  Version: \" + panel.getVersion());\n    context.println(\"  Eagle Map: \" + panel.getMapEagle());\n    context.println(\"  Eagle BCFs: \" + panel.getRefEagle());\n    context.println(\"  Beagle Bref3: \" + panel.getRefBeagle());\n    context.println(\"  Beagle Map: \" + panel.getMapBeagle());\n    context.println(\"  Minimac Map: \" + panel.getMapMinimac());\n    context.println(\"  Populations:\");\n    for (Map.Entry<String, String> entry : panel.getPopulations().entrySet()) {\n        context.println(\"    \" + entry.getKey() + \"/\" + entry.getValue());\n    }\n    context.println(\"  Samples:\");\n    for (Map.Entry<String, String> entry : panel.getSamples().entrySet()) {\n        context.println(\"    \" + entry.getKey() + \"/\" + entry.getValue());\n    }\n    if (panel.getQcFilter() != null) {\n        context.println(\"  QC Filters:\");\n        for (Map.Entry<String, String> entry : panel.getQcFilter().entrySet()) {\n            context.println(\"    \" + entry.getKey() + \"/\" + entry.getValue());\n        }\n    }\n    if (pgsPanel != null) {\n        context.println(\"  PGS: \" + pgsPanel.getScores().size() + \" scores\");\n    } else {\n        context.println(\"  PGS: no scores selected\");\n    }\n    try {\n        String[] chunkFiles = FileUtil.getFiles(input, \"*.*\");\n        context.beginTask(\"Start Imputation...\");\n        if (chunkFiles.length == 0) {\n            context.error(\"<br><b>Error:</b> No chunks found. Imputation cannot be started!\");\n            return false;\n        }\n        for (String chunkFile : chunkFiles) {\n            String[] tiles = chunkFile.split(\"/\");\n            String chr = tiles[tiles.length - 1];\n            ChunkFileConverterResult result = convertChunkfile(chunkFile, context.getHdfsTemp());\n            ImputationJob job = new ImputationJob(context.getJobId() + \"-chr-\" + chr, new ContextLog(context)) {\n                @Override\n                protected void readConfigFile() {\n                    File file = new File(folder + \"/\" + CONFIG_FILE);\n                    DefaultPreferenceStore preferenceStore = new DefaultPreferenceStore();\n                    if (file.exists()) {\n                        log.info(\"Loading distributed configuration file '\" + file.getAbsolutePath() + \"'...\");\n                        preferenceStore.load(file);\n                    } else {\n                        log.info(\"Configuration file '\" + file.getAbsolutePath() + \"' not available. Use default values\");\n                    }\n                    preferenceStore.write(getConfiguration());\n                    for (Object key : preferenceStore.getKeys()) {\n                        log.info(\"  \" + key + \": \" + preferenceStore.getString(key.toString()));\n                    }\n                }\n            };\n            job.setBinariesHDFS(binariesHDFS);\n            String hdfsFilenameChromosome = resolvePattern(panel.getHdfs(), chr);\n            job.setRefPanelHdfs(hdfsFilenameChromosome);\n            job.setR2Filter(r2Filter);\n            job.setBuild(panel.getBuild());\n            if (panel.getMapMinimac() != null) {\n                context.println(\"Setting up minimac map file...\");\n                job.setMapMinimac(panel.getMapMinimac());\n            } else {\n                context.println(\"Reference panel has no minimac map file.\");\n            }\n            if (result.needsPhasing) {\n                context.println(\"Input data is unphased.\");\n                if (phasing.equals(\"beagle\")) {\n                    context.println(\"  Setting up beagle reference and map files...\");\n                    String refBeagleFilenameChromosome = resolvePattern(panel.getRefBeagle(), chr);\n                    String mapBeagleFilenameChromosome = resolvePattern(panel.getMapBeagle(), chr);\n                    job.setRefBeagleHdfs(refBeagleFilenameChromosome);\n                    job.setMapBeagleHdfs(mapBeagleFilenameChromosome);\n                } else {\n                    if (!panel.checkEagleMap()) {\n                        context.error(\"Eagle map file not found.\");\n                        return false;\n                    }\n                    context.println(\"  Setting up eagle reference and map files...\");\n                    job.setMapEagleHdfs(panel.getMapEagle());\n                    String refEagleFilenameChromosome = resolvePattern(panel.getRefEagle(), chr);\n                    job.setRefEagleHdfs(refEagleFilenameChromosome);\n                }\n            } else {\n                context.println(\"Input data is phased.\");\n            }\n            if (mode != null && mode.equals(\"phasing\")) {\n                job.setPhasingOnly(\"true\");\n            } else {\n                job.setPhasingOnly(\"false\");\n            }\n            job.setPhasingEngine(phasing);\n            job.setInput(result.filename);\n            job.setOutput(HdfsUtil.path(output, chr));\n            if (outputScores != null) {\n                job.setOutputScores(outputScores);\n            }\n            if (pgsPanel != null) {\n                job.setScores(pgsPanel.getScores());\n            }\n            job.setRefPanel(reference);\n            job.setLogFilename(FileUtil.path(log, \"chr_\" + chr + \".log\"));\n            job.setJarByClass(ImputationJob.class);\n            executeJarInBackground(chr, context, job);\n            jobs.put(chr, job);\n        }\n        waitForAll();\n        running = false;\n        context.println(\"All jobs terminated.\");\n        if (error) {\n            context.println(\"Imputation on chromosome \" + errorChr + \" failed. Imputation was stopped.\");\n            updateProgress();\n            String text = updateMessage();\n            context.endTask(text, WorkflowContext.ERROR);\n            printSummary();\n            context.error(\"Imputation on chromosome \" + errorChr + \" failed. Imputation was stopped.\");\n            return false;\n        }\n        if (isCanceled()) {\n            context.println(\"Canceled by user.\");\n            updateProgress();\n            String text = updateMessage();\n            context.endTask(text, WorkflowContext.ERROR);\n            printSummary();\n            context.error(\"Canceled by user.\");\n            return false;\n        }\n        updateProgress();\n        printSummary();\n        String text = updateMessage();\n        context.endTask(text, ok ? WorkflowContext.OK : WorkflowContext.ERROR);\n        return ok;\n    } catch (Exception e) {\n        updateProgress();\n        printSummary();\n        e.printStackTrace();\n        context.updateTask(e.getMessage(), WorkflowContext.ERROR);\n        return false;\n    }\n}",
        "tuc": "@Test\npublic void testPipelineWithEagle() throws IOException, ZipException {\n    String configFolder = \"test-data/configs/hapmap-chr20\";\n    String inputFolder = \"test-data/data/chr20-unphased\";\n    WorkflowTestContext context = buildContext(inputFolder, \"hapmap2\");\n    QcStatisticsMock qcStats = new QcStatisticsMock(configFolder);\n    boolean result = run(context, qcStats);\n    assertTrue(result);\n    assertTrue(context.hasInMemory(\"Remaining sites in total: 7,735\"));\n    importRefPanel(FileUtil.path(configFolder, \"ref-panels\"));\n    importBinaries(\"files/bin\");\n    ImputationMinimac3Mock imputation = new ImputationMinimac3Mock(configFolder);\n    result = run(context, imputation);\n    assertTrue(result);\n    CompressionEncryptionMock export = new CompressionEncryptionMock(\"files\");\n    result = run(context, export);\n    assertTrue(result);\n    ZipFile zipFile = new ZipFile(\"test-data/tmp/local/chr_20.zip\", PASSWORD.toCharArray());\n    zipFile.extractAll(\"test-data/tmp\");\n    VcfFile file = VcfFileUtil.load(\"test-data/tmp/chr20.dose.vcf.gz\", 100000000, false);\n    assertEquals(\"20\", file.getChromosome());\n    assertEquals(51, file.getNoSamples());\n    assertEquals(true, file.isPhased());\n    assertEquals(TOTAL_REFPANEL_CHR20_B37, file.getNoSnps());\n    int snpInInfo = getLineCount(\"test-data/tmp/chr20.info.gz\") - 1;\n    assertEquals(snpInInfo, file.getNoSnps());\n    FileUtil.deleteDirectory(\"test-data/tmp\");\n}",
        "label": 1
    },
    {
        "repo_name": "genepi___imputationserver",
        "commit": "2d383cdf1f0f204da59c220a1bb27c53e29aa122",
        "commit_message": "Encrypt PGS score result",
        "p_path": "src/main/java/genepi/imputationserver/steps/CompressionEncryption.java",
        "t_path": "src/test/java/genepi/imputationserver/steps/ImputationTest.java",
        "p_name": "run",
        "t_name": "testPipelineWithEagleAndScores",
        "lpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    String workingDirectory = getFolder(CompressionEncryption.class);\n    String output = context.get(\"outputimputation\");\n    String outputScores = context.get(\"outputScores\");\n    String localOutput = context.get(\"local\");\n    String aesEncryption = context.get(\"aesEncryption\");\n    String mode = context.get(\"mode\");\n    String password = context.get(\"password\");\n    PgsPanel pgsPanel = PgsPanel.loadFromProperties(context.getData(\"pgsPanel\"));\n    boolean phasingOnly = false;\n    if (mode != null && mode.equals(\"phasing\")) {\n        phasingOnly = true;\n    }\n    // read config if mails should be sent\n    String folderConfig = getFolder(CompressionEncryption.class);\n    File jobConfig = new File(FileUtil.path(folderConfig, \"job.config\"));\n    DefaultPreferenceStore store = new DefaultPreferenceStore();\n    if (jobConfig.exists()) {\n        store.load(jobConfig);\n    } else {\n        context.log(\"Configuration file '\" + jobConfig.getAbsolutePath() + \"' not available. Use default values.\");\n    }\n    String notification = \"no\";\n    if (store.getString(\"minimac.sendmail\") != null && !store.getString(\"minimac.sendmail\").equals(\"\")) {\n        notification = store.getString(\"minimac.sendmail\");\n    }\n    String serverUrl = \"https://imputationserver.sph.umich.edu\";\n    if (store.getString(\"server.url\") != null && !store.getString(\"server.url\").isEmpty()) {\n        serverUrl = store.getString(\"server.url\");\n    }\n    String sanityCheck = \"yes\";\n    if (store.getString(\"sanitycheck\") != null && !store.getString(\"sanitycheck\").equals(\"\")) {\n        sanityCheck = store.getString(\"sanitycheck\");\n    }\n    if (password == null || (password != null && password.equals(\"auto\"))) {\n        password = PasswordCreator.createPassword();\n    }\n    try {\n        context.beginTask(\"Export data...\");\n        // get sorted directories\n        List<String> folders = HdfsUtil.getDirectories(output);\n        Map<String, ExportObject> chromosomes = new HashMap<String, ExportObject>();\n        for (String folder : folders) {\n            String name = FileUtil.getFilename(folder);\n            context.println(\"Prepare files for chromosome \" + name);\n            List<String> data = new Vector<String>();\n            List<String> header = new Vector<String>();\n            List<String> info = new Vector<String>();\n            header = findFiles(folder, \".header.dose.vcf.gz\");\n            if (phasingOnly) {\n                data = findFiles(folder, \".phased.vcf.gz\");\n            } else {\n                data = findFiles(folder, \".data.dose.vcf.gz\");\n                info = findFiles(folder, \".info\");\n            }\n            // combine all X. to one folder\n            if (name.startsWith(\"X.\")) {\n                name = \"X\";\n            }\n            ExportObject export = chromosomes.get(name);\n            if (export == null) {\n                export = new ExportObject();\n            }\n            ArrayList<String> currentDataList = export.getDataFiles();\n            currentDataList.addAll(data);\n            export.setDataFiles(currentDataList);\n            ArrayList<String> currentHeaderList = export.getHeaderFiles();\n            currentHeaderList.addAll(header);\n            export.setHeaderFiles(currentHeaderList);\n            ArrayList<String> currentInfoList = export.getInfoFiles();\n            currentInfoList.addAll(info);\n            export.setInfoFiles(currentInfoList);\n            chromosomes.put(name, export);\n        }\n        Set<String> chromosomesSet = chromosomes.keySet();\n        boolean lastChromosome = false;\n        int index = 0;\n        for (String name : chromosomesSet) {\n            index++;\n            if (index == chromosomesSet.size()) {\n                lastChromosome = true;\n            }\n            ExportObject entry = chromosomes.get(name);\n            context.println(\"Export and merge chromosome \" + name);\n            // resort for chrX only\n            if (name.equals(\"X\")) {\n                Collections.sort(entry.getDataFiles(), new ChrXComparator());\n                Collections.sort(entry.getInfoFiles(), new ChrXComparator());\n            }\n            // create temp fir\n            String temp = FileUtil.path(localOutput, \"temp\");\n            FileUtil.createDirectory(temp);\n            // output files\n            String dosageOutput;\n            String infoOutput = \"\";\n            if (phasingOnly) {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".phased.vcf.gz\");\n            } else {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".dose.vcf.gz\");\n                infoOutput = FileUtil.path(temp, \"chr\" + name + \".info.gz\");\n                FileMerger.mergeAndGzInfo(entry.getInfoFiles(), infoOutput);\n            }\n            MergedVcfFile vcfFile = new MergedVcfFile(dosageOutput);\n            // simple header check\n            String headerLine = null;\n            for (String file : entry.getHeaderFiles()) {\n                context.println(\"Read header file \" + file);\n                LineReader reader = null;\n                try {\n                    reader = new LineReader(HdfsUtil.open(file));\n                    while (reader.next()) {\n                        String line = reader.get();\n                        if (line.startsWith(\"#CHROM\")) {\n                            if (headerLine != null) {\n                                if (headerLine.equals(line)) {\n                                    context.println(\"  Header is the same as header of first file.\");\n                                } else {\n                                    context.println(\"  ERROR: Header is different as header of first file.\");\n                                    context.println(headerLine);\n                                    context.println(line);\n                                    throw new Exception(\"Different sample order in chunks.\");\n                                }\n                            } else {\n                                headerLine = line;\n                                vcfFile.addFile(HdfsUtil.open(file));\n                                context.println(\"  Keep this header as first header.\");\n                            }\n                        }\n                    }\n                    if (reader != null) {\n                        reader.close();\n                    }\n                    HdfsUtil.delete(file);\n                } catch (Exception e) {\n                    if (reader != null) {\n                        reader.close();\n                    }\n                    StringWriter errors = new StringWriter();\n                    e.printStackTrace(new PrintWriter(errors));\n                    context.println(\"Error reading header file: \" + errors.toString());\n                }\n            }\n            if (headerLine == null || headerLine.trim().isEmpty()) {\n                throw new Exception(\"No valid header file found\");\n            }\n            // add data files\n            for (String file : entry.getDataFiles()) {\n                context.println(\"Read file \" + file);\n                vcfFile.addFile(HdfsUtil.open(file));\n                HdfsUtil.delete(file);\n            }\n            vcfFile.close();\n            if (sanityCheck.equals(\"yes\") && lastChromosome) {\n                context.log(\"Run tabix on chromosome \" + name + \"...\");\n                Command tabix = new Command(FileUtil.path(workingDirectory, \"bin\", \"tabix\"));\n                tabix.setSilent(false);\n                tabix.setParams(\"-f\", dosageOutput);\n                if (tabix.execute() != 0) {\n                    context.endTask(\"Error during index creation: \" + tabix.getStdOut(), WorkflowContext.ERROR);\n                    return false;\n                }\n                context.log(\"Tabix done.\");\n            }\n            ZipParameters param = new ZipParameters();\n            param.setEncryptFiles(true);\n            param.setEncryptionMethod(EncryptionMethod.ZIP_STANDARD);\n            if (aesEncryption != null && aesEncryption.equals(\"yes\")) {\n                param.setEncryptionMethod(EncryptionMethod.AES);\n                param.setAesKeyStrength(AesKeyStrength.KEY_STRENGTH_256);\n                param.setCompressionMethod(CompressionMethod.DEFLATE);\n                param.setCompressionLevel(CompressionLevel.NORMAL);\n            }\n            // create zip file\n            ArrayList<File> files = new ArrayList<File>();\n            files.add(new File(dosageOutput));\n            if (!phasingOnly) {\n                files.add(new File(infoOutput));\n            }\n            String fileName = \"chr_\" + name + \".zip\";\n            String filePath = FileUtil.path(localOutput, fileName);\n            ZipFile file = new ZipFile(new File(filePath), password.toCharArray());\n            file.addFiles(files, param);\n            // delete temp dir\n            FileUtil.deleteDirectory(temp);\n            IExternalWorkspace externalWorkspace = context.getExternalWorkspace();\n            if (externalWorkspace != null) {\n                long start = System.currentTimeMillis();\n                context.log(\"External Workspace '\" + externalWorkspace.getName() + \"' found\");\n                context.log(\"Start file upload: \" + filePath);\n                String url = externalWorkspace.upload(\"local\", file.getFile());\n                long end = (System.currentTimeMillis() - start) / 1000;\n                context.log(\"Upload finished in  \" + end + \" sec. File Location: \" + url);\n                context.log(\"Add \" + localOutput + \" to custom download\");\n                String size = FileUtils.byteCountToDisplaySize(file.getFile().length());\n                context.addDownload(\"local\", fileName, size, url);\n                FileUtil.deleteFile(filePath);\n                context.log(\"File deleted: \" + filePath);\n            } else {\n                context.log(\"No external Workspace set.\");\n            }\n        }\n        // delete temporary files\n        HdfsUtil.delete(output);\n        // Export calculated risk scores\n        if (pgsPanel != null) {\n            context.println(\"Exporting PGS scores...\");\n            String temp2 = FileUtil.path(localOutput, \"temp2\");\n            FileUtil.createDirectory(temp2);\n            List<String> scoreList = HdfsUtil.getFiles(outputScores);\n            String[] scoresArray = new String[scoreList.size()];\n            int i = 0;\n            for (String score : scoreList) {\n                String scoreChunk = score.substring(score.lastIndexOf(\"/\"));\n                String localPath = FileUtil.path(temp2, scoreChunk);\n                HdfsUtil.get(score, localPath);\n                scoresArray[i] = localPath;\n                i++;\n            }\n            String outputFile = FileUtil.path(localOutput, \"scores.txt\");\n            MergeScoreTask task = new MergeScoreTask();\n            task.setInputs(scoresArray);\n            task.setOutput(outputFile);\n            task.run();\n            context.println(\"Exported PGS scores to \" + outputFile + \".\");\n            FileUtil.deleteDirectory(temp2);\n        }\n        context.endTask(\"Exported data.\", WorkflowContext.OK);\n    } catch (Exception e) {\n        e.printStackTrace();\n        context.endTask(\"Data export failed: \" + e.getMessage(), WorkflowContext.ERROR);\n        return false;\n    }\n    // submit counters!\n    context.submitCounter(\"samples\");\n    context.submitCounter(\"genotypes\");\n    context.submitCounter(\"chromosomes\");\n    context.submitCounter(\"runs\");\n    // submit panel and phasing method counters\n    String reference = context.get(\"refpanel\");\n    String phasing = context.get(\"phasing\");\n    context.submitCounter(\"refpanel_\" + reference);\n    context.submitCounter(\"phasing_\" + phasing);\n    context.submitCounter(\"23andme-input\");\n    // send email\n    if (notification.equals(\"yes\")) {\n        Object mail = context.getData(\"cloudgene.user.mail\");\n        Object name = context.getData(\"cloudgene.user.name\");\n        if (mail != null) {\n            String subject = \"Job \" + context.getJobId() + \" is complete.\";\n            String message = \"Dear \" + name + \",\\nthe password for the imputation results is: \" + password + \"\\n\\nThe results can be downloaded from \" + serverUrl + \"/start.html#!jobs/\" + context.getJobId() + \"/results\";\n            try {\n                context.sendMail(subject, message);\n                context.ok(\"We have sent an email to <b>\" + mail + \"</b> with the password.\");\n                return true;\n            } catch (Exception e) {\n                context.error(\"Data compression failed: \" + e.getMessage());\n                return false;\n            }\n        } else {\n            context.error(\"No email address found. Please enter your email address (Account -> Profile).\");\n            return false;\n        }\n    } else {\n        context.ok(\"Email notification is disabled. All results are encrypted with password <b>\" + password + \"</b>\");\n        return true;\n    }\n}",
        "rpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    String workingDirectory = getFolder(CompressionEncryption.class);\n    String output = context.get(\"outputimputation\");\n    String outputScores = context.get(\"outputScores\");\n    String localOutput = context.get(\"local\");\n    String aesEncryption = context.get(\"aesEncryption\");\n    String mode = context.get(\"mode\");\n    String password = context.get(\"password\");\n    PgsPanel pgsPanel = PgsPanel.loadFromProperties(context.getData(\"pgsPanel\"));\n    boolean phasingOnly = false;\n    if (mode != null && mode.equals(\"phasing\")) {\n        phasingOnly = true;\n    }\n    // read config if mails should be sent\n    String folderConfig = getFolder(CompressionEncryption.class);\n    File jobConfig = new File(FileUtil.path(folderConfig, \"job.config\"));\n    DefaultPreferenceStore store = new DefaultPreferenceStore();\n    if (jobConfig.exists()) {\n        store.load(jobConfig);\n    } else {\n        context.log(\"Configuration file '\" + jobConfig.getAbsolutePath() + \"' not available. Use default values.\");\n    }\n    String notification = \"no\";\n    if (store.getString(\"minimac.sendmail\") != null && !store.getString(\"minimac.sendmail\").equals(\"\")) {\n        notification = store.getString(\"minimac.sendmail\");\n    }\n    String serverUrl = \"https://imputationserver.sph.umich.edu\";\n    if (store.getString(\"server.url\") != null && !store.getString(\"server.url\").isEmpty()) {\n        serverUrl = store.getString(\"server.url\");\n    }\n    String sanityCheck = \"yes\";\n    if (store.getString(\"sanitycheck\") != null && !store.getString(\"sanitycheck\").equals(\"\")) {\n        sanityCheck = store.getString(\"sanitycheck\");\n    }\n    if (password == null || (password != null && password.equals(\"auto\"))) {\n        password = PasswordCreator.createPassword();\n    }\n    try {\n        context.beginTask(\"Export data...\");\n        // get sorted directories\n        List<String> folders = HdfsUtil.getDirectories(output);\n        Map<String, ExportObject> chromosomes = new HashMap<String, ExportObject>();\n        for (String folder : folders) {\n            String name = FileUtil.getFilename(folder);\n            context.println(\"Prepare files for chromosome \" + name);\n            List<String> data = new Vector<String>();\n            List<String> header = new Vector<String>();\n            List<String> info = new Vector<String>();\n            header = findFiles(folder, \".header.dose.vcf.gz\");\n            if (phasingOnly) {\n                data = findFiles(folder, \".phased.vcf.gz\");\n            } else {\n                data = findFiles(folder, \".data.dose.vcf.gz\");\n                info = findFiles(folder, \".info\");\n            }\n            // combine all X. to one folder\n            if (name.startsWith(\"X.\")) {\n                name = \"X\";\n            }\n            ExportObject export = chromosomes.get(name);\n            if (export == null) {\n                export = new ExportObject();\n            }\n            ArrayList<String> currentDataList = export.getDataFiles();\n            currentDataList.addAll(data);\n            export.setDataFiles(currentDataList);\n            ArrayList<String> currentHeaderList = export.getHeaderFiles();\n            currentHeaderList.addAll(header);\n            export.setHeaderFiles(currentHeaderList);\n            ArrayList<String> currentInfoList = export.getInfoFiles();\n            currentInfoList.addAll(info);\n            export.setInfoFiles(currentInfoList);\n            chromosomes.put(name, export);\n        }\n        Set<String> chromosomesSet = chromosomes.keySet();\n        boolean lastChromosome = false;\n        int index = 0;\n        ZipParameters param = new ZipParameters();\n        param.setEncryptFiles(true);\n        param.setEncryptionMethod(EncryptionMethod.ZIP_STANDARD);\n        for (String name : chromosomesSet) {\n            index++;\n            if (index == chromosomesSet.size()) {\n                lastChromosome = true;\n            }\n            ExportObject entry = chromosomes.get(name);\n            context.println(\"Export and merge chromosome \" + name);\n            // resort for chrX only\n            if (name.equals(\"X\")) {\n                Collections.sort(entry.getDataFiles(), new ChrXComparator());\n                Collections.sort(entry.getInfoFiles(), new ChrXComparator());\n            }\n            // create temp fir\n            String temp = FileUtil.path(localOutput, \"temp\");\n            FileUtil.createDirectory(temp);\n            // output files\n            String dosageOutput;\n            String infoOutput = \"\";\n            if (phasingOnly) {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".phased.vcf.gz\");\n            } else {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".dose.vcf.gz\");\n                infoOutput = FileUtil.path(temp, \"chr\" + name + \".info.gz\");\n                FileMerger.mergeAndGzInfo(entry.getInfoFiles(), infoOutput);\n            }\n            MergedVcfFile vcfFile = new MergedVcfFile(dosageOutput);\n            // simple header check\n            String headerLine = null;\n            for (String file : entry.getHeaderFiles()) {\n                context.println(\"Read header file \" + file);\n                LineReader reader = null;\n                try {\n                    reader = new LineReader(HdfsUtil.open(file));\n                    while (reader.next()) {\n                        String line = reader.get();\n                        if (line.startsWith(\"#CHROM\")) {\n                            if (headerLine != null) {\n                                if (headerLine.equals(line)) {\n                                    context.println(\"  Header is the same as header of first file.\");\n                                } else {\n                                    context.println(\"  ERROR: Header is different as header of first file.\");\n                                    context.println(headerLine);\n                                    context.println(line);\n                                    throw new Exception(\"Different sample order in chunks.\");\n                                }\n                            } else {\n                                headerLine = line;\n                                vcfFile.addFile(HdfsUtil.open(file));\n                                context.println(\"  Keep this header as first header.\");\n                            }\n                        }\n                    }\n                    if (reader != null) {\n                        reader.close();\n                    }\n                    HdfsUtil.delete(file);\n                } catch (Exception e) {\n                    if (reader != null) {\n                        reader.close();\n                    }\n                    StringWriter errors = new StringWriter();\n                    e.printStackTrace(new PrintWriter(errors));\n                    context.println(\"Error reading header file: \" + errors.toString());\n                }\n            }\n            if (headerLine == null || headerLine.trim().isEmpty()) {\n                throw new Exception(\"No valid header file found\");\n            }\n            // add data files\n            for (String file : entry.getDataFiles()) {\n                context.println(\"Read file \" + file);\n                vcfFile.addFile(HdfsUtil.open(file));\n                HdfsUtil.delete(file);\n            }\n            vcfFile.close();\n            if (sanityCheck.equals(\"yes\") && lastChromosome) {\n                context.log(\"Run tabix on chromosome \" + name + \"...\");\n                Command tabix = new Command(FileUtil.path(workingDirectory, \"bin\", \"tabix\"));\n                tabix.setSilent(false);\n                tabix.setParams(\"-f\", dosageOutput);\n                if (tabix.execute() != 0) {\n                    context.endTask(\"Error during index creation: \" + tabix.getStdOut(), WorkflowContext.ERROR);\n                    return false;\n                }\n                context.log(\"Tabix done.\");\n            }\n            if (aesEncryption != null && aesEncryption.equals(\"yes\")) {\n                param.setEncryptionMethod(EncryptionMethod.AES);\n                param.setAesKeyStrength(AesKeyStrength.KEY_STRENGTH_256);\n                param.setCompressionMethod(CompressionMethod.DEFLATE);\n                param.setCompressionLevel(CompressionLevel.NORMAL);\n            }\n            // create zip file\n            ArrayList<File> files = new ArrayList<File>();\n            files.add(new File(dosageOutput));\n            if (!phasingOnly) {\n                files.add(new File(infoOutput));\n            }\n            String fileName = \"chr_\" + name + \".zip\";\n            String filePath = FileUtil.path(localOutput, fileName);\n            ZipFile file = new ZipFile(new File(filePath), password.toCharArray());\n            file.addFiles(files, param);\n            // delete temp dir\n            FileUtil.deleteDirectory(temp);\n            IExternalWorkspace externalWorkspace = context.getExternalWorkspace();\n            if (externalWorkspace != null) {\n                long start = System.currentTimeMillis();\n                context.log(\"External Workspace '\" + externalWorkspace.getName() + \"' found\");\n                context.log(\"Start file upload: \" + filePath);\n                String url = externalWorkspace.upload(\"local\", file.getFile());\n                long end = (System.currentTimeMillis() - start) / 1000;\n                context.log(\"Upload finished in  \" + end + \" sec. File Location: \" + url);\n                context.log(\"Add \" + localOutput + \" to custom download\");\n                String size = FileUtils.byteCountToDisplaySize(file.getFile().length());\n                context.addDownload(\"local\", fileName, size, url);\n                FileUtil.deleteFile(filePath);\n                context.log(\"File deleted: \" + filePath);\n            } else {\n                context.log(\"No external Workspace set.\");\n            }\n        }\n        // delete temporary files\n        HdfsUtil.delete(output);\n        // Export calculated risk scores\n        if (pgsPanel != null) {\n            context.println(\"Exporting PGS scores...\");\n            String temp2 = FileUtil.path(localOutput, \"temp2\");\n            FileUtil.createDirectory(temp2);\n            List<String> scoreList = HdfsUtil.getFiles(outputScores);\n            String[] scoresArray = new String[scoreList.size()];\n            int i = 0;\n            for (String score : scoreList) {\n                String scoreChunk = score.substring(score.lastIndexOf(\"/\"));\n                String localPath = FileUtil.path(temp2, scoreChunk);\n                HdfsUtil.get(score, localPath);\n                scoresArray[i] = localPath;\n                i++;\n            }\n            String outputFile = FileUtil.path(temp2, \"scores.txt\");\n            MergeScoreTask task = new MergeScoreTask();\n            task.setInputs(scoresArray);\n            task.setOutput(outputFile);\n            task.run();\n            String fileName = \"scores.zip\";\n            String filePath = FileUtil.path(localOutput, fileName);\n            ArrayList<File> files = new ArrayList<File>();\n            files.add(new File(outputFile));\n            ZipFile file = new ZipFile(new File(filePath), password.toCharArray());\n            file.addFiles(files, param);\n            context.println(\"Exported PGS scores to \" + outputFile + \".\");\n            FileUtil.deleteDirectory(temp2);\n        }\n        context.endTask(\"Exported data.\", WorkflowContext.OK);\n    } catch (Exception e) {\n        e.printStackTrace();\n        context.endTask(\"Data export failed: \" + e.getMessage(), WorkflowContext.ERROR);\n        return false;\n    }\n    // submit counters!\n    context.submitCounter(\"samples\");\n    context.submitCounter(\"genotypes\");\n    context.submitCounter(\"chromosomes\");\n    context.submitCounter(\"runs\");\n    // submit panel and phasing method counters\n    String reference = context.get(\"refpanel\");\n    String phasing = context.get(\"phasing\");\n    context.submitCounter(\"refpanel_\" + reference);\n    context.submitCounter(\"phasing_\" + phasing);\n    context.submitCounter(\"23andme-input\");\n    // send email\n    if (notification.equals(\"yes\")) {\n        Object mail = context.getData(\"cloudgene.user.mail\");\n        Object name = context.getData(\"cloudgene.user.name\");\n        if (mail != null) {\n            String subject = \"Job \" + context.getJobId() + \" is complete.\";\n            String message = \"Dear \" + name + \",\\nthe password for the imputation results is: \" + password + \"\\n\\nThe results can be downloaded from \" + serverUrl + \"/start.html#!jobs/\" + context.getJobId() + \"/results\";\n            try {\n                context.sendMail(subject, message);\n                context.ok(\"We have sent an email to <b>\" + mail + \"</b> with the password.\");\n                return true;\n            } catch (Exception e) {\n                context.error(\"Data compression failed: \" + e.getMessage());\n                return false;\n            }\n        } else {\n            context.error(\"No email address found. Please enter your email address (Account -> Profile).\");\n            return false;\n        }\n    } else {\n        context.ok(\"Email notification is disabled. All results are encrypted with password <b>\" + password + \"</b>\");\n        return true;\n    }\n}",
        "tuc": "@Test\npublic void testPipelineWithEagleAndScores() throws IOException, ZipException {\n    String configFolder = \"test-data/configs/hapmap-chr20\";\n    String inputFolder = \"test-data/data/chr20-unphased\";\n    String score1 = PGSCatalog.getFilenameById(\"PGS000018\");\n    String score2 = PGSCatalog.getFilenameById(\"PGS000027\");\n    String targetScore1 = HdfsUtil.path(\"scores-hdfs\", \"PGS000018.txt.gz\");\n    HdfsUtil.put(score1, targetScore1);\n    String targetScore2 = HdfsUtil.path(\"scores-hdfs\", \"PGS000027.txt.gz\");\n    HdfsUtil.put(score2, targetScore2);\n    WorkflowTestContext context = buildContext(inputFolder, \"hapmap2\");\n    Map<String, Object> pgsPanel = new HashMap<String, Object>();\n    List<String> scores = new Vector<String>();\n    scores.add(\"PGS000018.txt.gz\");\n    scores.add(\"PGS000027.txt.gz\");\n    pgsPanel.put(\"location\", \"scores-hdfs\");\n    pgsPanel.put(\"scores\", scores);\n    context.setData(\"pgsPanel\", pgsPanel);\n    QcStatisticsMock qcStats = new QcStatisticsMock(configFolder);\n    boolean result = run(context, qcStats);\n    assertTrue(result);\n    assertTrue(context.hasInMemory(\"Remaining sites in total: 7,735\"));\n    importRefPanel(FileUtil.path(configFolder, \"ref-panels\"));\n    importBinaries(\"files/bin\");\n    ImputationMinimac3Mock imputation = new ImputationMinimac3Mock(configFolder);\n    result = run(context, imputation);\n    assertTrue(result);\n    CompressionEncryptionMock export = new CompressionEncryptionMock(\"files\");\n    result = run(context, export);\n    assertTrue(result);\n    ZipFile zipFile = new ZipFile(\"test-data/tmp/local/chr_20.zip\", PASSWORD.toCharArray());\n    zipFile.extractAll(\"test-data/tmp\");\n    VcfFile file = VcfFileUtil.load(\"test-data/tmp/chr20.dose.vcf.gz\", 100000000, false);\n    assertEquals(\"20\", file.getChromosome());\n    assertEquals(51, file.getNoSamples());\n    assertEquals(true, file.isPhased());\n    assertEquals(TOTAL_REFPANEL_CHR20_B37, file.getNoSnps());\n    int snpInInfo = getLineCount(\"test-data/tmp/chr20.info.gz\") - 1;\n    assertEquals(snpInInfo, file.getNoSnps());\n    String[] args = { \"test-data/tmp/chr20.dose.vcf.gz\", \"--ref\", \"PGS000018,PGS000027\", \"--out\", \"test-data/tmp/local/expected.txt\" };\n    int resultScore = new CommandLine(new ApplyScoreCommand()).execute(args);\n    assertEquals(0, resultScore);\n    CsvTableReader readerExpected = new CsvTableReader(\"test-data/tmp/local/expected.txt\", ',');\n    CsvTableReader readerActual = new CsvTableReader(\"test-data/tmp/local/scores.txt\", ',');\n    while (readerExpected.next() && readerActual.next()) {\n        assertEquals(readerExpected.getDouble(\"PGS000018\"), readerActual.getDouble(\"PGS000018\"), 0.00001);\n        assertEquals(readerExpected.getDouble(\"PGS000027\"), readerActual.getDouble(\"PGS000027\"), 0.00001);\n    }\n    readerExpected.close();\n    readerActual.close();\n}",
        "label": 1
    },
    {
        "repo_name": "genepi___imputationserver",
        "commit": "89726e73caaab295bef8f6d2f8e1ec69bc93840a",
        "commit_message": "Update Qc-Filter testcases",
        "p_path": "src/main/java/genepi/imputationserver/steps/FastQualityControl.java",
        "t_path": "src/test/java/genepi/imputationserver/steps/FastQualityControlTest.java",
        "p_name": "run",
        "t_name": "testQcStatisticsFilterSampleCallrate",
        "lpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    String folder = getFolder(FastQualityControl.class);\n    setupTabix(folder);\n    String inputFiles = context.get(\"files\");\n    String reference = context.get(\"refpanel\");\n    String population = context.get(\"population\");\n    String mafFile = context.get(\"mafFile\");\n    String chunkFileDir = context.get(\"chunkFileDir\");\n    String statDir = context.get(\"statisticDir\");\n    String chunksDir = context.get(\"chunksDir\");\n    String buildGwas = context.get(\"build\");\n    if (buildGwas == null) {\n        buildGwas = \"hg19\";\n    }\n    File jobConfig = new File(FileUtil.path(folder, \"job.config\"));\n    DefaultPreferenceStore store = new DefaultPreferenceStore();\n    if (jobConfig.exists()) {\n        store.load(jobConfig);\n    } else {\n        context.log(\"Configuration file '\" + jobConfig.getAbsolutePath() + \"' not available. Use default values.\");\n    }\n    int phasingWindow = Integer.parseInt(store.getString(\"phasing.window\"));\n    int chunkSize = Integer.parseInt(store.getString(\"chunksize\"));\n    RefPanelList panels = RefPanelList.loadFromFile(FileUtil.path(folder, RefPanelList.FILENAME));\n    RefPanel panel = null;\n    try {\n        panel = panels.getById(reference, context.getData(\"refpanel\"));\n        if (panel == null) {\n            context.error(\"Reference '\" + reference + \"' not found.\");\n            context.error(\"Available references:\");\n            for (RefPanel p : panels.getPanels()) {\n                context.error(p.getId());\n            }\n            return false;\n        }\n    } catch (Exception e) {\n        context.error(\"Unable to parse reference panel '\" + reference + \"': \" + e.getMessage());\n        return false;\n    }\n    String[] vcfFilenames = FileUtil.getFiles(inputFiles, \"*.vcf.gz$|*.vcf$\");\n    Arrays.sort(vcfFilenames);\n    LineWriter excludedSnpsWriter = null;\n    String excludedSnpsFile = FileUtil.path(statDir, \"snps-excluded.txt\");\n    try {\n        excludedSnpsWriter = new LineWriter(excludedSnpsFile);\n        excludedSnpsWriter.write(\"#Position\" + \"\\t\" + \"FilterType\" + \"\\t\" + \" Info\", false);\n    } catch (Exception e) {\n        context.error(\"Error creating file writer\");\n        return false;\n    }\n    if (!buildGwas.equals(panel.getBuild())) {\n        context.warning(\"Uploaded data is \" + buildGwas + \" and reference is \" + panel.getBuild() + \".\");\n        String chainFile = store.getString(buildGwas + \"To\" + panel.getBuild());\n        if (chainFile == null) {\n            context.error(\"Currently we do not support liftOver from \" + buildGwas + \" to \" + panel.getBuild());\n            return false;\n        }\n        String fullPathChainFile = FileUtil.path(folder, chainFile);\n        if (!new File(fullPathChainFile).exists()) {\n            context.error(\"Chain file \" + fullPathChainFile + \" not found.\");\n            return false;\n        }\n        LiftOverTask task = new LiftOverTask();\n        task.setVcfFilenames(vcfFilenames);\n        task.setChainFile(fullPathChainFile);\n        task.setChunksDir(chunksDir);\n        task.setExcludedSnpsWriter(excludedSnpsWriter);\n        TaskResults results = runTask(context, task);\n        if (results.isSuccess()) {\n            vcfFilenames = task.getNewVcfFilenames();\n        } else {\n            return false;\n        }\n    }\n    StatisticsTask task = new StatisticsTask();\n    task.setVcfFilenames(vcfFilenames);\n    task.setExcludedSnpsWriter(excludedSnpsWriter);\n    task.setChunkSize(chunkSize);\n    task.setPhasingWindow(phasingWindow);\n    task.setPopulation(population);\n    String legend = panel.getLegend();\n    if (!legend.startsWith(\"/\")) {\n        legend = FileUtil.path(folder, legend);\n    }\n    if (!panel.supportsPopulation(population)) {\n        StringBuilder report = new StringBuilder();\n        report.append(\"Population '\" + population + \"' is not supported by reference panel '\" + reference + \"'.\\n\");\n        if (panel.getPopulations() != null) {\n            report.append(\"Available populations:\");\n            for (String pop : panel.getPopulations().values()) {\n                report.append(\"\\n - \" + pop);\n            }\n        }\n        context.error(report.toString());\n        return false;\n    }\n    int refSamples = panel.getSamplesByPopulation(population);\n    if (refSamples <= 0) {\n        context.warning(\"Skip allele frequency check.\");\n        task.setAlleleFrequencyCheck(false);\n    }\n    double referenceOverlap = panel.getQcFilterByKey(\"overlap\");\n    int minSnps = (int) panel.getQcFilterByKey(\"minSnps\");\n    double sampleCallrate = panel.getQcFilterByKey(\"sampleCallrate\");\n    double mixedGenotypesChrX = panel.getQcFilterByKey(\"mixedGenotypeschrX\");\n    int strandFlips = (int) (panel.getQcFilterByKey(\"strandFlips\"));\n    task.setLegendFile(legend);\n    task.setRefSamples(refSamples);\n    task.setMafFile(mafFile);\n    task.setChunkFileDir(chunkFileDir);\n    task.setChunksDir(chunksDir);\n    task.setStatDir(statDir);\n    task.setBuild(panel.getBuild());\n    task.setReferenceOverlap(referenceOverlap);\n    task.setMinSnps(minSnps);\n    task.setSampleCallrate(sampleCallrate);\n    task.setMixedGenotypeschrX(mixedGenotypesChrX);\n    TaskResults results = runTask(context, task);\n    if (!results.isSuccess()) {\n        return false;\n    }\n    try {\n        excludedSnpsWriter.close();\n        if (!excludedSnpsWriter.hasData()) {\n            FileUtil.deleteFile(excludedSnpsFile);\n        }\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n    DecimalFormat df = new DecimalFormat(\"#.00\");\n    DecimalFormat formatter = new DecimalFormat(\"###,###.###\");\n    StringBuffer text = new StringBuffer();\n    text.append(\"<b>Statistics:</b> <br>\");\n    text.append(\"Alternative allele frequency > 0.5 sites: \" + formatter.format(task.getAlternativeAlleles()) + \"<br>\");\n    text.append(\"Reference Overlap: \" + df.format(task.getFoundInLegend() / (double) (task.getFoundInLegend() + task.getNotFoundInLegend()) * 100) + \" %\" + \"<br>\");\n    text.append(\"Match: \" + formatter.format(task.getMatch()) + \"<br>\");\n    text.append(\"Allele switch: \" + formatter.format(task.getAlleleSwitch()) + \"<br>\");\n    text.append(\"Strand flip: \" + formatter.format(task.getStrandFlipSimple()) + \"<br>\");\n    text.append(\"Strand flip and allele switch: \" + formatter.format(task.getStrandFlipAndAlleleSwitch()) + \"<br>\");\n    text.append(\"A/T, C/G genotypes: \" + formatter.format(task.getComplicatedGenotypes()) + \"<br>\");\n    text.append(\"<b>Filtered sites:</b> <br>\");\n    text.append(\"Filter flag set: \" + formatter.format(task.getFilterFlag()) + \"<br>\");\n    text.append(\"Invalid alleles: \" + formatter.format(task.getInvalidAlleles()) + \"<br>\");\n    text.append(\"Multiallelic sites: \" + formatter.format(task.getMultiallelicSites()) + \"<br>\");\n    text.append(\"Duplicated sites: \" + formatter.format(task.getDuplicates()) + \"<br>\");\n    text.append(\"NonSNP sites: \" + formatter.format(task.getNoSnps()) + \"<br>\");\n    text.append(\"Monomorphic sites: \" + formatter.format(task.getMonomorphic()) + \"<br>\");\n    text.append(\"Allele mismatch: \" + formatter.format(task.getAlleleMismatch()) + \"<br>\");\n    text.append(\"SNPs call rate < 90%: \" + formatter.format(task.getLowCallRate()));\n    context.ok(text.toString());\n    text = new StringBuffer();\n    text.append(\"Excluded sites in total: \" + formatter.format(task.getFiltered()) + \"<br>\");\n    text.append(\"Remaining sites in total: \" + formatter.format(task.getOverallSnps()) + \"<br>\");\n    if (task.getFiltered() > 0) {\n        text.append(\"See \" + context.createLinkToFile(\"statisticDir\", \"snps-excluded.txt\") + \" for details\" + \"<br>\");\n    }\n    if (task.getNotFoundInLegend() > 0) {\n        text.append(\"Typed only sites: \" + formatter.format(task.getNotFoundInLegend()) + \"<br>\");\n        text.append(\"See \" + context.createLinkToFile(\"statisticDir\", \"typed-only.txt\") + \" for details\" + \"<br>\");\n    }\n    if (task.getRemovedChunksSnps() > 0) {\n        text.append(\"<br><b>Warning:</b> \" + formatter.format(task.getRemovedChunksSnps()) + \" Chunk(s) excluded: < \" + minSnps + \" SNPs (see \" + context.createLinkToFile(\"statisticDir\", \"chunks-excluded.txt\") + \"  for details).\");\n    }\n    if (task.getRemovedChunksCallRate() > 0) {\n        text.append(\"<br><b>Warning:</b> \" + formatter.format(task.getRemovedChunksCallRate()) + \" Chunk(s) excluded: at least one sample has a call rate < \" + sampleCallrate + \"% (see \" + context.createLinkToFile(\"statisticDir\", \"chunks-excluded.txt\") + \" for details).\");\n    }\n    if (task.getRemovedChunksOverlap() > 0) {\n        text.append(\"<br><b>Warning:</b> \" + formatter.format(task.getRemovedChunksOverlap()) + \" Chunk(s) excluded: reference overlap < \" + (referenceOverlap * 100) + \"% (see \" + context.createLinkToFile(\"statisticDir\", \"chunks-excluded.txt\") + \" for details).\");\n    }\n    long excludedChunks = task.getRemovedChunksSnps() + task.getRemovedChunksCallRate() + task.getRemovedChunksOverlap();\n    long overallChunks = task.getOverallChunks();\n    if (excludedChunks > 0) {\n        text.append(\"<br>Remaining chunk(s): \" + formatter.format(overallChunks - excludedChunks));\n    }\n    if (excludedChunks == overallChunks) {\n        text.append(\"<br><b>Error:</b> No chunks passed the QC step. Imputation cannot be started!\");\n        context.error(text.toString());\n        return false;\n    } else if (task.getStrandFlipSimple() + task.getStrandFlipAndAlleleSwitch() > strandFlips) {\n        text.append(\"<br><b>Error:</b> More than \" + strandFlips + \" obvious strand flips have been detected. Please check strand. Imputation cannot be started!\");\n        context.error(text.toString());\n        return false;\n    } else if (task.isChrXMissingRate()) {\n        text.append(\"<br><b>Error:</b> Chromosome X nonPAR region includes > 10 % mixed genotypes. Imputation cannot be started!\");\n        context.error(text.toString());\n        return false;\n    } else if (task.isChrXPloidyError()) {\n        text.append(\"<br><b>Error:</b> ChrX nonPAR region includes ambiguous samples (haploid and diploid positions). Imputation cannot be started! See \" + context.createLinkToFile(\"statisticDir\", \"chrX-info.txt\"));\n        context.error(text.toString());\n        return false;\n    } else {\n        text.append(results.getMessage());\n        context.warning(text.toString());\n        return true;\n    }\n}",
        "rpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    String folder = getFolder(FastQualityControl.class);\n    setupTabix(folder);\n    String inputFiles = context.get(\"files\");\n    String reference = context.get(\"refpanel\");\n    String population = context.get(\"population\");\n    String mafFile = context.get(\"mafFile\");\n    String chunkFileDir = context.get(\"chunkFileDir\");\n    String statDir = context.get(\"statisticDir\");\n    String chunksDir = context.get(\"chunksDir\");\n    String buildGwas = context.get(\"build\");\n    if (buildGwas == null) {\n        buildGwas = \"hg19\";\n    }\n    File jobConfig = new File(FileUtil.path(folder, \"job.config\"));\n    DefaultPreferenceStore store = new DefaultPreferenceStore();\n    if (jobConfig.exists()) {\n        store.load(jobConfig);\n    } else {\n        context.log(\"Configuration file '\" + jobConfig.getAbsolutePath() + \"' not available. Use default values.\");\n    }\n    int phasingWindow = Integer.parseInt(store.getString(\"phasing.window\"));\n    int chunkSize = Integer.parseInt(store.getString(\"chunksize\"));\n    RefPanelList panels = RefPanelList.loadFromFile(FileUtil.path(folder, RefPanelList.FILENAME));\n    RefPanel panel = null;\n    try {\n        panel = panels.getById(reference, context.getData(\"refpanel\"));\n        if (panel == null) {\n            context.error(\"Reference '\" + reference + \"' not found.\");\n            context.error(\"Available references:\");\n            for (RefPanel p : panels.getPanels()) {\n                context.error(p.getId());\n            }\n            return false;\n        }\n    } catch (Exception e) {\n        context.error(\"Unable to parse reference panel '\" + reference + \"': \" + e.getMessage());\n        return false;\n    }\n    String[] vcfFilenames = FileUtil.getFiles(inputFiles, \"*.vcf.gz$|*.vcf$\");\n    Arrays.sort(vcfFilenames);\n    LineWriter excludedSnpsWriter = null;\n    String excludedSnpsFile = FileUtil.path(statDir, \"snps-excluded.txt\");\n    try {\n        excludedSnpsWriter = new LineWriter(excludedSnpsFile);\n        excludedSnpsWriter.write(\"#Position\" + \"\\t\" + \"FilterType\" + \"\\t\" + \" Info\", false);\n    } catch (Exception e) {\n        context.error(\"Error creating file writer\");\n        return false;\n    }\n    if (!buildGwas.equals(panel.getBuild())) {\n        context.warning(\"Uploaded data is \" + buildGwas + \" and reference is \" + panel.getBuild() + \".\");\n        String chainFile = store.getString(buildGwas + \"To\" + panel.getBuild());\n        if (chainFile == null) {\n            context.error(\"Currently we do not support liftOver from \" + buildGwas + \" to \" + panel.getBuild());\n            return false;\n        }\n        String fullPathChainFile = FileUtil.path(folder, chainFile);\n        if (!new File(fullPathChainFile).exists()) {\n            context.error(\"Chain file \" + fullPathChainFile + \" not found.\");\n            return false;\n        }\n        LiftOverTask task = new LiftOverTask();\n        task.setVcfFilenames(vcfFilenames);\n        task.setChainFile(fullPathChainFile);\n        task.setChunksDir(chunksDir);\n        task.setExcludedSnpsWriter(excludedSnpsWriter);\n        TaskResults results = runTask(context, task);\n        if (results.isSuccess()) {\n            vcfFilenames = task.getNewVcfFilenames();\n        } else {\n            return false;\n        }\n    }\n    StatisticsTask task = new StatisticsTask();\n    task.setVcfFilenames(vcfFilenames);\n    task.setExcludedSnpsWriter(excludedSnpsWriter);\n    task.setChunkSize(chunkSize);\n    task.setPhasingWindow(phasingWindow);\n    task.setPopulation(population);\n    String legend = panel.getLegend();\n    if (!legend.startsWith(\"/\")) {\n        legend = FileUtil.path(folder, legend);\n    }\n    if (!panel.supportsPopulation(population)) {\n        StringBuilder report = new StringBuilder();\n        report.append(\"Population '\" + population + \"' is not supported by reference panel '\" + reference + \"'.\\n\");\n        if (panel.getPopulations() != null) {\n            report.append(\"Available populations:\");\n            for (String pop : panel.getPopulations().values()) {\n                report.append(\"\\n - \" + pop);\n            }\n        }\n        context.error(report.toString());\n        return false;\n    }\n    int refSamples = panel.getSamplesByPopulation(population);\n    if (refSamples <= 0) {\n        context.warning(\"Skip allele frequency check.\");\n        task.setAlleleFrequencyCheck(false);\n    }\n    task.setLegendFile(legend);\n    task.setRefSamples(refSamples);\n    task.setMafFile(mafFile);\n    task.setChunkFileDir(chunkFileDir);\n    task.setChunksDir(chunksDir);\n    task.setStatDir(statDir);\n    task.setBuild(panel.getBuild());\n    double referenceOverlap = panel.getQcFilterByKey(\"overlap\");\n    int minSnps = (int) panel.getQcFilterByKey(\"minSnps\");\n    double sampleCallrate = panel.getQcFilterByKey(\"sampleCallrate\");\n    double mixedGenotypesChrX = panel.getQcFilterByKey(\"mixedGenotypeschrX\");\n    int strandFlips = (int) (panel.getQcFilterByKey(\"strandFlips\"));\n    task.setReferenceOverlap(referenceOverlap);\n    task.setMinSnps(minSnps);\n    task.setSampleCallrate(sampleCallrate);\n    task.setMixedGenotypeschrX(mixedGenotypesChrX);\n    TaskResults results = runTask(context, task);\n    if (!results.isSuccess()) {\n        return false;\n    }\n    try {\n        excludedSnpsWriter.close();\n        if (!excludedSnpsWriter.hasData()) {\n            FileUtil.deleteFile(excludedSnpsFile);\n        }\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n    DecimalFormat df = new DecimalFormat(\"#.00\");\n    DecimalFormat formatter = new DecimalFormat(\"###,###.###\");\n    StringBuffer text = new StringBuffer();\n    text.append(\"<b>Statistics:</b> <br>\");\n    text.append(\"Alternative allele frequency > 0.5 sites: \" + formatter.format(task.getAlternativeAlleles()) + \"<br>\");\n    text.append(\"Reference Overlap: \" + df.format(task.getFoundInLegend() / (double) (task.getFoundInLegend() + task.getNotFoundInLegend()) * 100) + \" %\" + \"<br>\");\n    text.append(\"Match: \" + formatter.format(task.getMatch()) + \"<br>\");\n    text.append(\"Allele switch: \" + formatter.format(task.getAlleleSwitch()) + \"<br>\");\n    text.append(\"Strand flip: \" + formatter.format(task.getStrandFlipSimple()) + \"<br>\");\n    text.append(\"Strand flip and allele switch: \" + formatter.format(task.getStrandFlipAndAlleleSwitch()) + \"<br>\");\n    text.append(\"A/T, C/G genotypes: \" + formatter.format(task.getComplicatedGenotypes()) + \"<br>\");\n    text.append(\"<b>Filtered sites:</b> <br>\");\n    text.append(\"Filter flag set: \" + formatter.format(task.getFilterFlag()) + \"<br>\");\n    text.append(\"Invalid alleles: \" + formatter.format(task.getInvalidAlleles()) + \"<br>\");\n    text.append(\"Multiallelic sites: \" + formatter.format(task.getMultiallelicSites()) + \"<br>\");\n    text.append(\"Duplicated sites: \" + formatter.format(task.getDuplicates()) + \"<br>\");\n    text.append(\"NonSNP sites: \" + formatter.format(task.getNoSnps()) + \"<br>\");\n    text.append(\"Monomorphic sites: \" + formatter.format(task.getMonomorphic()) + \"<br>\");\n    text.append(\"Allele mismatch: \" + formatter.format(task.getAlleleMismatch()) + \"<br>\");\n    text.append(\"SNPs call rate < 90%: \" + formatter.format(task.getLowCallRate()));\n    context.ok(text.toString());\n    text = new StringBuffer();\n    text.append(\"Excluded sites in total: \" + formatter.format(task.getFiltered()) + \"<br>\");\n    text.append(\"Remaining sites in total: \" + formatter.format(task.getOverallSnps()) + \"<br>\");\n    if (task.getFiltered() > 0) {\n        text.append(\"See \" + context.createLinkToFile(\"statisticDir\", \"snps-excluded.txt\") + \" for details\" + \"<br>\");\n    }\n    if (task.getNotFoundInLegend() > 0) {\n        text.append(\"Typed only sites: \" + formatter.format(task.getNotFoundInLegend()) + \"<br>\");\n        text.append(\"See \" + context.createLinkToFile(\"statisticDir\", \"typed-only.txt\") + \" for details\" + \"<br>\");\n    }\n    if (task.getRemovedChunksSnps() > 0) {\n        text.append(\"<br><b>Warning:</b> \" + formatter.format(task.getRemovedChunksSnps()) + \" Chunk(s) excluded: < \" + minSnps + \" SNPs (see \" + context.createLinkToFile(\"statisticDir\", \"chunks-excluded.txt\") + \"  for details).\");\n    }\n    if (task.getRemovedChunksCallRate() > 0) {\n        text.append(\"<br><b>Warning:</b> \" + formatter.format(task.getRemovedChunksCallRate()) + \" Chunk(s) excluded: at least one sample has a call rate < \" + (sampleCallrate * 100) + \"% (see \" + context.createLinkToFile(\"statisticDir\", \"chunks-excluded.txt\") + \" for details).\");\n    }\n    if (task.getRemovedChunksOverlap() > 0) {\n        text.append(\"<br><b>Warning:</b> \" + formatter.format(task.getRemovedChunksOverlap()) + \" Chunk(s) excluded: reference overlap < \" + (referenceOverlap * 100) + \"% (see \" + context.createLinkToFile(\"statisticDir\", \"chunks-excluded.txt\") + \" for details).\");\n    }\n    long excludedChunks = task.getRemovedChunksSnps() + task.getRemovedChunksCallRate() + task.getRemovedChunksOverlap();\n    long overallChunks = task.getOverallChunks();\n    if (excludedChunks > 0) {\n        text.append(\"<br>Remaining chunk(s): \" + formatter.format(overallChunks - excludedChunks));\n    }\n    if (excludedChunks == overallChunks) {\n        text.append(\"<br><b>Error:</b> No chunks passed the QC step. Imputation cannot be started!\");\n        context.error(text.toString());\n        return false;\n    } else if (task.getStrandFlipSimple() + task.getStrandFlipAndAlleleSwitch() > strandFlips) {\n        text.append(\"<br><b>Error:</b> More than \" + strandFlips + \" obvious strand flips have been detected. Please check strand. Imputation cannot be started!\");\n        context.error(text.toString());\n        return false;\n    } else if (task.isChrXMissingRate()) {\n        text.append(\"<br><b>Error:</b> Chromosome X nonPAR region includes > 10 % mixed genotypes. Imputation cannot be started!\");\n        context.error(text.toString());\n        return false;\n    } else if (task.isChrXPloidyError()) {\n        text.append(\"<br><b>Error:</b> ChrX nonPAR region includes ambiguous samples (haploid and diploid positions). Imputation cannot be started! See \" + context.createLinkToFile(\"statisticDir\", \"chrX-info.txt\"));\n        context.error(text.toString());\n        return false;\n    } else {\n        text.append(results.getMessage());\n        context.warning(text.toString());\n        return true;\n    }\n}",
        "tuc": "public void testQcStatisticsFilterSampleCallrate() throws IOException {\n    String configFolder = \"test-data/configs/hapmap-3chr\";\n    String inputFolder = \"test-data/data/simulated-chip-3chr-imputation\";\n    WorkflowTestContext context = buildContext(inputFolder, \"hapmap2-qcfilter-low-callrate\");\n    FastQualityControlMock qcStats = new FastQualityControlMock(configFolder);\n    run(context, qcStats);\n    assertTrue(context.hasInMemory(\"<b>Warning:</b> 36 Chunk(s) excluded: at least one sample has a call rate < 80.0% (see [NOT AVAILABLE] for details)\"));\n}",
        "label": 1
    },
    {
        "repo_name": "genepi___imputationserver",
        "commit": "ad47b6c4a8f3d4c9990443c7492ab62d27430e03",
        "commit_message": "adding eagle parameter to write compressed output. ",
        "p_path": "src/main/java/genepi/imputationserver/steps/imputationMinimac3/ImputationPipelineMinimac3.java",
        "t_path": "src/test/java/genepi/imputationserver/steps/ImputationMinimac3Test.java",
        "p_name": "phaseWithEagle",
        "t_name": "testPipelineWithEagle",
        "lpfc": "public boolean phaseWithEagle(VcfChunk input, VcfChunkOutput output, String reference, String mapFilename) {\n    int start = input.getStart() - phasingWindow;\n    if (start < 1) {\n        start = 1;\n    }\n    int end = input.getEnd() + phasingWindow;\n    if (!new File(output.getVcfFilename()).exists()) {\n        System.out.println(\"vcf file not created!\");\n        return false;\n    }\n    try {\n        boolean first = true;\n        LineReader reader = new LineReader(output.getVcfFilename());\n        BlockCompressedOutputStream out = new BlockCompressedOutputStream(output.getVcfFilename() + \".gz\");\n        while (reader.next()) {\n            if (!first) {\n                out.write(\"\\n\".getBytes());\n            }\n            out.write(reader.get().getBytes());\n            first = false;\n        }\n        reader.close();\n        out.close();\n    } catch (Exception e) {\n        System.out.println(\"my bgzip failed.\");\n        e.printStackTrace();\n        return false;\n    }\n    if (!new File(output.getVcfFilename() + \".gz\").exists()) {\n        System.out.println(\"vcf.gz file not created!\");\n        return false;\n    }\n    Command tabix = new Command(tabixCommand);\n    tabix.setSilent(false);\n    tabix.setParams(output.getVcfFilename() + \".gz\");\n    System.out.println(\"Command: \" + tabix.getExecutedCommand());\n    if (tabix.execute() != 0) {\n        System.out.println(\"Error during index creation: \" + tabix.getStdOut());\n        return false;\n    }\n    Command eagle = new Command(eagleCommand);\n    eagle.setSilent(false);\n    eagle.setParams(\"--vcfRef\", reference, \"--vcfTarget\", output.getVcfFilename() + \".gz\", \"--geneticMapFile\", mapFilename, \"--outPrefix\", output.getPrefix(), \"--chrom\", output.getChromosome(), \"--bpStart\", start + \"\", \"--bpEnd\", end + \"\", \"--allowRefAltSwap\");\n    eagle.saveStdOut(output.getPrefix() + \".eagle.out\");\n    eagle.saveStdErr(output.getPrefix() + \".eagle.err\");\n    System.out.println(\"Command: \" + eagle.getExecutedCommand());\n    if (eagle.execute() != 0) {\n        return false;\n    }\n    new File(output.getPrefix() + \".vcf.gz\").renameTo(new File(output.getPhasedVcfFilename()));\n    return true;\n}",
        "rpfc": "public boolean phaseWithEagle(VcfChunk input, VcfChunkOutput output, String reference, String mapFilename) {\n    int start = input.getStart() - phasingWindow;\n    if (start < 1) {\n        start = 1;\n    }\n    int end = input.getEnd() + phasingWindow;\n    if (!new File(output.getVcfFilename()).exists()) {\n        System.out.println(\"vcf file not created!\");\n        return false;\n    }\n    try {\n        boolean first = true;\n        LineReader reader = new LineReader(output.getVcfFilename());\n        BlockCompressedOutputStream out = new BlockCompressedOutputStream(output.getVcfFilename() + \".gz\");\n        while (reader.next()) {\n            if (!first) {\n                out.write(\"\\n\".getBytes());\n            }\n            out.write(reader.get().getBytes());\n            first = false;\n        }\n        reader.close();\n        out.close();\n    } catch (Exception e) {\n        System.out.println(\"my bgzip failed.\");\n        e.printStackTrace();\n        return false;\n    }\n    if (!new File(output.getVcfFilename() + \".gz\").exists()) {\n        System.out.println(\"vcf.gz file not created!\");\n        return false;\n    }\n    Command tabix = new Command(tabixCommand);\n    tabix.setSilent(false);\n    tabix.setParams(output.getVcfFilename() + \".gz\");\n    System.out.println(\"Command: \" + tabix.getExecutedCommand());\n    if (tabix.execute() != 0) {\n        System.out.println(\"Error during index creation: \" + tabix.getStdOut());\n        return false;\n    }\n    Command eagle = new Command(eagleCommand);\n    eagle.setSilent(false);\n    String phasedPrefix = \".eagle.phased\";\n    eagle.setParams(\"--vcfRef\", reference, \"--vcfTarget\", output.getVcfFilename() + \".gz\", \"--geneticMapFile\", mapFilename, \"--outPrefix\", output.getPrefix() + phasedPrefix, \"--chrom\", output.getChromosome(), \"--bpStart\", start + \"\", \"--bpEnd\", end + \"\", \"--allowRefAltSwap\", \"--vcfOutFormat\", \"z\");\n    eagle.saveStdOut(output.getPrefix() + \".eagle.out\");\n    eagle.saveStdErr(output.getPrefix() + \".eagle.err\");\n    System.out.println(\"Command: \" + eagle.getExecutedCommand());\n    if (eagle.execute() != 0) {\n        return false;\n    }\n    new File(output.getPrefix() + phasedPrefix + \".vcf.gz\").renameTo(new File(output.getPhasedVcfFilename()));\n    return true;\n}",
        "tuc": "@Test\npublic void testPipelineWithEagle() throws IOException, ZipException {\n    String configFolder = \"test-data/configs/hapmap-chr20\";\n    String inputFolder = \"test-data/data/chr20-unphased\";\n    WorkflowTestContext context = buildContext(inputFolder, \"hapmap2\", \"eagle\");\n    QcStatisticsMock qcStats = new QcStatisticsMock(configFolder);\n    boolean result = run(context, qcStats);\n    assertTrue(result);\n    assertTrue(context.hasInMemory(\"Remaining sites in total: 7,735\"));\n    importRefPanel(FileUtil.path(configFolder, \"ref-panels\"));\n    importBinaries(\"files/minimac/bin\");\n    ImputationMinimac3Mock imputation = new ImputationMinimac3Mock(configFolder);\n    result = run(context, imputation);\n    assertTrue(result);\n    CompressionEncryptionMock export = new CompressionEncryptionMock(\"files/minimac\");\n    result = run(context, export);\n    assertTrue(result);\n    ZipFile zipFile = new ZipFile(\"test-data/tmp/local/chr_20.zip\");\n    if (zipFile.isEncrypted()) {\n        zipFile.setPassword(CompressionEncryption.DEFAULT_PASSWORD);\n    }\n    zipFile.extractAll(\"test-data/tmp\");\n    VcfFile file = VcfFileUtil.load(\"test-data/tmp/chr20.dose.vcf.gz\", 100000000, false);\n    assertEquals(\"20\", file.getChromosome());\n    assertEquals(51, file.getNoSamples());\n    assertEquals(true, file.isPhased());\n    assertEquals(TOTAL_REFPANEL_CHR20 - FILTER_REFPANEL + ONLY_IN_INPUT, file.getNoSnps());\n}",
        "label": 1
    },
    {
        "repo_name": "treelogic-swe___aws-mock",
        "commit": "1d1058c0ebbd0615efa28695481e36ae3ffc58a0",
        "commit_message": "Merge pull request #57 from Dynatrace/bugfix/fix-persistence-of-volumes\n\nFix issue in persistence of volumeArray that caused crash during loading ",
        "p_path": "src/main/java/com/tlswe/awsmock/common/listener/AppServletContextListener.java",
        "t_path": "src/test/java/com/tlswe/awsmock/common/listener/AppServletContextListenerTest.java",
        "p_name": "contextDestroyed",
        "t_name": "Test_contextDestroyedPersistenceEnabled",
        "lpfc": "@Override\npublic final void contextDestroyed(final ServletContextEvent sce) {\n    Collection<AbstractMockEc2Instance> instances = MockEc2Controller.getInstance().getAllMockEc2Instances();\n    for (AbstractMockEc2Instance instance : instances) {\n        instance.destroyInternalTimer();\n    }\n    if (persistenceEnabled) {\n        AbstractMockEc2Instance[] array = new AbstractMockEc2Instance[instances.size()];\n        instances.toArray(array);\n        PersistenceUtils.saveAll(array, PersistenceStoreType.EC2);\n        Collection<MockVpc> vpcs = MockVpcController.getInstance().describeVpcs();\n        MockVpc[] vpcArray = new MockVpc[vpcs.size()];\n        vpcs.toArray(vpcArray);\n        PersistenceUtils.saveAll(vpcArray, PersistenceStoreType.VPC);\n        Collection<MockVolume> volumes = MockVolumeController.getInstance().describeVolumes();\n        MockVolume[] volumeArray = new MockVolume[volumes.size()];\n        volumes.toArray(volumeArray);\n        PersistenceUtils.saveAll(volumeArray, PersistenceStoreType.VPC);\n        Collection<MockTags> tags = MockTagsController.getInstance().describeTags();\n        MockTags[] tagArray = new MockTags[tags.size()];\n        tags.toArray(tagArray);\n        PersistenceUtils.saveAll(tagArray, PersistenceStoreType.TAGS);\n        Collection<MockSubnet> subnets = MockSubnetController.getInstance().describeSubnets();\n        MockSubnet[] subnetArray = new MockSubnet[subnets.size()];\n        subnets.toArray(subnetArray);\n        PersistenceUtils.saveAll(subnetArray, PersistenceStoreType.SUBNET);\n        Collection<MockRouteTable> routetables = MockRouteTableController.getInstance().describeRouteTables();\n        MockRouteTable[] routetableArray = new MockRouteTable[routetables.size()];\n        routetables.toArray(routetableArray);\n        PersistenceUtils.saveAll(routetableArray, PersistenceStoreType.ROUTETABLE);\n        Collection<MockInternetGateway> internetgateways = MockInternetGatewayController.getInstance().describeInternetGateways();\n        MockInternetGateway[] internetgatewayArray = new MockInternetGateway[internetgateways.size()];\n        internetgateways.toArray(internetgatewayArray);\n        PersistenceUtils.saveAll(internetgatewayArray, PersistenceStoreType.INTERNETGATEWAY);\n    }\n    MockEc2Controller.getInstance().destroyCleanupTerminatedInstanceTimer();\n    log.info(\"aws-mock stopped.\");\n}",
        "rpfc": "@Override\npublic final void contextDestroyed(final ServletContextEvent sce) {\n    Collection<AbstractMockEc2Instance> instances = MockEc2Controller.getInstance().getAllMockEc2Instances();\n    for (AbstractMockEc2Instance instance : instances) {\n        instance.destroyInternalTimer();\n    }\n    if (persistenceEnabled) {\n        AbstractMockEc2Instance[] array = new AbstractMockEc2Instance[instances.size()];\n        instances.toArray(array);\n        PersistenceUtils.saveAll(array, PersistenceStoreType.EC2);\n        Collection<MockVpc> vpcs = MockVpcController.getInstance().describeVpcs();\n        MockVpc[] vpcArray = new MockVpc[vpcs.size()];\n        vpcs.toArray(vpcArray);\n        PersistenceUtils.saveAll(vpcArray, PersistenceStoreType.VPC);\n        Collection<MockVolume> volumes = MockVolumeController.getInstance().describeVolumes();\n        MockVolume[] volumeArray = new MockVolume[volumes.size()];\n        volumes.toArray(volumeArray);\n        PersistenceUtils.saveAll(volumeArray, PersistenceStoreType.VOLUME);\n        Collection<MockTags> tags = MockTagsController.getInstance().describeTags();\n        MockTags[] tagArray = new MockTags[tags.size()];\n        tags.toArray(tagArray);\n        PersistenceUtils.saveAll(tagArray, PersistenceStoreType.TAGS);\n        Collection<MockSubnet> subnets = MockSubnetController.getInstance().describeSubnets();\n        MockSubnet[] subnetArray = new MockSubnet[subnets.size()];\n        subnets.toArray(subnetArray);\n        PersistenceUtils.saveAll(subnetArray, PersistenceStoreType.SUBNET);\n        Collection<MockRouteTable> routetables = MockRouteTableController.getInstance().describeRouteTables();\n        MockRouteTable[] routetableArray = new MockRouteTable[routetables.size()];\n        routetables.toArray(routetableArray);\n        PersistenceUtils.saveAll(routetableArray, PersistenceStoreType.ROUTETABLE);\n        Collection<MockInternetGateway> internetgateways = MockInternetGatewayController.getInstance().describeInternetGateways();\n        MockInternetGateway[] internetgatewayArray = new MockInternetGateway[internetgateways.size()];\n        internetgateways.toArray(internetgatewayArray);\n        PersistenceUtils.saveAll(internetgatewayArray, PersistenceStoreType.INTERNETGATEWAY);\n    }\n    MockEc2Controller.getInstance().destroyCleanupTerminatedInstanceTimer();\n    log.info(\"aws-mock stopped.\");\n}",
        "tuc": "@Test\npublic void Test_contextDestroyedPersistenceEnabled() {\n    Whitebox.setInternalState(AppServletContextListener.class, \"persistenceEnabled\", true);\n    acl.contextDestroyed(sce);\n    Whitebox.setInternalState(AppServletContextListener.class, \"persistenceEnabled\", false);\n}",
        "label": 1
    },
    {
        "repo_name": "treelogic-swe___aws-mock",
        "commit": "b0c783565efd1f7bbe37d815f3b39acfebfa0b29",
        "commit_message": "Fix issue in persistence of volumeArray that caused crash during loading. Store correct data type.\n",
        "p_path": "src/main/java/com/tlswe/awsmock/common/listener/AppServletContextListener.java",
        "t_path": "src/test/java/com/tlswe/awsmock/common/listener/AppServletContextListenerTest.java",
        "p_name": "contextDestroyed",
        "t_name": "Test_contextDestroyedPersistenceEnabled",
        "lpfc": "@Override\npublic final void contextDestroyed(final ServletContextEvent sce) {\n    Collection<AbstractMockEc2Instance> instances = MockEc2Controller.getInstance().getAllMockEc2Instances();\n    for (AbstractMockEc2Instance instance : instances) {\n        instance.destroyInternalTimer();\n    }\n    if (persistenceEnabled) {\n        AbstractMockEc2Instance[] array = new AbstractMockEc2Instance[instances.size()];\n        instances.toArray(array);\n        PersistenceUtils.saveAll(array, PersistenceStoreType.EC2);\n        Collection<MockVpc> vpcs = MockVpcController.getInstance().describeVpcs();\n        MockVpc[] vpcArray = new MockVpc[vpcs.size()];\n        vpcs.toArray(vpcArray);\n        PersistenceUtils.saveAll(vpcArray, PersistenceStoreType.VPC);\n        Collection<MockVolume> volumes = MockVolumeController.getInstance().describeVolumes();\n        MockVolume[] volumeArray = new MockVolume[volumes.size()];\n        volumes.toArray(volumeArray);\n        PersistenceUtils.saveAll(volumeArray, PersistenceStoreType.VPC);\n        Collection<MockTags> tags = MockTagsController.getInstance().describeTags();\n        MockTags[] tagArray = new MockTags[tags.size()];\n        tags.toArray(tagArray);\n        PersistenceUtils.saveAll(tagArray, PersistenceStoreType.TAGS);\n        Collection<MockSubnet> subnets = MockSubnetController.getInstance().describeSubnets();\n        MockSubnet[] subnetArray = new MockSubnet[subnets.size()];\n        subnets.toArray(subnetArray);\n        PersistenceUtils.saveAll(subnetArray, PersistenceStoreType.SUBNET);\n        Collection<MockRouteTable> routetables = MockRouteTableController.getInstance().describeRouteTables();\n        MockRouteTable[] routetableArray = new MockRouteTable[routetables.size()];\n        routetables.toArray(routetableArray);\n        PersistenceUtils.saveAll(routetableArray, PersistenceStoreType.ROUTETABLE);\n        Collection<MockInternetGateway> internetgateways = MockInternetGatewayController.getInstance().describeInternetGateways();\n        MockInternetGateway[] internetgatewayArray = new MockInternetGateway[internetgateways.size()];\n        internetgateways.toArray(internetgatewayArray);\n        PersistenceUtils.saveAll(internetgatewayArray, PersistenceStoreType.INTERNETGATEWAY);\n    }\n    MockEc2Controller.getInstance().destroyCleanupTerminatedInstanceTimer();\n    log.info(\"aws-mock stopped.\");\n}",
        "rpfc": "@Override\npublic final void contextDestroyed(final ServletContextEvent sce) {\n    Collection<AbstractMockEc2Instance> instances = MockEc2Controller.getInstance().getAllMockEc2Instances();\n    for (AbstractMockEc2Instance instance : instances) {\n        instance.destroyInternalTimer();\n    }\n    if (persistenceEnabled) {\n        AbstractMockEc2Instance[] array = new AbstractMockEc2Instance[instances.size()];\n        instances.toArray(array);\n        PersistenceUtils.saveAll(array, PersistenceStoreType.EC2);\n        Collection<MockVpc> vpcs = MockVpcController.getInstance().describeVpcs();\n        MockVpc[] vpcArray = new MockVpc[vpcs.size()];\n        vpcs.toArray(vpcArray);\n        PersistenceUtils.saveAll(vpcArray, PersistenceStoreType.VPC);\n        Collection<MockVolume> volumes = MockVolumeController.getInstance().describeVolumes();\n        MockVolume[] volumeArray = new MockVolume[volumes.size()];\n        volumes.toArray(volumeArray);\n        PersistenceUtils.saveAll(volumeArray, PersistenceStoreType.VOLUME);\n        Collection<MockTags> tags = MockTagsController.getInstance().describeTags();\n        MockTags[] tagArray = new MockTags[tags.size()];\n        tags.toArray(tagArray);\n        PersistenceUtils.saveAll(tagArray, PersistenceStoreType.TAGS);\n        Collection<MockSubnet> subnets = MockSubnetController.getInstance().describeSubnets();\n        MockSubnet[] subnetArray = new MockSubnet[subnets.size()];\n        subnets.toArray(subnetArray);\n        PersistenceUtils.saveAll(subnetArray, PersistenceStoreType.SUBNET);\n        Collection<MockRouteTable> routetables = MockRouteTableController.getInstance().describeRouteTables();\n        MockRouteTable[] routetableArray = new MockRouteTable[routetables.size()];\n        routetables.toArray(routetableArray);\n        PersistenceUtils.saveAll(routetableArray, PersistenceStoreType.ROUTETABLE);\n        Collection<MockInternetGateway> internetgateways = MockInternetGatewayController.getInstance().describeInternetGateways();\n        MockInternetGateway[] internetgatewayArray = new MockInternetGateway[internetgateways.size()];\n        internetgateways.toArray(internetgatewayArray);\n        PersistenceUtils.saveAll(internetgatewayArray, PersistenceStoreType.INTERNETGATEWAY);\n    }\n    MockEc2Controller.getInstance().destroyCleanupTerminatedInstanceTimer();\n    log.info(\"aws-mock stopped.\");\n}",
        "tuc": "@Test\npublic void Test_contextDestroyedPersistenceEnabled() {\n    Whitebox.setInternalState(AppServletContextListener.class, \"persistenceEnabled\", true);\n    acl.contextDestroyed(sce);\n    Whitebox.setInternalState(AppServletContextListener.class, \"persistenceEnabled\", false);\n}",
        "label": 1
    },
    {
        "repo_name": "vivo-project___Vitro",
        "commit": "6d46883865b0fead3486db0169b8b0c479ba2039",
        "commit_message": "Allow rdfs:label on ARM migration\n",
        "p_path": "api/src/main/java/edu/cornell/mannlib/vitro/webapp/migration/auth/ArmMigrator.java",
        "t_path": "api/src/test/java/edu/cornell/mannlib/vitro/webapp/migration/auth/ArmMigratorTest.java",
        "p_name": "getRoleOperationObjectTypedStatementsToAdd",
        "t_name": "migrateConfigurationTest",
        "lpfc": "private void getRoleOperationObjectTypedStatementsToAdd(Map<AccessObjectType, Set<String>> entityTypeMap, String role, String newRole, Set<String> armEntities, OperationGroup og, Set<String> addFauxOP, Set<String> addFauxDP, AccessObjectType type) {\n    Set<String> allTypeEntitities = entityTypeMap.get(type);\n    HashSet<String> intersectionEntities = new HashSet<>(armEntities);\n    intersectionEntities.retainAll(allTypeEntitities);\n    if (AccessObjectType.OBJECT_PROPERTY.equals(type)) {\n        for (String entity : intersectionEntities) {\n            Set<String> faux = getFauxByBase(entity);\n            addFauxOP.addAll(faux);\n        }\n    }\n    if (AccessObjectType.DATA_PROPERTY.equals(type)) {\n        for (String entity : intersectionEntities) {\n            Set<String> faux = getFauxByBase(entity);\n            addFauxDP.addAll(faux);\n        }\n    }\n    if (AccessObjectType.FAUX_OBJECT_PROPERTY.equals(type)) {\n        intersectionEntities.addAll(addFauxOP);\n    }\n    if (AccessObjectType.FAUX_DATA_PROPERTY.equals(type)) {\n        intersectionEntities.addAll(addFauxDP);\n    }\n    for (AccessOperation ao : OperationGroup.getOperations(og)) {\n        for (String entityUri : intersectionEntities) {\n            log.info(String.format(\"Allow %s role to %s %s entity <%s>\", role, ao, type, entityUri));\n            EntityPolicyController.getDataValueStatements(entityUri, type, ao, Collections.singleton(newRole), additions);\n        }\n    }\n}",
        "rpfc": "private void getRoleOperationObjectTypedStatementsToAdd(Map<AccessObjectType, Set<String>> entityTypeMap, String role, String newRole, Set<String> armEntities, OperationGroup og, Set<String> addFauxOP, Set<String> addFauxDP, AccessObjectType type) {\n    Set<String> allTypeEntitities = entityTypeMap.get(type);\n    HashSet<String> intersectionEntities = new HashSet<>(armEntities);\n    intersectionEntities.retainAll(allTypeEntitities);\n    if (AccessObjectType.OBJECT_PROPERTY.equals(type)) {\n        for (String entity : intersectionEntities) {\n            Set<String> faux = getFauxByBase(entity);\n            addFauxOP.addAll(faux);\n        }\n    }\n    if (AccessObjectType.DATA_PROPERTY.equals(type)) {\n        for (String entity : intersectionEntities) {\n            Set<String> faux = getFauxByBase(entity);\n            addFauxDP.addAll(faux);\n        }\n        if (!(ARM_PUBLIC.equals(role) && !OperationGroup.DISPLAY_GROUP.equals(og))) {\n            intersectionEntities.add(VitroVocabulary.LABEL);\n        }\n    }\n    if (AccessObjectType.FAUX_OBJECT_PROPERTY.equals(type)) {\n        intersectionEntities.addAll(addFauxOP);\n    }\n    if (AccessObjectType.FAUX_DATA_PROPERTY.equals(type)) {\n        intersectionEntities.addAll(addFauxDP);\n    }\n    for (AccessOperation ao : OperationGroup.getOperations(og)) {\n        for (String entityUri : intersectionEntities) {\n            log.info(String.format(\"Allow %s role to %s %s entity <%s>\", role, ao, type, entityUri));\n            EntityPolicyController.getDataValueStatements(entityUri, type, ao, Collections.singleton(newRole), additions);\n        }\n    }\n}",
        "tuc": "@Test\npublic void migrateConfigurationTest() {\n    addUserAccountsStatement(PolicyTest.CUSTOM, VitroVocabulary.PERMISSIONSET, VitroVocabulary.RDF_TYPE);\n    armMigrator = new ArmMigrator(new RDFServiceModel(contentModel), new RDFServiceModel(configurationDataSet), userAccountsModel);\n    addArmStatement(ArmMigrator.ARM_ADMIN, ArmMigrator.DISPLAY, OBJECT_PROPERTY_URI);\n    addArmStatement(ArmMigrator.ARM_EDITOR, ArmMigrator.PUBLISH, OBJECT_PROPERTY_URI);\n    addArmStatement(ArmMigrator.ARM_CURATOR, ArmMigrator.UPDATE, OBJECT_PROPERTY_URI);\n    addArmStatement(ArmMigrator.ARM_SELF_EDITOR, ArmMigrator.DISPLAY, CLASS_URI);\n    addArmStatement(ArmMigrator.ARM_EDITOR, ArmMigrator.PUBLISH, CLASS_URI);\n    addArmStatement(ArmMigrator.ARM_ADMIN, ArmMigrator.DISPLAY, DATA_PROPERTY_URI);\n    addArmStatement(ArmMigrator.ARM_EDITOR, ArmMigrator.PUBLISH, DATA_PROPERTY_URI);\n    addArmStatement(ArmMigrator.ARM_CURATOR, ArmMigrator.UPDATE, DATA_PROPERTY_URI);\n    addArmStatement(ArmMigrator.ARM_SELF_EDITOR, ArmMigrator.DISPLAY, FAUX_DATA_PROPERTY_URI);\n    addArmStatement(ArmMigrator.ARM_EDITOR, ArmMigrator.PUBLISH, FAUX_DATA_PROPERTY_URI);\n    addArmStatement(ArmMigrator.ARM_ADMIN, ArmMigrator.UPDATE, FAUX_DATA_PROPERTY_URI);\n    addArmStatement(ArmMigrator.ARM_PUBLIC, ArmMigrator.DISPLAY, FAUX_OBJECT_PROPERTY_URI);\n    addArmStatement(ArmMigrator.ARM_EDITOR, ArmMigrator.PUBLISH, FAUX_OBJECT_PROPERTY_URI);\n    addArmStatement(ArmMigrator.ARM_CURATOR, ArmMigrator.UPDATE, FAUX_OBJECT_PROPERTY_URI);\n    addArmStatement(ArmMigrator.ARM_CUSTOM, ArmMigrator.DISPLAY, OBJECT_PROPERTY_URI);\n    addArmStatement(ArmMigrator.ARM_CUSTOM, ArmMigrator.UPDATE, FAUX_DATA_PROPERTY_URI);\n    addArmStatement(ArmMigrator.ARM_CUSTOM, ArmMigrator.DISPLAY, CLASS_URI);\n    Map<AccessObjectType, Set<String>> entityTypeMap = armMigrator.getEntityMap();\n    armMigrator.collectAdditions(entityTypeMap);\n    String stringResult = armMigrator.additions.toString();\n    assertFalse(stringResult.isEmpty());\n    assertEquals(33, getCount(\"\\n\", stringResult));\n    assertEquals(33, getCount(value, stringResult));\n    assertEquals(6, getCount(OBJECT_PROPERTY_URI, stringResult));\n    assertEquals(3, getCount(CLASS_URI, stringResult));\n    assertEquals(5, getCount(DATA_PROPERTY_URI, stringResult));\n    assertEquals(12, getCount(FAUX_DATA_PROPERTY_URI, stringResult));\n    assertEquals(7, getCount(FAUX_OBJECT_PROPERTY_URI, stringResult));\n}",
        "label": 1
    },
    {
        "repo_name": "apache___commons-functor",
        "commit": "962b860747dd56b73317503e15d9eec82a7e4649",
        "commit_message": "return int instead of Integer\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/jakarta/commons/sandbox/functor/trunk@144492 13f79535-47bb-0310-9956-ffa450edef68\n",
        "p_path": "src/java/org/apache/commons/functor/util/BinarySearch.java",
        "t_path": "src/test/org/apache/commons/functor/util/TestBinarySearch.java",
        "p_name": "execute",
        "t_name": "testBasicSearch",
        "lpfc": "public static Integer execute(List list, Comparable item) {\n    return (Integer) (new BinarySearch(list, item)).recurse();\n}",
        "rpfc": "public static int execute(List list, Comparable item) {\n    return ((Number) (new BinarySearch(list, item)).recurse()).intValue();\n}",
        "tuc": "public void testBasicSearch() {\n    List list = new ArrayList();\n    for (int i = 0; i < 100; i++) {\n        list.add(new Integer(i));\n    }\n    Integer position = (Integer) new BinarySearch(list, new Integer(10)).recurse();\n    assertEquals(new Integer(10), position);\n    position = (Integer) new BinarySearch(list, new Integer(86)).recurse();\n    assertEquals(new Integer(86), position);\n    position = (Integer) new BinarySearch(list, new Integer(-1)).recurse();\n    assertEquals(new Integer(-1), position);\n    position = BinarySearch.execute(list, new Integer(86));\n    assertEquals(new Integer(86), position);\n    position = (Integer) new BinarySearch(new ArrayList(), new Integer(10)).recurse();\n    assertEquals(new Integer(-1), position);\n}",
        "label": 1
    },
    {
        "repo_name": "apache___commons-functor",
        "commit": "1c8c802d25890b218157e142e45b4f8ee03c1aea",
        "commit_message": "fix bug, add test, add todo\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/jakarta/commons/sandbox/functor/trunk@144491 13f79535-47bb-0310-9956-ffa450edef68\n",
        "p_path": "src/java/org/apache/commons/functor/util/BinarySearch.java",
        "t_path": "src/test/org/apache/commons/functor/util/TestBinarySearch.java",
        "p_name": "evaluate",
        "t_name": "testBasicSearch",
        "lpfc": "/**\n * ************************************************\n *   Instance methods\n * *************************************************\n */\n/**\n * Either returns the next BinarySearch function or position of the item\n * when it is found. If the item is not found -1 (as Integer) is returned.\n */\npublic Object evaluate() {\n    if (lower == upper) {\n        return list.get(upper).equals(item) ? new Integer(upper) : new Integer(-1);\n    } else {\n        int middle = (lower + upper) / 2;\n        if (item.compareTo(list.get(middle)) > 0) {\n            return new BinarySearch(list, item, middle + 1, upper);\n        } else {\n            return new BinarySearch(list, item, lower, middle);\n        }\n    }\n}",
        "rpfc": "/**\n * ************************************************\n *   Instance methods\n * *************************************************\n */\n/**\n * Either returns the next BinarySearch function or position of the item\n * when it is found. If the item is not found -1 (as Integer) is returned.\n */\npublic Object evaluate() {\n    // TODO: should be using compareTo instead of equals\n    if (lower == upper) {\n        if (upper >= list.size()) {\n            return new Integer(-1);\n        } else if (list.get(upper).equals(item)) {\n            return new Integer(upper);\n        } else {\n            return new Integer(-1);\n        }\n    } else {\n        int middle = (lower + upper) / 2;\n        if (item.compareTo(list.get(middle)) > 0) {\n            return new BinarySearch(list, item, middle + 1, upper);\n        } else {\n            return new BinarySearch(list, item, lower, middle);\n        }\n    }\n}",
        "tuc": "public void testBasicSearch() {\n    List list = new ArrayList();\n    for (int i = 0; i < 100; i++) {\n        list.add(new Integer(i));\n    }\n    Integer position = (Integer) new BinarySearch(list, new Integer(10)).recurse();\n    assertEquals(new Integer(10), position);\n    position = (Integer) new BinarySearch(list, new Integer(86)).recurse();\n    assertEquals(new Integer(86), position);\n    position = (Integer) new BinarySearch(list, new Integer(-1)).recurse();\n    assertEquals(new Integer(-1), position);\n    position = BinarySearch.execute(list, new Integer(86));\n    assertEquals(new Integer(86), position);\n}",
        "label": 1
    },
    {
        "repo_name": "code-not-found___spring-jms",
        "commit": "289086c87ed4efda8976d60445105bb950171445",
        "commit_message": "update test\n",
        "p_path": "spring-jms-message-selector/src/main/java/com/codenotfound/jms/Receiver.java",
        "t_path": "spring-jms-message-selector/src/test/java/com/codenotfound/jms/SpringJmsApplicationTest.java",
        "p_name": "receiveLow",
        "t_name": "testReceive",
        "lpfc": "@JmsListener(destination = \"${queue.boot}\", selector = \"priority = 'low'\")\npublic void receiveLow(String message) {\n    LOGGER.info(\"received low priority message='{}'\", message);\n    latch.countDown();\n}",
        "rpfc": "@JmsListener(destination = \"${queue.boot}\", selector = \"priority = 'low'\")\npublic void receiveLow(String message) {\n    LOGGER.info(\"received low priority message='{}'\", message);\n}",
        "tuc": "@Test\npublic void testReceive() throws Exception {\n    sender.send(\"selector.q\", \"High priority!\", true);\n    sender.send(\"selector.q\", \"Low priority!\", false);\n    receiver.getLatch().await(10000, TimeUnit.MILLISECONDS);\n    assertThat(receiver.getLatch().getCount()).isEqualTo(0);\n}",
        "label": 1
    },
    {
        "repo_name": "AxonFramework___extension-kafka",
        "commit": "e8c2c569b95b68ef7d3cd4de61c2387b266016f8",
        "commit_message": "Drop the overloading of the start method\n\nThe SubscribableKafkaMessageSource has a start() and start(List<String),\n where the latter allowed a user to start with a different list of\n topics. However, there are no real use cases when somebody would (1)\n build a SubscribableKafkaMessageSource *with* the desired topics and\n (2) start the source with *different* topics after that. Hence, the\n start(List<String>) method should be removed in favor of start()\n\n#17-3\n",
        "p_path": "kafka/src/main/java/org/axonframework/extensions/kafka/eventhandling/consumer/subscribable/SubscribableKafkaMessageSource.java",
        "t_path": "kafka/src/test/java/org/axonframework/extensions/kafka/eventhandling/consumer/subscribable/SubscribableKafkaMessageSourceTest.java",
        "p_name": "start",
        "t_name": "testStartSubscribesConsumerToAllProvidedTopics",
        "lpfc": "public void start(List<String> topics) {\n    inProgress.set(true);\n    eventProcessors.forEach(eventProcessor -> {\n        Consumer<K, V> consumer = consumerFactory.createConsumer(groupId);\n        consumer.subscribe(topics);\n        Registration closeConsumer = fetcher.poll(consumer, consumerRecords -> StreamSupport.stream(consumerRecords.spliterator(), false).map(messageConverter::readKafkaMessage).filter(Optional::isPresent).map(Optional::get).collect(Collectors.toList()), eventMessages -> {\n            if (eventMessages.isEmpty()) {\n                return;\n            }\n            eventProcessor.accept(eventMessages);\n        });\n        fetcherRegistrations.put(eventProcessor, closeConsumer);\n    });\n}",
        "rpfc": "public void start() {\n    inProgress.set(true);\n    eventProcessors.forEach(eventProcessor -> {\n        Consumer<K, V> consumer = consumerFactory.createConsumer(groupId);\n        consumer.subscribe(topics);\n        Registration closeConsumer = fetcher.poll(consumer, consumerRecords -> StreamSupport.stream(consumerRecords.spliterator(), false).map(messageConverter::readKafkaMessage).filter(Optional::isPresent).map(Optional::get).collect(Collectors.toList()), eventMessages -> {\n            if (eventMessages.isEmpty()) {\n                return;\n            }\n            eventProcessor.accept(eventMessages);\n        });\n        fetcherRegistrations.put(eventProcessor, closeConsumer);\n    });\n}",
        "tuc": "@Test\nvoid testStartSubscribesConsumerToAllProvidedTopics() {\n    when(fetcher.poll(eq(mockConsumer), any(), any())).thenReturn(NO_OP_FETCHER_REGISTRATION);\n    List<String> testTopics = new ArrayList<>();\n    testTopics.add(\"topicOne\");\n    testTopics.add(\"topicTwo\");\n    testSubject.subscribe(NO_OP_EVENT_PROCESSOR);\n    testSubject.start(testTopics);\n    verify(consumerFactory).createConsumer(DEFAULT_GROUP_ID);\n    verify(mockConsumer).subscribe(testTopics);\n    verify(fetcher).poll(eq(mockConsumer), any(), any());\n}",
        "label": 1
    },
    {
        "repo_name": "intuit___QuickBooks-V3-Java-SDK",
        "commit": "afce8bf6d84a0bd1a87c4a3a8116a37e647823aa",
        "commit_message": "update minor version 40\n",
        "p_path": "ipp-v3-java-devkit/src/main/java/com/intuit/ipp/interceptors/PrepareRequestInterceptor.java",
        "t_path": "ipp-v3-java-devkit/src/test/java/com/intuit/ipp/interceptors/PrepareRequestInterceptorTest.java",
        "p_name": "prepareQBOUri",
        "t_name": "testExecute_QBO_URI",
        "lpfc": "private <T extends IEntity> String prepareQBOUri(String entityName, Context context, Map<String, String> requestParameters) throws FMSException {\n    StringBuilder uri = new StringBuilder();\n    if (entityName.equalsIgnoreCase(\"Taxservice\")) {\n        entityName = entityName + \"/\" + \"taxcode\";\n    }\n    uri.append(getBaseUrl(Config.getProperty(Config.BASE_URL_QBO))).append(\"/\").append(context.getRealmID()).append(\"/\").append(entityName);\n    addEntityID(requestParameters, uri);\n    addEntitySelector(requestParameters, uri);\n    addParentID(requestParameters, uri);\n    uri.append(\"?\").append(buildRequestParams(requestParameters));\n    uri.append(\"requestid\").append(\"=\").append(context.getRequestID()).append(\"&\");\n    context.setRequestID(null);\n    if (context.getMinorVersion() == null) {\n        context.setMinorVersion(\"38\");\n    }\n    uri.append(\"minorversion\").append(\"=\").append(context.getMinorVersion()).append(\"&\");\n    if (context.getIncludeParam() != null) {\n        int includeval = context.getIncludeParam().size();\n        String includeParam = \"\";\n        if (includeval > 0) {\n            for (int i = 0; i < includeval; i++) {\n                includeParam = includeParam + context.getIncludeParam().get(i) + \",\";\n            }\n            uri.append(\"include\").append(\"=\").append(includeParam.substring(0, includeParam.length() - 1)).append(\"&\");\n        }\n    }\n    return uri.toString();\n}",
        "rpfc": "private <T extends IEntity> String prepareQBOUri(String entityName, Context context, Map<String, String> requestParameters) throws FMSException {\n    StringBuilder uri = new StringBuilder();\n    if (entityName.equalsIgnoreCase(\"Taxservice\")) {\n        entityName = entityName + \"/\" + \"taxcode\";\n    }\n    uri.append(getBaseUrl(Config.getProperty(Config.BASE_URL_QBO))).append(\"/\").append(context.getRealmID()).append(\"/\").append(entityName);\n    addEntityID(requestParameters, uri);\n    addEntitySelector(requestParameters, uri);\n    addParentID(requestParameters, uri);\n    uri.append(\"?\").append(buildRequestParams(requestParameters));\n    uri.append(\"requestid\").append(\"=\").append(context.getRequestID()).append(\"&\");\n    context.setRequestID(null);\n    if (context.getMinorVersion() == null) {\n        context.setMinorVersion(\"40\");\n    }\n    uri.append(\"minorversion\").append(\"=\").append(context.getMinorVersion()).append(\"&\");\n    if (context.getIncludeParam() != null) {\n        int includeval = context.getIncludeParam().size();\n        String includeParam = \"\";\n        if (includeval > 0) {\n            for (int i = 0; i < includeval; i++) {\n                includeParam = includeParam + context.getIncludeParam().get(i) + \",\";\n            }\n            uri.append(\"include\").append(\"=\").append(includeParam.substring(0, includeParam.length() - 1)).append(\"&\");\n        }\n    }\n    return uri.toString();\n}",
        "tuc": "@Test\npublic void testExecute_QBO_URI() throws FMSException {\n    instance.execute(message);\n    String actual = message.getRequestElements().getRequestParameters().get(\"uri\");\n    Assert.assertEquals(actual, Config.getProperty(Config.BASE_URL_QBO) + \"/fakeRealm/fakeAction?requestid=anyRequestID&minorversion=38&\");\n}",
        "label": 1
    },
    {
        "repo_name": "intuit___QuickBooks-V3-Java-SDK",
        "commit": "928e1e89c5c55a15d54f9623836229a359dc35fc",
        "commit_message": "update minor version\n",
        "p_path": "ipp-v3-java-devkit/src/main/java/com/intuit/ipp/interceptors/PrepareRequestInterceptor.java",
        "t_path": "ipp-v3-java-devkit/src/test/java/com/intuit/ipp/interceptors/PrepareRequestInterceptorTest.java",
        "p_name": "prepareQBOUri",
        "t_name": "testExecute_QBO_URI",
        "lpfc": "private <T extends IEntity> String prepareQBOUri(String entityName, Context context, Map<String, String> requestParameters) throws FMSException {\n    StringBuilder uri = new StringBuilder();\n    if (entityName.equalsIgnoreCase(\"Taxservice\")) {\n        entityName = entityName + \"/\" + \"taxcode\";\n    }\n    uri.append(getBaseUrl(Config.getProperty(Config.BASE_URL_QBO))).append(\"/\").append(context.getRealmID()).append(\"/\").append(entityName);\n    addEntityID(requestParameters, uri);\n    addEntitySelector(requestParameters, uri);\n    addParentID(requestParameters, uri);\n    uri.append(\"?\").append(buildRequestParams(requestParameters));\n    uri.append(\"requestid\").append(\"=\").append(context.getRequestID()).append(\"&\");\n    context.setRequestID(null);\n    if (context.getMinorVersion() == null) {\n        context.setMinorVersion(\"37\");\n    }\n    uri.append(\"minorversion\").append(\"=\").append(context.getMinorVersion()).append(\"&\");\n    if (context.getIncludeParam() != null) {\n        int includeval = context.getIncludeParam().size();\n        String includeParam = \"\";\n        if (includeval > 0) {\n            for (int i = 0; i < includeval; i++) {\n                includeParam = includeParam + context.getIncludeParam().get(i) + \",\";\n            }\n            uri.append(\"include\").append(\"=\").append(includeParam.substring(0, includeParam.length() - 1)).append(\"&\");\n        }\n    }\n    return uri.toString();\n}",
        "rpfc": "private <T extends IEntity> String prepareQBOUri(String entityName, Context context, Map<String, String> requestParameters) throws FMSException {\n    StringBuilder uri = new StringBuilder();\n    if (entityName.equalsIgnoreCase(\"Taxservice\")) {\n        entityName = entityName + \"/\" + \"taxcode\";\n    }\n    uri.append(getBaseUrl(Config.getProperty(Config.BASE_URL_QBO))).append(\"/\").append(context.getRealmID()).append(\"/\").append(entityName);\n    addEntityID(requestParameters, uri);\n    addEntitySelector(requestParameters, uri);\n    addParentID(requestParameters, uri);\n    uri.append(\"?\").append(buildRequestParams(requestParameters));\n    uri.append(\"requestid\").append(\"=\").append(context.getRequestID()).append(\"&\");\n    context.setRequestID(null);\n    if (context.getMinorVersion() == null) {\n        context.setMinorVersion(\"38\");\n    }\n    uri.append(\"minorversion\").append(\"=\").append(context.getMinorVersion()).append(\"&\");\n    if (context.getIncludeParam() != null) {\n        int includeval = context.getIncludeParam().size();\n        String includeParam = \"\";\n        if (includeval > 0) {\n            for (int i = 0; i < includeval; i++) {\n                includeParam = includeParam + context.getIncludeParam().get(i) + \",\";\n            }\n            uri.append(\"include\").append(\"=\").append(includeParam.substring(0, includeParam.length() - 1)).append(\"&\");\n        }\n    }\n    return uri.toString();\n}",
        "tuc": "@Test\npublic void testExecute_QBO_URI() throws FMSException {\n    instance.execute(message);\n    String actual = message.getRequestElements().getRequestParameters().get(\"uri\");\n    Assert.assertEquals(actual, Config.getProperty(Config.BASE_URL_QBO) + \"/fakeRealm/fakeAction?requestid=anyRequestID&minorversion=37&\");\n}",
        "label": 1
    },
    {
        "repo_name": "intuit___QuickBooks-V3-Java-SDK",
        "commit": "a43f9f291dad7bb578ef9ac671e66e833019c168",
        "commit_message": "update to latest minor version\n",
        "p_path": "ipp-v3-java-devkit/src/main/java/com/intuit/ipp/interceptors/PrepareRequestInterceptor.java",
        "t_path": "ipp-v3-java-devkit/src/test/java/com/intuit/ipp/interceptors/PrepareRequestInterceptorTest.java",
        "p_name": "prepareQBOUri",
        "t_name": "testExecute_QBO_URI",
        "lpfc": "private <T extends IEntity> String prepareQBOUri(String entityName, Context context, Map<String, String> requestParameters) throws FMSException {\n    StringBuilder uri = new StringBuilder();\n    if (entityName.equalsIgnoreCase(\"Taxservice\")) {\n        entityName = entityName + \"/\" + \"taxcode\";\n    }\n    uri.append(getBaseUrl(Config.getProperty(Config.BASE_URL_QBO))).append(\"/\").append(context.getRealmID()).append(\"/\").append(entityName);\n    addEntityID(requestParameters, uri);\n    addEntitySelector(requestParameters, uri);\n    addParentID(requestParameters, uri);\n    uri.append(\"?\").append(buildRequestParams(requestParameters));\n    uri.append(\"requestid\").append(\"=\").append(context.getRequestID()).append(\"&\");\n    context.setRequestID(null);\n    if (context.getMinorVersion() == null) {\n        context.setMinorVersion(\"36\");\n    }\n    uri.append(\"minorversion\").append(\"=\").append(context.getMinorVersion()).append(\"&\");\n    if (context.getIncludeParam() != null) {\n        int includeval = context.getIncludeParam().size();\n        String includeParam = \"\";\n        if (includeval > 0) {\n            for (int i = 0; i < includeval; i++) {\n                includeParam = includeParam + context.getIncludeParam().get(i) + \",\";\n            }\n            uri.append(\"include\").append(\"=\").append(includeParam.substring(0, includeParam.length() - 1)).append(\"&\");\n        }\n    }\n    return uri.toString();\n}",
        "rpfc": "private <T extends IEntity> String prepareQBOUri(String entityName, Context context, Map<String, String> requestParameters) throws FMSException {\n    StringBuilder uri = new StringBuilder();\n    if (entityName.equalsIgnoreCase(\"Taxservice\")) {\n        entityName = entityName + \"/\" + \"taxcode\";\n    }\n    uri.append(getBaseUrl(Config.getProperty(Config.BASE_URL_QBO))).append(\"/\").append(context.getRealmID()).append(\"/\").append(entityName);\n    addEntityID(requestParameters, uri);\n    addEntitySelector(requestParameters, uri);\n    addParentID(requestParameters, uri);\n    uri.append(\"?\").append(buildRequestParams(requestParameters));\n    uri.append(\"requestid\").append(\"=\").append(context.getRequestID()).append(\"&\");\n    context.setRequestID(null);\n    if (context.getMinorVersion() == null) {\n        context.setMinorVersion(\"37\");\n    }\n    uri.append(\"minorversion\").append(\"=\").append(context.getMinorVersion()).append(\"&\");\n    if (context.getIncludeParam() != null) {\n        int includeval = context.getIncludeParam().size();\n        String includeParam = \"\";\n        if (includeval > 0) {\n            for (int i = 0; i < includeval; i++) {\n                includeParam = includeParam + context.getIncludeParam().get(i) + \",\";\n            }\n            uri.append(\"include\").append(\"=\").append(includeParam.substring(0, includeParam.length() - 1)).append(\"&\");\n        }\n    }\n    return uri.toString();\n}",
        "tuc": "@Test\npublic void testExecute_QBO_URI() throws FMSException {\n    instance.execute(message);\n    String actual = message.getRequestElements().getRequestParameters().get(\"uri\");\n    Assert.assertEquals(actual, Config.getProperty(Config.BASE_URL_QBO) + \"/fakeRealm/fakeAction?requestid=anyRequestID&minorversion=36&\");\n}",
        "label": 1
    },
    {
        "repo_name": "intuit___QuickBooks-V3-Java-SDK",
        "commit": "f469306b91ee60ca3777c4c01c4b83f7e00b45c2",
        "commit_message": "update minor version\n",
        "p_path": "ipp-v3-java-devkit/src/main/java/com/intuit/ipp/interceptors/PrepareRequestInterceptor.java",
        "t_path": "ipp-v3-java-devkit/src/test/java/com/intuit/ipp/interceptors/PrepareRequestInterceptorTest.java",
        "p_name": "prepareQBOUri",
        "t_name": "testExecute_QBO_URI",
        "lpfc": "private <T extends IEntity> String prepareQBOUri(String entityName, Context context, Map<String, String> requestParameters) throws FMSException {\n    StringBuilder uri = new StringBuilder();\n    if (entityName.equalsIgnoreCase(\"Taxservice\")) {\n        entityName = entityName + \"/\" + \"taxcode\";\n    }\n    uri.append(getBaseUrl(Config.getProperty(Config.BASE_URL_QBO))).append(\"/\").append(context.getRealmID()).append(\"/\").append(entityName);\n    addEntityID(requestParameters, uri);\n    addEntitySelector(requestParameters, uri);\n    addParentID(requestParameters, uri);\n    uri.append(\"?\").append(buildRequestParams(requestParameters));\n    uri.append(\"requestid\").append(\"=\").append(context.getRequestID()).append(\"&\");\n    context.setRequestID(null);\n    if (context.getMinorVersion() == null) {\n        context.setMinorVersion(\"35\");\n    }\n    uri.append(\"minorversion\").append(\"=\").append(context.getMinorVersion()).append(\"&\");\n    if (context.getIncludeParam() != null) {\n        int includeval = context.getIncludeParam().size();\n        String includeParam = \"\";\n        if (includeval > 0) {\n            for (int i = 0; i < includeval; i++) {\n                includeParam = includeParam + context.getIncludeParam().get(i) + \",\";\n            }\n            uri.append(\"include\").append(\"=\").append(includeParam.substring(0, includeParam.length() - 1)).append(\"&\");\n        }\n    }\n    return uri.toString();\n}",
        "rpfc": "private <T extends IEntity> String prepareQBOUri(String entityName, Context context, Map<String, String> requestParameters) throws FMSException {\n    StringBuilder uri = new StringBuilder();\n    if (entityName.equalsIgnoreCase(\"Taxservice\")) {\n        entityName = entityName + \"/\" + \"taxcode\";\n    }\n    uri.append(getBaseUrl(Config.getProperty(Config.BASE_URL_QBO))).append(\"/\").append(context.getRealmID()).append(\"/\").append(entityName);\n    addEntityID(requestParameters, uri);\n    addEntitySelector(requestParameters, uri);\n    addParentID(requestParameters, uri);\n    uri.append(\"?\").append(buildRequestParams(requestParameters));\n    uri.append(\"requestid\").append(\"=\").append(context.getRequestID()).append(\"&\");\n    context.setRequestID(null);\n    if (context.getMinorVersion() == null) {\n        context.setMinorVersion(\"36\");\n    }\n    uri.append(\"minorversion\").append(\"=\").append(context.getMinorVersion()).append(\"&\");\n    if (context.getIncludeParam() != null) {\n        int includeval = context.getIncludeParam().size();\n        String includeParam = \"\";\n        if (includeval > 0) {\n            for (int i = 0; i < includeval; i++) {\n                includeParam = includeParam + context.getIncludeParam().get(i) + \",\";\n            }\n            uri.append(\"include\").append(\"=\").append(includeParam.substring(0, includeParam.length() - 1)).append(\"&\");\n        }\n    }\n    return uri.toString();\n}",
        "tuc": "@Test\npublic void testExecute_QBO_URI() throws FMSException {\n    instance.execute(message);\n    String actual = message.getRequestElements().getRequestParameters().get(\"uri\");\n    Assert.assertEquals(actual, Config.getProperty(Config.BASE_URL_QBO) + \"/fakeRealm/fakeAction?requestid=anyRequestID&minorversion=35&\");\n}",
        "label": 1
    },
    {
        "repo_name": "intuit___QuickBooks-V3-Java-SDK",
        "commit": "d4fe78ecdff38c99e1e9417fc87c8ec1d958e21b",
        "commit_message": "update minor version 33\n",
        "p_path": "ipp-v3-java-devkit/src/main/java/com/intuit/ipp/interceptors/PrepareRequestInterceptor.java",
        "t_path": "ipp-v3-java-devkit/src/test/java/com/intuit/ipp/interceptors/PrepareRequestInterceptorTest.java",
        "p_name": "prepareQBOUri",
        "t_name": "testExecute_QBO_URI",
        "lpfc": "private <T extends IEntity> String prepareQBOUri(String entityName, Context context, Map<String, String> requestParameters) throws FMSException {\n    StringBuilder uri = new StringBuilder();\n    if (entityName.equalsIgnoreCase(\"Taxservice\")) {\n        entityName = entityName + \"/\" + \"taxcode\";\n    }\n    uri.append(getBaseUrl(Config.getProperty(Config.BASE_URL_QBO))).append(\"/\").append(context.getRealmID()).append(\"/\").append(entityName);\n    addEntityID(requestParameters, uri);\n    addEntitySelector(requestParameters, uri);\n    uri.append(\"?\").append(buildRequestParams(requestParameters));\n    uri.append(\"requestid\").append(\"=\").append(context.getRequestID()).append(\"&\");\n    context.setRequestID(null);\n    if (context.getMinorVersion() == null) {\n        context.setMinorVersion(\"30\");\n    }\n    uri.append(\"minorversion\").append(\"=\").append(context.getMinorVersion()).append(\"&\");\n    if (context.getIncludeParam() != null) {\n        int includeval = context.getIncludeParam().size();\n        String includeParam = \"\";\n        if (includeval > 0) {\n            for (int i = 0; i < includeval; i++) {\n                includeParam = includeParam + context.getIncludeParam().get(i) + \",\";\n            }\n            uri.append(\"include\").append(\"=\").append(includeParam.substring(0, includeParam.length() - 1)).append(\"&\");\n        }\n    }\n    return uri.toString();\n}",
        "rpfc": "private <T extends IEntity> String prepareQBOUri(String entityName, Context context, Map<String, String> requestParameters) throws FMSException {\n    StringBuilder uri = new StringBuilder();\n    if (entityName.equalsIgnoreCase(\"Taxservice\")) {\n        entityName = entityName + \"/\" + \"taxcode\";\n    }\n    uri.append(getBaseUrl(Config.getProperty(Config.BASE_URL_QBO))).append(\"/\").append(context.getRealmID()).append(\"/\").append(entityName);\n    addEntityID(requestParameters, uri);\n    addEntitySelector(requestParameters, uri);\n    uri.append(\"?\").append(buildRequestParams(requestParameters));\n    uri.append(\"requestid\").append(\"=\").append(context.getRequestID()).append(\"&\");\n    context.setRequestID(null);\n    if (context.getMinorVersion() == null) {\n        context.setMinorVersion(\"33\");\n    }\n    uri.append(\"minorversion\").append(\"=\").append(context.getMinorVersion()).append(\"&\");\n    if (context.getIncludeParam() != null) {\n        int includeval = context.getIncludeParam().size();\n        String includeParam = \"\";\n        if (includeval > 0) {\n            for (int i = 0; i < includeval; i++) {\n                includeParam = includeParam + context.getIncludeParam().get(i) + \",\";\n            }\n            uri.append(\"include\").append(\"=\").append(includeParam.substring(0, includeParam.length() - 1)).append(\"&\");\n        }\n    }\n    return uri.toString();\n}",
        "tuc": "@Test\npublic void testExecute_QBO_URI() throws FMSException {\n    instance.execute(message);\n    String actual = message.getRequestElements().getRequestParameters().get(\"uri\");\n    Assert.assertEquals(actual, Config.getProperty(Config.BASE_URL_QBO) + \"/fakeRealm/fakeAction?requestid=anyRequestID&minorversion=30&\");\n}",
        "label": 1
    },
    {
        "repo_name": "intuit___QuickBooks-V3-Java-SDK",
        "commit": "fe38b9403d0483d8363d4ff8d8849203009793d5",
        "commit_message": "update to minor version 30\n",
        "p_path": "ipp-v3-java-devkit/src/main/java/com/intuit/ipp/interceptors/PrepareRequestInterceptor.java",
        "t_path": "ipp-v3-java-devkit/src/test/java/com/intuit/ipp/interceptors/PrepareRequestInterceptorTest.java",
        "p_name": "prepareQBOUri",
        "t_name": "testExecute_QBO_URI",
        "lpfc": "private <T extends IEntity> String prepareQBOUri(String entityName, Context context, Map<String, String> requestParameters) throws FMSException {\n    StringBuilder uri = new StringBuilder();\n    if (entityName.equalsIgnoreCase(\"Taxservice\")) {\n        entityName = entityName + \"/\" + \"taxcode\";\n    }\n    uri.append(getBaseUrl(Config.getProperty(Config.BASE_URL_QBO))).append(\"/\").append(context.getRealmID()).append(\"/\").append(entityName);\n    addEntityID(requestParameters, uri);\n    addEntitySelector(requestParameters, uri);\n    uri.append(\"?\").append(buildRequestParams(requestParameters));\n    uri.append(\"requestid\").append(\"=\").append(context.getRequestID()).append(\"&\");\n    context.setRequestID(null);\n    if (context.getMinorVersion() == null) {\n        context.setMinorVersion(\"29\");\n    }\n    uri.append(\"minorversion\").append(\"=\").append(context.getMinorVersion()).append(\"&\");\n    if (context.getIncludeParam() != null) {\n        int includeval = context.getIncludeParam().size();\n        String includeParam = \"\";\n        if (includeval > 0) {\n            for (int i = 0; i < includeval; i++) {\n                includeParam = includeParam + context.getIncludeParam().get(i) + \",\";\n            }\n            uri.append(\"include\").append(\"=\").append(includeParam.substring(0, includeParam.length() - 1)).append(\"&\");\n        }\n    }\n    return uri.toString();\n}",
        "rpfc": "private <T extends IEntity> String prepareQBOUri(String entityName, Context context, Map<String, String> requestParameters) throws FMSException {\n    StringBuilder uri = new StringBuilder();\n    if (entityName.equalsIgnoreCase(\"Taxservice\")) {\n        entityName = entityName + \"/\" + \"taxcode\";\n    }\n    uri.append(getBaseUrl(Config.getProperty(Config.BASE_URL_QBO))).append(\"/\").append(context.getRealmID()).append(\"/\").append(entityName);\n    addEntityID(requestParameters, uri);\n    addEntitySelector(requestParameters, uri);\n    uri.append(\"?\").append(buildRequestParams(requestParameters));\n    uri.append(\"requestid\").append(\"=\").append(context.getRequestID()).append(\"&\");\n    context.setRequestID(null);\n    if (context.getMinorVersion() == null) {\n        context.setMinorVersion(\"30\");\n    }\n    uri.append(\"minorversion\").append(\"=\").append(context.getMinorVersion()).append(\"&\");\n    if (context.getIncludeParam() != null) {\n        int includeval = context.getIncludeParam().size();\n        String includeParam = \"\";\n        if (includeval > 0) {\n            for (int i = 0; i < includeval; i++) {\n                includeParam = includeParam + context.getIncludeParam().get(i) + \",\";\n            }\n            uri.append(\"include\").append(\"=\").append(includeParam.substring(0, includeParam.length() - 1)).append(\"&\");\n        }\n    }\n    return uri.toString();\n}",
        "tuc": "@Test\npublic void testExecute_QBO_URI() throws FMSException {\n    instance.execute(message);\n    String actual = message.getRequestElements().getRequestParameters().get(\"uri\");\n    Assert.assertEquals(actual, Config.getProperty(Config.BASE_URL_QBO) + \"/fakeRealm/fakeAction?requestid=anyRequestID&minorversion=29&\");\n}",
        "label": 1
    },
    {
        "repo_name": "intuit___QuickBooks-V3-Java-SDK",
        "commit": "42916189dc4072f55b7d6ffa5dec63ad53102d88",
        "commit_message": "update to minor version 23\n",
        "p_path": "ipp-v3-java-devkit/src/main/java/com/intuit/ipp/interceptors/PrepareRequestInterceptor.java",
        "t_path": "ipp-v3-java-devkit/src/test/java/com/intuit/ipp/interceptors/PrepareRequestInterceptorTest.java",
        "p_name": "prepareQBOUri",
        "t_name": "testExecute_QBO_URI",
        "lpfc": "private <T extends IEntity> String prepareQBOUri(String entityName, Context context, Map<String, String> requestParameters) throws FMSException {\n    StringBuilder uri = new StringBuilder();\n    if (entityName.equalsIgnoreCase(\"Taxservice\")) {\n        entityName = entityName + \"/\" + \"taxcode\";\n    }\n    uri.append(getBaseUrlQBO()).append(\"/\").append(context.getRealmID()).append(\"/\").append(entityName);\n    addEntityID(requestParameters, uri);\n    addEntitySelector(requestParameters, uri);\n    uri.append(\"?\").append(buildRequestParams(requestParameters));\n    uri.append(\"requestid\").append(\"=\").append(context.getRequestID()).append(\"&\");\n    context.setRequestID(null);\n    if (context.getMinorVersion() == null) {\n        context.setMinorVersion(\"21\");\n    }\n    uri.append(\"minorversion\").append(\"=\").append(context.getMinorVersion()).append(\"&\");\n    if (context.getIncludeParam() != null) {\n        int includeval = context.getIncludeParam().size();\n        String includeParam = \"\";\n        if (includeval > 0) {\n            for (int i = 0; i < includeval; i++) {\n                includeParam = includeParam + context.getIncludeParam().get(i) + \",\";\n            }\n            uri.append(\"include\").append(\"=\").append(includeParam.substring(0, includeParam.length() - 1)).append(\"&\");\n        }\n    }\n    return uri.toString();\n}",
        "rpfc": "private <T extends IEntity> String prepareQBOUri(String entityName, Context context, Map<String, String> requestParameters) throws FMSException {\n    StringBuilder uri = new StringBuilder();\n    if (entityName.equalsIgnoreCase(\"Taxservice\")) {\n        entityName = entityName + \"/\" + \"taxcode\";\n    }\n    uri.append(getBaseUrlQBO()).append(\"/\").append(context.getRealmID()).append(\"/\").append(entityName);\n    addEntityID(requestParameters, uri);\n    addEntitySelector(requestParameters, uri);\n    uri.append(\"?\").append(buildRequestParams(requestParameters));\n    uri.append(\"requestid\").append(\"=\").append(context.getRequestID()).append(\"&\");\n    context.setRequestID(null);\n    if (context.getMinorVersion() == null) {\n        context.setMinorVersion(\"23\");\n    }\n    uri.append(\"minorversion\").append(\"=\").append(context.getMinorVersion()).append(\"&\");\n    if (context.getIncludeParam() != null) {\n        int includeval = context.getIncludeParam().size();\n        String includeParam = \"\";\n        if (includeval > 0) {\n            for (int i = 0; i < includeval; i++) {\n                includeParam = includeParam + context.getIncludeParam().get(i) + \",\";\n            }\n            uri.append(\"include\").append(\"=\").append(includeParam.substring(0, includeParam.length() - 1)).append(\"&\");\n        }\n    }\n    return uri.toString();\n}",
        "tuc": "@Test\npublic void testExecute_QBO_URI() throws FMSException {\n    instance.execute(message);\n    String actual = message.getRequestElements().getRequestParameters().get(\"uri\");\n    Assert.assertEquals(actual, Config.getProperty(Config.BASE_URL_QBO) + \"/fakeRealm/fakeAction?requestid=anyRequestID&minorversion=21&\");\n}",
        "label": 1
    },
    {
        "repo_name": "jutzig___jabylon",
        "commit": "88c5bd26f82eccd2675cc0900992a00558155a97",
        "commit_message": "made tests run on linux",
        "p_path": "org.jabylon.properties/src/main/java/org/jabylon/properties/util/scanner/AbstractScanFileAcceptor.java",
        "t_path": "org.jabylon.properties/src/test/java/org/jabylon/properties/util/scanner/PartialScanFileAcceptorTest.java",
        "p_name": "calculateLocation",
        "t_name": "testComputeLocationWindows3",
        "lpfc": "protected URI calculateLocation(File file) {\n    /*\n    \t * we need to get rid of the base directory and version\n    \t */\n    String relativePath = file.getAbsolutePath().substring(versionPath.toFileString().length());\n    if (relativePath.startsWith(\"/\") || relativePath.startsWith(\"\\\\\"))\n        relativePath = relativePath.substring(1);\n    URI location = URI.createFileURI(relativePath);\n    return location;\n}",
        "rpfc": "protected URI calculateLocation(File file) {\n    String versionPathString = versionPath.toFileString();\n    String absolutePath = file.getAbsolutePath();\n    //normalize\n    if (!(absolutePath.startsWith(\"/\") || absolutePath.startsWith(\"\\\\\")))\n        absolutePath = \"/\" + absolutePath;\n    if (!(versionPathString.startsWith(\"/\") || versionPathString.startsWith(\"\\\\\")))\n        versionPathString = \"/\" + versionPathString;\n    /*\n    \t * we need to get rid of the base directory and version\n    \t */\n    String relativePath = absolutePath.substring(versionPathString.length());\n    if (relativePath.startsWith(\"/\") || relativePath.startsWith(\"\\\\\"))\n        relativePath = relativePath.substring(1);\n    URI location = URI.createFileURI(relativePath);\n    return location;\n}",
        "tuc": "@Test\npublic void testComputeLocationWindows3() {\n    ProjectVersion projectVersion = Mockito.mock(ProjectVersion.class);\n    Mockito.when(projectVersion.absolutPath()).thenReturn(URI.createFileURI(\"\\\\C:\\\\tests\\\\jabylon\\\\workspace\\\\test\\\\master\\\\\"));\n    PartialScanFileAcceptor acceptor = new PartialScanFileAcceptor(projectVersion, null, null);\n    URI location = acceptor.calculateLocation(new File(\"c:/tests/jabylon/workspace/test/master/core/build/internalartifacts.properties\"));\n    assertEquals(\"back slash or forward slash should not matter\", \"core/build/internalartifacts.properties\", location.toString());\n}",
        "label": 1
    },
    {
        "repo_name": "quantiply___rico",
        "commit": "0d1bd67a285974a2376aa671d36de7344e34070d",
        "commit_message": "Added case where reporter is ahead by one\n",
        "p_path": "core/src/main/java/com/quantiply/samza/metrics/WindowedMapGauge.java",
        "t_path": "core/src/test/java/com/quantiply/samza/metrics/WindowedMapGaugeTest.java",
        "p_name": "getValue",
        "t_name": "testUpdate",
        "lpfc": "@Override\npublic Map<String, Object> getValue() {\n    Windows newWindows = getWindowStartTimes(clock.currentTimeMillis());\n    long activeStartMs = windows.activeStartMs;\n    Map<String, Object> data = new HashMap<>();\n    if (newWindows.activeStartMs == activeStartMs || newWindows.prevStartMs == activeStartMs) {\n        data = Collections.unmodifiableMap(prevWindowMap);\n    }\n    Map<String, Object> value = new HashMap<>();\n    value.put(\"type\", \"windowed-map\");\n    value.put(\"window-duration-ms\", windowDurationMs);\n    value.put(\"data\", data);\n    return value;\n}",
        "rpfc": "@Override\npublic Map<String, Object> getValue() {\n    Windows newWindows = getWindowStartTimes(clock.currentTimeMillis());\n    long prevStartMs = windows.prevStartMs;\n    long activeStartMs = windows.activeStartMs;\n    Map<String, Object> data = new HashMap<>();\n    if (newWindows.activeStartMs == activeStartMs || newWindows.prevStartMs == activeStartMs || newWindows.activeStartMs == prevStartMs) {\n        data = Collections.unmodifiableMap(prevWindowMap);\n    }\n    Map<String, Object> value = new HashMap<>();\n    value.put(\"type\", \"windowed-map\");\n    value.put(\"window-duration-ms\", windowDurationMs);\n    value.put(\"data\", data);\n    return value;\n}",
        "tuc": "@Test\npublic void testUpdate() throws Exception {\n    final long windowMs = 60000L;\n    when(clock.currentTimeMillis()).thenReturn(0L);\n    WindowedMapGauge<Long> gauge = new WindowedMapGauge<>(\"wtf\", windowMs, Long::max, clock);\n    gauge.update(\"key1\", 5L);\n    gauge.update(\"key1\", 6L);\n    gauge.update(\"key1\", 4L);\n    assertEquals(0, ((Map) gauge.getValue().get(\"data\")).size());\n    when(clock.currentTimeMillis()).thenReturn(windowMs);\n    assertEquals(0, ((Map) gauge.getValue().get(\"data\")).size());\n    gauge.update(\"key1\", 20L);\n    System.out.println(gauge.getValue());\n    assertEquals(6L, ((Map<String, Long>) gauge.getValue().get(\"data\")).get(\"key1\").longValue());\n    when(clock.currentTimeMillis()).thenReturn(windowMs * 2);\n    assertEquals(6L, ((Map<String, Long>) gauge.getValue().get(\"data\")).get(\"key1\").longValue());\n    gauge.update(\"key1\", 0L);\n    assertEquals(20L, ((Map<String, Long>) gauge.getValue().get(\"data\")).get(\"key1\").longValue());\n    when(clock.currentTimeMillis()).thenReturn(windowMs * 4);\n    assertEquals(0, ((Map) gauge.getValue().get(\"data\")).size());\n}",
        "label": 1
    },
    {
        "repo_name": "koocyton___reactor-guice",
        "commit": "baed1caef86c9be17559810f93be2bd0692b76dd",
        "commit_message": "\u4e0a\u4f20\u6587\u4ef6\u4fdd\u5b58\u6210\u529f\n",
        "p_path": "src/main/java/com/doopp/reactor/guice/publisher/HandlePublisher.java",
        "t_path": "src/test/java/com/doopp/reactor/guice/test/AppServerTest.java",
        "p_name": "classCastFileUploadValue",
        "t_name": "testFileUpload",
        "lpfc": "private <T> T classCastFileUploadValue(List<MemoryFileUpload> value, String path, Class<T> clazz) {\n    if (value == null) {\n        return clazz.cast(null);\n    } else if (clazz == File.class) {\n        File saveDirPath = new File(path);\n        if (saveDirPath.isDirectory()) {\n            File file = new File(saveDirPath.getPath() + \"/aaa.txt\");\n            FileOutputStream out;\n            try {\n                out = new FileOutputStream(file);\n                ObjectOutputStream objOut = new ObjectOutputStream(out);\n                objOut.write(value.get(0).get());\n                objOut.flush();\n                objOut.close();\n                return clazz.cast(file);\n            } catch (IOException e) {\n                e.printStackTrace();\n                return clazz.cast(null);\n            }\n        }\n        return null;\n    } else if (clazz == File[].class) {\n        return null;\n    } else if (clazz == FileUpload.class) {\n        return clazz.cast(value.get(0));\n    } else if (clazz == FileUpload[].class) {\n        return clazz.cast(value.toArray(new FileUpload[0]));\n    } else if (clazz == MemoryFileUpload.class) {\n        return clazz.cast(value.get(0));\n    } else if (clazz == MemoryFileUpload[].class) {\n        return clazz.cast(value.toArray(new MemoryFileUpload[0]));\n    } else if (clazz == byte[].class) {\n        return clazz.cast(value.get(0).get());\n    } else if (clazz == byte[][].class) {\n        ArrayList<byte[]> byteValues = new ArrayList<>();\n        for (MemoryFileUpload s : value) {\n            byteValues.add(s.get());\n        }\n        return clazz.cast(byteValues.toArray());\n    } else {\n        return clazz.cast(null);\n    }\n}",
        "rpfc": "private <T> T classCastFileUploadValue(List<MemoryFileUpload> value, String path, Class<T> clazz) {\n    if (value == null) {\n        return clazz.cast(null);\n    } else if (clazz == File.class) {\n        File file = new File(path + \"/aaa.jpg\");\n        try (FileOutputStream fs = new FileOutputStream(file)) {\n            fs.write(value.get(0).get());\n            fs.close();\n            return clazz.cast(file);\n        } catch (Exception e) {\n            return null;\n        }\n    } else if (clazz == File[].class) {\n        return null;\n    } else if (clazz == FileUpload.class) {\n        return clazz.cast(value.get(0));\n    } else if (clazz == FileUpload[].class) {\n        return clazz.cast(value.toArray(new FileUpload[0]));\n    } else if (clazz == MemoryFileUpload.class) {\n        return clazz.cast(value.get(0));\n    } else if (clazz == MemoryFileUpload[].class) {\n        return clazz.cast(value.toArray(new MemoryFileUpload[0]));\n    } else if (clazz == byte[].class) {\n        return clazz.cast(value.get(0).get());\n    } else if (clazz == byte[][].class) {\n        ArrayList<byte[]> byteValues = new ArrayList<>();\n        for (MemoryFileUpload s : value) {\n            byteValues.add(s.get());\n        }\n        return clazz.cast(byteValues.toArray());\n    } else {\n        return clazz.cast(null);\n    }\n}",
        "tuc": "@Test\npublic void testFileUpload() {\n    String hhe = HttpClient.create().post().uri(\"http://127.0.0.1:8083/kreactor/test/post-bean\").sendForm((req, form) -> form.multipart(true).attr(\"id\", \"123123121312312\").attr(\"account\", \"account\").attr(\"password\", \"password\").attr(\"name\", \"name\").file(\"image\", new File(\"D:\\\\project\\\\reactor-guice\\\\application.properties\"))).responseSingle((res, content) -> content).map(byteBuf -> byteBuf.toString(CharsetUtil.UTF_8)).block();\n    System.out.println(hhe);\n}",
        "label": 1
    },
    {
        "repo_name": "helpermethod___connor",
        "commit": "5e54c99e00a4914c2e16dfe6488edc9f3ee31299",
        "commit_message": "Add kafka-clients library\n",
        "p_path": "src/main/java/com/github/helpermethod/kafka/connect/reset/KafkaConnectReset.java",
        "t_path": "src/test/java/com/github/helpermethod/kafka/connect/reset/KafkaConnectResetTest.java",
        "p_name": "call",
        "t_name": "test",
        "lpfc": "@Override\npublic Integer call() {\n    System.out.println(bootstrapServers);\n    return 0;\n}",
        "rpfc": "@Override\npublic Integer call() {\n    var consumer = new KafkaConsumer<>(Map.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers, ConsumerConfig.GROUP_ID_CONFIG, \"connect-reset-\" + new Random().nextInt(100000), ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false, ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class, ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class));\n    return 0;\n}",
        "tuc": "@Test\nvoid test() {\n    var bootstrapServers = kafka.getBootstrapServers();\n}",
        "label": 1
    },
    {
        "repo_name": "kakao___hbase-tools",
        "commit": "fc8b18031381d167f2a220760d27a2ea5d2e3edd",
        "commit_message": "should not delete snapshots which was not created by hbase-snapshot\n",
        "p_path": "hbase0.98/hbase-snapshot-0.98/src/main/java/com/kakao/hbase/snapshot/Snapshot.java",
        "t_path": "hbase0.98/hbase-snapshot-0.98/src/test/java/com/kakao/hbase/snapshot/SnapshotOptionTest.java",
        "p_name": "deleteSnapshotsForNotExistingTables",
        "t_name": "testDeleteSnapshotsForNotExistingTables",
        "lpfc": "private void deleteSnapshotsForNotExistingTables() throws IOException {\n    List<SnapshotDescription> snapshots = admin.listSnapshots();\n    for (SnapshotDescription snapshot : snapshots) {\n        String tableName = snapshot.getTable();\n        String snapshotName = snapshot.getName();\n        if (!admin.tableExists(tableName)) {\n            System.out.print(timestamp(TimestampFormat.log) + \" - Table \\\"\" + tableName + \"\\\" - Delete snapshot - Not existing table - \\\"\" + snapshotName + \"\\\"\");\n            admin.deleteSnapshot(snapshotName);\n            System.out.println(\" - OK\");\n        }\n    }\n}",
        "rpfc": "private void deleteSnapshotsForNotExistingTables() throws IOException {\n    List<SnapshotDescription> snapshots = admin.listSnapshots();\n    for (SnapshotDescription snapshot : snapshots) {\n        String tableName = snapshot.getTable();\n        String snapshotName = snapshot.getName();\n        if (snapshotName.startsWith(getPrefix(tableName))) {\n            if (!admin.tableExists(tableName)) {\n                System.out.print(timestamp(TimestampFormat.log) + \" - Table \\\"\" + tableName + \"\\\" - Delete snapshot - Not existing table - \\\"\" + snapshotName + \"\\\"\");\n                admin.deleteSnapshot(snapshotName);\n                System.out.println(\" - OK\");\n            }\n        } else {\n            System.out.println(timestamp(TimestampFormat.log) + \" - Table \\\"\" + tableName + \"\\\" - Delete snapshot - \\\"\" + snapshotName + \"\\\" - SKIPPED\");\n        }\n    }\n}",
        "tuc": "@Test\npublic void testDeleteSnapshotsForNotExistingTables() throws Exception {\n    List<HBaseProtos.SnapshotDescription> snapshotDescriptions;\n    String[] argsParam;\n    SnapshotArgs args;\n    Snapshot app;\n    String tableName2 = createAdditionalTable(tableName + \"2\");\n    argsParam = new String[] { \"localhost\", \".*\", \"--keep=1\", \"--test\", \"--delete-snapshot-for-not-existing-table\" };\n    args = new SnapshotArgs(argsParam);\n    app = new Snapshot(admin, args);\n    app.run();\n    snapshotDescriptions = listSnapshots(tableName + \".*\");\n    assertEquals(2, snapshotDescriptions.size());\n    assertEquals(tableName2, snapshotDescriptions.get(0).getTable());\n    assertEquals(tableName, snapshotDescriptions.get(1).getTable());\n    Thread.sleep(1000);\n    app.run();\n    snapshotDescriptions = listSnapshots(tableName + \".*\");\n    assertEquals(2, snapshotDescriptions.size());\n    assertEquals(tableName2, snapshotDescriptions.get(0).getTable());\n    assertEquals(tableName, snapshotDescriptions.get(1).getTable());\n    dropTable(tableName2);\n    argsParam = new String[] { \"localhost\", \".*\", \"--keep=1\", \"--test\" };\n    args = new SnapshotArgs(argsParam);\n    app = new Snapshot(admin, args);\n    Thread.sleep(1000);\n    app.run();\n    snapshotDescriptions = listSnapshots(tableName + \".*\");\n    assertEquals(2, snapshotDescriptions.size());\n    assertEquals(tableName2, snapshotDescriptions.get(0).getTable());\n    assertEquals(tableName, snapshotDescriptions.get(1).getTable());\n    argsParam = new String[] { \"localhost\", \".*\", \"--keep=1\", \"--test\", \"--delete-snapshot-for-not-existing-table\" };\n    args = new SnapshotArgs(argsParam);\n    app = new Snapshot(admin, args);\n    Thread.sleep(1000);\n    app.run();\n    snapshotDescriptions = listSnapshots(tableName + \".*\");\n    assertEquals(1, snapshotDescriptions.size());\n    assertEquals(tableName, snapshotDescriptions.get(0).getTable());\n}",
        "label": 1
    },
    {
        "repo_name": "joshgontijo___rest-client",
        "commit": "314944c8df1806b481aa3f8615cbab2eea547ae7",
        "commit_message": "Merge pull request #48 from sensational/default-user-agent\n\nallow setDefaultHeaders to override User-Agent",
        "p_path": "src/main/java/com/mashape/unirest/http/HttpClientHelper.java",
        "t_path": "src/test/java/com/mashape/unirest/test/http/UnirestTest.java",
        "p_name": "prepareRequest",
        "t_name": "testDefaultHeaders",
        "lpfc": "private static HttpRequestBase prepareRequest(HttpRequest request, boolean async) {\n    if (!request.getHeaders().containsKey(USER_AGENT_HEADER)) {\n        request.header(USER_AGENT_HEADER, USER_AGENT);\n    }\n    if (!request.getHeaders().containsKey(ACCEPT_ENCODING_HEADER)) {\n        request.header(ACCEPT_ENCODING_HEADER, \"gzip\");\n    }\n    Object defaultHeaders = Options.getOption(Option.DEFAULT_HEADERS);\n    if (defaultHeaders != null) {\n        @SuppressWarnings(\"unchecked\")\n        Set<Entry<String, String>> entrySet = ((Map<String, String>) defaultHeaders).entrySet();\n        for (Entry<String, String> entry : entrySet) {\n            request.header(entry.getKey(), entry.getValue());\n        }\n    }\n    HttpRequestBase reqObj = null;\n    String urlToRequest = null;\n    try {\n        URL url = new URL(request.getUrl());\n        URI uri = new URI(url.getProtocol(), url.getUserInfo(), url.getHost(), url.getPort(), URLDecoder.decode(url.getPath(), \"UTF-8\"), \"\", url.getRef());\n        urlToRequest = uri.toURL().toString();\n        if (url.getQuery() != null && !url.getQuery().trim().equals(\"\")) {\n            if (!urlToRequest.substring(urlToRequest.length() - 1).equals(\"?\")) {\n                urlToRequest += \"?\";\n            }\n            urlToRequest += url.getQuery();\n        } else if (urlToRequest.substring(urlToRequest.length() - 1).equals(\"?\")) {\n            urlToRequest = urlToRequest.substring(0, urlToRequest.length() - 1);\n        }\n    } catch (Exception e) {\n        throw new RuntimeException(e);\n    }\n    switch(request.getHttpMethod()) {\n        case GET:\n            reqObj = new HttpGet(urlToRequest);\n            break;\n        case POST:\n            reqObj = new HttpPost(urlToRequest);\n            break;\n        case PUT:\n            reqObj = new HttpPut(urlToRequest);\n            break;\n        case DELETE:\n            reqObj = new HttpDeleteWithBody(urlToRequest);\n            break;\n        case PATCH:\n            reqObj = new HttpPatchWithBody(urlToRequest);\n            break;\n        case OPTIONS:\n            reqObj = new HttpOptions(urlToRequest);\n            break;\n        case HEAD:\n            reqObj = new HttpHead(urlToRequest);\n            break;\n    }\n    Set<Entry<String, List<String>>> entrySet = request.getHeaders().entrySet();\n    for (Entry<String, List<String>> entry : entrySet) {\n        List<String> values = entry.getValue();\n        if (values != null) {\n            for (String value : values) {\n                reqObj.addHeader(entry.getKey(), value);\n            }\n        }\n    }\n    if (!(request.getHttpMethod() == HttpMethod.GET || request.getHttpMethod() == HttpMethod.HEAD)) {\n        if (request.getBody() != null) {\n            HttpEntity entity = request.getBody().getEntity();\n            if (async) {\n                reqObj.setHeader(entity.getContentType());\n                try {\n                    ByteArrayOutputStream output = new ByteArrayOutputStream();\n                    entity.writeTo(output);\n                    NByteArrayEntity en = new NByteArrayEntity(output.toByteArray());\n                    ((HttpEntityEnclosingRequestBase) reqObj).setEntity(en);\n                } catch (IOException e) {\n                    throw new RuntimeException(e);\n                }\n            } else {\n                ((HttpEntityEnclosingRequestBase) reqObj).setEntity(entity);\n            }\n        }\n    }\n    return reqObj;\n}",
        "rpfc": "private static HttpRequestBase prepareRequest(HttpRequest request, boolean async) {\n    Object defaultHeaders = Options.getOption(Option.DEFAULT_HEADERS);\n    if (defaultHeaders != null) {\n        @SuppressWarnings(\"unchecked\")\n        Set<Entry<String, String>> entrySet = ((Map<String, String>) defaultHeaders).entrySet();\n        for (Entry<String, String> entry : entrySet) {\n            request.header(entry.getKey(), entry.getValue());\n        }\n    }\n    if (!request.getHeaders().containsKey(USER_AGENT_HEADER)) {\n        request.header(USER_AGENT_HEADER, USER_AGENT);\n    }\n    if (!request.getHeaders().containsKey(ACCEPT_ENCODING_HEADER)) {\n        request.header(ACCEPT_ENCODING_HEADER, \"gzip\");\n    }\n    HttpRequestBase reqObj = null;\n    String urlToRequest = null;\n    try {\n        URL url = new URL(request.getUrl());\n        URI uri = new URI(url.getProtocol(), url.getUserInfo(), url.getHost(), url.getPort(), URLDecoder.decode(url.getPath(), \"UTF-8\"), \"\", url.getRef());\n        urlToRequest = uri.toURL().toString();\n        if (url.getQuery() != null && !url.getQuery().trim().equals(\"\")) {\n            if (!urlToRequest.substring(urlToRequest.length() - 1).equals(\"?\")) {\n                urlToRequest += \"?\";\n            }\n            urlToRequest += url.getQuery();\n        } else if (urlToRequest.substring(urlToRequest.length() - 1).equals(\"?\")) {\n            urlToRequest = urlToRequest.substring(0, urlToRequest.length() - 1);\n        }\n    } catch (Exception e) {\n        throw new RuntimeException(e);\n    }\n    switch(request.getHttpMethod()) {\n        case GET:\n            reqObj = new HttpGet(urlToRequest);\n            break;\n        case POST:\n            reqObj = new HttpPost(urlToRequest);\n            break;\n        case PUT:\n            reqObj = new HttpPut(urlToRequest);\n            break;\n        case DELETE:\n            reqObj = new HttpDeleteWithBody(urlToRequest);\n            break;\n        case PATCH:\n            reqObj = new HttpPatchWithBody(urlToRequest);\n            break;\n        case OPTIONS:\n            reqObj = new HttpOptions(urlToRequest);\n            break;\n        case HEAD:\n            reqObj = new HttpHead(urlToRequest);\n            break;\n    }\n    Set<Entry<String, List<String>>> entrySet = request.getHeaders().entrySet();\n    for (Entry<String, List<String>> entry : entrySet) {\n        List<String> values = entry.getValue();\n        if (values != null) {\n            for (String value : values) {\n                reqObj.addHeader(entry.getKey(), value);\n            }\n        }\n    }\n    if (!(request.getHttpMethod() == HttpMethod.GET || request.getHttpMethod() == HttpMethod.HEAD)) {\n        if (request.getBody() != null) {\n            HttpEntity entity = request.getBody().getEntity();\n            if (async) {\n                reqObj.setHeader(entity.getContentType());\n                try {\n                    ByteArrayOutputStream output = new ByteArrayOutputStream();\n                    entity.writeTo(output);\n                    NByteArrayEntity en = new NByteArrayEntity(output.toByteArray());\n                    ((HttpEntityEnclosingRequestBase) reqObj).setEntity(en);\n                } catch (IOException e) {\n                    throw new RuntimeException(e);\n                }\n            } else {\n                ((HttpEntityEnclosingRequestBase) reqObj).setEntity(entity);\n            }\n        }\n    }\n    return reqObj;\n}",
        "tuc": "@Test\npublic void testDefaultHeaders() throws UnirestException, JSONException {\n    Unirest.setDefaultHeader(\"X-Custom-Header\", \"hello\");\n    HttpResponse<JsonNode> jsonResponse = Unirest.get(\"http://httpbin.org/headers\").asJson();\n    assertTrue(jsonResponse.getHeaders().size() > 0);\n    assertTrue(jsonResponse.getBody().toString().length() > 0);\n    assertFalse(jsonResponse.getRawBody() == null);\n    assertEquals(200, jsonResponse.getCode());\n    JsonNode json = jsonResponse.getBody();\n    assertFalse(json.isArray());\n    assertTrue(jsonResponse.getBody().getObject().getJSONObject(\"headers\").has(\"X-Custom-Header\"));\n    assertEquals(\"hello\", json.getObject().getJSONObject(\"headers\").getString(\"X-Custom-Header\"));\n    jsonResponse = Unirest.get(\"http://httpbin.org/headers\").asJson();\n    assertTrue(jsonResponse.getBody().getObject().getJSONObject(\"headers\").has(\"X-Custom-Header\"));\n    assertEquals(\"hello\", jsonResponse.getBody().getObject().getJSONObject(\"headers\").getString(\"X-Custom-Header\"));\n    Unirest.clearDefaultHeaders();\n    jsonResponse = Unirest.get(\"http://httpbin.org/headers\").asJson();\n    assertFalse(jsonResponse.getBody().getObject().getJSONObject(\"headers\").has(\"X-Custom-Header\"));\n}",
        "label": 1
    },
    {
        "repo_name": "joshgontijo___rest-client",
        "commit": "67077fc5d54871b1b67a1d6c4b1860592a9b1979",
        "commit_message": "allow setDefaultHeaders to override User-Agent\n\nThe User-Agent header is one of the prime candidates of headers to be\ncustomized by default for every request, so it makes sense to allow\ncustomizing it via setDefaultHeaders.\n\nBefore this commit, request specific headers would be set first and\nthe default user agent would then be added in addition to the request\nspecific header\n",
        "p_path": "src/main/java/com/mashape/unirest/http/HttpClientHelper.java",
        "t_path": "src/test/java/com/mashape/unirest/test/http/UnirestTest.java",
        "p_name": "prepareRequest",
        "t_name": "testDefaultHeaders",
        "lpfc": "private static HttpRequestBase prepareRequest(HttpRequest request, boolean async) {\n    if (!request.getHeaders().containsKey(USER_AGENT_HEADER)) {\n        request.header(USER_AGENT_HEADER, USER_AGENT);\n    }\n    if (!request.getHeaders().containsKey(ACCEPT_ENCODING_HEADER)) {\n        request.header(ACCEPT_ENCODING_HEADER, \"gzip\");\n    }\n    Object defaultHeaders = Options.getOption(Option.DEFAULT_HEADERS);\n    if (defaultHeaders != null) {\n        @SuppressWarnings(\"unchecked\")\n        Set<Entry<String, String>> entrySet = ((Map<String, String>) defaultHeaders).entrySet();\n        for (Entry<String, String> entry : entrySet) {\n            request.header(entry.getKey(), entry.getValue());\n        }\n    }\n    HttpRequestBase reqObj = null;\n    String urlToRequest = null;\n    try {\n        URL url = new URL(request.getUrl());\n        URI uri = new URI(url.getProtocol(), url.getUserInfo(), url.getHost(), url.getPort(), URLDecoder.decode(url.getPath(), \"UTF-8\"), \"\", url.getRef());\n        urlToRequest = uri.toURL().toString();\n        if (url.getQuery() != null && !url.getQuery().trim().equals(\"\")) {\n            if (!urlToRequest.substring(urlToRequest.length() - 1).equals(\"?\")) {\n                urlToRequest += \"?\";\n            }\n            urlToRequest += url.getQuery();\n        } else if (urlToRequest.substring(urlToRequest.length() - 1).equals(\"?\")) {\n            urlToRequest = urlToRequest.substring(0, urlToRequest.length() - 1);\n        }\n    } catch (Exception e) {\n        throw new RuntimeException(e);\n    }\n    switch(request.getHttpMethod()) {\n        case GET:\n            reqObj = new HttpGet(urlToRequest);\n            break;\n        case POST:\n            reqObj = new HttpPost(urlToRequest);\n            break;\n        case PUT:\n            reqObj = new HttpPut(urlToRequest);\n            break;\n        case DELETE:\n            reqObj = new HttpDeleteWithBody(urlToRequest);\n            break;\n        case PATCH:\n            reqObj = new HttpPatchWithBody(urlToRequest);\n            break;\n        case OPTIONS:\n            reqObj = new HttpOptions(urlToRequest);\n            break;\n        case HEAD:\n            reqObj = new HttpHead(urlToRequest);\n            break;\n    }\n    Set<Entry<String, List<String>>> entrySet = request.getHeaders().entrySet();\n    for (Entry<String, List<String>> entry : entrySet) {\n        List<String> values = entry.getValue();\n        if (values != null) {\n            for (String value : values) {\n                reqObj.addHeader(entry.getKey(), value);\n            }\n        }\n    }\n    if (!(request.getHttpMethod() == HttpMethod.GET || request.getHttpMethod() == HttpMethod.HEAD)) {\n        if (request.getBody() != null) {\n            HttpEntity entity = request.getBody().getEntity();\n            if (async) {\n                reqObj.setHeader(entity.getContentType());\n                try {\n                    ByteArrayOutputStream output = new ByteArrayOutputStream();\n                    entity.writeTo(output);\n                    NByteArrayEntity en = new NByteArrayEntity(output.toByteArray());\n                    ((HttpEntityEnclosingRequestBase) reqObj).setEntity(en);\n                } catch (IOException e) {\n                    throw new RuntimeException(e);\n                }\n            } else {\n                ((HttpEntityEnclosingRequestBase) reqObj).setEntity(entity);\n            }\n        }\n    }\n    return reqObj;\n}",
        "rpfc": "private static HttpRequestBase prepareRequest(HttpRequest request, boolean async) {\n    Object defaultHeaders = Options.getOption(Option.DEFAULT_HEADERS);\n    if (defaultHeaders != null) {\n        @SuppressWarnings(\"unchecked\")\n        Set<Entry<String, String>> entrySet = ((Map<String, String>) defaultHeaders).entrySet();\n        for (Entry<String, String> entry : entrySet) {\n            request.header(entry.getKey(), entry.getValue());\n        }\n    }\n    if (!request.getHeaders().containsKey(USER_AGENT_HEADER)) {\n        request.header(USER_AGENT_HEADER, USER_AGENT);\n    }\n    if (!request.getHeaders().containsKey(ACCEPT_ENCODING_HEADER)) {\n        request.header(ACCEPT_ENCODING_HEADER, \"gzip\");\n    }\n    HttpRequestBase reqObj = null;\n    String urlToRequest = null;\n    try {\n        URL url = new URL(request.getUrl());\n        URI uri = new URI(url.getProtocol(), url.getUserInfo(), url.getHost(), url.getPort(), URLDecoder.decode(url.getPath(), \"UTF-8\"), \"\", url.getRef());\n        urlToRequest = uri.toURL().toString();\n        if (url.getQuery() != null && !url.getQuery().trim().equals(\"\")) {\n            if (!urlToRequest.substring(urlToRequest.length() - 1).equals(\"?\")) {\n                urlToRequest += \"?\";\n            }\n            urlToRequest += url.getQuery();\n        } else if (urlToRequest.substring(urlToRequest.length() - 1).equals(\"?\")) {\n            urlToRequest = urlToRequest.substring(0, urlToRequest.length() - 1);\n        }\n    } catch (Exception e) {\n        throw new RuntimeException(e);\n    }\n    switch(request.getHttpMethod()) {\n        case GET:\n            reqObj = new HttpGet(urlToRequest);\n            break;\n        case POST:\n            reqObj = new HttpPost(urlToRequest);\n            break;\n        case PUT:\n            reqObj = new HttpPut(urlToRequest);\n            break;\n        case DELETE:\n            reqObj = new HttpDeleteWithBody(urlToRequest);\n            break;\n        case PATCH:\n            reqObj = new HttpPatchWithBody(urlToRequest);\n            break;\n        case OPTIONS:\n            reqObj = new HttpOptions(urlToRequest);\n            break;\n        case HEAD:\n            reqObj = new HttpHead(urlToRequest);\n            break;\n    }\n    Set<Entry<String, List<String>>> entrySet = request.getHeaders().entrySet();\n    for (Entry<String, List<String>> entry : entrySet) {\n        List<String> values = entry.getValue();\n        if (values != null) {\n            for (String value : values) {\n                reqObj.addHeader(entry.getKey(), value);\n            }\n        }\n    }\n    if (!(request.getHttpMethod() == HttpMethod.GET || request.getHttpMethod() == HttpMethod.HEAD)) {\n        if (request.getBody() != null) {\n            HttpEntity entity = request.getBody().getEntity();\n            if (async) {\n                reqObj.setHeader(entity.getContentType());\n                try {\n                    ByteArrayOutputStream output = new ByteArrayOutputStream();\n                    entity.writeTo(output);\n                    NByteArrayEntity en = new NByteArrayEntity(output.toByteArray());\n                    ((HttpEntityEnclosingRequestBase) reqObj).setEntity(en);\n                } catch (IOException e) {\n                    throw new RuntimeException(e);\n                }\n            } else {\n                ((HttpEntityEnclosingRequestBase) reqObj).setEntity(entity);\n            }\n        }\n    }\n    return reqObj;\n}",
        "tuc": "@Test\npublic void testDefaultHeaders() throws UnirestException, JSONException {\n    Unirest.setDefaultHeader(\"X-Custom-Header\", \"hello\");\n    HttpResponse<JsonNode> jsonResponse = Unirest.get(\"http://httpbin.org/headers\").asJson();\n    assertTrue(jsonResponse.getHeaders().size() > 0);\n    assertTrue(jsonResponse.getBody().toString().length() > 0);\n    assertFalse(jsonResponse.getRawBody() == null);\n    assertEquals(200, jsonResponse.getCode());\n    JsonNode json = jsonResponse.getBody();\n    assertFalse(json.isArray());\n    assertTrue(jsonResponse.getBody().getObject().getJSONObject(\"headers\").has(\"X-Custom-Header\"));\n    assertEquals(\"hello\", json.getObject().getJSONObject(\"headers\").getString(\"X-Custom-Header\"));\n    jsonResponse = Unirest.get(\"http://httpbin.org/headers\").asJson();\n    assertTrue(jsonResponse.getBody().getObject().getJSONObject(\"headers\").has(\"X-Custom-Header\"));\n    assertEquals(\"hello\", jsonResponse.getBody().getObject().getJSONObject(\"headers\").getString(\"X-Custom-Header\"));\n    Unirest.clearDefaultHeaders();\n    jsonResponse = Unirest.get(\"http://httpbin.org/headers\").asJson();\n    assertFalse(jsonResponse.getBody().getObject().getJSONObject(\"headers\").has(\"X-Custom-Header\"));\n}",
        "label": 1
    },
    {
        "repo_name": "joshgontijo___rest-client",
        "commit": "5005d834bdc5483cf017294967bb6462a572c7fb",
        "commit_message": "Fixed a bug where parameters were being unnecessarily added to the url\n",
        "p_path": "src/main/java/com/mashape/client/http/utils/UrlUtils.java",
        "t_path": "src/test/java/com/mashape/client/test/http/UrlUtilsTest.java",
        "p_name": "prepareRequest",
        "t_name": "testPrepareRequest",
        "lpfc": "public static RequestPrepareResult prepareRequest(String url, Map<String, String> parameters, boolean addRegularQueryStringParameters) throws UnsupportedEncodingException {\n    if (parameters == null) {\n        parameters = new HashMap<String, String>();\n    }\n    Set<String> keySet = new HashSet<String>(parameters.keySet());\n    for (String key : keySet) {\n        if (parameters.get(key) == null) {\n            parameters.remove(key);\n        }\n    }\n    Pattern p = Pattern.compile(\"\\\\{([\\\\w\\\\.]+)\\\\}\");\n    Matcher matcher = p.matcher(url);\n    String finalUrl = url;\n    while (matcher.find()) {\n        String key = matcher.group(1);\n        if (parameters.containsKey(key)) {\n            String parameterValue = parameters.get(key);\n            finalUrl = finalUrl.replaceAll(\"(\\\\?.+)\\\\{\" + key + \"\\\\}\", \"$1\" + URLEncoder.encode(parameterValue, \"UTF-8\"));\n            finalUrl = finalUrl.replaceAll(\"\\\\{\" + key + \"\\\\}\", UriUtils.encodeUri(parameterValue, \"UTF-8\"));\n        } else {\n            finalUrl = finalUrl.replaceAll(\"&?[\\\\w]*=?\\\\{\" + key + \"\\\\}\", \"\");\n        }\n    }\n    finalUrl = finalUrl.replaceAll(\"\\\\?&\", \"?\");\n    finalUrl = finalUrl.replaceAll(\"\\\\?$\", \"\");\n    if (addRegularQueryStringParameters) {\n        addRegularQueryStringParameters(finalUrl, parameters);\n    } else {\n        for (String key : parameters.keySet()) {\n            String delimiter = (finalUrl.indexOf(\"?\") > 0) ? \"&\" : \"?\";\n            finalUrl += delimiter + key + \"=\" + parameters.get(key);\n        }\n    }\n    return new RequestPrepareResult(finalUrl, parameters);\n}",
        "rpfc": "public static RequestPrepareResult prepareRequest(String url, Map<String, String> parameters, boolean addRegularQueryStringParameters) throws UnsupportedEncodingException {\n    if (parameters == null) {\n        parameters = new HashMap<String, String>();\n    }\n    Set<String> keySet = new HashSet<String>(parameters.keySet());\n    for (String key : keySet) {\n        if (parameters.get(key) == null) {\n            parameters.remove(key);\n        }\n    }\n    Pattern p = Pattern.compile(\"\\\\{([\\\\w\\\\.]+)\\\\}\");\n    Matcher matcher = p.matcher(url);\n    String finalUrl = url;\n    while (matcher.find()) {\n        String key = matcher.group(1);\n        if (parameters.containsKey(key)) {\n            String parameterValue = parameters.get(key);\n            finalUrl = finalUrl.replaceAll(\"(\\\\?.+)\\\\{\" + key + \"\\\\}\", \"$1\" + URLEncoder.encode(parameterValue, \"UTF-8\"));\n            finalUrl = finalUrl.replaceAll(\"\\\\{\" + key + \"\\\\}\", UriUtils.encodeUri(parameterValue, \"UTF-8\"));\n            parameters.remove(key);\n        } else {\n            finalUrl = finalUrl.replaceAll(\"&?[\\\\w]*=?\\\\{\" + key + \"\\\\}\", \"\");\n        }\n    }\n    finalUrl = finalUrl.replaceAll(\"\\\\?&\", \"?\");\n    finalUrl = finalUrl.replaceAll(\"\\\\?$\", \"\");\n    if (addRegularQueryStringParameters) {\n        addRegularQueryStringParameters(finalUrl, parameters);\n    } else {\n        for (String key : parameters.keySet()) {\n            String delimiter = (finalUrl.indexOf(\"?\") > 0) ? \"&\" : \"?\";\n            finalUrl += delimiter + key + \"=\" + parameters.get(key);\n        }\n    }\n    return new RequestPrepareResult(finalUrl, parameters);\n}",
        "tuc": "@Test\npublic void testPrepareRequest() throws UnsupportedEncodingException {\n    RequestPrepareResult prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com\", null, false);\n    assertEquals(\"http://www.ciao.com\", prepareRequest.getUrl());\n    assertEquals(new HashMap<String, String>(), prepareRequest.getParameters());\n    prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com\", new HashMap<String, String>(), false);\n    assertEquals(\"http://www.ciao.com\", prepareRequest.getUrl());\n    assertEquals(new HashMap<String, String>(), prepareRequest.getParameters());\n    prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com/{id}\", null, false);\n    assertEquals(\"http://www.ciao.com/\", prepareRequest.getUrl());\n    assertEquals(new HashMap<String, String>(), prepareRequest.getParameters());\n    prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com/{id}?name={name}\", null, false);\n    assertEquals(\"http://www.ciao.com/\", prepareRequest.getUrl());\n    assertEquals(new HashMap<String, String>(), prepareRequest.getParameters());\n    Map<String, String> parameters = new HashMap<String, String>();\n    parameters.put(\"id\", \"12\");\n    prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com/{id}?name={name}\", parameters, false);\n    assertEquals(\"http://www.ciao.com/12\", prepareRequest.getUrl());\n    assertEquals(parameters, prepareRequest.getParameters());\n    parameters = new HashMap<String, String>();\n    parameters.put(\"id\", \"12\");\n    parameters.put(\"name\", \"tom\");\n    prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com/{id}?name={name}\", parameters, false);\n    assertEquals(\"http://www.ciao.com/12?name=tom\", prepareRequest.getUrl());\n    assertEquals(parameters, prepareRequest.getParameters());\n    parameters = new HashMap<String, String>();\n    parameters.put(\"id\", \"12\");\n    parameters.put(\"name\", \"tom\");\n    prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com/{id}?name={name}&opt=1\", parameters, false);\n    assertEquals(\"http://www.ciao.com/12?name=tom&opt=1\", prepareRequest.getUrl());\n    assertEquals(parameters, prepareRequest.getParameters());\n    parameters = new HashMap<String, String>();\n    parameters.put(\"id\", \"12\");\n    parameters.put(\"name\", \"tom jerry\");\n    prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com/{id}?name={name}\", parameters, false);\n    assertEquals(\"http://www.ciao.com/12?name=tom+jerry\", prepareRequest.getUrl());\n    assertEquals(parameters, prepareRequest.getParameters());\n    parameters = new HashMap<String, String>();\n    parameters.put(\"id\", \"12\");\n    parameters.put(\"name\", \"tom jerry\");\n    prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com/{id}?name={name}&opt=1&nick={nick}\", parameters, false);\n    assertEquals(\"http://www.ciao.com/12?name=tom+jerry&opt=1\", prepareRequest.getUrl());\n    assertEquals(parameters, prepareRequest.getParameters());\n    parameters = new HashMap<String, String>();\n    parameters.put(\"id\", \"12\");\n    parameters.put(\"name\", \"tom jerry\");\n    parameters.put(\"nick\", \"sinz\");\n    prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com/{id}?name={name}&opt={opt}&nick={nick}\", parameters, false);\n    assertEquals(\"http://www.ciao.com/12?name=tom+jerry&nick=sinz\", prepareRequest.getUrl());\n    assertEquals(parameters, prepareRequest.getParameters());\n    parameters = new HashMap<String, String>();\n    parameters.put(\"id\", \"12\");\n    parameters.put(\"name\", \"tom jerry\");\n    parameters.put(\"opt\", \"yes\");\n    parameters.put(\"nick\", \"sinz\");\n    prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com/{id}?name={name}&opt={opt}&nick={nick}\", parameters, false);\n    assertEquals(\"http://www.ciao.com/12?name=tom+jerry&opt=yes&nick=sinz\", prepareRequest.getUrl());\n    assertEquals(parameters, prepareRequest.getParameters());\n    parameters = new HashMap<String, String>();\n    parameters.put(\"id\", \"12\");\n    parameters.put(\"opt\", \"yes\");\n    parameters.put(\"nick\", \"sinz\");\n    prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com/{id}?name={name}&opt={opt}&nick={nick}\", parameters, false);\n    assertEquals(\"http://www.ciao.com/12?opt=yes&nick=sinz\", prepareRequest.getUrl());\n    assertEquals(parameters, prepareRequest.getParameters());\n    parameters = new HashMap<String, String>();\n    parameters.put(\"id\", \"12\");\n    parameters.put(\"opt\", \"yes\");\n    prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com/{id}?name={name}&opt={opt}&nick={nick}\", parameters, false);\n    assertEquals(\"http://www.ciao.com/12?opt=yes\", prepareRequest.getUrl());\n    assertEquals(parameters, prepareRequest.getParameters());\n    parameters = new HashMap<String, String>();\n    parameters.put(\"id\", \"12\");\n    prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com/{id}?name={name}&opt={opt}&nick={nick}\", parameters, false);\n    assertEquals(\"http://www.ciao.com/12\", prepareRequest.getUrl());\n    assertEquals(parameters, prepareRequest.getParameters());\n    parameters = new HashMap<String, String>();\n    parameters.put(\"id\", \"12\");\n    parameters.put(\"pippo\", null);\n    prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com/{id}?name={name}&opt={opt}&nick={nick}\", parameters, false);\n    assertEquals(\"http://www.ciao.com/12\", prepareRequest.getUrl());\n    assertEquals(1, parameters.size());\n    assertEquals(parameters, prepareRequest.getParameters());\n    parameters = new HashMap<String, String>();\n    parameters.put(\"id\", \"ciao marco\");\n    parameters.put(\"name\", \"ciao pippo\");\n    parameters.put(\"opt\", \"2\");\n    prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com/{id}?name={name}&opt={opt}&nick=some+nick\", parameters, false);\n    assertEquals(\"http://www.ciao.com/ciao%20marco?name=ciao+pippo&opt=2&nick=some+nick\", prepareRequest.getUrl());\n    assertEquals(3, parameters.size());\n    assertEquals(parameters, prepareRequest.getParameters());\n    parameters = new HashMap<String, String>();\n    parameters.put(\"id\", \"ciao marco\");\n    parameters.put(\"name\", \"ciao pippo\");\n    parameters.put(\"opt\", \"{this is opt}\");\n    prepareRequest = UrlUtils.prepareRequest(\"http://www.ciao.com/{id}?name={name}&opt={opt}&nick=some+nick\", parameters, false);\n    assertEquals(\"http://www.ciao.com/ciao%20marco?name=ciao+pippo&opt=%7Bthis+is+opt%7D&nick=some+nick\", prepareRequest.getUrl());\n    assertEquals(3, parameters.size());\n    assertEquals(parameters, prepareRequest.getParameters());\n}",
        "label": 1
    },
    {
        "repo_name": "joshgontijo___rest-client",
        "commit": "375e2e5156f77c8ae18712639e917becdc420d46",
        "commit_message": "new auth\n",
        "p_path": "src/main/java/com/mashape/client/http/AuthUtil.java",
        "t_path": "src/test/java/com/mashape/client/test/http/AuthUtilTest.java",
        "p_name": "generateAuthenticationHeader",
        "t_name": "testRequestToken",
        "lpfc": "public static Header generateAuthenticationHeader(String publicKey, String privateKey) {\n    String uuid = UUID.randomUUID().toString();\n    String hash = CryptUtils.getHMAC_SHA1(uuid, privateKey);\n    String headerValue = publicKey + \":\" + hash + uuid;\n    return new BasicHeader(\"X-Mashape-Authorization\", Base64.encodeBase64String(headerValue.getBytes()).replace(\"\\r\\n\", \"\"));\n}",
        "rpfc": "public static Header generateAuthenticationHeader(String publicKey, String privateKey) {\n    String hash = CryptUtils.getHMAC_SHA1(publicKey, privateKey);\n    String headerValue = publicKey + \":\" + hash;\n    return new BasicHeader(\"Proxy-Authorization\", Base64.encodeBase64String(headerValue.getBytes()).replace(\"\\r\\n\", \"\"));\n}",
        "tuc": "@Test\npublic void testRequestToken() {\n    assertEquals(108, AuthUtil.generateAuthenticationHeader(\"ciao\", \"marco\").getValue().length());\n    assertEquals(108, AuthUtil.generateAuthenticationHeader(\"ciao\", \"piipopopopopopopo\").getValue().length());\n}",
        "label": 1
    },
    {
        "repo_name": "joshgontijo___rest-client",
        "commit": "dcdf6d434524248869413f81c2d7f569f00ecbe5",
        "commit_message": "removing old code\n",
        "p_path": "src/main/java/com/mashape/client/http/TokenUtil.java",
        "t_path": "src/test/java/com/mashape/client/test/http/TokenUtilTest.java",
        "p_name": "requestToken",
        "t_name": "testRequestToken",
        "lpfc": "public static String requestToken(String developerKey) throws MashapeClientException {\n    Map<String, String> parameters = new HashMap<String, String>();\n    parameters.put(\"devkey\", developerKey);\n    JSONObject response = (JSONObject) HttpClient.doRequest(HttpMethod.POST, TOKEN_URL, parameters, null);\n    try {\n        JSONArray errors = response.getJSONArray(\"errors\");\n        if (errors.length() > 0) {\n            JSONObject error = errors.getJSONObject(0);\n            throw new MashapeClientException(error.getString(\"message\"), error.getInt(\"code\"));\n        } else {\n            return response.getString(\"token\");\n        }\n    } catch (MashapeClientException e1) {\n        throw e1;\n    } catch (Exception e) {\n        throw new RuntimeException(e);\n    }\n}",
        "rpfc": null,
        "tuc": "@Test\npublic void testRequestToken() {\n    try {\n        TokenUtil.requestToken(null);\n        fail();\n    } catch (MashapeClientException e) {\n        // Ok\n    }\n    try {\n        TokenUtil.requestToken(\"\");\n        fail();\n    } catch (MashapeClientException e) {\n        // Ok\n    }\n    try {\n        TokenUtil.requestToken(\"bla\");\n        fail();\n    } catch (MashapeClientException e) {\n        // Ok\n    }\n}",
        "label": 1
    },
    {
        "repo_name": "yahoo___validatar",
        "commit": "19dbd3bee9a9904fa2b30525f32d670574f89b85",
        "commit_message": "Use Number instead of Long and Double\n",
        "p_path": "src/main/java/com/yahoo/validatar/execution/rest/JSON.java",
        "t_path": "src/test/java/com/yahoo/validatar/execution/rest/JSONTest.java",
        "p_name": "type",
        "t_name": "testJSONMapTypeConversions",
        "lpfc": "private TypedObject type(Object object) {\n    if (object == null) {\n        log.info(\"Value: null\");\n        return null;\n    }\n    TypedObject typed;\n    if (object instanceof String) {\n        typed = new TypedObject((String) object, TypeSystem.Type.STRING);\n    } else if (object instanceof Integer) {\n        typed = new TypedObject(((Integer) object).longValue(), TypeSystem.Type.LONG);\n    } else if (object instanceof Double) {\n        typed = new TypedObject((Double) object, TypeSystem.Type.DOUBLE);\n    } else if (object instanceof Long) {\n        typed = new TypedObject((Long) object, TypeSystem.Type.LONG);\n    } else if (object instanceof Boolean) {\n        typed = new TypedObject((Boolean) object, TypeSystem.Type.BOOLEAN);\n    } else {\n        log.info(\"Object {} has an unsupported type {}. Nulling...\", object, object.getClass().getCanonicalName());\n        return null;\n    }\n    log.info(\"Value: {}\\tType: {}\", typed.data, typed.type);\n    return typed;\n}",
        "rpfc": "private TypedObject type(Object object) {\n    if (object == null) {\n        log.info(\"Value: null\");\n        return null;\n    }\n    TypedObject typed;\n    if (object instanceof String) {\n        typed = new TypedObject((String) object, TypeSystem.Type.STRING);\n    } else if (object instanceof Integer) {\n        typed = new TypedObject(((Integer) object).longValue(), TypeSystem.Type.LONG);\n    } else if (object instanceof Number) {\n        typed = new TypedObject(((Number) object).doubleValue(), TypeSystem.Type.DOUBLE);\n    } else if (object instanceof Boolean) {\n        typed = new TypedObject((Boolean) object, TypeSystem.Type.BOOLEAN);\n    } else {\n        log.info(\"Object {} has an unsupported type {}. Nulling...\", object, object.getClass().getCanonicalName());\n        return null;\n    }\n    log.info(\"Value: {}\\tType: {}\", typed.data, typed.type);\n    return typed;\n}",
        "tuc": "@Test\npublic void testJSONMapTypeConversions() {\n    Query query = new Query();\n    String jsonData = \"{'users' : ['user1', 'user2'], 'count': [10], 'ratio': [0.14], 'longs': [123123123123123],\" + \" 'booleans': [true, false], 'mixed': [[1, 2], {'a': 1}, null, 2]}\";\n    Map<String, List<TypedObject>> actual = json.convertToMap(jsonData, query);\n    Assert.assertEquals(actual.size(), 6);\n    List<TypedObject> users = actual.get(\"users\");\n    Assert.assertEquals(users.get(0).type, TypeSystem.Type.STRING);\n    Assert.assertEquals(users.get(0).data, \"user1\");\n    Assert.assertEquals(users.get(1).type, TypeSystem.Type.STRING);\n    Assert.assertEquals(users.get(1).data, \"user2\");\n    List<TypedObject> counts = actual.get(\"count\");\n    Assert.assertEquals(counts.get(0).type, TypeSystem.Type.LONG);\n    Assert.assertEquals(counts.get(0).data, 10L);\n    List<TypedObject> ratios = actual.get(\"ratio\");\n    Assert.assertEquals(ratios.get(0).type, TypeSystem.Type.DOUBLE);\n    Assert.assertEquals(ratios.get(0).data, 0.14);\n    List<TypedObject> longs = actual.get(\"longs\");\n    Assert.assertEquals(longs.get(0).type, TypeSystem.Type.LONG);\n    Assert.assertEquals(longs.get(0).data, 123123123123123L);\n    List<TypedObject> booleans = actual.get(\"booleans\");\n    Assert.assertEquals(booleans.get(0).type, TypeSystem.Type.BOOLEAN);\n    Assert.assertEquals(booleans.get(0).data, true);\n    Assert.assertEquals(booleans.get(1).type, TypeSystem.Type.BOOLEAN);\n    Assert.assertEquals(booleans.get(1).data, false);\n    List<TypedObject> mixeds = actual.get(\"mixed\");\n    Assert.assertNull(mixeds.get(0));\n    Assert.assertNull(mixeds.get(1));\n    Assert.assertNull(mixeds.get(2));\n    Assert.assertEquals(mixeds.get(3).type, TypeSystem.Type.LONG);\n    Assert.assertEquals(mixeds.get(3).data, 2L);\n}",
        "label": 1
    },
    {
        "repo_name": "apache___oodt",
        "commit": "5df39551471f4f25cfa9f259bf63f4141a9e17dc",
        "commit_message": "OODT-606 Make it so only abolute path is required to config. Allow environment variables to flow through\n\ngit-svn-id: https://svn.apache.org/repos/asf/oodt/trunk@1476669 13f79535-47bb-0310-9956-ffa450edef68\n",
        "p_path": "pge/src/main/java/org/apache/oodt/cas/pge/writers/VelocityConfigFileWriter.java",
        "t_path": "pge/src/test/java/org/apache/oodt/cas/pge/writers/VelocityConfigFileWriterTest.java",
        "p_name": "generateFile",
        "t_name": "testCreateConfigFile",
        "lpfc": "/*\n   * (non-Javadoc)\n   * \n   * @see\n   * org.apache.oodt.cas.pge.writers.DynamicConfigFileWriter#generateFile(java\n   * .lang.String, org.apache.oodt.cas.metadata.Metadata,\n   * java.util.logging.Logger, java.lang.Object[])\n   */\n@Override\npublic File generateFile(String filePath, Metadata metadata, Logger logger, Object... args) throws Exception {\n    File configFile = new File(filePath);\n    VelocityMetadata velocityMetadata = new VelocityMetadata(metadata);\n    try {\n        // Velocity requires you to set a path of where to look for\n        // templates.\n        // This path defaults to . if not set.\n        Velocity.setProperty(\"file.resource.loader.path\", args[0]);\n        Velocity.init();\n        VelocityContext context = new VelocityContext();\n        context.put(\"metadata\", velocityMetadata);\n        Template template = Velocity.getTemplate((String) args[1]);\n        StringWriter sw = new StringWriter();\n        template.merge(context, sw);\n        FileUtils.writeStringToFile(configFile, sw.toString());\n    } catch (Exception e) {\n        e.printStackTrace();\n    }\n    return configFile;\n}",
        "rpfc": "/*\n   * (non-Javadoc)\n   * \n   * @see\n   * org.apache.oodt.cas.pge.writers.DynamicConfigFileWriter#generateFile(java\n   * .lang.String, org.apache.oodt.cas.metadata.Metadata,\n   * java.util.logging.Logger, java.lang.Object[])\n   */\n@Override\npublic File generateFile(String filePath, Metadata metadata, Logger logger, Object... args) throws Exception {\n    File configFile = new File(filePath);\n    VelocityMetadata velocityMetadata = new VelocityMetadata(metadata);\n    try {\n        // Velocity requires you to set a path of where to look for\n        // templates. This path defaults to . if not set.\n        int slashIndex = ((String) args[0]).lastIndexOf('/');\n        String templatePath = ((String) args[0]).substring(0, slashIndex);\n        Velocity.setProperty(\"file.resource.loader.path\", templatePath);\n        Velocity.init();\n        VelocityContext context = new VelocityContext();\n        context.put(\"metadata\", velocityMetadata);\n        context.put(\"env\", System.getenv());\n        String templateName = ((String) args[0]).substring(slashIndex);\n        Template template = Velocity.getTemplate(templateName);\n        StringWriter sw = new StringWriter();\n        template.merge(context, sw);\n        FileUtils.writeStringToFile(configFile, sw.toString());\n    } catch (Exception e) {\n        e.printStackTrace();\n    }\n    return configFile;\n}",
        "tuc": "public void testCreateConfigFile() throws IOException {\n    URL url = this.getClass().getResource(\"/test-config.vm\");\n    VelocityConfigFileWriter vcfw = new VelocityConfigFileWriter();\n    VelocityMetadata metadata = new VelocityMetadata(new Metadata());\n    metadata.addMetadata(\"name\", \"Chris\");\n    metadata.addMetadata(\"name\", \"Paul\");\n    metadata.addMetadata(\"conference\", \"ApacheCon\");\n    File config = File.createTempFile(\"config\", \".out\");\n    try {\n        vcfw.generateFile(config.toString(), metadata, LOG, url.getFile().substring(0, url.getFile().lastIndexOf('/')), \"test-config.vm\");\n    } catch (Exception e) {\n        e.printStackTrace();\n        fail(e.getMessage());\n    }\n    String output = FileUtils.readFileToString(config);\n    assertEquals(\"Welcome to ApacheCon Chris Paul!\", output);\n    config.delete();\n}",
        "label": 1
    },
    {
        "repo_name": "apache___oodt",
        "commit": "9023c6b5a005e4c98e584163cc39d67283f50f05",
        "commit_message": "- fix for OODT-195 XMLValidationLayer: Elements Map and ProductType to Element Map can become inconsistent\n\ngit-svn-id: https://svn.apache.org/repos/asf/oodt/trunk@1128940 13f79535-47bb-0310-9956-ffa450edef68\n",
        "p_path": "filemgr/src/main/java/org/apache/oodt/cas/filemgr/validation/XMLValidationLayer.java",
        "t_path": "filemgr/src/test/org/apache/oodt/cas/filemgr/validation/TestXMLValidationLayer.java",
        "p_name": "modifyElement",
        "t_name": "testModifyElement",
        "lpfc": "public void modifyElement(Element element) throws ValidationLayerException {\n    elementMap.put(element.getElementId(), element);\n    saveElementsAndMappings();\n}",
        "rpfc": "public void modifyElement(Element element) throws ValidationLayerException {\n    for (Element elem : elementMap.values()) {\n        if (elem.getElementId().equals(element.getElementId())) {\n            elem.setElementName(element.getElementName());\n            elem.setDescription(elem.getDescription());\n            elem.setDCElement(element.getDCElement());\n        }\n    }\n    saveElementsAndMappings();\n}",
        "tuc": "/**\n * @since OODT-195\n */\npublic void testModifyElement() {\n    Element elem = new Element();\n    String elemName = \"TestFilename\";\n    elem.setElementName(elemName);\n    elem.setElementId(\"urn:oodt:Filename\");\n    try {\n        validationLayer.modifyElement(elem);\n    } catch (Exception e) {\n        fail(e.getMessage());\n    }\n    ProductType type = new ProductType();\n    type.setName(\"urn:oodt:GenericFile\");\n    List<Element> retrievedElems = null;\n    try {\n        retrievedElems = validationLayer.getElements(type);\n    } catch (Exception e) {\n        fail(e.getMessage());\n    }\n    assertNotNull(retrievedElems);\n    boolean found = false;\n    for (Element e : retrievedElems) {\n        if (e.getElementName().equals(elemName)) {\n            found = true;\n        }\n    }\n    assertTrue(\"Unable to find updated element: [\" + elemName + \"]: Set contains : [\" + retrievedElems + \"]\", found);\n}",
        "label": 1
    },
    {
        "repo_name": "apache___oodt",
        "commit": "29f44586e498463b21dc463196063044f8df4a32",
        "commit_message": "- fix for OODT-108 Ability for the file manager to ingest a file in place: made it work for hierarchical products too, added unit test\n\ngit-svn-id: https://svn.apache.org/repos/asf/oodt/trunk@1057466 13f79535-47bb-0310-9956-ffa450edef68\n",
        "p_path": "filemgr/src/main/java/org/apache/oodt/cas/filemgr/versioning/InPlaceVersioner.java",
        "t_path": "filemgr/src/test/org/apache/oodt/cas/filemgr/versioning/TestInPlaceVersioner.java",
        "p_name": "createDataStoreReferences",
        "t_name": "testVersioner",
        "lpfc": "/*\n   * (non-Javadoc)\n   * \n   * @see\n   * org.apache.oodt.cas.versioning.Versioner#createDataStoreReferences(org.\n   * apache.oodt.cas.data.structs.Product)\n   */\npublic void createDataStoreReferences(Product product, Metadata metadata) throws VersioningException {\n    if (product.getProductStructure().equals(Product.STRUCTURE_FLAT)) {\n        for (Reference r : product.getProductReferences()) {\n            r.setDataStoreReference(r.getOrigReference());\n            LOG.log(Level.INFO, \"in-place ingestion at datastore path: \" + r.getDataStoreReference() + \".which is the same as the product's origin: \" + r.getOrigReference());\n        }\n    } else {\n        throw new VersioningException(\"Unsupported product structure: \" + product.getProductStructure());\n    }\n}",
        "rpfc": "/*\n   * (non-Javadoc)\n   * \n   * @see\n   * org.apache.oodt.cas.versioning.Versioner#createDataStoreReferences(org.\n   * apache.oodt.cas.data.structs.Product)\n   */\npublic void createDataStoreReferences(Product product, Metadata metadata) throws VersioningException {\n    for (Reference r : product.getProductReferences()) {\n        r.setDataStoreReference(r.getOrigReference());\n        LOG.log(Level.INFO, \"in-place ingestion at datastore path: \" + r.getDataStoreReference() + \".which is the same as the product's origin: \" + r.getOrigReference());\n    }\n}",
        "tuc": "/**\n * @since OODT-108\n */\npublic void testVersioner() {\n    Product p = Product.getDefaultFlatProduct(\"test\", \"urn:oodt:GenericFile\");\n    Reference r = new Reference(\"file:///tmp/test.txt\", null, 0L);\n    p.getProductReferences().add(r);\n    InPlaceVersioner versioner = new InPlaceVersioner();\n    Metadata met = new Metadata();\n    try {\n        versioner.createDataStoreReferences(p, met);\n    } catch (Exception e) {\n        fail(e.getMessage());\n    }\n    assertTrue(r.getDataStoreReference().equals(r.getOrigReference()));\n}",
        "label": 1
    },
    {
        "repo_name": "mstritt___orbit-image-analysis",
        "commit": "2deab902e0326eff223028862f0c455b6a966733",
        "commit_message": "Allow dynamically changing the number of classes.\n",
        "p_path": "src/main/java/com/actelion/research/orbit/imageAnalysis/deeplearning/playground/maskRCNN/MaskRCNNSegment.java",
        "t_path": "src/test/java/com/actelion/research/orbit/imageAnalysis/test/deeplearning/playground/maskRCNN/TestCorpusCallosumSegment.java",
        "p_name": "getMaskRCNNRawDetections",
        "t_name": "testSegmentationAnnotationsCustom",
        "lpfc": "public RawDetections getMaskRCNNRawDetections(final Tensor<Float> inputTensor, final int inputWidth, final int inputHeight, final int maxDetections, final int maskWidth, final int maskHeight, final int numClasses) {\n    final FloatBuffer metas = FloatBuffer.wrap(new float[] { 0, inputWidth, inputHeight, 3, inputWidth, inputHeight, 3, 0, 0, inputWidth, inputHeight, 1, 0, 0, 0, 0, 0 });\n    final Tensor<Float> meta_data = Tensor.create(new long[] { 1, 17 }, metas);\n    List<Tensor<?>> res = s.runner().feed(\"input_image\", inputTensor).feed(\"input_image_meta\", meta_data).feed(\"input_anchors\", getAnchors(inputWidth)).fetch(\"mrcnn_detection/Reshape_1\").fetch(\"mrcnn_mask/Reshape_1\").run();\n    float[][][] res_detection = new float[1][maxDetections][6];\n    float[][][][][] res_mask = new float[1][maxDetections][maskHeight][maskWidth][numClasses];\n    Tensor<Float> mrcnn_detection = res.get(0).expect(Float.class);\n    Tensor<Float> mrcnn_mask = res.get(1).expect(Float.class);\n    mrcnn_detection.copyTo(res_detection);\n    mrcnn_mask.copyTo(res_mask);\n    RawDetections rawDetections = new RawDetections();\n    rawDetections.objectBB = res_detection;\n    rawDetections.masks = res_mask;\n    return rawDetections;\n}",
        "rpfc": "public RawDetections getMaskRCNNRawDetections(final Tensor<Float> inputTensor, final int inputWidth, final int inputHeight, final int maxDetections, final int maskWidth, final int maskHeight, final int numClasses) {\n    final FloatBuffer metas = FloatBuffer.wrap(new float[] { 0, inputWidth, inputHeight, 3, inputWidth, inputHeight, 3, 0, 0, inputWidth, inputHeight, 1, 0 });\n    for (int i = 0; i < numClasses; i++) {\n        metas.put(0);\n    }\n    int metaLength = 12 + numClasses;\n    final Tensor<Float> meta_data = Tensor.create(new long[] { 1, metaLength }, metas);\n    List<Tensor<?>> res = s.runner().feed(\"input_image\", inputTensor).feed(\"input_image_meta\", meta_data).feed(\"input_anchors\", getAnchors(inputWidth)).fetch(\"mrcnn_detection/Reshape_1\").fetch(\"mrcnn_mask/Reshape_1\").run();\n    float[][][] res_detection = new float[1][maxDetections][6];\n    float[][][][][] res_mask = new float[1][maxDetections][maskHeight][maskWidth][numClasses];\n    Tensor<Float> mrcnn_detection = res.get(0).expect(Float.class);\n    Tensor<Float> mrcnn_mask = res.get(1).expect(Float.class);\n    mrcnn_detection.copyTo(res_detection);\n    mrcnn_mask.copyTo(res_mask);\n    RawDetections rawDetections = new RawDetections();\n    rawDetections.objectBB = res_detection;\n    rawDetections.masks = res_mask;\n    return rawDetections;\n}",
        "tuc": "@Test\npublic void testSegmentationAnnotationsCustom() throws Exception {\n    int[] images = { 19340922 };\n    RawDataFile rdf = DALConfig.getImageProvider().LoadRawDataFile(19340922);\n    RecognitionFrame rf = new RecognitionFrame(rdf);\n    BufferedImage smallImage = rf.bimg.getMipMaps()[rf.bimg.getMipMaps().length - 1].getImage().getAsBufferedImage();\n    float imageScale = rf.bimg.getWidth() / 1024;\n    File maskRCNNBrainModel = new File(\"D:/deeplearning/corpus_callosum/finalbrainDetect2.pb\");\n    File maskRCNNCorpusCallosumModel = new File(\"D:/deeplearning/corpus_callosum/finalbrain15-56b.pb\");\n    MaskRCNNSegmentationSettings brainSettings = new MaskRCNNSegmentationSettings(512, 512, 1f, 1, 28, 28, 2, \"Brain\");\n    MaskRCNNSegmentationSettings corpusCallosumSettings = new MaskRCNNSegmentationSettings(1024, 1024, imageScale, 1, 56, 56, 2, \"Corpus_Callosum\");\n    MaskRCNNSegment brainModel = new MaskRCNNSegment(maskRCNNBrainModel, MaskRCNNSegment.PostProcessMethod.CUSTOM, brainSettings);\n    float xScale = smallImage.getWidth() / 512f;\n    float yScale = smallImage.getHeight() / 512f;\n    BufferedImage image512 = DLHelpers.resize(smallImage, brainSettings.getImageWidth(), brainSettings.getImageHeight());\n    Tensor<Float> input = DLHelpers.convertBufferedImageToTensor(image512, brainSettings.getImageWidth(), brainSettings.getImageHeight());\n    RawDetections rawBrain = brainModel.getMaskRCNNRawDetections(input);\n    MaskRCNNDetections brainz = brainModel.processDetections(brainSettings.getImageWidth(), brainSettings.getImageHeight(), rawBrain);\n    assertEquals(brainz.getDetections().size(), 1);\n    Rectangle brainBB = brainz.getBoundingBoxes().get(0);\n    BufferedImage brainImg = smallImage.getSubimage((int) (xScale * brainBB.x), (int) (yScale * brainBB.y), (int) (brainBB.width * xScale), (int) (brainBB.height * yScale));\n    brainImg = DLHelpers.resize(brainImg, 1024, 1024);\n    MaskRCNNSegment ccModel = new MaskRCNNSegment(maskRCNNCorpusCallosumModel, MaskRCNNSegment.PostProcessMethod.CUSTOM, corpusCallosumSettings);\n    Tensor<Float> input2 = DLHelpers.convertBufferedImageToTensor(brainImg, corpusCallosumSettings.getImageWidth(), corpusCallosumSettings.getImageHeight());\n    RawDetections rawCC = ccModel.getMaskRCNNRawDetections(input2);\n    MaskRCNNDetections cc = ccModel.processDetections(corpusCallosumSettings.getImageWidth(), corpusCallosumSettings.getImageHeight(), rawCC);\n    ccModel.storeShapes(cc, corpusCallosumSettings, 19340922, \"AutomatedAnnotation\");\n    assertEquals(cc.getDetections().size(), 1);\n    assertEquals(cc.getDetections().get(0).getBoundingBox().getBounds(), new Rectangle(464, 164, 214, 699));\n}",
        "label": 1
    },
    {
        "repo_name": "diggsweden___dgc-java",
        "commit": "3ec36f79fd4999b66eae346deca7fe64cc509c4a",
        "commit_message": "Merge pull request #45 from DIGGSweden/feature/cose-fix\n\nMaking sure that CWT times are encoded as ints.",
        "p_path": "create-validate/src/main/java/se/digg/dgc/signatures/cwt/support/CBORInstantConverter.java",
        "t_path": "create-validate/src/test/java/se/digg/dgc/cwt/CwtTest.java",
        "p_name": "ToCBORObject",
        "t_name": "testCreate",
        "lpfc": "/**\n * {@inheritDoc}\n */\n@Override\npublic CBORObject ToCBORObject(final Instant obj) {\n    if (obj == null) {\n        return null;\n    }\n    return untaggedDateConverter.ToCBORObject(new Date(obj.toEpochMilli()));\n}",
        "rpfc": "/**\n * {@inheritDoc}\n */\n@Override\npublic CBORObject ToCBORObject(final Instant obj) {\n    if (obj == null) {\n        return null;\n    }\n    return untaggedDateConverter.ToCBORObject(new Date(obj.getEpochSecond() * 1000L));\n}",
        "tuc": "@Test\npublic void testCreate() {\n    final Instant now = Instant.now();\n    final long seconds = now.getEpochSecond();\n    final CBORObject object = CBORObject.FromObject(\"value\");\n    Cwt cwt = Cwt.builder().issuer(\"Kalle\").issuedAt(now).expiration(now.plus(Duration.ofDays(30))).claim(\"98\", object.EncodeToBytes()).claim(99, object).build();\n    System.out.println(Hex.encodeHexString(cwt.encode()));\n    Cwt cwt2 = Cwt.decode(cwt.encode());\n    Assert.assertEquals(\"Kalle\", cwt2.getIssuer());\n    Assert.assertEquals(seconds, cwt2.getIssuedAt().getEpochSecond());\n    Assert.assertEquals(\"value\", cwt2.getClaim(\"98\").AsString());\n    Assert.assertEquals(\"value\", cwt2.getClaim(99).AsString());\n}",
        "label": 1
    },
    {
        "repo_name": "diggsweden___dgc-java",
        "commit": "67fe9cc2e4e1588a3d435b34252fa1632d41dfc3",
        "commit_message": "Making sure that CWT times are encoded as ints.\n",
        "p_path": "create-validate/src/main/java/se/digg/dgc/signatures/cwt/support/CBORInstantConverter.java",
        "t_path": "create-validate/src/test/java/se/digg/dgc/cwt/CwtTest.java",
        "p_name": "ToCBORObject",
        "t_name": "testCreate",
        "lpfc": "/**\n * {@inheritDoc}\n */\n@Override\npublic CBORObject ToCBORObject(final Instant obj) {\n    if (obj == null) {\n        return null;\n    }\n    return untaggedDateConverter.ToCBORObject(new Date(obj.toEpochMilli()));\n}",
        "rpfc": "/**\n * {@inheritDoc}\n */\n@Override\npublic CBORObject ToCBORObject(final Instant obj) {\n    if (obj == null) {\n        return null;\n    }\n    return untaggedDateConverter.ToCBORObject(new Date(obj.getEpochSecond() * 1000L));\n}",
        "tuc": "@Test\npublic void testCreate() {\n    final Instant now = Instant.now();\n    final long seconds = now.getEpochSecond();\n    final CBORObject object = CBORObject.FromObject(\"value\");\n    Cwt cwt = Cwt.builder().issuer(\"Kalle\").issuedAt(now).expiration(now.plus(Duration.ofDays(30))).claim(\"98\", object.EncodeToBytes()).claim(99, object).build();\n    System.out.println(Hex.encodeHexString(cwt.encode()));\n    Cwt cwt2 = Cwt.decode(cwt.encode());\n    Assert.assertEquals(\"Kalle\", cwt2.getIssuer());\n    Assert.assertEquals(seconds, cwt2.getIssuedAt().getEpochSecond());\n    Assert.assertEquals(\"value\", cwt2.getClaim(\"98\").AsString());\n    Assert.assertEquals(\"value\", cwt2.getClaim(99).AsString());\n}",
        "label": 1
    },
    {
        "repo_name": "KengoTODA___findbugs-slf4j",
        "commit": "509f0cbb235b7dddaa57d1bcf983ac6f31081a10",
        "commit_message": "reproduce additional false negative\n",
        "p_path": "test-case/src/main/java/pkg/UsingMarker.java",
        "t_path": "test-case/src/test/java/jp/skypencil/findbugs/slf4j/UsingMarkerTest.java",
        "p_name": "method",
        "t_name": "test",
        "lpfc": "void method() {\n    Marker marker = MarkerFactory.getMarker(\"my marker\");\n    logger.error(marker, \"Hello, marker\");\n    logger.error(marker, \"Hello, {}\");\n    logger.error(marker, \"Hello, {}\", \"world\");\n}",
        "rpfc": "void method() {\n    Marker marker = MarkerFactory.getMarker(\"my marker\");\n    logger.error(marker, \"Hello, marker\");\n    logger.error(marker, \"Hello, {}\");\n    logger.error(marker, \"Hello, {}\", \"world\");\n    logger.error(marker, \"Hello, {}\", new RuntimeException());\n}",
        "tuc": "@Test\npublic void test() {\n    Map<String, Integer> expected = new HashMap<>();\n    expected.put(\"SLF4J_PLACE_HOLDER_MISMATCH\", 1);\n    new XmlParser().expect(pkg.UsingMarker.class, expected);\n}",
        "label": 1
    },
    {
        "repo_name": "KengoTODA___findbugs-slf4j",
        "commit": "f716040a0c6df11c883f50bf68dee88b5265f29c",
        "commit_message": "stop using null as marker\n",
        "p_path": "test-case/src/main/java/pkg/UsingMarker.java",
        "t_path": "test-case/src/test/java/jp/skypencil/findbugs/slf4j/UsingMarkerTest.java",
        "p_name": "method",
        "t_name": "test",
        "lpfc": "void method() {\n    Marker marker = null;\n    logger.error(marker, \"Hello, marker\");\n    logger.error(marker, \"Hello, {}\");\n    logger.error(marker, \"Hello, {}\", \"world\");\n}",
        "rpfc": "void method() {\n    Marker marker = MarkerFactory.getMarker(\"my marker\");\n    logger.error(marker, \"Hello, marker\");\n    logger.error(marker, \"Hello, {}\");\n    logger.error(marker, \"Hello, {}\", \"world\");\n}",
        "tuc": "@Test\npublic void test() {\n    Map<String, Integer> expected = new HashMap<>();\n    expected.put(\"NP_LOAD_OF_KNOWN_NULL_VALUE\", 1);\n    expected.put(\"SLF4J_PLACE_HOLDER_MISMATCH\", 1);\n    new XmlParser().expect(pkg.UsingMarker.class, expected);\n}",
        "label": 1
    },
    {
        "repo_name": "KengoTODA___findbugs-slf4j",
        "commit": "f7ae9d198916601a1024601110db757fec7a9963",
        "commit_message": "make sure that typo does not affect behaviour\n",
        "p_path": "test-case/src/main/java/pkg/UsingMarker.java",
        "t_path": "test-case/src/test/java/jp/skypencil/findbugs/slf4j/UsingMarkerTest.java",
        "p_name": "method",
        "t_name": "test",
        "lpfc": "void method() {\n    Marker marker = null;\n    logger.error(marker, \"Hello, marker\");\n    logger.error(marker, \"Hello, {}\", \"world\");\n}",
        "rpfc": "void method() {\n    Marker marker = null;\n    logger.error(marker, \"Hello, marker\");\n    logger.error(marker, \"Hello, {}\");\n    logger.error(marker, \"Hello, {}\", \"world\");\n}",
        "tuc": "@Test\npublic void test() {\n    Map<String, Integer> expected = Collections.singletonMap(\"NP_LOAD_OF_KNOWN_NULL_VALUE\", 1);\n    new XmlParser().expect(pkg.UsingMarker.class, expected);\n}",
        "label": 1
    },
    {
        "repo_name": "KengoTODA___findbugs-slf4j",
        "commit": "767f19409a530f8214e497a9a9db2b07d1a60e61",
        "commit_message": "fixed incorrect spelling. refs #15\n",
        "p_path": "bug-pattern/src/main/java/jp/skypencil/findbugs/slf4j/WrongPlaceholderDetector.java",
        "t_path": "bug-pattern/src/test/java/jp/skypencil/findbugs/slf4j/WrongPlaceholderDetectorTest.java",
        "p_name": "countParameter",
        "t_name": "testCountParameterWithoutArray",
        "lpfc": "int countParameter(OpcodeStack stack, String methodSignature) {\n    String[] signatures = splitSignature(methodSignature);\n    if (signatures[signatures.length - 1].equals(\"[Ljava/lang/Object;\")) {\n        ArrayData arrayData = (ArrayData) stack.getStackItem(0).getUserValue();\n        if (arrayData == null || arrayData.getSize() < 0) {\n            throw new IllegalStateException(\"no array initializer found\");\n        }\n        int parameterCount = arrayData.getSize();\n        if (arrayData.hasThrowableAtLast()) {\n            --parameterCount;\n        }\n        return parameterCount;\n    }\n    int parameterCount = signatures.length - 1;\n    if (signatures[0].equals(\"Lorg/slf4j/Maker;\")) {\n        --parameterCount;\n    }\n    Item lastItem = stack.getStackItem(0);\n    if (throwableHandler.checkThrowable(lastItem)) {\n        --parameterCount;\n    }\n    return parameterCount;\n}",
        "rpfc": "int countParameter(OpcodeStack stack, String methodSignature) {\n    String[] signatures = splitSignature(methodSignature);\n    if (signatures[signatures.length - 1].equals(\"[Ljava/lang/Object;\")) {\n        ArrayData arrayData = (ArrayData) stack.getStackItem(0).getUserValue();\n        if (arrayData == null || arrayData.getSize() < 0) {\n            throw new IllegalStateException(\"no array initializer found\");\n        }\n        int parameterCount = arrayData.getSize();\n        if (arrayData.hasThrowableAtLast()) {\n            --parameterCount;\n        }\n        return parameterCount;\n    }\n    int parameterCount = signatures.length - 1;\n    if (signatures[0].equals(\"Lorg/slf4j/Marker;\")) {\n        --parameterCount;\n    }\n    Item lastItem = stack.getStackItem(0);\n    if (throwableHandler.checkThrowable(lastItem)) {\n        --parameterCount;\n    }\n    return parameterCount;\n}",
        "tuc": "@Test\npublic void testCountParameterWithoutArray() {\n    WrongPlaceholderDetector detector = new WrongPlaceholderDetector(null);\n    OpcodeStack stack = mock(OpcodeStack.class);\n    Item itemInStack = mock(Item.class);\n    doReturn(itemInStack).when(stack).getStackItem(0);\n    assertThat(detector.countParameter(stack, \"(Ljava/lang/String;Ljava/lang/Object;)V\"), is(1));\n    assertThat(detector.countParameter(stack, \"(Ljava/lang/String;Ljava/lang/Object;Ljava/lang/Object;)V\"), is(2));\n    assertThat(detector.countParameter(stack, \"(Lorg/slf4j/Maker;Ljava/lang/String;Ljava/lang/Object;)V\"), is(1));\n    assertThat(detector.countParameter(stack, \"(Lorg/slf4j/Maker;Ljava/lang/String;Ljava/lang/Object;Ljava/lang/Object;)V\"), is(2));\n    Item exceptionInStack = mock(Item.class);\n    doReturn(\"IS_THROWABLE\").when(exceptionInStack).getUserValue();\n    doReturn(exceptionInStack).when(stack).getStackItem(0);\n    assertThat(detector.countParameter(stack, \"(Ljava/lang/String;Ljava/lang/Object;)V\"), is(0));\n    assertThat(detector.countParameter(stack, \"(Ljava/lang/String;Ljava/lang/Object;Ljava/lang/Object;)V\"), is(1));\n    assertThat(detector.countParameter(stack, \"(Lorg/slf4j/Maker;Ljava/lang/String;Ljava/lang/Object;)V\"), is(0));\n    assertThat(detector.countParameter(stack, \"(Lorg/slf4j/Maker;Ljava/lang/String;Ljava/lang/Object;Ljava/lang/Object;)V\"), is(1));\n}",
        "label": 1
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "0d4019859aa942d48e9d630163a8286c51dea428",
        "commit_message": "Merge remote-tracking branch 'origin/master'\n",
        "p_path": "QiunetUtils/src/main/java/org/qiunet/utils/timer/TimerManager.java",
        "t_path": "Quartz/src/test/java/org/qiunet/quartz/test/TestQuartz.java",
        "p_name": "shutdown",
        "t_name": "testCron",
        "lpfc": "public void shutdown() {\n    schedule.shutdown();\n}",
        "rpfc": "public void shutdown() {\n    schedule.shutdownNow();\n}",
        "tuc": "@Test\npublic void testCron() throws InterruptedException {\n    ClassScanner.getInstance().scanner();\n    Thread.sleep(100000);\n}",
        "label": 1
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "50485b9b39193944b23078aff2800a212593dc2b",
        "commit_message": "\u4f18\u5316quartz\u7684\u8c03\u7528\n",
        "p_path": "QiunetUtils/src/main/java/org/qiunet/utils/timer/TimerManager.java",
        "t_path": "Quartz/src/test/java/org/qiunet/quartz/test/TestQuartz.java",
        "p_name": "shutdown",
        "t_name": "testCron",
        "lpfc": "public void shutdown() {\n    schedule.shutdown();\n}",
        "rpfc": "public void shutdown() {\n    schedule.shutdownNow();\n}",
        "tuc": "@Test\npublic void testCron() throws InterruptedException {\n    ClassScanner.getInstance().scanner();\n    Thread.sleep(100000);\n}",
        "label": 1
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "53bd0de4076aa51bcb87d40afd5d3e507908cb1a",
        "commit_message": "\u589e\u52a0\u8fc7\u6ee4\u8bcd\u6d4b\u8bd5\u7528\u4f8b\n",
        "p_path": "QiunetUtils/src/main/java/org/qiunet/utils/badword/BadWordFilter.java",
        "t_path": "QiunetUtils/src/test/java/org/qiunet/utils/badword/TestBadWord.java",
        "p_name": "powerFind",
        "t_name": "testBadWord",
        "lpfc": "public String powerFind(String content) {\n    Matcher m = NOT_CHINESE_REGEX.get().matcher(content);\n    String tempContent = m.replaceAll(\"\");\n    String ret = find(tempContent);\n    if (ret == null)\n        ret = find(content);\n    return ret;\n}",
        "rpfc": "public String powerFind(String content) {\n    Matcher m = NOT_CHINESE_REGEX.get().matcher(content);\n    String tempContent = m.replaceAll(\"\");\n    String ret = find(tempContent);\n    if (ret == null && tempContent.length() != content.length())\n        ret = find(content);\n    return ret;\n}",
        "tuc": "@Test\npublic void testBadWord() {\n    BadWordFilter.getInstance().loadBadWord(new DefaultBadWord(new String[] { \"\u67df\", \"\u738b\u5c90\u5c71\", \"\u738b\u7389\u521a\", \"\u6bdb\u6cfd\u4e1c\", \"www.qq.com\" }));\n    Assert.assertEquals(\"\u6bdb\u6cfd\u4e1c\", BadWordFilter.getInstance().powerFind(\"sss\u6bdb2\u6cfd3\u4e1c7--\"));\n    Assert.assertEquals(\"\u67df\", BadWordFilter.getInstance().find(\"sss\u738b\u67df\u5c717--\"));\n    Assert.assertEquals(\"\u738b\u5c90\u5c71\", BadWordFilter.getInstance().find(\"s\u738bss\u738b\u5c90\u5c717--\"));\n    Assert.assertEquals(\"ss\u738bs***7\u6211***--\", BadWordFilter.getInstance().doFilter(\"ss\u738bs\u738b\u5c90\u5c717\u6211\u6bdb\u6cfd\u4e1c--\"));\n    Assert.assertEquals(\"\u8bbf\u95ee**********\u5c31\u884c\", BadWordFilter.getInstance().doFilter(\"\u8bbf\u95eewww.qq.com\u5c31\u884c\"));\n}",
        "label": 1
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "c9c290328cc37524995c20b32e75fde9da385a52",
        "commit_message": "\u89e3\u51b3\u5173\u952e\u5b57\u4e3a\u4e00\u4e2a\u5b57\u65f6\u5019\u7684bug\n",
        "p_path": "QiunetUtils/src/main/java/org/qiunet/utils/badword/BadWordFilter.java",
        "t_path": "QiunetUtils/src/test/java/org/qiunet/utils/badword/TestBadWord.java",
        "p_name": "loadBadWord",
        "t_name": "testBadWord",
        "lpfc": "public void loadBadWord(IBadWord badWords) {\n    rootNode = new RootNode();\n    for (String badWord : badWords.getBadWordList()) {\n        int index = 0;\n        INode node = rootNode;\n        do {\n            INode currNode = node.find(badWord.charAt(index));\n            if (currNode == null) {\n                node.addNode(node = new CharNode(badWord.charAt(index), index == (badWord.length() - 1)));\n            } else {\n                node = currNode;\n            }\n            if (node.endChar())\n                break;\n        } while (++index < badWord.length());\n    }\n}",
        "rpfc": "public void loadBadWord(IBadWord badWords) {\n    rootNode = new RootNode();\n    for (String badWord : badWords.getBadWordList()) {\n        if (badWord.length() == 0)\n            continue;\n        int index = 0;\n        INode node = rootNode;\n        do {\n            INode currNode = node.find(badWord.charAt(index));\n            if (currNode == null) {\n                node.addNode(node = new CharNode(badWord.charAt(index), index == (badWord.length() - 1)));\n            } else {\n                node = currNode;\n            }\n            if (node.endChar())\n                break;\n        } while (++index < badWord.length());\n    }\n}",
        "tuc": "@Test\npublic void testBadWord() {\n    BadWordFilter.getInstance().loadBadWord(new DefaultBadWord(new String[] { \"\u738b\u5c90\u5c71\", \"\u738b\u7389\u521a\", \"\u6bdb\u6cfd\u4e1c\", \"www.qq.com\" }));\n    Assert.assertEquals(\"\u738b\u5c90\u5c71\", BadWordFilter.getInstance().find(\"sss\u738b\u5c90\u5c717--\"));\n    Assert.assertEquals(\"ss\u738bs***7\u6211***--\", BadWordFilter.getInstance().doFilter(\"ss\u738bs\u738b\u5c90\u5c717\u6211\u6bdb\u6cfd\u4e1c--\"));\n    Assert.assertEquals(\"\u8bbf\u95ee**********\u5c31\u884c\", BadWordFilter.getInstance().doFilter(\"\u8bbf\u95eewww.qq.com\u5c31\u884c\"));\n}",
        "label": 1
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "7ee72929d5ba6ab2cc6ea75ab7bbadbed9f1b5b0",
        "commit_message": "subList \u6539\u5199\n",
        "p_path": "QiunetUtils/src/main/java/org/qiunet/utils/common/CommonUtil.java",
        "t_path": "QiunetUtils/src/test/java/org/qiunet/utils/common/TestCommonUtil.java",
        "p_name": "getSubListPage",
        "t_name": "testSubList",
        "lpfc": "public static <T> List<T> getSubListPage(List<T> list, int skip, int count) {\n    if (list == null || list.isEmpty()) {\n        return null;\n    }\n    int startIndex = skip;\n    int endIndex = skip + count;\n    if (startIndex > endIndex || startIndex > list.size()) {\n        return null;\n    }\n    if (endIndex > list.size()) {\n        endIndex = list.size();\n    }\n    return list.subList(startIndex, endIndex);\n}",
        "rpfc": "public static <T> List<T> getSubListPage(List<T> list, int skip, int count) {\n    if (list == null || list.isEmpty()) {\n        return null;\n    }\n    return list.stream().skip(skip).limit(count).collect(Collectors.toList());\n}",
        "tuc": "@Test\npublic void testSubList() {\n    List<Integer> list = new ArrayList<>(Arrays.asList(new Integer[] { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 }));\n    List<Integer> subList = CommonUtil.getSubListPage(list, 1, 4);\n    Assert.assertTrue(subList.get(0) == 1);\n}",
        "label": 1
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "0d28d658d6efc6bc584905924f162ebf39d8db67",
        "commit_message": "magic \u7684\u5224\u65ad\u4fee\u6539\u4e3ajdk\u81ea\u5df1\u7684\u65b9\u6cd5\n",
        "p_path": "FlashHandler/src/main/java/org/qiunet/flash/handler/context/header/ProtocolHeader.java",
        "t_path": "FlashHandler/src/test/java/org/qiunet/flash/handler/header/TestProtocolHeader.java",
        "p_name": "isMagicValid",
        "t_name": "testheader",
        "lpfc": "public boolean isMagicValid() {\n    for (int i = 0; i < MAGIC_CONTENTS.length; i++) {\n        if (this.magic[i] != MAGIC_CONTENTS[i]) {\n            return false;\n        }\n    }\n    return true;\n}",
        "rpfc": "public boolean isMagicValid() {\n    return Arrays.equals(this.magic, MAGIC_CONTENTS);\n}",
        "tuc": "@Test\npublic void testheader() {\n    ByteBuf byteBuf = Unpooled.buffer();\n    ProtocolHeader header1 = new ProtocolHeader(100, 1, 44664323);\n    header1.writeToByteBuf(byteBuf);\n    ProtocolHeader header2 = new ProtocolHeader(byteBuf);\n    Assert.assertArrayEquals(header1.getMagic(), header2.getMagic());\n    Assert.assertEquals(header1.getLength(), header2.getLength());\n    Assert.assertEquals(header1.getProtocolId(), header2.getProtocolId());\n    Assert.assertTrue(header2.crcIsValid(44664323L));\n}",
        "label": 1
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "c802851cb8be508e9427810d0ace897b4b1732e5",
        "commit_message": "\u5916\u9762\u4f20\u5165maxLockedCount\n",
        "p_path": "QiunetUtils/src/main/java/org/qiunet/utils/lock/UserLockManager.java",
        "t_path": "QiunetUtils/src/test/java/org/qiunet/utils/lock/TestUserLock.java",
        "p_name": "tryLock",
        "t_name": "testLock",
        "lpfc": "private synchronized boolean tryLock() {\n    boolean canLock = lockedCount.get() < MAX_THREAD_COUNT_HOLD_LOCK;\n    if (canLock) {\n        lockedCount.incrementAndGet();\n    }\n    return canLock;\n}",
        "rpfc": "private synchronized boolean tryLock() {\n    boolean canLock = lockedCount.get() < maxLockedCount;\n    if (canLock) {\n        lockedCount.incrementAndGet();\n    }\n    return canLock;\n}",
        "tuc": "@Test\npublic void testLock() throws InterruptedException {\n    final UserLockManager<String> manager = new UserLockManager<>();\n    final AtomicInteger fastCount = new AtomicInteger();\n    final AtomicInteger handleCount = new AtomicInteger();\n    int threadCount = 100;\n    final CountDownLatch latch = new CountDownLatch(threadCount);\n    for (int i = 0; i < threadCount; i++) {\n        new Thread(new Runnable() {\n            @Override\n            public void run() {\n                try {\n                    if (!manager.lock(\"qiunet\")) {\n                        fastCount.incrementAndGet();\n                        latch.countDown();\n                        return;\n                    }\n                    handleCount.incrementAndGet();\n                    Thread.sleep(10);\n                    latch.countDown();\n                } catch (InterruptedException e) {\n                    e.printStackTrace();\n                } finally {\n                    manager.releaseLock(\"qiunet\");\n                }\n            }\n        }, String.valueOf(i)).start();\n    }\n    latch.await();\n    Assert.assertTrue(manager.getLockedCount(\"qiunet\") == 0);\n    Assert.assertTrue(fastCount.get() == (threadCount - UserLockManager.MAX_THREAD_COUNT_HOLD_LOCK));\n    Assert.assertTrue(handleCount.get() == (UserLockManager.MAX_THREAD_COUNT_HOLD_LOCK));\n}",
        "label": 1
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "e233e3deea2bb86f3c90ebd40adea7109053ed0f",
        "commit_message": "\u662f\u5426\u5728\u67d0\u4e2a\u533a\u95f4\n",
        "p_path": "QiunetUtils/src/main/java/org/qiunet/utils/date/DateUtil.java",
        "t_path": "QiunetUtils/src/test/java/org/qiunet/utils/date/TestDateUtil.java",
        "p_name": "isBetweenDays",
        "t_name": "testIsBetweenDay",
        "lpfc": "public static boolean isBetweenDays(Date date, Date date1, Date date2) {\n    if (date == null || date1 == null || date2 == null) {\n        return false;\n    }\n    if (date1.before(date2) && date.after(date1) && date.before(date2)) {\n        return true;\n    } else if (date.before(date1) && date.after(date2)) {\n        return true;\n    }\n    return false;\n}",
        "rpfc": "public static boolean isBetweenDays(Date date, Date dateBefore, Date dateLast) {\n    long d = date.getTime();\n    long d1 = dateBefore.getTime();\n    long d2 = dateLast.getTime();\n    return d >= d1 && d < d2;\n}",
        "tuc": "@Test\npublic void testIsBetweenDay() {\n    try {\n        Date date1 = DateUtil.stringToDate(\"2016-05-16 00:00:00\");\n        Date date2 = DateUtil.stringToDate(\"2016-05-26 00:00:00\");\n        Date dt = DateUtil.stringToDate(\"2016-05-20 00:00:00\");\n        Assert.assertTrue(DateUtil.isBetweenDays(dt, date1, date2));\n    } catch (ParseException e) {\n        e.printStackTrace();\n    }\n}",
        "label": 1
    },
    {
        "repo_name": "halo-dev___plugin-s3",
        "commit": "565d3cfcaad1f2a2d95d6a1f98532aae5a17e9e3",
        "commit_message": "Fix findUrlSuffix NPE when upgrading from old version (#94)\n\nfixes https://github.com/halo-dev/plugin-s3/issues/93\r\n```release-note\r\n\u4fee\u590d\u4ece\u65e7\u7248\u672c\u5347\u7ea7\u540e\u4e0a\u4f20\u6587\u4ef6\u7684NPE\u9519\u8bef\r\n```\r\n/kind bug\n",
        "p_path": "src/main/java/run/halo/s3os/UrlUtils.java",
        "t_path": "src/test/java/run/halo/s3os/UrlUtilsTest.java",
        "p_name": "findUrlSuffix",
        "t_name": "testFindUrlSuffix",
        "lpfc": "public static String findUrlSuffix(List<S3OsProperties.urlSuffixItem> urlSuffixList, String fileName) {\n    if (StringUtils.isBlank(fileName)) {\n        return null;\n    }\n    fileName = fileName.toLowerCase();\n    for (S3OsProperties.urlSuffixItem item : urlSuffixList) {\n        String[] fileSuffixes = item.getFileSuffix().split(\",\");\n        for (String suffix : fileSuffixes) {\n            if (fileName.endsWith(\".\" + suffix.trim().toLowerCase())) {\n                return item.getUrlSuffix();\n            }\n        }\n    }\n    return null;\n}",
        "rpfc": "public static String findUrlSuffix(List<S3OsProperties.urlSuffixItem> urlSuffixList, String fileName) {\n    if (StringUtils.isBlank(fileName) || urlSuffixList == null) {\n        return null;\n    }\n    fileName = fileName.toLowerCase();\n    for (S3OsProperties.urlSuffixItem item : urlSuffixList) {\n        String[] fileSuffixes = item.getFileSuffix().split(\",\");\n        for (String suffix : fileSuffixes) {\n            if (fileName.endsWith(\".\" + suffix.trim().toLowerCase())) {\n                return item.getUrlSuffix();\n            }\n        }\n    }\n    return null;\n}",
        "tuc": "@Test\npublic void testFindUrlSuffix() {\n    List<S3OsProperties.urlSuffixItem> urlSuffixList = List.of(new S3OsProperties.urlSuffixItem(\"jpg,png,gif\", \"?imageMogr2/format/webp\"), new S3OsProperties.urlSuffixItem(\"pdf\", \"?123=123\"), new S3OsProperties.urlSuffixItem(\"jpg\", \"?456=456\"));\n    String fileName1 = \"example.jpg\";\n    String result1 = UrlUtils.findUrlSuffix(urlSuffixList, fileName1);\n    assertEquals(\"?imageMogr2/format/webp\", result1);\n    String fileName2 = \"Document.PDF\";\n    String result2 = UrlUtils.findUrlSuffix(urlSuffixList, fileName2);\n    assertEquals(\"?123=123\", result2);\n    String fileName3 = \"unknown.txt\";\n    String result3 = UrlUtils.findUrlSuffix(urlSuffixList, fileName3);\n    assertNull(result3);\n    String fileName4 = \"example\";\n    String result4 = UrlUtils.findUrlSuffix(urlSuffixList, fileName4);\n    assertNull(result4);\n    String fileName5 = \"\";\n    String result5 = UrlUtils.findUrlSuffix(urlSuffixList, fileName5);\n    assertNull(result5);\n}",
        "label": 1
    },
    {
        "repo_name": "apache___oodt",
        "commit": "c212b1a53d6ac19d657951d972005b5287eba271",
        "commit_message": "fix about 1000 code quality issues\n",
        "p_path": "filemgr/src/main/java/org/apache/oodt/cas/filemgr/catalog/DataSourceCatalog.java",
        "t_path": "filemgr/src/test/java/org/apache/oodt/cas/filemgr/catalog/TestDataSourceCatalog.java",
        "p_name": "removeMetadata",
        "t_name": "testRemoveMetadata",
        "lpfc": "public synchronized void removeMetadata(Metadata m, Product product) throws CatalogException {\n    List<Element> metadataTypes = null;\n    try {\n        metadataTypes = validationLayer.getElements(product.getProductType());\n    } catch (ValidationLayerException e) {\n        e.printStackTrace();\n        throw new CatalogException(\"ValidationLayerException when trying to obtain element list for product type: \" + product.getProductType().getName() + \": Message: \" + e.getMessage());\n    }\n    for (Iterator<Element> i = metadataTypes.iterator(); i.hasNext(); ) {\n        Element element = i.next();\n        List<String> values = m.getAllMetadata(element.getElementName());\n        if (values != null) {\n            for (Iterator<String> j = values.iterator(); j.hasNext(); ) {\n                String value = j.next();\n                try {\n                    removeMetadataValue(element, product, value);\n                } catch (Exception e) {\n                    e.printStackTrace();\n                    LOG.log(Level.WARNING, \"Exception removing metadata. Error deleting field: [\" + element.getElementId() + \"=>\" + value + \"]: for product: [\" + product.getProductName() + \"]: Message: \" + e.getMessage() + \": Attempting to continue processing metadata\");\n                }\n            }\n        }\n    }\n}",
        "rpfc": "public synchronized void removeMetadata(Metadata m, Product product) throws CatalogException {\n    List<Element> metadataTypes;\n    try {\n        metadataTypes = validationLayer.getElements(product.getProductType());\n    } catch (ValidationLayerException e) {\n        e.printStackTrace();\n        throw new CatalogException(\"ValidationLayerException when trying to obtain element list for product type: \" + product.getProductType().getName() + \": Message: \" + e.getMessage());\n    }\n    for (Element element : metadataTypes) {\n        List<String> values = m.getAllMetadata(element.getElementName());\n        if (values != null) {\n            for (String value : values) {\n                try {\n                    removeMetadataValue(element, product, value);\n                } catch (Exception e) {\n                    e.printStackTrace();\n                    LOG.log(Level.WARNING, \"Exception removing metadata. Error deleting field: [\" + element.getElementId() + \"=>\" + value + \"]: for product: [\" + product.getProductName() + \"]: Message: \" + e.getMessage() + \": Attempting to continue processing metadata\");\n                }\n            }\n        }\n    }\n}",
        "tuc": "public void testRemoveMetadata() {\n    Metadata met = new Metadata();\n    met.addMetadata(\"Filename\", \"tempProduct\");\n    Product testProduct = getTestProduct();\n    testProduct.setProductId(\"1\");\n    try {\n        myCat.addMetadata(met, testProduct);\n    } catch (Exception e) {\n        e.printStackTrace();\n        fail(e.getMessage());\n    }\n    try {\n        myCat.removeMetadata(met, testProduct);\n    } catch (Exception e) {\n        e.printStackTrace();\n        fail(e.getMessage());\n    }\n    try {\n        Metadata retMet = myCat.getMetadata(testProduct);\n        String retValue = retMet.getMetadata(\"Filename\");\n        assertNull(retValue);\n    } catch (CatalogException e) {\n        fail(e.getMessage());\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "karussell___Jetwick",
        "commit": "ef88cc9785c1ee7234428bd61bbb39c8d83f6d25",
        "commit_message": "minor wording change;; max 10 minutes for jtag\n",
        "p_path": "src/main/java/de/jetwick/data/JTag.java",
        "t_path": "src/test/java/de/jetwick/data/JTagTest.java",
        "p_name": "optimizeQueryFrequency",
        "t_name": "testOptimizeQueryFrequency",
        "lpfc": "public void optimizeQueryFrequency(int newTweets) {\n    if (newTweets == 0)\n        queryInterval *= 20;\n    else\n        queryInterval = (long) (20.0 / newTweets) * queryInterval;\n    queryInterval = Math.max(queryInterval, 5 * 1001);\n    queryInterval = Math.min(queryInterval, 5 * 60 * 1001);\n}",
        "rpfc": "public void optimizeQueryFrequency(int newTweets) {\n    if (newTweets == 0)\n        queryInterval *= 20;\n    else\n        queryInterval = (long) (20.0 / newTweets) * queryInterval;\n    queryInterval = Math.max(queryInterval, 5 * 1001);\n    queryInterval = Math.min(queryInterval, 10 * 60 * 1001);\n}",
        "tuc": "@Test\npublic void testOptimizeQueryFrequency() {\n    JTag st = new JTag(\"java\");\n    st.optimizeQueryFrequency(10);\n    assertTrue(st.getQueryInterval() > 1000);\n    st.optimizeQueryFrequency(1);\n    assertTrue(st.getQueryInterval() > 1000);\n    st.optimizeQueryFrequency(100);\n    assertTrue(st.getQueryInterval() > 1000);\n    st.optimizeQueryFrequency(1000);\n    assertTrue(st.getQueryInterval() > 1000);\n}",
        "label": 0
    },
    {
        "repo_name": "eldur___jwbf",
        "commit": "0068320c4ef681c878d499fe5db85b4440574630",
        "commit_message": "Removed inner mutating state for debuging reasons\n",
        "p_path": "src/main/java/net/sourceforge/jwbf/core/actions/HttpActionClient.java",
        "t_path": "src/test/java/net/sourceforge/jwbf/core/actions/HttpActionClientTest.java",
        "p_name": "debug",
        "t_name": "testDebug",
        "lpfc": "@VisibleForTesting\nObject[] debug(HttpUriRequest request, HttpAction ha, ReturningTextProcessor cp) {\n    if (cp != null) {\n        final String continueing = debugContinueingMsg(cp);\n        final String path = debugRequestPathOf(request);\n        final String type = debugTypeOf(ha, cp, continueing);\n        return new String[] { type, path, ha.getRequest() };\n    }\n    return new String[0];\n}",
        "rpfc": "@VisibleForTesting\nObject[] debug(HttpUriRequest request, HttpAction ha, ReturningTextProcessor cp) {\n    if (cp != null) {\n        final String path = debugRequestPathOf(request);\n        final String type = debugTypeOf(ha, cp);\n        return new String[] { type, path, ha.getRequest() };\n    }\n    return new String[0];\n}",
        "tuc": "@Test\npublic void testDebug() {\n    testee = HttpActionClient.of(\"http://localhost/\");\n    HttpAction action = new Get(\"a\");\n    HttpUriRequest request = mock(HttpUriRequest.class);\n    when(request.getURI()).thenReturn(JWBF.toUri(\"http://localhost/wiki/api.php\"));\n    ImmutableList<String> result = ImmutableList.copyOf((String[]) testee.debug(request, action, MOCK_HANDLER));\n    ImmutableList<String> expected = ImmutableList.<String>builder().add(\"(GET net.sourceforge.jwbf.core.actions.HttpActionClientTest$2)\").add(\"http://localhost/wiki\").add(\"a\").build();\n    GAssert.assertEquals(expected, result);\n}",
        "label": 0
    },
    {
        "repo_name": "corona-warn-app___cwa-verification-portal",
        "commit": "3f9acce09724090e99320cee56c222f8da55282c",
        "commit_message": "feat: add password \u00e4ndern button (#82)\n\nFix: A password can be changed by a \"password \u00e4ndern\" button\r\nCo-authored-by: Maximilian Laue <65015235+mlaue-tech@users.noreply.github.com>",
        "p_path": "src/main/java/app/coronawarn/verification/portal/controller/VerificationPortalController.java",
        "t_path": "src/test/java/app/coronawarn/verification/portal/controller/VerificationPortalControllerTest.java",
        "p_name": "start",
        "t_name": "testStart",
        "lpfc": "@RequestMapping(value = ROUTE_START, method = { RequestMethod.GET, RequestMethod.POST })\npublic String start(HttpServletRequest request, Model model) {\n    KeycloakAuthenticationToken principal = (KeycloakAuthenticationToken) request.getUserPrincipal();\n    String user = ((KeycloakPrincipal) principal.getPrincipal()).getName();\n    if (model != null) {\n        model.addAttribute(ATTR_USER, user.replace(\"<\", \"\").replace(\">\", \"\"));\n    }\n    HttpSession session = request.getSession();\n    if (session != null) {\n        session.setAttribute(SESSION_ATTR_TELETAN, \"TeleTAN\");\n    }\n    return TEMPLATE_START;\n}",
        "rpfc": "@RequestMapping(value = ROUTE_START, method = { RequestMethod.GET, RequestMethod.POST })\npublic String start(HttpServletRequest request, Model model) {\n    KeycloakAuthenticationToken principal = (KeycloakAuthenticationToken) request.getUserPrincipal();\n    String user = ((KeycloakPrincipal) principal.getPrincipal()).getName();\n    if (model != null) {\n        model.addAttribute(ATTR_USER, user.replace(\"<\", \"\").replace(\">\", \"\"));\n        model.addAttribute(ATTR_PW_RESET_URL, pwResetUrl);\n    }\n    HttpSession session = request.getSession();\n    if (session != null) {\n        session.setAttribute(SESSION_ATTR_TELETAN, \"TeleTAN\");\n    }\n    return TEMPLATE_START;\n}",
        "tuc": "@Test\n@WithMockKeycloakAuth(name = \"tester\", value = \"Role_Test\")\npublic void testStart() throws Exception {\n    log.info(\"process testStart() RequestMethod.GET\");\n    mockMvc.perform(get(\"/cwa/start\")).andExpect(status().isOk()).andExpect(view().name(\"start\")).andExpect(model().attribute(\"userName\", equalTo(\"tester\"))).andExpect(request().sessionAttribute(TELETAN_NAME, equalTo(TELETAN_VALUE)));\n    String TOKEN_ATTR_NAME = \"org.springframework.security.web.csrf.HttpSessionCsrfTokenRepository.CSRF_TOKEN\";\n    HttpSessionCsrfTokenRepository httpSessionCsrfTokenRepository = new HttpSessionCsrfTokenRepository();\n    CsrfToken csrfToken = httpSessionCsrfTokenRepository.generateToken(new MockHttpServletRequest());\n    log.info(\"process testStart() RequestMethod.POST\");\n    mockMvc.perform(post(\"/cwa/start\").sessionAttr(TOKEN_ATTR_NAME, csrfToken).param(csrfToken.getParameterName(), csrfToken.getToken()).sessionAttr(TELETAN_NAME, TELETAN_VALUE).param(TELETAN_NAME, TELETAN_VALUE)).andExpect(status().isOk()).andExpect(view().name(\"start\")).andExpect(model().attribute(\"userName\", equalTo(\"tester\"))).andExpect(request().sessionAttribute(TELETAN_NAME, equalTo(TELETAN_VALUE)));\n}",
        "label": 0
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "ab2438f3c08711718e418732edcfca071a24023f",
        "commit_message": "\u4f18\u5316shellUtil\n",
        "p_path": "QiunetUtils/src/main/java/org/qiunet/utils/shell/ShellUtil.java",
        "t_path": "QiunetUtils/src/test/java/org/qiunet/utils/shell/TestShellUtil.java",
        "p_name": "execShell",
        "t_name": "testExecShell",
        "lpfc": "/**\n * \u6267\u884c\u7cfb\u7edf\u811a\u672c \u4e3b\u8981\u662flinux \u5e26\u53c2\u6570\n * @param shell \u6570\u7ec4\u7b2c\u4e00\u4f4d\u4e3a\u547d\u4ee4 \u540e\u9762\u662f\u53c2\u6570\n * @return\n */\npublic static String execShell(String... shell) {\n    if (!\"/\".equals(File.separator))\n        return \"\";\n    shell[0] = shell[0].replace(\"~\", System.getProperty(\"user.home\"));\n    StringJoiner sb = new StringJoiner(\"\\n\");\n    Process process = null;\n    try {\n        process = Runtime.getRuntime().exec(shell);\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n    Preconditions.checkArgument(process != null, \"process is null\");\n    try (InputStreamReader ir = new InputStreamReader(process.getInputStream());\n        LineNumberReader input = new LineNumberReader(ir);\n        InputStreamReader irError = new InputStreamReader(process.getErrorStream());\n        LineNumberReader inputError = new LineNumberReader(irError)) {\n        String line;\n        while ((line = input.readLine()) != null) {\n            sb.add(line);\n        }\n        StringJoiner sbError = new StringJoiner(\"\\n\");\n        while ((line = inputError.readLine()) != null) {\n            sbError.add(line);\n        }\n        if (sbError.length() > 0) {\n            sb.add(\"\u811a\u672c\u9519\u8bef\u8f93\u51fa:\");\n            sb.add(sbError.toString());\n        }\n    } catch (Exception e) {\n        e.printStackTrace();\n    }\n    return sb.toString();\n}",
        "rpfc": "/**\n * \u6267\u884c\u7cfb\u7edf\u811a\u672c \u4e3b\u8981\u662flinux \u5e26\u53c2\u6570\n * @param shell \u6570\u7ec4\u7b2c\u4e00\u4f4d\u4e3a\u547d\u4ee4 \u540e\u9762\u662f\u53c2\u6570\n * @return\n */\npublic static String execShell(String... shell) {\n    if (!\"/\".equals(File.separator))\n        return \"\";\n    shell[0] = shell[0].replace(\"~\", System.getProperty(\"user.home\"));\n    StringJoiner sb = new StringJoiner(\"\\n\");\n    Process process = null;\n    try {\n        process = Runtime.getRuntime().exec(shell);\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n    Preconditions.checkNotNull(process, \"process is null\");\n    try (InputStreamReader ir = new InputStreamReader(process.getInputStream());\n        LineNumberReader input = new LineNumberReader(ir);\n        InputStreamReader irError = new InputStreamReader(process.getErrorStream());\n        LineNumberReader inputError = new LineNumberReader(irError)) {\n        String line;\n        while ((line = input.readLine()) != null) {\n            sb.add(line);\n        }\n        StringJoiner sbError = new StringJoiner(\"\\n\");\n        while ((line = inputError.readLine()) != null) {\n            sbError.add(line);\n        }\n        if (sbError.length() > 0) {\n            sb.add(\"\u811a\u672c\u9519\u8bef\u8f93\u51fa:\");\n            sb.add(sbError.toString());\n        }\n    } catch (Exception e) {\n        e.printStackTrace();\n    }\n    return sb.toString();\n}",
        "tuc": "@Test\npublic void testExecShell() {\n    String[] cmd = { \"ls\" };\n    String ret = ShellUtil.execShell(cmd);\n    logger.info(ret);\n    Assert.assertNotNull(ret);\n}",
        "label": 0
    },
    {
        "repo_name": "intuit___QuickBooks-V3-Java-SDK",
        "commit": "b271f2b0aae2d0e5b84a3cb49b130d52fd05bab7",
        "commit_message": "Using UTF-8 for data compression\n",
        "p_path": "ipp-v3-java-devkit/src/main/java/com/intuit/ipp/compression/GZIPCompressor.java",
        "t_path": "ipp-v3-java-devkit/src/test/java/com/intuit/ipp/compression/GZIPCompressorTest.java",
        "p_name": "compress",
        "t_name": "testCompress",
        "lpfc": "/**\n * {@inheritDoc}\n */\npublic byte[] compress(final String data, byte[] uploadFile) throws CompressionException {\n    if (!StringUtils.hasText(data)) {\n        return null;\n    }\n    ByteArrayOutputStream baos = null;\n    OutputStream gzout = null;\n    byte[] compressedData = null;\n    try {\n        baos = new ByteArrayOutputStream();\n        gzout = new GZIPOutputStream(baos);\n        gzout.write(data.getBytes());\n        if (uploadFile != null) {\n            gzout.write(uploadFile);\n        }\n        gzout.close();\n        compressedData = baos.toByteArray();\n        return compressedData;\n    } catch (IOException ioe) {\n        LOG.error(\"IOException while compress the data using GZIP compression.\", ioe);\n        throw new CompressionException(ioe);\n    } finally {\n        if (baos != null) {\n            try {\n                baos.close();\n            } catch (IOException e) {\n                LOG.error(\"Unable to close ByteArrayOutputStream.\");\n            }\n        }\n    }\n}",
        "rpfc": "/**\n * {@inheritDoc}\n */\npublic byte[] compress(final String data, byte[] uploadFile) throws CompressionException {\n    if (!StringUtils.hasText(data)) {\n        return null;\n    }\n    ByteArrayOutputStream baos = null;\n    OutputStream gzout = null;\n    byte[] compressedData = null;\n    try {\n        baos = new ByteArrayOutputStream();\n        gzout = new GZIPOutputStream(baos);\n        gzout.write(data.getBytes(StandardCharsets.UTF_8));\n        if (uploadFile != null) {\n            gzout.write(uploadFile);\n        }\n        gzout.close();\n        compressedData = baos.toByteArray();\n        return compressedData;\n    } catch (IOException ioe) {\n        LOG.error(\"IOException while compress the data using GZIP compression.\", ioe);\n        throw new CompressionException(ioe);\n    } finally {\n        if (baos != null) {\n            try {\n                baos.close();\n            } catch (IOException e) {\n                LOG.error(\"Unable to close ByteArrayOutputStream.\");\n            }\n        }\n    }\n}",
        "tuc": "@Test\npublic void testCompress() {\n    String data = \"Hello World!\";\n    try {\n        GZIPCompressor compressor = new GZIPCompressor();\n        byte[] compressed = compressor.compress(data, null);\n        Assert.assertNotEquals(data, compressed, \"GZIPCompressor : given data did not compress.\");\n    } catch (CompressionException e) {\n        e.printStackTrace();\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "byzer-org___byzer-notebook",
        "commit": "dbf48a0b93c97a95901993d1f0ef2a52a0237c3f",
        "commit_message": "support parsing python log in `JobService.getJobLog`\n",
        "p_path": "src/main/java/io/kyligence/notebook/console/service/JobService.java",
        "t_path": "src/test/java/io/kyligence/notebook/console/service/JobServiceTest.java",
        "p_name": "getJobLog",
        "t_name": "testGetJobLog",
        "lpfc": "@Transactional\npublic JobLog getJobLog(String user, String jobId, Long offset) {\n    Integer jobStatus = getJobStatus(jobId);\n    if (!isRunning(jobStatus)) {\n        return null;\n    }\n    String groupId = getGroupOrJobId(jobId);\n    String response = null;\n    try {\n        response = engineService.runScript(new EngineService.RunScriptParams().withOwner(user).withSql(String.format(\"load _mlsql_.`log/%d` where filePath=\\\"engine_log\\\" as output;\", offset)).withAsync(\"false\").with(\"sessionPerRequest\", \"true\"));\n    } catch (Exception e) {\n        log.error(ExceptionUtils.getRootCause(e));\n    }\n    if (StringUtils.isEmpty(response) || response.equals(\"[]\")) {\n        return null;\n    }\n    List<JobLog> resultsMap = JacksonUtils.readJsonArray(response, JobLog.class);\n    JobLog jobLog = Objects.requireNonNull(resultsMap).get(0);\n    if (jobLog.getValue() != null) {\n        jobLog.setValue(jobLog.getValue().stream().filter(s -> s.contains(String.format(\"[owner] [%s] [groupId] [%s]\", user, groupId)) && !s.contains(\"DefaultConsoleClient\")).collect(Collectors.toList()));\n    } else {\n        jobLog.setValue(Lists.newArrayList());\n    }\n    return jobLog;\n}",
        "rpfc": "public JobLog getJobLog(String user, String jobId, Long offset) {\n    String groupId = getGroupOrJobId(jobId);\n    String response = null;\n    try {\n        response = engineService.runScript(new EngineService.RunScriptParams().withOwner(user).withSql(String.format(\"load _mlsql_.`log/%d` where filePath=\\\"engine_log\\\" as output;\", offset)).withAsync(\"false\").with(\"sessionPerRequest\", \"true\"));\n    } catch (Exception e) {\n        log.error(ExceptionUtils.getRootCause(e));\n    }\n    if (StringUtils.isEmpty(response) || response.equals(\"[]\")) {\n        return null;\n    }\n    List<JobLog> resultsMap = JacksonUtils.readJsonArray(response, JobLog.class);\n    JobLog jobLog = Objects.requireNonNull(resultsMap).get(0);\n    if (jobLog.getValue() != null) {\n        jobLog.setValue(jobLog.getValue().stream().filter(s -> s.contains(String.format(\"[owner] [%s] [groupId] [%s]\", user, groupId)) || s.contains(String.format(\"DriverLogServer: [owner] [%s]\", user))).collect(Collectors.toList()));\n    } else {\n        jobLog.setValue(Lists.newArrayList());\n    }\n    return jobLog;\n}",
        "tuc": "@Test\npublic void testGetJobLog() {\n    when(mockEngineService.runScript(any())).thenReturn(getResponseContent(MOCK_JOB_LOG));\n    JobLog jobLog = mockJobService.getJobLog(\"admin\", mockJobId, -1L);\n    Assert.assertNotNull(jobLog);\n}",
        "label": 0
    },
    {
        "repo_name": "killbilling___recurly-java-library",
        "commit": "f4b0d28dedefbd89a55b934bdeb9fe13ffe6b15e",
        "commit_message": "Added url encoding\n",
        "p_path": "src/main/java/com/ning/billing/recurly/RecurlyClient.java",
        "t_path": "src/test/java/com/ning/billing/recurly/TestRecurlyClient.java",
        "p_name": "reactivateItem",
        "t_name": "testReactivateItem",
        "lpfc": "public Item reactivateItem(final String itemCode) {\n    return doPUT(Item.ITEMS_RESOURCE + \"/\" + itemCode + \"/reactivate\", null, Item.class);\n}",
        "rpfc": "public Item reactivateItem(final String itemCode) {\n    return doPUT(Item.ITEMS_RESOURCE + \"/\" + urlEncode(itemCode) + \"/reactivate\", null, Item.class);\n}",
        "tuc": "@Test(groups = \"integration\")\npublic void testReactivateItem() throws Exception {\n    final Item itemData = TestUtils.createRandomItem();\n    try {\n        final Item item = recurlyClient.createItem(itemData);\n        Assert.assertNotNull(item);\n        Assert.assertTrue(recurlyClient.getItems().size() > 0);\n        recurlyClient.deleteItem(item.getItemCode());\n        final Item deletedItem = recurlyClient.getItem(item.getItemCode());\n        Assert.assertEquals(deletedItem.getState(), \"inactive\");\n        recurlyClient.reactivateItem(item.getItemCode());\n        final Item reactivatedItem = recurlyClient.getItem(item.getItemCode());\n        Assert.assertEquals(reactivatedItem.getState(), \"active\");\n    } catch (Exception e) {\n        System.out.println(e.getMessage());\n    } finally {\n        recurlyClient.deleteItem(itemData.getItemCode());\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "CityOfNewYork___geoclient",
        "commit": "cadf0ea30630e2b98321f4a9dd7c393fdb070208",
        "commit_message": "Update/remove old TODO comments.\n",
        "p_path": "geoclient-core/src/main/java/gov/nyc/doitt/gis/geoclient/api/Boroughs.java",
        "t_path": "geoclient-core/src/test/java/gov/nyc/doitt/gis/geoclient/api/BoroughsTest.java",
        "p_name": "fromCode",
        "t_name": "testFromCode",
        "lpfc": "public static final Borough fromCode(String code) {\n    // TODO Decide if this method should trim whitespace (currently, does not)\n    return findFirst((Borough b) -> b.getCode().equalsIgnoreCase(code));\n}",
        "rpfc": "public static final Borough fromCode(String code) {\n    // Note: this equality test does not consider whitespace.\n    return findFirst((Borough b) -> b.getCode().equalsIgnoreCase(code));\n}",
        "tuc": "@Test\nvoid testFromCode() {\n    assertSame(Boroughs.MANHATTAN, Boroughs.fromCode(\"1\"));\n    assertSame(Boroughs.BRONX, Boroughs.fromCode(\"2\"));\n    assertSame(Boroughs.BROOKLYN, Boroughs.fromCode(\"3\"));\n    assertSame(Boroughs.QUEENS, Boroughs.fromCode(\"4\"));\n    assertSame(Boroughs.STATEN_ISLAND, Boroughs.fromCode(\"5\"));\n    assertNull(Boroughs.fromCode(\" 1\"));\n    assertNull(Boroughs.fromCode(\"8\"));\n    assertNull(Boroughs.fromCode(\"woof\"));\n}",
        "label": 0
    },
    {
        "repo_name": "byzer-org___byzer-notebook",
        "commit": "6db95eb0ff2a385bf885672e5183a81ef446edfa",
        "commit_message": "update\n",
        "p_path": "src/main/java/io/kyligence/notebook/console/service/JobService.java",
        "t_path": "src/test/java/io/kyligence/notebook/console/service/JobServiceTest.java",
        "p_name": "getJobLog",
        "t_name": "testGetJobLog",
        "lpfc": "public JobLog getJobLog(String user, String jobId, Long offset) {\n    String groupId = getGroupOrJobId(jobId);\n    String response = null;\n    try {\n        response = engineService.runScript(new EngineService.RunScriptParams().withOwner(user).withSql(String.format(\"load _mlsql_.`log/%d` where filePath=\\\"engine_log\\\" as output;\", offset)).withAsync(\"false\").with(\"sessionPerRequest\", \"true\"));\n    } catch (Exception e) {\n        log.error(ExceptionUtils.getRootCause(e));\n    }\n    if (StringUtils.isEmpty(response) || response.equals(\"[]\")) {\n        return null;\n    }\n    List<JobLog> resultsMap = JacksonUtils.readJsonArray(response, JobLog.class);\n    JobLog jobLog = Objects.requireNonNull(resultsMap).get(0);\n    if (jobLog.getValue() != null) {\n        jobLog.setValue(jobLog.getValue().stream().filter(s -> s.contains(String.format(\"[owner] [%s] [groupId]\", user))).map(s -> {\n            String[] arr = s.split(\"__MMMMMM__\", 2);\n            if (arr.length == 2) {\n                return arr[1];\n            } else {\n                return s;\n            }\n        }).collect(Collectors.toList()));\n    } else {\n        jobLog.setValue(Lists.newArrayList());\n    }\n    return jobLog;\n}",
        "rpfc": "public JobLog getJobLog(String user, String jobId, Long offset) {\n    String groupId = getGroupOrJobId(jobId);\n    String response = null;\n    try {\n        response = engineService.runScript(new EngineService.RunScriptParams().withOwner(user).withSql(String.format(\"load _mlsql_.`log/%d` where filePath=\\\"engine_log\\\" as output;\", offset)).withAsync(\"false\").with(\"sessionPerRequest\", \"true\"));\n    } catch (Exception e) {\n        log.error(ExceptionUtils.getRootCause(e));\n    }\n    if (StringUtils.isEmpty(response) || response.equals(\"[]\")) {\n        return null;\n    }\n    List<JobLog> resultsMap = JacksonUtils.readJsonArray(response, JobLog.class);\n    JobLog jobLog = Objects.requireNonNull(resultsMap).get(0);\n    if (jobLog.getValue() != null) {\n        jobLog.setValue(jobLog.getValue().stream().filter(s -> s.contains(String.format(\"[owner] [%s] [groupId]\", user))).filter(s -> !s.contains(\"run command as ShowCommand.`jobs/v2\")).map(s -> {\n            String[] arr = s.split(\"__MMMMMM__\", 2);\n            if (arr.length == 2) {\n                return arr[1];\n            } else {\n                return s;\n            }\n        }).collect(Collectors.toList()));\n    } else {\n        jobLog.setValue(Lists.newArrayList());\n    }\n    return jobLog;\n}",
        "tuc": "@Test\npublic void testGetJobLog() {\n    when(mockEngineService.runScript(any())).thenReturn(getResponseContent(MOCK_JOB_LOG));\n    JobLog jobLog = mockJobService.getJobLog(\"admin\", mockJobId, -1L);\n    Assert.assertNotNull(jobLog);\n}",
        "label": 0
    },
    {
        "repo_name": "apache___oodt",
        "commit": "c212b1a53d6ac19d657951d972005b5287eba271",
        "commit_message": "fix about 1000 code quality issues\n",
        "p_path": "filemgr/src/main/java/org/apache/oodt/cas/filemgr/catalog/DataSourceCatalog.java",
        "t_path": "filemgr/src/test/java/org/apache/oodt/cas/filemgr/catalog/TestDataSourceCatalog.java",
        "p_name": "modifyProduct",
        "t_name": "testModifyProduct",
        "lpfc": "public synchronized void modifyProduct(Product product) throws CatalogException {\n    Connection conn = null;\n    Statement statement = null;\n    try {\n        conn = dataSource.getConnection();\n        conn.setAutoCommit(false);\n        statement = conn.createStatement();\n        String modifyProductSql = \"UPDATE products SET product_name='\" + product.getProductName() + \"', product_structure='\" + product.getProductStructure() + \"', product_transfer_status='\" + product.getTransferStatus() + \"' \" + \"WHERE product_id = \" + quoteIt(product.getProductId());\n        LOG.log(Level.FINE, \"modifyProduct: Executing: \" + modifyProductSql);\n        statement.execute(modifyProductSql);\n        conn.commit();\n        updateReferences(product);\n    } catch (Exception e) {\n        e.printStackTrace();\n        LOG.log(Level.WARNING, \"Exception modifying product. Message: \" + e.getMessage());\n        try {\n            conn.rollback();\n        } catch (SQLException e2) {\n            LOG.log(Level.SEVERE, \"Unable to rollback modifyProduct transaction. Message: \" + e2.getMessage());\n        }\n        throw new CatalogException(e.getMessage());\n    } finally {\n        if (statement != null) {\n            try {\n                statement.close();\n            } catch (SQLException ignore) {\n            }\n            statement = null;\n        }\n        if (conn != null) {\n            try {\n                conn.close();\n            } catch (SQLException ignore) {\n            }\n            conn = null;\n        }\n    }\n}",
        "rpfc": "public synchronized void modifyProduct(Product product) throws CatalogException {\n    Connection conn = null;\n    Statement statement = null;\n    try {\n        conn = dataSource.getConnection();\n        conn.setAutoCommit(false);\n        statement = conn.createStatement();\n        String modifyProductSql = \"UPDATE products SET product_name='\" + product.getProductName() + \"', product_structure='\" + product.getProductStructure() + \"', product_transfer_status='\" + product.getTransferStatus() + \"' \" + \"WHERE product_id = \" + quoteIt(product.getProductId());\n        LOG.log(Level.FINE, \"modifyProduct: Executing: \" + modifyProductSql);\n        statement.execute(modifyProductSql);\n        conn.commit();\n        updateReferences(product);\n    } catch (Exception e) {\n        e.printStackTrace();\n        LOG.log(Level.WARNING, \"Exception modifying product. Message: \" + e.getMessage());\n        try {\n            assert conn != null;\n            conn.rollback();\n        } catch (SQLException e2) {\n            LOG.log(Level.SEVERE, \"Unable to rollback modifyProduct transaction. Message: \" + e2.getMessage());\n        }\n        throw new CatalogException(e.getMessage());\n    } finally {\n        if (statement != null) {\n            try {\n                statement.close();\n            } catch (SQLException ignore) {\n            }\n        }\n        if (conn != null) {\n            try {\n                conn.close();\n            } catch (SQLException ignore) {\n            }\n        }\n    }\n}",
        "tuc": "public void testModifyProduct() {\n    Product testProduct = getTestProduct();\n    try {\n        myCat.addProduct(testProduct);\n    } catch (Exception e) {\n        fail(e.getMessage());\n    }\n    assertNotNull(testProduct);\n    assertEquals(\"test\", testProduct.getProductName());\n    testProduct.setProductName(\"f002\");\n    try {\n        myCat.modifyProduct(testProduct);\n    } catch (Exception e) {\n        fail(e.getMessage());\n    }\n    assertNotNull(testProduct);\n    Product retProduct;\n    try {\n        retProduct = myCat.getProductByName(\"f002\");\n        assertNotNull(retProduct);\n        assertEquals(\"f002\", retProduct.getProductName());\n    } catch (Exception e) {\n        fail(e.getMessage());\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "apache___oodt",
        "commit": "e43c6c0076e4c75598389db881e1e0a1716fc91d",
        "commit_message": "Fix numHits error in Lucene Catalog\n",
        "p_path": "filemgr/src/main/java/org/apache/oodt/cas/filemgr/catalog/LuceneCatalog.java",
        "t_path": "filemgr/src/test/java/org/apache/oodt/cas/filemgr/catalog/TestLuceneCatalog.java",
        "p_name": "getTopNProducts",
        "t_name": "testGetTopNProducts",
        "lpfc": "public List<Product> getTopNProducts(int n, ProductType type) throws CatalogException {\n    int numPages = 1;\n    if (n > this.pageSize) {\n        numPages = n / this.pageSize + (n % this.pageSize == 0 ? 0 : 1);\n    }\n    List<Product> products = new Vector<Product>(n);\n    Query query = new Query();\n    for (int pageNum = 1; pageNum < numPages + 1; pageNum++) {\n        List<Product> pageProducts = paginateQuery(query, type, pageNum, null);\n        products.addAll(pageProducts);\n    }\n    if (n <= products.size()) {\n        return products.subList(0, n);\n    }\n    return products;\n}",
        "rpfc": "public List<Product> getTopNProducts(int n, ProductType type) throws CatalogException {\n    int numPages = 1;\n    if (n > this.pageSize) {\n        numPages = n / this.pageSize + (n % this.pageSize == 0 ? 0 : 1);\n    }\n    List<Product> products = new Vector<Product>(n);\n    Query query = new Query();\n    for (int pageNum = 1; pageNum < numPages + 1; pageNum++) {\n        List<Product> pageProducts = paginateQuery(query, type, pageNum, null);\n        if (pageProducts != null) {\n            products.addAll(pageProducts);\n        }\n    }\n    if (n <= products.size()) {\n        return products.subList(0, n);\n    }\n    return products;\n}",
        "tuc": "public void testGetTopNProducts() {\n    Product testProd = getTestProduct();\n    Metadata met = getTestMetadata(\"test\");\n    for (int i = 0; i < catPageSize; i++) {\n        try {\n            myCat.addProduct(testProd);\n            myCat.addMetadata(met, testProd);\n        } catch (Exception e) {\n            fail(e.getMessage());\n        }\n    }\n    testProd.setProductName(\"ShouldBeFirstForPage.txt\");\n    met.replaceMetadata(\"CAS.ProdutName\", \"ShouldBeFirstForPage.txt\");\n    try {\n        myCat.addProduct(testProd);\n        myCat.addMetadata(met, testProd);\n    } catch (Exception e) {\n        fail(e.getMessage());\n    }\n    try {\n        assertNotNull(myCat.getProducts());\n        assertEquals(21, myCat.getProducts().size());\n    } catch (Exception e) {\n        fail(e.getMessage());\n    }\n    try {\n        assertNotNull(myCat.getTopNProducts(5));\n        assertEquals(5, myCat.getTopNProducts(5).size());\n        Product retProd = myCat.getTopNProducts(5).get(0);\n        assertEquals(\"test\", retProd.getProductName());\n    } catch (CatalogException e) {\n        LOG.log(Level.SEVERE, e.getMessage());\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "47f6ac1da11c6e8e4f2f6245e16fe2108a3f9376",
        "commit_message": "Merge remote-tracking branch 'origin/master'\n",
        "p_path": "QiunetUtils/src/main/java/org/qiunet/utils/file/FileUtil.java",
        "t_path": "QiunetUtils/src/test/java/org/qiunet/utils/file/TestFileUtil.java",
        "p_name": "copy",
        "t_name": "testCopy",
        "lpfc": "public static void copy(File oldFile, String newPath) {\n    if (!oldFile.exists() || !oldFile.isFile())\n        throw new IllegalArgumentException(\"file [\" + oldFile.getAbsolutePath() + \"] is not exist or is not a file!\");\n    try (FileOutputStream fs = new FileOutputStream(newPath)) {\n        fs.write(Files.readAllBytes(oldFile.toPath()));\n    } catch (Exception e) {\n        logger.error(\"Exception\", e);\n    }\n}",
        "rpfc": "public static void copy(File oldFile, String newPath) {\n    if (!oldFile.exists() || !oldFile.isFile())\n        throw new IllegalArgumentException(\"file [\" + oldFile.getAbsolutePath() + \"] is not exist or is not a file!\");\n    try (FileOutputStream fs = new FileOutputStream(newPath)) {\n        fs.write(Files.readAllBytes(oldFile.toPath()));\n    } catch (Exception e) {\n        throw new CustomException(e, \"File Copy exception!\");\n    }\n}",
        "tuc": "@Test\npublic void testCopy() {\n    String baseDir = System.getProperty(\"user.dir\");\n    if (!baseDir.endsWith(File.separator))\n        baseDir += File.separator;\n    String targetFilePath = baseDir + \"clazzes/org/test/\";\n    String targetFile = targetFilePath + \"ObjectB.class\";\n    String currPath = getClass().getResource(\".\").getPath();\n    String currPathFile = currPath + \"ObjectB.class\";\n    FileUtil.copy(targetFile, currPathFile);\n    File file = new File(currPathFile);\n    Assert.assertTrue(file.exists());\n    file.delete();\n    //\n    FileUtil.move(targetFile, currPath);\n    file = new File(currPathFile);\n    Assert.assertTrue(file.exists());\n    FileUtil.move(file, targetFilePath);\n    file = new File(targetFile);\n    Assert.assertTrue(file.exists());\n    FileUtil.copy(targetFile, currPathFile);\n    file = new File(currPathFile);\n    Assert.assertTrue(file.exists());\n    file.delete();\n}",
        "label": 0
    },
    {
        "repo_name": "apache___empire-db",
        "commit": "9ef8f13ef0542e76ce8eb523e023efcd061b2d1b",
        "commit_message": "EMPIREDB-190\nimprovement\n\ngit-svn-id: https://svn.apache.org/repos/asf/empire-db/trunk@1516891 13f79535-47bb-0310-9956-ffa450edef68\n",
        "p_path": "empire-db/src/main/java/org/apache/empire/commons/ObjectUtils.java",
        "t_path": "empire-db/src/test/java/org/apache/empire/commons/ObjectUtilsTest.java",
        "p_name": "compareEqual",
        "t_name": "testCompareEqual",
        "lpfc": "@SuppressWarnings(\"unchecked\")\npublic static boolean compareEqual(Object o1, Object o2) {\n    if (isEmpty(o1))\n        return isEmpty(o2);\n    if (isEmpty(o2))\n        return isEmpty(o1);\n    if (o1.getClass().equals(o2.getClass())) {\n        if (o1 instanceof Comparable)\n            return (((Comparable<Object>) o1).compareTo(o2) == 0);\n        else\n            return o1.equals(o2);\n    }\n    if (o1.equals(o2) || o2.equals(o1))\n        return true;\n    if (o1 instanceof Number && o2 instanceof Number) {\n        double d1 = ((Number) o1).doubleValue();\n        double d2 = ((Number) o2).doubleValue();\n        return (d1 == d2);\n    }\n    return o1.toString().equals(o2.toString());\n}",
        "rpfc": "@SuppressWarnings(\"unchecked\")\npublic static boolean compareEqual(Object o1, Object o2) {\n    if (o1 == o2)\n        return true;\n    if (isEmpty(o1))\n        return isEmpty(o2);\n    if (isEmpty(o2))\n        return isEmpty(o1);\n    if (o1.getClass().equals(o2.getClass())) {\n        if (o1 instanceof Comparable)\n            return (((Comparable<Object>) o1).compareTo(o2) == 0);\n        else\n            return o1.equals(o2);\n    }\n    if (o1.equals(o2) || o2.equals(o1))\n        return true;\n    if (o1 instanceof Number && o2 instanceof Number) {\n        double d1 = ((Number) o1).doubleValue();\n        double d2 = ((Number) o2).doubleValue();\n        return (d1 == d2);\n    }\n    return o1.toString().equals(o2.toString());\n}",
        "tuc": "@Test\npublic void testCompareEqual() {\n    assertTrue(ObjectUtils.compareEqual(null, null));\n    Object object = new Object();\n    assertTrue(ObjectUtils.compareEqual(object, object));\n    assertTrue(ObjectUtils.compareEqual(\"\", \"\"));\n    assertTrue(ObjectUtils.compareEqual(\"abc\", String.valueOf(\"abc\")));\n    assertTrue(ObjectUtils.compareEqual(\"abc\", new String(\"abc\")));\n    assertTrue(ObjectUtils.compareEqual(\"\", null));\n    assertTrue(ObjectUtils.compareEqual(null, \"\"));\n    assertFalse(ObjectUtils.compareEqual(\"\", \" \"));\n    assertTrue(ObjectUtils.compareEqual(Long.valueOf(100), Integer.valueOf(100)));\n    assertTrue(ObjectUtils.compareEqual(Float.valueOf(100), Integer.valueOf(100)));\n    assertTrue(ObjectUtils.compareEqual(Float.valueOf(100.0123f), Double.valueOf(100.0123f)));\n    assertFalse(ObjectUtils.compareEqual(Float.valueOf(100.0123f), Long.valueOf(100)));\n    Date date = new Date();\n    Date dateEq = new Date(date.getTime());\n    Date dateDiff = new Date(123);\n    assertTrue(ObjectUtils.compareEqual(date, dateEq));\n    assertFalse(ObjectUtils.compareEqual(date, dateDiff));\n    Object o1 = new Object() {\n        @Override\n        public String toString() {\n            return \"JUnit\";\n        }\n    };\n    Object oEq = new Object() {\n        @Override\n        public String toString() {\n            return \"JUnit\";\n        }\n    };\n    Object oDiff = new Object() {\n        @Override\n        public String toString() {\n            return \"JUnitDiff\";\n        }\n    };\n    assertTrue(ObjectUtils.compareEqual(o1, oEq));\n    assertFalse(ObjectUtils.compareEqual(o1, oDiff));\n}",
        "label": 0
    },
    {
        "repo_name": "killbilling___recurly-java-library",
        "commit": "e54ca075ccf0befbf5b5e6be75dc1bd59229647b",
        "commit_message": "Merge pull request #213 from killbilling/fix_extra_slash_in_account_balance_request\n\nResolves issue #211",
        "p_path": "src/main/java/com/ning/billing/recurly/RecurlyClient.java",
        "t_path": "src/test/java/com/ning/billing/recurly/TestRecurlyClient.java",
        "p_name": "getAccountBalance",
        "t_name": "testGetAccountBalance",
        "lpfc": "public AccountBalance getAccountBalance(final String accountCode) {\n    return doGET(Account.ACCOUNT_RESOURCE + \"/\" + accountCode + \"/\" + AccountBalance.ACCOUNT_BALANCE_RESOURCE, AccountBalance.class);\n}",
        "rpfc": "public AccountBalance getAccountBalance(final String accountCode) {\n    return doGET(Account.ACCOUNT_RESOURCE + \"/\" + accountCode + AccountBalance.ACCOUNT_BALANCE_RESOURCE, AccountBalance.class);\n}",
        "tuc": "@Test(groups = \"integration\")\npublic void testGetAccountBalance() throws Exception {\n    final Account accountData = TestUtils.createRandomAccount();\n    final BillingInfo billingInfoData = TestUtils.createRandomBillingInfo();\n    try {\n        final Account account = recurlyClient.createAccount(accountData);\n        billingInfoData.setAccount(account);\n        final BillingInfo billingInfo = recurlyClient.createOrUpdateBillingInfo(billingInfoData);\n        Assert.assertNotNull(billingInfo);\n        final BillingInfo retrievedBillingInfo = recurlyClient.getBillingInfo(account.getAccountCode());\n        Assert.assertNotNull(retrievedBillingInfo);\n        final Adjustment adjustment = new Adjustment();\n        adjustment.setUnitAmountInCents(150);\n        adjustment.setCurrency(CURRENCY);\n        recurlyClient.createAccountAdjustment(account.getAccountCode(), adjustment);\n        final AccountBalance balance = recurlyClient.getAccountBalance(account.getAccountCode());\n        Assert.assertEquals(balance.getBalanceInCents().getUnitAmountUSD(), new Integer(150));\n        Assert.assertEquals(balance.getPastDue(), Boolean.FALSE);\n    } finally {\n        recurlyClient.clearBillingInfo(accountData.getAccountCode());\n        recurlyClient.closeAccount(accountData.getAccountCode());\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "apache___oodt",
        "commit": "c212b1a53d6ac19d657951d972005b5287eba271",
        "commit_message": "fix about 1000 code quality issues\n",
        "p_path": "filemgr/src/main/java/org/apache/oodt/cas/filemgr/catalog/DataSourceCatalog.java",
        "t_path": "filemgr/src/test/java/org/apache/oodt/cas/filemgr/catalog/TestDataSourceCatalog.java",
        "p_name": "addMetadata",
        "t_name": "testAddMetadata",
        "lpfc": "/*\n     * (non-Javadoc)\n     * \n     * @see org.apache.oodt.cas.filemgr.catalog.Catalog#addMetadata(org.apache.oodt.cas.metadata.Metadata,\n     *      org.apache.oodt.cas.filemgr.structs.Product)\n     */\npublic synchronized void addMetadata(Metadata m, Product product) throws CatalogException {\n    List<Element> metadataTypes = null;\n    try {\n        metadataTypes = validationLayer.getElements(product.getProductType());\n    } catch (ValidationLayerException e) {\n        e.printStackTrace();\n        throw new CatalogException(\"ValidationLayerException when trying to obtain element list for product type: \" + product.getProductType().getName() + \": Message: \" + e.getMessage());\n    }\n    for (Iterator<Element> i = metadataTypes.iterator(); i.hasNext(); ) {\n        Element element = i.next();\n        List<String> values = m.getAllMetadata(element.getElementName());\n        if (values == null) {\n            LOG.log(Level.WARNING, \"No Metadata specified for product [\" + product.getProductName() + \"] for required field [\" + element.getElementName() + \"]: Attempting to continue processing metadata\");\n            continue;\n        }\n        for (Iterator<String> j = values.iterator(); j.hasNext(); ) {\n            String value = j.next();\n            try {\n                addMetadataValue(element, product, value);\n            } catch (Exception e) {\n                e.printStackTrace();\n                LOG.log(Level.WARNING, \"Exception ingesting metadata. Error inserting field: [\" + element.getElementId() + \"=>\" + value + \"]: for product: [\" + product.getProductName() + \"]: Message: \" + e.getMessage() + \": Attempting to continue processing metadata\");\n            }\n        }\n    }\n}",
        "rpfc": "/*\n     * (non-Javadoc)\n     * \n     * @see org.apache.oodt.cas.filemgr.catalog.Catalog#addMetadata(org.apache.oodt.cas.metadata.Metadata,\n     *      org.apache.oodt.cas.filemgr.structs.Product)\n     */\npublic synchronized void addMetadata(Metadata m, Product product) throws CatalogException {\n    List<Element> metadataTypes = null;\n    try {\n        metadataTypes = validationLayer.getElements(product.getProductType());\n    } catch (ValidationLayerException e) {\n        e.printStackTrace();\n        throw new CatalogException(\"ValidationLayerException when trying to obtain element list for product type: \" + product.getProductType().getName() + \": Message: \" + e.getMessage());\n    }\n    for (Element element : metadataTypes) {\n        List<String> values = m.getAllMetadata(element.getElementName());\n        if (values == null) {\n            LOG.log(Level.WARNING, \"No Metadata specified for product [\" + product.getProductName() + \"] for required field [\" + element.getElementName() + \"]: Attempting to continue processing metadata\");\n            continue;\n        }\n        for (String value : values) {\n            try {\n                addMetadataValue(element, product, value);\n            } catch (Exception e) {\n                e.printStackTrace();\n                LOG.log(Level.WARNING, \"Exception ingesting metadata. Error inserting field: [\" + element.getElementId() + \"=>\" + value + \"]: for product: [\" + product.getProductName() + \"]: Message: \" + e.getMessage() + \": Attempting to continue processing metadata\");\n            }\n        }\n    }\n}",
        "tuc": "public void testAddMetadata() {\n    Metadata met = new Metadata();\n    met.addMetadata(\"ProductStructure\", Product.STRUCTURE_FLAT);\n    Product testProduct = getTestProduct();\n    testProduct.setProductId(\"1\");\n    try {\n        myCat.addMetadata(met, testProduct);\n    } catch (Exception e) {\n        e.printStackTrace();\n        fail(e.getMessage());\n    }\n    try {\n        Metadata retMet = myCat.getMetadata(testProduct);\n        assertNotNull(retMet);\n        assertTrue(retMet.containsKey(CoreMetKeys.PRODUCT_STRUCTURE));\n        assertEquals(Product.STRUCTURE_FLAT, retMet.getMetadata(CoreMetKeys.PRODUCT_STRUCTURE));\n    } catch (CatalogException e) {\n        fail(e.getMessage());\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "karussell___Jetwick",
        "commit": "b4fccac95f443e3d42bc0ffd7d320eefc3be4970",
        "commit_message": "jtag: max 5 minutes to wait; first search pages 5 times\n",
        "p_path": "src/main/java/de/jetwick/data/JTag.java",
        "t_path": "src/test/java/de/jetwick/data/JTagTest.java",
        "p_name": "optimizeQueryFrequency",
        "t_name": "testOptimizeQueryFrequency",
        "lpfc": "public void optimizeQueryFrequency(int newTweets) {\n    if (newTweets == 0)\n        queryInterval *= 20;\n    else\n        queryInterval = (long) (20.0 / newTweets) * queryInterval;\n    queryInterval = Math.max(queryInterval, 5 * 1001);\n    queryInterval = Math.min(queryInterval, 20 * 60 * 1001);\n    if (\"#jetwick\".equalsIgnoreCase(term) || \"jetwick\".equalsIgnoreCase(term))\n        queryInterval = Math.min(queryInterval, 5 * 60 * 1001);\n}",
        "rpfc": "public void optimizeQueryFrequency(int newTweets) {\n    if (newTweets == 0)\n        queryInterval *= 20;\n    else\n        queryInterval = (long) (20.0 / newTweets) * queryInterval;\n    queryInterval = Math.max(queryInterval, 5 * 1001);\n    queryInterval = Math.min(queryInterval, 5 * 60 * 1001);\n}",
        "tuc": "@Test\npublic void testOptimizeQueryFrequency() {\n    JTag st = new JTag(\"java\");\n    st.optimizeQueryFrequency(10);\n    assertTrue(st.getQueryInterval() > 1000);\n    st.optimizeQueryFrequency(1);\n    assertTrue(st.getQueryInterval() > 1000);\n    st.optimizeQueryFrequency(100);\n    assertTrue(st.getQueryInterval() > 1000);\n    st.optimizeQueryFrequency(1000);\n    assertTrue(st.getQueryInterval() > 1000);\n}",
        "label": 0
    },
    {
        "repo_name": "apache___oodt",
        "commit": "c212b1a53d6ac19d657951d972005b5287eba271",
        "commit_message": "fix about 1000 code quality issues\n",
        "p_path": "filemgr/src/main/java/org/apache/oodt/cas/filemgr/catalog/DataSourceCatalog.java",
        "t_path": "filemgr/src/test/java/org/apache/oodt/cas/filemgr/catalog/TestDataSourceCatalog.java",
        "p_name": "addProductReferences",
        "t_name": "testAddProductReferences",
        "lpfc": "public synchronized void addProductReferences(Product product) throws CatalogException {\n    Connection conn = null;\n    Statement statement = null;\n    ResultSet rs = null;\n    String productRefTable = product.getProductType().getName() + \"_reference\";\n    try {\n        conn = dataSource.getConnection();\n        conn.setAutoCommit(false);\n        statement = conn.createStatement();\n        for (Iterator<Reference> i = product.getProductReferences().iterator(); i.hasNext(); ) {\n            Reference r = i.next();\n            String addRefSql = \"INSERT INTO \" + productRefTable + \" \" + \"(product_id, product_orig_reference, product_datastore_reference, product_reference_filesize, product_reference_mimetype) \" + \"VALUES (\" + quoteIt(product.getProductId()) + \", '\" + r.getOrigReference() + \"', '\" + r.getDataStoreReference() + \"', \" + r.getFileSize() + \",'\" + ((r.getMimeType() == null) ? \"\" : r.getMimeType().getName()) + \"')\";\n            LOG.log(Level.FINE, \"addProductReferences: Executing: \" + addRefSql);\n            statement.execute(addRefSql);\n        }\n        conn.commit();\n    } catch (Exception e) {\n        e.printStackTrace();\n        LOG.log(Level.WARNING, \"Exception adding product references. Message: \" + e.getMessage());\n        try {\n            conn.rollback();\n        } catch (SQLException e2) {\n            LOG.log(Level.SEVERE, \"Unable to rollback addProductReferences transaction. Message: \" + e2.getMessage());\n        }\n        throw new CatalogException(e.getMessage());\n    } finally {\n        if (rs != null) {\n            try {\n                rs.close();\n            } catch (SQLException ignore) {\n            }\n            rs = null;\n        }\n        if (statement != null) {\n            try {\n                statement.close();\n            } catch (SQLException ignore) {\n            }\n            statement = null;\n        }\n        if (conn != null) {\n            try {\n                conn.close();\n            } catch (SQLException ignore) {\n            }\n            conn = null;\n        }\n    }\n}",
        "rpfc": "public synchronized void addProductReferences(Product product) throws CatalogException {\n    Connection conn = null;\n    Statement statement = null;\n    String productRefTable = product.getProductType().getName() + \"_reference\";\n    try {\n        conn = dataSource.getConnection();\n        conn.setAutoCommit(false);\n        statement = conn.createStatement();\n        for (Iterator<Reference> i = product.getProductReferences().iterator(); i.hasNext(); ) {\n            Reference r = i.next();\n            String addRefSql = \"INSERT INTO \" + productRefTable + \" \" + \"(product_id, product_orig_reference, product_datastore_reference, product_reference_filesize, product_reference_mimetype) \" + \"VALUES (\" + quoteIt(product.getProductId()) + \", '\" + r.getOrigReference() + \"', '\" + r.getDataStoreReference() + \"', \" + r.getFileSize() + \",'\" + ((r.getMimeType() == null) ? \"\" : r.getMimeType().getName()) + \"')\";\n            LOG.log(Level.FINE, \"addProductReferences: Executing: \" + addRefSql);\n            statement.execute(addRefSql);\n        }\n        conn.commit();\n    } catch (Exception e) {\n        e.printStackTrace();\n        LOG.log(Level.WARNING, \"Exception adding product references. Message: \" + e.getMessage());\n        try {\n            if (conn != null) {\n                conn.rollback();\n            }\n        } catch (SQLException e2) {\n            LOG.log(Level.SEVERE, \"Unable to rollback addProductReferences transaction. Message: \" + e2.getMessage());\n        }\n        throw new CatalogException(e.getMessage());\n    } finally {\n        if (statement != null) {\n            try {\n                statement.close();\n            } catch (SQLException ignore) {\n            }\n        }\n        if (conn != null) {\n            try {\n                conn.close();\n            } catch (SQLException ignore) {\n            }\n        }\n    }\n}",
        "tuc": "public void testAddProductReferences() {\n    Product testProduct = getTestProduct();\n    testProduct.setProductId(\"1\");\n    Reference ref = new Reference();\n    ref.setMimeType(\"text/plain\");\n    ref.setFileSize(12345);\n    List<Reference> refs = new ArrayList<Reference>();\n    refs.add(ref);\n    testProduct.setProductReferences(refs);\n    try {\n        myCat.addProductReferences(testProduct);\n    } catch (Exception e) {\n        e.printStackTrace();\n        fail(e.getMessage());\n    }\n    try {\n        List<Reference> productReferences = myCat.getProductReferences(testProduct);\n        assertNotNull(productReferences);\n        assertFalse(productReferences.isEmpty());\n        assertEquals(productReferences.get(0).getMimeType().getName(), \"text/plain\");\n        assertEquals(productReferences.get(0).getFileSize(), 12345);\n    } catch (CatalogException e) {\n        fail(e.getMessage());\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "killbilling___recurly-java-library",
        "commit": "b8d8c1c7aa39f47ebe51beadf19f18f1c54f4115",
        "commit_message": "Merge pull request #382 from saasquatch/issue20200225\n\nAdd URL encoding to URL path segments",
        "p_path": "src/main/java/com/ning/billing/recurly/RecurlyClient.java",
        "t_path": "src/test/java/com/ning/billing/recurly/TestRecurlyClient.java",
        "p_name": "getAccountBalance",
        "t_name": "testGetAccountBalance",
        "lpfc": "public AccountBalance getAccountBalance(final String accountCode) {\n    return doGET(Account.ACCOUNT_RESOURCE + \"/\" + accountCode + AccountBalance.ACCOUNT_BALANCE_RESOURCE, AccountBalance.class);\n}",
        "rpfc": "public AccountBalance getAccountBalance(final String accountCode) {\n    return doGET(Account.ACCOUNT_RESOURCE + \"/\" + urlEncode(accountCode) + AccountBalance.ACCOUNT_BALANCE_RESOURCE, AccountBalance.class);\n}",
        "tuc": "@Test(groups = \"integration\")\npublic void testGetAccountBalance() throws Exception {\n    final Account accountData = TestUtils.createRandomAccount();\n    final BillingInfo billingInfoData = TestUtils.createRandomBillingInfo();\n    try {\n        final Account account = recurlyClient.createAccount(accountData);\n        billingInfoData.setAccount(account);\n        final BillingInfo billingInfo = recurlyClient.createOrUpdateBillingInfo(billingInfoData);\n        Assert.assertNotNull(billingInfo);\n        final BillingInfo retrievedBillingInfo = recurlyClient.getBillingInfo(account.getAccountCode());\n        Assert.assertNotNull(retrievedBillingInfo);\n        final Adjustment adjustment = new Adjustment();\n        adjustment.setUnitAmountInCents(150);\n        adjustment.setCurrency(CURRENCY);\n        recurlyClient.createAccountAdjustment(account.getAccountCode(), adjustment);\n        final AccountBalance balance = recurlyClient.getAccountBalance(account.getAccountCode());\n        Assert.assertEquals(balance.getBalanceInCents().getUnitAmountUSD(), new Integer(150));\n        Assert.assertEquals(balance.getPastDue(), Boolean.FALSE);\n    } finally {\n        recurlyClient.clearBillingInfo(accountData.getAccountCode());\n        recurlyClient.closeAccount(accountData.getAccountCode());\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "killbilling___recurly-java-library",
        "commit": "b8d8c1c7aa39f47ebe51beadf19f18f1c54f4115",
        "commit_message": "Merge pull request #382 from saasquatch/issue20200225\n\nAdd URL encoding to URL path segments",
        "p_path": "src/main/java/com/ning/billing/recurly/RecurlyClient.java",
        "t_path": "src/test/java/com/ning/billing/recurly/TestRecurlyClient.java",
        "p_name": "redeemCoupon",
        "t_name": "testRedeemCoupon",
        "lpfc": "public Redemption redeemCoupon(final String couponCode, final Redemption redemption) {\n    return doPOST(Coupon.COUPON_RESOURCE + \"/\" + couponCode + Redemption.REDEEM_RESOURCE, redemption, Redemption.class);\n}",
        "rpfc": "public Redemption redeemCoupon(final String couponCode, final Redemption redemption) {\n    return doPOST(Coupon.COUPON_RESOURCE + \"/\" + urlEncode(couponCode) + Redemption.REDEEM_RESOURCE, redemption, Redemption.class);\n}",
        "tuc": "@Test(groups = \"integration\")\npublic void testRedeemCoupon() throws Exception {\n    final Account accountData = TestUtils.createRandomAccount();\n    final BillingInfo billingInfoData = TestUtils.createRandomBillingInfo();\n    final Plan planData = TestUtils.createRandomPlan(CURRENCY);\n    final Coupon couponData = TestUtils.createRandomCoupon();\n    final Coupon secondCouponData = TestUtils.createRandomCoupon();\n    final Coupon subscriptionLevelCouponData = TestUtils.createRandomCoupon();\n    subscriptionLevelCouponData.setRedemptionResource(RedemptionResource.subscription);\n    try {\n        final Account account = recurlyClient.createAccount(accountData);\n        final Plan plan = recurlyClient.createPlan(planData);\n        final Coupon coupon = recurlyClient.createCoupon(couponData);\n        final Coupon secondCoupon = recurlyClient.createCoupon(secondCouponData);\n        final Coupon subscriptionLevelCoupon = recurlyClient.createCoupon(subscriptionLevelCouponData);\n        billingInfoData.setAccount(account);\n        final BillingInfo billingInfo = recurlyClient.createOrUpdateBillingInfo(billingInfoData);\n        Assert.assertNotNull(billingInfo);\n        final Subscription subscriptionData = new Subscription();\n        subscriptionData.setPlanCode(plan.getPlanCode());\n        subscriptionData.setAccount(accountData);\n        subscriptionData.setCurrency(CURRENCY);\n        subscriptionData.setUnitAmountInCents(1242);\n        final Subscription subscription = recurlyClient.createSubscription(subscriptionData);\n        Assert.assertNotNull(subscription);\n        try {\n            recurlyClient.getCouponRedemptionByAccount(account.getAccountCode());\n            Assert.fail(\"Coupon should not be found.\");\n        } catch (RecurlyAPIException expected) {\n            Assert.assertTrue(true);\n        }\n        final Redemption redemptionData = new Redemption();\n        redemptionData.setAccountCode(account.getAccountCode());\n        redemptionData.setCurrency(CURRENCY);\n        Redemption redemption = recurlyClient.redeemCoupon(coupon.getCouponCode(), redemptionData);\n        Assert.assertNotNull(redemption);\n        Assert.assertEquals(redemption.getCoupon().getCouponCode(), coupon.getCouponCode());\n        Assert.assertEquals(redemption.getAccount().getAccountCode(), account.getAccountCode());\n        Assert.assertFalse(redemption.getSingleUse());\n        Assert.assertEquals(redemption.getTotalDiscountedInCents(), (Integer) 0);\n        Assert.assertEquals(redemption.getState(), \"active\");\n        Assert.assertEquals(redemption.getCurrency(), CURRENCY);\n        redemption = recurlyClient.getCouponRedemptionByAccount(account.getAccountCode());\n        Assert.assertNotNull(redemption);\n        Assert.assertEquals(redemption.getCoupon().getCouponCode(), coupon.getCouponCode());\n        Assert.assertEquals(redemption.getAccount().getAccountCode(), account.getAccountCode());\n        final Redemption secondRedemptionData = new Redemption();\n        secondRedemptionData.setAccountCode(account.getAccountCode());\n        secondRedemptionData.setCurrency(CURRENCY);\n        Redemption secondRedemption = recurlyClient.redeemCoupon(secondCoupon.getCouponCode(), secondRedemptionData);\n        Assert.assertNotNull(secondRedemption);\n        Assert.assertEquals(secondRedemption.getCoupon().getCouponCode(), secondCoupon.getCouponCode());\n        Assert.assertEquals(secondRedemption.getAccount().getAccountCode(), account.getAccountCode());\n        Assert.assertFalse(secondRedemption.getSingleUse());\n        Assert.assertEquals(secondRedemption.getTotalDiscountedInCents(), (Integer) 0);\n        Assert.assertEquals(secondRedemption.getState(), \"active\");\n        Assert.assertEquals(secondRedemption.getCurrency(), CURRENCY);\n        Redemptions redemptions = recurlyClient.getCouponRedemptionsByAccount(account.getAccountCode());\n        Assert.assertEquals(redemptions.size(), 2);\n        recurlyClient.deleteCouponRedemption(account.getAccountCode(), redemption.getUuid());\n        recurlyClient.deleteCouponRedemption(account.getAccountCode(), secondRedemption.getUuid());\n        try {\n            recurlyClient.getCouponRedemptionByAccount(account.getAccountCode());\n            Assert.fail(\"Coupon should be removed.\");\n        } catch (RecurlyAPIException expected) {\n            Assert.assertTrue(true);\n        }\n        final Redemption redemptionData2 = new Redemption();\n        redemptionData2.setAccountCode(account.getAccountCode());\n        redemptionData2.setCurrency(CURRENCY);\n        redemption = recurlyClient.redeemCoupon(coupon.getCouponCode(), redemptionData2);\n        Assert.assertNotNull(redemption);\n        redemption = recurlyClient.getCouponRedemptionByAccount(account.getAccountCode());\n        Assert.assertEquals(redemption.getCoupon().getCouponCode(), coupon.getCouponCode());\n        Assert.assertEquals(redemption.getAccount().getAccountCode(), account.getAccountCode());\n        Assert.assertFalse(redemption.getSingleUse());\n        Assert.assertEquals(redemption.getTotalDiscountedInCents(), (Integer) 0);\n        Assert.assertEquals(redemption.getState(), \"active\");\n        Assert.assertEquals(redemption.getCurrency(), CURRENCY);\n        final Redemption subscriptionLevelRedemptionData = new Redemption();\n        subscriptionLevelRedemptionData.setAccountCode(account.getAccountCode());\n        subscriptionLevelRedemptionData.setCurrency(CURRENCY);\n        subscriptionLevelRedemptionData.setSubscriptionUuid(subscription.getUuid());\n        Redemption subscriptionLevelRedemption = recurlyClient.redeemCoupon(subscriptionLevelCoupon.getCouponCode(), subscriptionLevelRedemptionData);\n        Assert.assertNotNull(subscriptionLevelRedemption.getUuid());\n        Redemptions subRedemptions = recurlyClient.getCouponRedemptionsBySubscription(subscription.getUuid(), new QueryParams());\n        Assert.assertEquals(subRedemptions.size(), 1);\n    } finally {\n        recurlyClient.closeAccount(accountData.getAccountCode());\n        recurlyClient.deletePlan(planData.getPlanCode());\n        recurlyClient.deleteCoupon(couponData.getCouponCode());\n        recurlyClient.deleteCoupon(secondCouponData.getCouponCode());\n        recurlyClient.deleteCoupon(subscriptionLevelCouponData.getCouponCode());\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "killbilling___recurly-java-library",
        "commit": "b8d8c1c7aa39f47ebe51beadf19f18f1c54f4115",
        "commit_message": "Merge pull request #382 from saasquatch/issue20200225\n\nAdd URL encoding to URL path segments",
        "p_path": "src/main/java/com/ning/billing/recurly/RecurlyClient.java",
        "t_path": "src/test/java/com/ning/billing/recurly/TestRecurlyClient.java",
        "p_name": "updatePlan",
        "t_name": "testUpdatePlan",
        "lpfc": "public Plan updatePlan(final Plan plan) {\n    return doPUT(Plan.PLANS_RESOURCE + \"/\" + plan.getPlanCode(), plan, Plan.class);\n}",
        "rpfc": "public Plan updatePlan(final Plan plan) {\n    return doPUT(Plan.PLANS_RESOURCE + \"/\" + urlEncode(plan.getPlanCode()), plan, Plan.class);\n}",
        "tuc": "@Test(groups = \"integration\")\npublic void testUpdatePlan() throws Exception {\n    final Plan planData = TestUtils.createRandomPlan();\n    try {\n        final DateTime creationDateTime = new DateTime(DateTimeZone.UTC);\n        final Plan plan = recurlyClient.createPlan(planData);\n        final Plan planChanges = new Plan();\n        Assert.assertNotNull(plan);\n        planChanges.setPlanCode(planData.getPlanCode());\n        planChanges.setName(\"A new name\");\n        planChanges.setDescription(\"A new description\");\n        final Plan updatedPlan = recurlyClient.updatePlan(planChanges);\n        Assert.assertNotNull(updatedPlan);\n        Assert.assertEquals(updatedPlan.getName(), \"A new name\");\n        Assert.assertEquals(updatedPlan.getDescription(), \"A new description\");\n    } finally {\n        recurlyClient.deletePlan(planData.getPlanCode());\n        try {\n            final Plan retrievedPlan2 = recurlyClient.getPlan(planData.getPlanCode());\n            Assert.fail(\"Failed to delete the Plan\");\n        } catch (final RecurlyAPIException e) {\n        }\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "CDCgov___prime-simplereport",
        "commit": "2dfa6cfa9901d246201fb8774d98a84cfebf892a",
        "commit_message": "remove redundant where clause in findMostRecentByTestOrderIdIn (#4319)\n\n",
        "p_path": "backend/src/main/java/gov/cdc/usds/simplereport/db/repository/PatientLinkRepository.java",
        "t_path": "backend/src/test/java/gov/cdc/usds/simplereport/db/repository/PatientLinkRepositoryTest.java",
        "p_name": "findMostRecentByTestOrderIdIn",
        "t_name": "findMostRecentByTestOrderIdInTest",
        "lpfc": "@Query(value = \"select DISTINCT on (ordered.test_order_id) * from (select * from {h-schema}patient_link where test_order_id in :testOrderIds order by created_at desc ) ordered where test_order_id in :testOrderIds\", nativeQuery = true)\nList<PatientLink> findMostRecentByTestOrderIdIn(Collection<UUID> testOrderIds);",
        "rpfc": "@Query(value = \"select DISTINCT on (ordered.test_order_id) * from (select * from {h-schema}patient_link where test_order_id in :testOrderIds order by created_at desc ) ordered\", nativeQuery = true)\nList<PatientLink> findMostRecentByTestOrderIdIn(Collection<UUID> testOrderIds);",
        "tuc": "@Test\nvoid findMostRecentByTestOrderIdInTest() {\n    // GIVEN\n    mockCreationTime(\"2020-01-01 00:00\");\n    Organization org = testDataFactory.createValidOrg();\n    Facility facility = testDataFactory.createValidFacility(org, \"PatientLinkTest Facility\");\n    Person patient = testDataFactory.createFullPerson(org);\n    Person patient2 = testDataFactory.createFullPerson(org);\n    Person patient3 = testDataFactory.createFullPerson(org);\n    TestOrder testOrder1 = testDataFactory.createTestOrder(patient, facility);\n    TestOrder testOrder2 = testDataFactory.createTestOrder(patient2, facility);\n    TestOrder testOrderNoPatientLink = testDataFactory.createTestOrderNoPatientLink(patient3, facility);\n    mockCreationTime(\"2020-02-17 00:00\");\n    PatientLink testOrder1OlderPatientLink = testDataFactory.createPatientLink(testOrder1);\n    mockCreationTime(\"2020-05-17 00:00\");\n    PatientLink testOrder1MostRecentPatientLink = testDataFactory.createPatientLink(testOrder1);\n    mockCreationTime(\"2022-05-20 00:00\");\n    PatientLink testOrder2OlderPatientLink = testDataFactory.createPatientLink(testOrder2);\n    mockCreationTime(\"2022-06-23 00:00\");\n    PatientLink testOrder2MostRecentPatientLink = testDataFactory.createPatientLink(testOrder2);\n    // WHEN\n    List<PatientLink> found = patientLinkRepository.findMostRecentByTestOrderIdIn(List.of(testOrder1.getInternalId(), testOrder2.getInternalId(), testOrderNoPatientLink.getInternalId()));\n    // THEN\n    assertThat(found).hasSize(2);\n    assertThat(found).contains(testOrder1MostRecentPatientLink, testOrder2MostRecentPatientLink);\n    assertThat(found).doesNotContain(testOrder1OlderPatientLink, testOrder2OlderPatientLink);\n}",
        "label": 0
    },
    {
        "repo_name": "vert-x3___vertx-mongo-client",
        "commit": "eb6cb5837e9d713275516e92d9595615d2f308e9",
        "commit_message": "Expose expireAfterSeconds in CreateCollectionOptions\n\nFixes #305\n\nSigned-off-by: Thomas Segismont <tsegismont@gmail.com>\n",
        "p_path": "src/main/java/io/vertx/ext/mongo/CreateCollectionOptions.java",
        "t_path": "src/test/java/io/vertx/ext/mongo/CreateCollectionOptionsTest.java",
        "p_name": "equals",
        "t_name": "testEquals",
        "lpfc": "@Override\npublic boolean equals(Object o) {\n    if (this == o)\n        return true;\n    if (o == null || getClass() != o.getClass())\n        return false;\n    CreateCollectionOptions that = (CreateCollectionOptions) o;\n    return Objects.equals(maxDocuments, that.maxDocuments) && Objects.equals(capped, that.capped) && Objects.equals(sizeInBytes, that.sizeInBytes) && Objects.equals(storageEngineOptions, that.storageEngineOptions) && Objects.equals(indexOptionDefaults, that.indexOptionDefaults) && Objects.equals(validationOptions, that.validationOptions) && Objects.equals(collation, that.collation);\n}",
        "rpfc": "@Override\npublic boolean equals(Object o) {\n    if (this == o)\n        return true;\n    if (o == null || getClass() != o.getClass())\n        return false;\n    CreateCollectionOptions that = (CreateCollectionOptions) o;\n    return Objects.equals(maxDocuments, that.maxDocuments) && Objects.equals(capped, that.capped) && Objects.equals(timeSeriesOptions, that.timeSeriesOptions) && Objects.equals(sizeInBytes, that.sizeInBytes) && Objects.equals(storageEngineOptions, that.storageEngineOptions) && Objects.equals(indexOptionDefaults, that.indexOptionDefaults) && Objects.equals(validationOptions, that.validationOptions) && Objects.equals(collation, that.collation) && Objects.equals(expireAfterSeconds, that.expireAfterSeconds);\n}",
        "tuc": "@Test\npublic void testEquals() {\n    assertEquals(new CreateCollectionOptions(), new CreateCollectionOptions());\n    assertNotEqual((a, b) -> {\n        a.setCapped(true);\n        b.setCapped(false);\n    });\n    assertNotEqual((a, b) -> {\n        a.setCollation(new CollationOptions().setLocale(\"de_AT\"));\n        b.setCollation(new CollationOptions().setLocale(\"en_US\"));\n    });\n    assertNotEqual((a, b) -> {\n        a.setIndexOptionDefaults(new JsonObject().put(\"some\", \"option\"));\n        b.setIndexOptionDefaults(new JsonObject());\n    });\n    assertNotEqual((a, b) -> {\n        a.setValidationOptions(new ValidationOptions().setValidationAction(ValidationAction.WARN));\n        b.setValidationOptions(new ValidationOptions().setValidationAction(ValidationAction.ERROR));\n    });\n    assertNotEqual((a, b) -> {\n        a.setMaxDocuments(12345L);\n        b.setMaxDocuments(10L);\n    });\n    assertNotEqual((a, b) -> {\n        a.setSizeInBytes(1024L);\n        b.setSizeInBytes(2048L);\n    });\n    assertNotEqual((a, b) -> {\n        a.setStorageEngineOptions(new JsonObject().put(\"some\", \"option\"));\n        b.setStorageEngineOptions(new JsonObject());\n    });\n    assertNotEquals(new CreateCollectionOptions(), null);\n}",
        "label": 0
    },
    {
        "repo_name": "Longi94___JavaSteam",
        "commit": "c98c20320c6281003a4ca16fcd4e9c1d5f2e992d",
        "commit_message": "Fixed FileManifestProvider not properly saving/loading depot manifests. Removed the checksum from the return of IManifestProvider and the deserialize/serialize functions in DepotManifest. Ran a clean build to apply lint. Suppressed some warnings. Reverted change in BinaryReader of readNBytes back to read.\n",
        "p_path": "src/main/java/in/dragonbra/javasteam/util/stream/BinaryReader.java",
        "t_path": "src/test/java/in/dragonbra/javasteam/util/stream/BinaryReaderTest.java",
        "p_name": "readLong",
        "t_name": "testReadLong",
        "lpfc": "public long readLong() throws IOException {\n    in.readNBytes(readBuffer, 0, 8);\n    position += 8;\n    return (((long) readBuffer[7] << 56) + ((long) (readBuffer[6] & 255) << 48) + ((long) (readBuffer[5] & 255) << 40) + ((long) (readBuffer[4] & 255) << 32) + ((long) (readBuffer[3] & 255) << 24) + ((readBuffer[2] & 255) << 16) + ((readBuffer[1] & 255) << 8) + (readBuffer[0] & 255));\n}",
        "rpfc": "public long readLong() throws IOException {\n    in.read(readBuffer, 0, 8);\n    position += 8;\n    return (((long) readBuffer[7] << 56) + ((long) (readBuffer[6] & 255) << 48) + ((long) (readBuffer[5] & 255) << 40) + ((long) (readBuffer[4] & 255) << 32) + ((long) (readBuffer[3] & 255) << 24) + ((readBuffer[2] & 255) << 16) + ((readBuffer[1] & 255) << 8) + (readBuffer[0] & 255));\n}",
        "tuc": "@Test\nvoid testReadLong() throws IOException {\n    byte[] data = { 1, 0, 0, 0, 0, 0, 0, 0 };\n    binaryReader = new BinaryReader(new MemoryStream(data));\n    var result = binaryReader.readLong();\n    Assertions.assertEquals(1L, result);\n    Assertions.assertEquals(8, binaryReader.getPosition());\n}",
        "label": 0
    },
    {
        "repo_name": "apache___oodt",
        "commit": "95e04f9a223d076beee3d4d5604b5296cbb2e315",
        "commit_message": "OODT-887 remove URL network dependency\n",
        "p_path": "filemgr/src/main/java/org/apache/oodt/cas/filemgr/ingest/CachedIngester.java",
        "t_path": "filemgr/src/test/java/org/apache/oodt/cas/filemgr/ingest/TestCachedIngester.java",
        "p_name": "hasProduct",
        "t_name": "testHasProduct",
        "lpfc": "public boolean hasProduct(URL fmUrl, String productName) throws CatalogException {\n    if (cache.getFileManagerUrl().equals(fmUrl)) {\n        return cache.contains(productName);\n    } else {\n        cache.setFileManager(fmUrl);\n        try {\n            cache.sync();\n        } catch (CacheException e) {\n            LOG.log(Level.WARNING, \"Exception re-syncing cache to file manager: [\" + fmUrl + \"]: Message: \" + e.getMessage());\n            throw new CatalogException(\"Exception re-syncing cache to file manager: [\" + fmUrl + \"]: Message: \" + e.getMessage());\n        }\n        return cache.contains(productName);\n    }\n}",
        "rpfc": "public boolean hasProduct(URL fmUrl, String productName) throws CatalogException {\n    try {\n        if (cache.getFileManagerUrl().toURI().equals(fmUrl.toURI())) {\n            return cache.contains(productName);\n        } else {\n            cache.setFileManager(fmUrl);\n            try {\n                cache.sync();\n            } catch (CacheException e) {\n                LOG.log(Level.WARNING, \"Exception re-syncing cache to file manager: [\" + fmUrl + \"]: Message: \" + e.getMessage());\n                throw new CatalogException(\"Exception re-syncing cache to file manager: [\" + fmUrl + \"]: Message: \" + e.getMessage());\n            }\n            return cache.contains(productName);\n        }\n    } catch (URISyntaxException e) {\n        LOG.log(Level.SEVERE, \"Exception getting URI from URL\");\n        throw new CatalogException(\"Exception getting URL from URL: Message: \" + e.getMessage());\n    }\n}",
        "tuc": "public void testHasProduct() {\n    try {\n        ingester.resynsc();\n    } catch (CacheException e) {\n        fail(e.getMessage());\n    }\n    try {\n        assertTrue(ingester.hasProduct(new URL(\"http://localhost:\" + FM_PORT), \"test.txt\"));\n    } catch (Exception e) {\n        fail(e.getMessage());\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "SvenEwald___xmlbeam",
        "commit": "cd8ac4388f2a5d00c06a591e31ce8c0d3ae726e0",
        "commit_message": "Fixing missleading import.",
        "p_path": "src/main/java/org/xmlbeam/util/intern/org/objectweb/asm/Frame.java",
        "t_path": "src/test/java/org/xmlbeam/refcards/SpecifyProjectorUsage.java",
        "p_name": "type",
        "t_name": "testfluentapi",
        "lpfc": "private static int type(final ClassWriter cw, final String desc) {\n    String t;\n    int index = desc.charAt(0) == '(' ? desc.indexOf(')') + 1 : 0;\n    switch(desc.charAt(index)) {\n        case 'V':\n            return 0;\n        case 'Z':\n        case 'C':\n        case 'B':\n        case 'S':\n        case 'I':\n            return INTEGER;\n        case 'F':\n            return FLOAT;\n        case 'J':\n            return LONG;\n        case 'D':\n            return DOUBLE;\n        case 'L':\n            t = desc.substring(index + 1, desc.length() - 1);\n            return OBJECT | cw.addType(t);\n        default:\n            int data;\n            int dims = index + 1;\n            while (desc.charAt(dims) == '[') {\n                ++dims;\n            }\n            switch(desc.charAt(dims)) {\n                case 'Z':\n                    data = BOOLEAN;\n                    break;\n                case 'C':\n                    data = CHAR;\n                    break;\n                case 'B':\n                    data = BYTE;\n                    break;\n                case 'S':\n                    data = SHORT;\n                    break;\n                case 'I':\n                    data = INTEGER;\n                    break;\n                case 'F':\n                    data = FLOAT;\n                    break;\n                case 'J':\n                    data = LONG;\n                    break;\n                case 'D':\n                    data = DOUBLE;\n                    break;\n                default:\n                    t = desc.substring(dims + 1, desc.length() - 1);\n                    data = OBJECT | cw.addType(t);\n            }\n            return ((dims - index) << 28) | data;\n    }\n}",
        "rpfc": "private static int type(final ClassWriter cw, final String desc) {\n    String t;\n    final int index = desc.charAt(0) == '(' ? desc.indexOf(')') + 1 : 0;\n    switch(desc.charAt(index)) {\n        case 'V':\n            return 0;\n        case 'Z':\n        case 'C':\n        case 'B':\n        case 'S':\n        case 'I':\n            return INTEGER;\n        case 'F':\n            return FLOAT;\n        case 'J':\n            return LONG;\n        case 'D':\n            return DOUBLE;\n        case 'L':\n            t = desc.substring(index + 1, desc.length() - 1);\n            return OBJECT | cw.addType(t);\n        default:\n            int data;\n            int dims = index + 1;\n            while (desc.charAt(dims) == '[') {\n                ++dims;\n            }\n            switch(desc.charAt(dims)) {\n                case 'Z':\n                    data = BOOLEAN;\n                    break;\n                case 'C':\n                    data = CHAR;\n                    break;\n                case 'B':\n                    data = BYTE;\n                    break;\n                case 'S':\n                    data = SHORT;\n                    break;\n                case 'I':\n                    data = INTEGER;\n                    break;\n                case 'F':\n                    data = FLOAT;\n                    break;\n                case 'J':\n                    data = LONG;\n                    break;\n                case 'D':\n                    data = DOUBLE;\n                    break;\n                default:\n                    t = desc.substring(dims + 1, desc.length() - 1);\n                    data = OBJECT | cw.addType(t);\n            }\n            return ((dims - index) << 28) | data;\n    }\n}",
        "tuc": "public void testFluentAPI() throws IOException {\n    new XBProjector(Flags.SYNCHRONIZE_ON_DOCUMENTS).projectEmptyDocument(null);\n    new XBProjector(Flags.SYNCHRONIZE_ON_DOCUMENTS, Flags.TO_STRING_RENDERS_XML).io().fromURLAnnotation(null);\n}",
        "label": 0
    },
    {
        "repo_name": "clulab___reach-banner",
        "commit": "4950bb2a2bf733ecc89bbc49a9d02e3331bbea5a",
        "commit_message": "simplified BannerWrapper.tag()\n",
        "p_path": "src/main/java/banner/BannerWrapper.java",
        "t_path": "src/test/java/TestBanner.java",
        "p_name": "tag",
        "t_name": "testbanner",
        "lpfc": "public List<Mention> tag(String sentenceText) {\n    String originalText = new String(sentenceText);\n    Sentence sentence = new Sentence(sentenceText);\n    tokenizer.tokenize(sentence);\n    //System.out.println(\"original text: \" + sentenceText);\n    //System.out.println(\"Sentence text: \" + sentence.getText());\n    //for(Token t: sentence.getTokens()) {\n    //  System.out.println(\"\\t\" + t.getText() + \" \" + t.getStart());\n    //}\n    tagger.tag(sentence);\n    if (postProcessor != null)\n        postProcessor.postProcess(sentence);\n    // make sure the text of the sentence did not change!\n    //   if it did, it is impossible to align the mentions with the original text...\n    if (!originalText.equals(sentence.getText())) {\n        throw new RuntimeException(\"ERROR: input sentence [\" + originalText + \"] is different from sentence output by Banner [\" + sentence.getText() + \"]!\");\n    }\n    return sentence.getMentions();\n}",
        "rpfc": "public List<Mention> tag(String sentenceText) {\n    Sentence sentence = new Sentence(sentenceText);\n    tokenizer.tokenize(sentence);\n    //System.out.println(\"original text: \" + sentenceText);\n    //System.out.println(\"Sentence text: \" + sentence.getText());\n    //for(Token t: sentence.getTokens()) {\n    //  System.out.println(\"\\t\" + t.getText() + \" \" + t.getStart());\n    //}\n    tagger.tag(sentence);\n    if (postProcessor != null)\n        postProcessor.postProcess(sentence);\n    return sentence.getMentions();\n}",
        "tuc": "@Test\npublic void testBanner() {\n    BannerWrapper banner = new BannerWrapper();\n    List<Mention> mentions = banner.tag(\"Co-immunoprecipitation analysis confirmed that Bis interacted with Bcl-2 in vivo.\");\n    for (Mention m : mentions) {\n        System.out.println(\"\\tMENTION: \" + m.getText() + \" \" + m.getType() + \" \" + m.getStartChar() + \" \" + m.getEndChar());\n        assertEquals(m.getText(), \"Bcl-2\");\n        assertEquals(m.getStartChar(), 67);\n        assertEquals(m.getEndChar(), 72);\n    }\n    mentions = banner.tag(\"Only p105 and human c-Rel (hc-Rel) are common to complexes precipitated with antiserum directed against either p105 or hc-Rel.\");\n    int i = 0;\n    for (Mention m : mentions) {\n        System.out.println(\"\\tMENTION: \" + m.getText() + \" \" + m.getType() + \" \" + m.getStartChar() + \" \" + m.getEndChar());\n        if (i == 0) {\n            assertEquals(m.getText(), \"p105\");\n            assertEquals(m.getStartChar(), 5);\n        }\n        i++;\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "kakao___hbase-tools",
        "commit": "01500dd86cc411bcf3181e07cdc2e1306d762de1",
        "commit_message": "update message\n",
        "p_path": "hbase0.98/hbase-manager-0.98/src/main/java/com/kakao/hbase/manager/command/MC.java",
        "t_path": "hbase0.98/hbase-manager-0.98/src/test/java/com/kakao/hbase/manager/command/MCTest.java",
        "p_name": "mc",
        "t_name": "testmc",
        "lpfc": "private void mc(boolean tableLevel, String tableOrRegion) throws InterruptedException, IOException {\n    if (args.has(Args.OPTION_CF)) {\n        String cf = (String) args.valueOf(Args.OPTION_CF);\n        try {\n            System.out.print(\"Major compaction on \" + cf + \" CF of \" + (tableLevel ? \"table \" : \"region \") + tableOrRegion + (tableLevel ? \"\" : \" - \" + getRegionInfo(tableOrRegion) + \" - \"));\n            if (!askProceedInteractively())\n                return;\n            admin.majorCompact(tableOrRegion, cf);\n            mcCounter.getAndIncrement();\n        } catch (IOException e) {\n            String message = \"column family \" + cf + \" does not exist\";\n            if (e.getMessage().contains(message)) {\n                System.out.println(\"WARNING - \" + message + \" on \" + tableOrRegion);\n            } else {\n                throw e;\n            }\n        }\n    } else {\n        System.out.print(\"Major compaction on \" + (tableLevel ? \"table \" : \"region \") + tableOrRegion + (tableLevel ? \"\" : \" - \" + getRegionInfo(tableOrRegion) + \" - \"));\n        if (!askProceedInteractively())\n            return;\n        admin.majorCompact(tableOrRegion);\n        mcCounter.getAndIncrement();\n    }\n}",
        "rpfc": "private void mc(boolean tableLevel, String tableOrRegion) throws InterruptedException, IOException {\n    if (args.has(Args.OPTION_CF)) {\n        String cf = (String) args.valueOf(Args.OPTION_CF);\n        try {\n            System.out.print(\"Major compaction on \" + cf + \" CF of \" + (tableLevel ? \"table \" : \"region \") + tableOrRegion + (tableLevel ? \"\" : \" - \" + getRegionInfo(tableOrRegion)));\n            if (!askProceedInteractively())\n                return;\n            admin.majorCompact(tableOrRegion, cf);\n            mcCounter.getAndIncrement();\n        } catch (IOException e) {\n            String message = \"column family \" + cf + \" does not exist\";\n            if (e.getMessage().contains(message)) {\n                System.out.println(\"WARNING - \" + message + \" on \" + tableOrRegion);\n            } else {\n                throw e;\n            }\n        }\n    } else {\n        System.out.print(\"Major compaction on \" + (tableLevel ? \"table \" : \"region \") + tableOrRegion + (tableLevel ? \"\" : \" - \" + getRegionInfo(tableOrRegion)));\n        if (!askProceedInteractively())\n            return;\n        admin.majorCompact(tableOrRegion);\n        mcCounter.getAndIncrement();\n    }\n}",
        "tuc": "@Test\npublic void testMC() throws Exception {\n    // move a region to the first RS\n    ArrayList<ServerName> serverNameList = getServerNameList();\n    assertTrue(serverNameList.size() >= 2);\n    ArrayList<HRegionInfo> regionInfoList = getRegionInfoList(tableName);\n    assertEquals(1, regionInfoList.size());\n    HRegionInfo regionInfo = regionInfoList.get(0);\n    ServerName serverName = serverNameList.get(0);\n    move(regionInfo, serverName);\n    // make 2 store files\n    putData(table, \"a\".getBytes());\n    admin.flush(tableName);\n    putData(table, \"b\".getBytes());\n    admin.flush(tableName);\n    Thread.sleep(3000);\n    assertEquals(2, getRegionLoad(regionInfo, serverName).getStorefiles());\n    // run MC\n    String[] argsParam = { \"zookeeper\", tableName, \"--force-proceed\", \"--wait\", \"--test\" };\n    Args args = new ManagerArgs(argsParam);\n    MC command = new MC(admin, args);\n    command.run();\n    // should be 1 store file\n    assertEquals(1, getRegionLoad(regionInfo, serverName).getStorefiles());\n}",
        "label": 0
    },
    {
        "repo_name": "pkiraly___metadata-qa-api",
        "commit": "796d4394384e89f63c118bba0f60448789d36178",
        "commit_message": "Issue #8: improving saturation result.\n",
        "p_path": "src/main/java/de/gwdg/metadataqa/api/calculator/CalculatorFacade.java",
        "t_path": "src/test/java/de/gwdg/metadataqa/api/calculator/CalculatorFacadeTest.java",
        "p_name": "configure",
        "t_name": "testchanged",
        "lpfc": "public void configure() {\n    calculators = new ArrayList<>();\n    fieldExtractor = new FieldExtractor();\n    EdmSchema schema = new EdmOaiPmhXmlSchema();\n    if (completenessMeasurementEnabled) {\n        completenessCalculator = new CompletenessCalculator(schema);\n        completenessCalculator.collectFields(completenessCollectFields);\n        calculators.add(completenessCalculator);\n    }\n    if (tfIdfMeasurementEnabled) {\n        tfidfCalculator = new TfIdfCalculator(schema);\n        tfidfCalculator.setDoCollectTerms(collectTfIdfTerms);\n        calculators.add(tfidfCalculator);\n    }\n    if (problemCatalogMeasurementEnabled) {\n        ProblemCatalog problemCatalog = new ProblemCatalog(schema);\n        LongSubject longSubject = new LongSubject(problemCatalog);\n        TitleAndDescriptionAreSame titleAndDescriptionAreSame = new TitleAndDescriptionAreSame(problemCatalog);\n        EmptyStrings emptyStrings = new EmptyStrings(problemCatalog);\n        calculators.add(problemCatalog);\n    }\n    if (languageMeasurementEnabled) {\n        languageCalculator = new LanguageCalculator(schema);\n        calculators.add(languageCalculator);\n    }\n    if (languageSaturationMeasurementEnabled) {\n        languageSaturationCalculator = new LanguageSaturationCalculator(schema);\n        calculators.add(languageSaturationCalculator);\n    }\n}",
        "rpfc": "public void configure() {\n    calculators = new ArrayList<>();\n    fieldExtractor = new FieldExtractor();\n    EdmSchema schema = new EdmOaiPmhXmlSchema();\n    if (completenessMeasurementEnabled) {\n        completenessCalculator = new CompletenessCalculator(schema);\n        completenessCalculator.collectFields(completenessCollectFields);\n        calculators.add(completenessCalculator);\n    }\n    if (tfIdfMeasurementEnabled) {\n        tfidfCalculator = new TfIdfCalculator(schema);\n        tfidfCalculator.setDoCollectTerms(collectTfIdfTerms);\n        calculators.add(tfidfCalculator);\n    }\n    if (problemCatalogMeasurementEnabled) {\n        ProblemCatalog problemCatalog = new ProblemCatalog(schema);\n        LongSubject longSubject = new LongSubject(problemCatalog);\n        TitleAndDescriptionAreSame titleAndDescriptionAreSame = new TitleAndDescriptionAreSame(problemCatalog);\n        EmptyStrings emptyStrings = new EmptyStrings(problemCatalog);\n        calculators.add(problemCatalog);\n    }\n    if (languageMeasurementEnabled) {\n        languageCalculator = new LanguageCalculator(schema);\n        calculators.add(languageCalculator);\n    }\n    if (languageSaturationMeasurementEnabled) {\n        languageSaturationCalculator = new LanguageSaturationCalculator(schema);\n        if (saturationExtendedResult)\n            languageSaturationCalculator.setResultType(LanguageSaturationCalculator.ResultTypes.EXTENDED);\n        calculators.add(languageSaturationCalculator);\n    }\n}",
        "tuc": "@Test\npublic void testChanged() {\n    CalculatorFacade calculator = new CalculatorFacade();\n    assertFalse(calculator.isTfIdfMeasurementEnabled());\n    calculator.configure();\n    List<Calculator> calculators = calculator.getCalculators();\n    assertEquals(1, calculators.size());\n    calculator.enableTfIdfMeasurement(true);\n    calculator.changed();\n    calculators = calculator.getCalculators();\n    assertEquals(2, calculators.size());\n    calculator.changed();\n    calculators = calculator.getCalculators();\n    assertEquals(2, calculators.size());\n}",
        "label": 0
    },
    {
        "repo_name": "apache___empire-db",
        "commit": "c7365bc3fcf804f14ff1c985891a794f8c68c22b",
        "commit_message": "EMPIREDB-157 \nOverload for simple create changed to final to avoid ambigousity\n\ngit-svn-id: https://svn.apache.org/repos/asf/empire-db/trunk@1382299 13f79535-47bb-0310-9956-ffa450edef68\n",
        "p_path": "empire-db/src/main/java/org/apache/empire/db/DBRecord.java",
        "t_path": "empire-db/src/test/java/org/apache/empire/db/IntegerTest.java",
        "p_name": "create",
        "t_name": "testderby",
        "lpfc": "public void create(DBRowSet table) {\n    create(table, null);\n}",
        "rpfc": "public final void create(DBRowSet table) {\n    create(table, null);\n}",
        "tuc": "@Test\npublic void testDerby() {\n    SampleConfig config = new SampleConfig();\n    config.databaseProvider = \"derby\";\n    config.jdbcClass = \"org.apache.derby.jdbc.EmbeddedDriver\";\n    config.jdbcURL = \"jdbc:derby:memory:data/derby/test;create=true\";\n    testLongInteger(config);\n}",
        "label": 0
    },
    {
        "repo_name": "Nastel___remoraj",
        "commit": "b90d7c3576a1a0b894d135e7286a20f84168fd11",
        "commit_message": "Moving writing to queue distributed in Threads\n",
        "p_path": "remora-core/src/main/java/com/jkoolcloud/remora/core/output/ChronicleOutput.java",
        "t_path": "remora-core/src/test/java/com/jkoolcloud/remora/core/output/ChronicleOutputTest.java",
        "p_name": "send",
        "t_name": "testrolling",
        "lpfc": "@Override\npublic void send(EntryDefinition entry) {\n    try {\n        appender.writeDocument(entry);\n    } catch (UnrecoverableTimeoutException e) {\n        logger.error(e);\n    }\n}",
        "rpfc": "@Override\npublic void send(EntryDefinition entry) {\n    queueWorkers.submit(new Runnable() {\n        @Override\n        public void run() {\n            try {\n                ((ChronicleAppenderThread) Thread.currentThread()).getAppender().writeDocument(entry);\n            } catch (Exception e) {\n                failCount.incrementAndGet();\n            }\n        }\n    });\n}",
        "tuc": "@Test\npublic void testRolling() throws InterruptedException {\n    ChronicleOutput output = new ChronicleOutput();\n    output.rollCycle = RollCycles.TEST_SECONDLY;\n    output.keepQueueRolls = 2;\n    File tempDir = Files.createTempDir();\n    tempDir.deleteOnExit();\n    System.out.println(tempDir.getAbsolutePath());\n    output.queuePath = tempDir.getPath();\n    output.init();\n    for (int i = 0; i <= 50; i++) {\n        output.send(new EntryDefinition(ChronicleOutputTest.class));\n        Thread.sleep(100);\n    }\n    // +1 = metadata; +1 = current\n    assertEquals(tempDir.list().length, output.keepQueueRolls.intValue() + 1 + 1);\n    output.shutdown();\n    // +1 = current after shutdown\n    assertEquals(tempDir.list().length, output.keepQueueRolls.intValue() + 1);\n    // queue\n    Arrays.asList(tempDir.listFiles()).forEach(file -> {\n        while (!file.delete()) {\n            // make sure it deletes\n        }\n    });\n    tempDir.delete();\n}",
        "label": 0
    },
    {
        "repo_name": "pkiraly___metadata-qa-api",
        "commit": "aa5c529f7b5a7bddb506fd79128c9417d8c90f59",
        "commit_message": "issue #54: addin List<String> measureAsList(), and Map<String, Object> measureAsMap methods\n",
        "p_path": "src/main/java/de/gwdg/metadataqa/api/calculator/CalculatorFacade.java",
        "t_path": "src/test/java/de/gwdg/metadataqa/api/calculator/CalculatorFacadeTest.java",
        "p_name": "measure",
        "t_name": "testnoabbreviate",
        "lpfc": "public String measure(String jsonRecord) throws InvalidJsonException {\n    return this.<XmlFieldInstance>measureWithGenerics(jsonRecord);\n}",
        "rpfc": "public String measure(String jsonRecord) throws InvalidJsonException {\n    return (String) this.<XmlFieldInstance>measureWithGenerics(jsonRecord);\n}",
        "tuc": "@Test\npublic void testNoAbbreviate() throws URISyntaxException, IOException {\n    CalculatorFacade calculatorFacade = new CalculatorFacade(true, true, true, false, true);\n    calculatorFacade.setSchema(new EdmOaiPmhJsonSchema());\n    calculatorFacade.configure();\n    String expected = \"0.184,1.0,0.181818,0.388889,0.272727,0.5,0.357143,0.75,0.363636,0.4,1,1,1,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,1,1,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,0,1,1,0,0,0,0,5,0,0,0,0,0,0,0,1,0,0,1,0,0,1,1,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,12,0,0,0.0,0.0,0.0\";\n    assertEquals(expected, calculatorFacade.measure(FileUtils.readFirstLine(\"general/test.json\")));\n}",
        "label": 0
    },
    {
        "repo_name": "karussell___Jetwick",
        "commit": "022d4ff1401111e1b11fcf89c200c5d2ec0b8a1f",
        "commit_message": "using bulk api of ES\n",
        "p_path": "src/main/java/de/jetwick/es/ElasticTweetSearch.java",
        "t_path": "src/test/java/de/jetwick/es/ElasticTweetSearchTest.java",
        "p_name": "update",
        "t_name": "testaddoldtweetsifpersistent",
        "lpfc": "public void update(Collection<SolrTweet> tweets) {\n    try {\n        tweets = new SerialCommandExecutor(tweets).add(new TermCreateCommand()).execute();\n        for (SolrTweet tw : tweets) {\n            feedDoc(Long.toString(tw.getTwitterId()), createDoc(tw));\n        }\n    } catch (Exception e) {\n        logger.error(\"Exception while updating.\", e);\n    }\n}",
        "rpfc": "public void update(Collection<SolrTweet> tweets) {\n    try {\n        if (tweets.isEmpty())\n            return;\n        tweets = new SerialCommandExecutor(tweets).add(new TermCreateCommand()).execute();\n        BulkRequestBuilder brb = client.prepareBulk();\n        for (SolrTweet tw : tweets) {\n            String id = Long.toString(tw.getTwitterId());\n            XContentBuilder source = createDoc(tw);\n            brb.add(Requests.indexRequest(getIndexName()).type(getIndexType()).id(id).source(source));\n        }\n        if (brb.numberOfActions() > 0)\n            brb.execute().actionGet();\n    } catch (Exception e) {\n        logger.error(\"Exception while updating.\", e);\n    }\n}",
        "tuc": "@Test\npublic void testAddOldTweetsIfPersistent() throws SolrServerException {\n    SolrTweet tw = createTweet(2L, \"RT @userA: bla bli blu\", \"userB\");\n    Date dt = new MyDate().minusDays(2).toDate();\n    tw.setUpdatedAt(dt);\n    tw.setCreatedAt(dt);\n    assertEquals(1, twSearch.update(tw).size());\n    // testOverwriteTweetsIfPersistent\n    tw = createTweet(2L, \"totally new\", \"userB\");\n    dt = new MyDate().minusDays(2).toDate();\n    tw.setUpdatedAt(dt);\n    tw.setCreatedAt(dt);\n    assertEquals(1, twSearch.update(tw).size());\n    assertEquals(0, twSearch.search(\"bla\").size());\n    assertEquals(1, twSearch.search(\"new\").size());\n}",
        "label": 0
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "3ba5ae9310759273fdb521487f105b3edc9111e3",
        "commit_message": "feat(QiunetDatas): DbEntityBo \u52a0\u5165delete\u7684\u72b6\u6001. \u5904\u7406delete\u540e\u53ef\u80fd\u8fdb\u884cupdate\u7684\u95ee\u9898.\n",
        "p_path": "QiunetDatas/src/main/java/org/qiunet/data/db/loader/DbEntityBo.java",
        "t_path": "QiunetDatas/src/test/java/org/qiunet/data/db/TestDbDataSupport.java",
        "p_name": "delete",
        "t_name": "testentity",
        "lpfc": "@Override\npublic void delete() {\n    if (playerDataLoader == null) {\n        getDo().update();\n        return;\n    }\n    EntityStatus entityStatus = entityStatus();\n    Object obj = playerDataLoader.dataCache.get(getClass());\n    if (obj instanceof Map) {\n        ((Map<?, ?>) obj).remove(((DbEntityList) getDo()).subKey());\n    } else {\n        playerDataLoader.dataCache.put(getClass(), PlayerDataLoader.NULL);\n    }\n    if (entityStatus == EntityStatus.INIT) {\n        return;\n    }\n    playerDataLoader.cacheAsyncToDb.add(PlayerDataLoader.EntityOperate.DELETE, this);\n}",
        "rpfc": "@Override\npublic void delete() {\n    if (playerDataLoader == null) {\n        getDo().update();\n        return;\n    }\n    if (delete) {\n        return;\n    }\n    this.delete = true;\n    EntityStatus entityStatus = entityStatus();\n    Object obj = playerDataLoader.dataCache.get(getClass());\n    if (obj instanceof Map) {\n        ((Map<?, ?>) obj).remove(((DbEntityList) getDo()).subKey());\n    } else {\n        playerDataLoader.dataCache.put(getClass(), PlayerDataLoader.NULL);\n    }\n    if (entityStatus == EntityStatus.INIT) {\n        return;\n    }\n    playerDataLoader.cacheAsyncToDb.add(PlayerDataLoader.EntityOperate.DELETE, this);\n}",
        "tuc": "@Test\npublic void testEntity() {\n    String name = \"\u79cb\u9633\";\n    PlayerDo playerDo = new PlayerDo();\n    playerDo.setExp(1111111111111L);\n    playerDo.setName(\"\u79cb\u96331\");\n    playerDo.setLevel(10);\n    playerDo.setUid(playerDataLoader.getPlayerId());\n    PlayerBo playerBo = playerDataLoader.insertDo(playerDo);\n    playerBo.getDo().setName(name);\n    playerBo.getDo().setLevel(100);\n    playerBo.update();\n    PlayerBo bo = playerDataLoader.getData(PlayerBo.class);\n    Assert.assertEquals(bo.getDo().getName(), name);\n    Assert.assertEquals(bo.getDo().getLevel(), 100);\n    bo.delete();\n    bo = playerDataLoader.getData(PlayerBo.class);\n    Assert.assertNull(bo);\n}",
        "label": 0
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "8300c03e3abfcca793982d48e828cfbaa61d671c",
        "commit_message": "\u5ba2\u6237\u7aefParam\u5bf9\u8c61\u548c\u670d\u52a1\u5668\u7edf\u4e00, \u90fd\u9ed8\u8ba4encryption = true\n",
        "p_path": "FlashHandler/src/main/java/org/qiunet/flash/handler/netty/client/param/AbstractClientParam.java",
        "t_path": "FlashHandler/src/test/java/org/qiunet/flash/handler/bootstrap/TestMuchWebSocketBootStrap.java",
        "p_name": "build",
        "t_name": "testmuchwebsocket",
        "lpfc": "public P build() {\n    if (address == null)\n        throw new NullPointerException(\"Must set port for Http Listener! \");\n    P p = newParams();\n    p.protocolHeaderAdapter = protocolHeaderAdapter;\n    p.maxReceivedLength = maxReceivedLength;\n    p.address = address;\n    this.buildInner(p);\n    return p;\n}",
        "rpfc": "public P build() {\n    if (address == null)\n        throw new NullPointerException(\"Must set port for Http Listener! \");\n    P p = newParams();\n    p.protocolHeaderAdapter = protocolHeaderAdapter;\n    p.maxReceivedLength = maxReceivedLength;\n    p.address = address;\n    p.encryption = encryption;\n    this.buildInner(p);\n    return p;\n}",
        "tuc": "@Test\npublic void testMuchWebSocket() throws InterruptedException {\n    long start = System.currentTimeMillis();\n    for (int i = 0; i < clientCount; i++) {\n        new Thread(() -> {\n            NettyWebsocketClient client = new NettyWebsocketClient(WebSocketClientParams.custom().setAddress(\"localhost\", 8080).setUriIPath(\"/ws\").build(), new Trigger());\n            for (int j = 0; j < requestCount; j++) {\n                String text = \"testMuchWebSocket: \" + j;\n                byte[] bytes = text.getBytes(CharsetUtil.UTF_8);\n                MessageContent content = new MessageContent(1005, bytes);\n                client.sendMessage(content);\n            }\n        }).start();\n    }\n    latch.await();\n    long end = System.currentTimeMillis();\n    System.out.println(\"All Time is:[\" + (end - start) + \"]ms\");\n}",
        "label": 0
    },
    {
        "repo_name": "GameRevision___GWLP-R",
        "commit": "983c942201f82cb02633c3ba2bec143b5ce1ccc9",
        "commit_message": "fixed serialization filters\n",
        "p_path": "actions/src/main/java/gwlpr/actions/SerializationFilter.java",
        "t_path": "actions/src/test/java/gwlpr/actions/DeserializationTest.java",
        "p_name": "register",
        "t_name": "test",
        "lpfc": "public void register(SerializationFilter filter) {\n    filters.add(filter);\n    iterator = filters.listIterator();\n}",
        "rpfc": "public void register(SerializationFilter filter) {\n    filters.add(filter);\n}",
        "tuc": "@Test\npublic void test() {\n    // create a new deserializer\n    SerializationFilter packetDeserializer = SerializationFilterFactory.produceSerializer(P008_TestPacket.class);\n    // build the fake inputstream\n    // the stream represents the data shown in the \"assertEqual\" tests\n    byte[] bytes = new byte[] { 8, 0, 1, 0, 0, 0, 2, 0, 0, 0, 3, 0, 20, 21, 22, 23, 24, 25, 4, 0, 116, 0, 111, 0, 116, 0, 111, 0, 4, 0, 26, 27, 28, 29, 5, 0, 0, 0, 5, 0, 30, 31, 32, 33, 34, 6, 7, 0, 0, 0, 4, 0, 116, 0, 105, 0, 116, 0, 105, 0 };\n    ByteBuffer buffer = ByteBuffer.wrap(bytes);\n    buffer.order(ByteOrder.LITTLE_ENDIAN);\n    // the header isn't parsed by using reflection\n    buffer.getShort();\n    // deserialize the object\n    P008_TestPacket testIncomingPacket = new P008_TestPacket();\n    packetDeserializer.deserialize(buffer, testIncomingPacket);\n    // test that the deserializer correctly extracted all values\n    assertEquals(1, testIncomingPacket.getUnsignedInteger1());\n    assertEquals(2, testIncomingPacket.getUnsignedInteger2());\n    assertEquals(3, testIncomingPacket.getUnsignedShort1());\n    assertEquals(1, testIncomingPacket.getConstantUnsignedByteArray1().length);\n    assertEquals(20, testIncomingPacket.getConstantUnsignedByteArray1()[0]);\n    assertEquals(2, testIncomingPacket.getConstantUnsignedByteArray2().length);\n    assertEquals(21, testIncomingPacket.getConstantUnsignedByteArray2()[0]);\n    assertEquals(22, testIncomingPacket.getConstantUnsignedByteArray2()[1]);\n    assertEquals(3, testIncomingPacket.getConstantUnsignedByteArray3().length);\n    assertEquals(23, testIncomingPacket.getConstantUnsignedByteArray3()[0]);\n    assertEquals(24, testIncomingPacket.getConstantUnsignedByteArray3()[1]);\n    assertEquals(25, testIncomingPacket.getConstantUnsignedByteArray3()[2]);\n    assertEquals(\"toto\", testIncomingPacket.getString1());\n    assertEquals(4, testIncomingPacket.getUnsignedShort2());\n    assertEquals(4, testIncomingPacket.getConstantUnsignedByteArray4().length);\n    assertEquals(26, testIncomingPacket.getConstantUnsignedByteArray4()[0]);\n    assertEquals(27, testIncomingPacket.getConstantUnsignedByteArray4()[1]);\n    assertEquals(28, testIncomingPacket.getConstantUnsignedByteArray4()[2]);\n    assertEquals(29, testIncomingPacket.getConstantUnsignedByteArray4()[3]);\n    assertEquals(5, testIncomingPacket.getUnsignedInteger3());\n    assertEquals(5, testIncomingPacket.getVariableUnsignedByteArray1().length);\n    assertEquals(30, testIncomingPacket.getVariableUnsignedByteArray1()[0]);\n    assertEquals(31, testIncomingPacket.getVariableUnsignedByteArray1()[1]);\n    assertEquals(32, testIncomingPacket.getVariableUnsignedByteArray1()[2]);\n    assertEquals(33, testIncomingPacket.getVariableUnsignedByteArray1()[3]);\n    assertEquals(34, testIncomingPacket.getVariableUnsignedByteArray1()[4]);\n    assertEquals(6, testIncomingPacket.getUnsignedByte1());\n    assertEquals(7, testIncomingPacket.getUnsignedInteger4());\n    assertEquals(\"titi\", testIncomingPacket.getString2());\n}",
        "label": 0
    },
    {
        "repo_name": "mixpanel___mixpanel-java",
        "commit": "012c62e10430c9aa65d611f4c8623103168f071e",
        "commit_message": "Adding automatic mp_lib to events\n",
        "p_path": "src/main/java/com/mixpanel/mixpanelapi/MessageBuilder.java",
        "t_path": "src/test/java/com/mixpanel/mixpanelapi/MixpanelAPITest.java",
        "p_name": "event",
        "t_name": "testclientdelivery",
        "lpfc": "/**\n * Creates a message tracking an event, for consumption by MixpanelAPI\n * See:\n *\n *    http://blog.mixpanel.com/2012/09/12/best-practices-updated/\n *\n * for a detailed discussion of event names, distinct ids, event properties, and how to use them\n * to get the most out of your metrics.\n *\n * @param distinctId a string uniquely identifying the individual cause associated with this event\n *           (for example, the user id of a signing-in user, or the hostname of a server)\n * @param eventName a human readable name for the event, for example \"Purchase\", or \"Threw Exception\"\n * @param properties a JSONObject associating properties with the event. These are useful\n *           for reporting and segmentation of events. It is often useful not only to include\n *           properties of the event itself (for example { 'Item Purchased' : 'Hat' } or\n *           { 'ExceptionType' : 'OutOfMemory' }), but also properties associated with the\n *           identified user (for example { 'MemberSince' : '2012-01-10' } or { 'TotalMemory' : '10TB' })\n */\npublic JSONObject event(String distinctId, String eventName, JSONObject properties) {\n    long time = System.currentTimeMillis() / 1000;\n    // Nothing below should EVER throw a JSONException.\n    try {\n        JSONObject dataObj = new JSONObject();\n        dataObj.put(\"event\", eventName);\n        JSONObject propertiesObj = null;\n        if (properties == null) {\n            propertiesObj = new JSONObject();\n        } else {\n            propertiesObj = new JSONObject(properties.toString());\n        }\n        if (!propertiesObj.has(\"token\"))\n            propertiesObj.put(\"token\", mToken);\n        if (!propertiesObj.has(\"time\"))\n            propertiesObj.put(\"time\", time);\n        if (distinctId != null)\n            propertiesObj.put(\"distinct_id\", distinctId);\n        dataObj.put(\"properties\", propertiesObj);\n        JSONObject envelope = new JSONObject();\n        envelope.put(\"envelope_version\", 1);\n        envelope.put(\"message_type\", \"event\");\n        envelope.put(\"message\", dataObj);\n        return envelope;\n    } catch (JSONException e) {\n        throw new RuntimeException(\"Can't construct a Mixpanel message\", e);\n    }\n}",
        "rpfc": "/**\n * Creates a message tracking an event, for consumption by MixpanelAPI\n * See:\n *\n *    http://blog.mixpanel.com/2012/09/12/best-practices-updated/\n *\n * for a detailed discussion of event names, distinct ids, event properties, and how to use them\n * to get the most out of your metrics.\n *\n * @param distinctId a string uniquely identifying the individual cause associated with this event\n *           (for example, the user id of a signing-in user, or the hostname of a server)\n * @param eventName a human readable name for the event, for example \"Purchase\", or \"Threw Exception\"\n * @param properties a JSONObject associating properties with the event. These are useful\n *           for reporting and segmentation of events. It is often useful not only to include\n *           properties of the event itself (for example { 'Item Purchased' : 'Hat' } or\n *           { 'ExceptionType' : 'OutOfMemory' }), but also properties associated with the\n *           identified user (for example { 'MemberSince' : '2012-01-10' } or { 'TotalMemory' : '10TB' })\n */\npublic JSONObject event(String distinctId, String eventName, JSONObject properties) {\n    long time = System.currentTimeMillis() / 1000;\n    // Nothing below should EVER throw a JSONException.\n    try {\n        JSONObject dataObj = new JSONObject();\n        dataObj.put(\"event\", eventName);\n        JSONObject propertiesObj = null;\n        if (properties == null) {\n            propertiesObj = new JSONObject();\n        } else {\n            propertiesObj = new JSONObject(properties.toString());\n        }\n        if (!propertiesObj.has(\"token\"))\n            propertiesObj.put(\"token\", mToken);\n        if (!propertiesObj.has(\"time\"))\n            propertiesObj.put(\"time\", time);\n        if (!propertiesObj.has(\"mp_lib\"))\n            propertiesObj.put(\"mp_lib\", \"jdk\");\n        if (distinctId != null)\n            propertiesObj.put(\"distinct_id\", distinctId);\n        dataObj.put(\"properties\", propertiesObj);\n        JSONObject envelope = new JSONObject();\n        envelope.put(\"envelope_version\", 1);\n        envelope.put(\"message_type\", \"event\");\n        envelope.put(\"message\", dataObj);\n        return envelope;\n    } catch (JSONException e) {\n        throw new RuntimeException(\"Can't construct a Mixpanel message\", e);\n    }\n}",
        "tuc": "public void testClientDelivery() {\n    ClientDelivery c = new ClientDelivery();\n    try {\n        c.addMessage(mSampleProps);\n        fail(\"addMessage did not throw\");\n    } catch (MixpanelMessageException e) {\n        // This is expected, we pass\n    }\n    try {\n        JSONObject event = mBuilder.event(\"a distinct id\", \"login\", mSampleProps);\n        c.addMessage(event);\n        JSONObject set = mBuilder.set(\"a distinct id\", mSampleProps);\n        c.addMessage(set);\n        Map<String, Long> increments = new HashMap<String, Long>();\n        increments.put(\"a key\", 24L);\n        JSONObject increment = mBuilder.increment(\"a distinct id\", increments);\n        c.addMessage(increment);\n    } catch (MixpanelMessageException e) {\n        fail(\"Threw exception on valid message\");\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "ModelDriven___fUML-Reference-Implementation",
        "commit": "9bfef755f4d61878f496485c5bbd5ff6df06ce69",
        "commit_message": "Updated resolution to OMG Issue FUML12-33/19008.",
        "p_path": "org.modeldriven.fuml/src/main/java/fUML/Semantics/Loci/LociL1/Locus.java",
        "t_path": "org.modeldriven.fuml/src/test/java/org/modeldriven/fuml/test/builtin/ConditionalNodeTestCase.java",
        "p_name": "add",
        "t_name": "testconditionalnode",
        "lpfc": "public void add(fUML.Semantics.Classes.Kernel.ExtensionalValue value) {\n    value.locus = this;\n    this.extensionalValues.addValue(value);\n    IntegerValue integerValue = new IntegerValue();\n    integerValue.value = this.nextValueId;\n    value.identifier = this.identifier + \"#\" + integerValue.toString();\n    this.nextValueId = this.nextValueId + 1;\n}",
        "rpfc": "public void add(fUML.Semantics.Classes.Kernel.ExtensionalValue value) {\n    value.locus = this;\n    value.identifier = this.identifier + \"#\" + this.makeIdentifier(value);\n    this.extensionalValues.addValue(value);\n}",
        "tuc": "public void testConditionalNode() throws Exception {\n    log.info(\"testConditionalNode\");\n    initTestEnv.testSuite.testConditionalNode();\n    log.info(\"done\");\n}",
        "label": 0
    },
    {
        "repo_name": "apache___empire-db",
        "commit": "4784793d539bd53e73e64de4acf5ff3357c0da7a",
        "commit_message": "EMPIREDB-362 small fix\n",
        "p_path": "empire-db/src/main/java/org/apache/empire/db/DBRecord.java",
        "t_path": "empire-db/src/test/java/org/apache/empire/db/IntegerTest.java",
        "p_name": "update",
        "t_name": "testderby",
        "lpfc": "public void update() {\n    if (!isValid())\n        throw new ObjectNotValidException(this);\n    if (!isModified())\n        return;\n    if (isReadOnly())\n        throw new RecordReadOnlyException(this);\n    checkUpdateable();\n    if (enableRollbackHandling)\n        getContext().appendRollbackHandler(createRollbackHandler());\n    getRowSet().updateRecord(this);\n}",
        "rpfc": "public void update() {\n    if (!isValid())\n        throw new ObjectNotValidException(this);\n    if (!isModified())\n        return;\n    checkUpdateable();\n    if (enableRollbackHandling)\n        getContext().appendRollbackHandler(createRollbackHandler());\n    getRowSet().updateRecord(this);\n}",
        "tuc": "@Test\npublic void testDerby() {\n    SampleConfig config = new SampleConfig();\n    config.databaseProvider = \"derby\";\n    config.jdbcClass = \"org.apache.derby.jdbc.EmbeddedDriver\";\n    config.jdbcURL = \"jdbc:derby:memory:data/derby/test;create=true\";\n    testLongInteger(config);\n}",
        "label": 0
    },
    {
        "repo_name": "CityOfNewYork___geoclient",
        "commit": "c40b14fc5fc7a70e178a351bf8e5f3e14f4be129",
        "commit_message": "Small log message and level adjustments and Java code reformatting\n",
        "p_path": "geoclient-service/src/main/java/gov/nyc/doitt/gis/geoclient/service/invoker/GeosupportServiceImpl.java",
        "t_path": "geoclient-service/src/test/java/gov/nyc/doitt/gis/geoclient/service/invoker/GeosupportServiceImplTest.java",
        "p_name": "execute",
        "t_name": "testcallfunction1b_withborough",
        "lpfc": "public Map<String, Object> execute() {\n    Map<String, Object> requiredArguments = requiredArguments(function);\n    LOGGER.debug(\"{} requiredArguments {}\", id(), requiredArguments);\n    Map<String, Object> userArguments = userArguments();\n    LOGGER.info(\"{} arguments {}\", id(), userArguments);\n    Map<String, Object> allArguments = new HashMap<String, Object>();\n    allArguments.putAll(requiredArguments);\n    allArguments.putAll(userArguments);\n    LOGGER.debug(\"{} allArguments {}\", id(), allArguments);\n    Map<String, Object> result = function.call(allArguments);\n    LOGGER.debug(\"{} Geosupport result {}\", id(), allArguments);\n    latLongEnhancer.addLatLong(result);\n    LOGGER.info(\"{} result {}\", id(), result);\n    return result;\n}",
        "rpfc": "public Map<String, Object> execute() {\n    Map<String, Object> requiredArguments = requiredArguments(function);\n    LOGGER.debug(\"{} requiredArguments {}\", id(), requiredArguments);\n    Map<String, Object> userArguments = userArguments();\n    LOGGER.debug(\"{} userArguments {}\", id(), userArguments);\n    Map<String, Object> allArguments = new HashMap<String, Object>();\n    allArguments.putAll(requiredArguments);\n    allArguments.putAll(userArguments);\n    LOGGER.debug(\"{} allArguments {}\", id(), allArguments);\n    Map<String, Object> result = function.call(allArguments);\n    LOGGER.debug(\"{} result {}\", id(), result);\n    latLongEnhancer.addLatLong(result);\n    LOGGER.debug(\"{} result post lat/lon enhancement {}\", id(), result);\n    return result;\n}",
        "tuc": "@Test\npublic void testCallFunction1B_withBorough() {\n    Map<String, Object> arguments = new HashMap<String, Object>();\n    arguments.put(HOUSE_NUMBER, \"280\");\n    arguments.put(STREET_NAME, \"RIVERSIDE DRIVE\");\n    arguments.put(BOROUGH_CODE, 1);\n    AssertResult assertResult = mockFunctionCall(F1B, arguments);\n    Map<String, Object> actualResult = geosupportServiceImpl.callFunction1B(\"280\", \"RIVERSIDE DRIVE\", \"Manhattan\", null);\n    assertCall(actualResult, assertResult);\n}",
        "label": 0
    },
    {
        "repo_name": "mixpanel___mixpanel-java",
        "commit": "bec2151121e292b79e5e939946efc0120c568dbf",
        "commit_message": "Merge pull request #37 from mixpanel/jared-ms-updates\n\nsend millisecond time and update junit and json dependencies",
        "p_path": "src/main/java/com/mixpanel/mixpanelapi/MessageBuilder.java",
        "t_path": "src/test/java/com/mixpanel/mixpanelapi/MixpanelAPITest.java",
        "p_name": "event",
        "t_name": "testclientdelivery",
        "lpfc": "/**\n * Creates a message tracking an event, for consumption by MixpanelAPI\n * See:\n *\n *    https://help.mixpanel.com/hc/en-us/articles/360000857366-Guide-to-Mixpanel-Basics\n *\n * for a detailed discussion of event names, distinct ids, event properties, and how to use them\n * to get the most out of your metrics.\n *\n * @param distinctId a string uniquely identifying the individual cause associated with this event\n *           (for example, the user id of a signing-in user, or the hostname of a server)\n * @param eventName a human readable name for the event, for example \"Purchase\", or \"Threw Exception\"\n * @param properties a JSONObject associating properties with the event. These are useful\n *           for reporting and segmentation of events. It is often useful not only to include\n *           properties of the event itself (for example { 'Item Purchased' : 'Hat' } or\n *           { 'ExceptionType' : 'OutOfMemory' }), but also properties associated with the\n *           identified user (for example { 'MemberSince' : '2012-01-10' } or { 'TotalMemory' : '10TB' })\n * @return event message for consumption by MixpanelAPI\n */\npublic JSONObject event(String distinctId, String eventName, JSONObject properties) {\n    long time = System.currentTimeMillis() / 1000;\n    // Nothing below should EVER throw a JSONException.\n    try {\n        JSONObject dataObj = new JSONObject();\n        dataObj.put(\"event\", eventName);\n        JSONObject propertiesObj = null;\n        if (properties == null) {\n            propertiesObj = new JSONObject();\n        } else {\n            propertiesObj = new JSONObject(properties.toString());\n        }\n        if (!propertiesObj.has(\"token\"))\n            propertiesObj.put(\"token\", mToken);\n        if (!propertiesObj.has(\"time\"))\n            propertiesObj.put(\"time\", time);\n        if (!propertiesObj.has(\"mp_lib\"))\n            propertiesObj.put(\"mp_lib\", \"jdk\");\n        if (distinctId != null)\n            propertiesObj.put(\"distinct_id\", distinctId);\n        dataObj.put(\"properties\", propertiesObj);\n        JSONObject envelope = new JSONObject();\n        envelope.put(\"envelope_version\", 1);\n        envelope.put(\"message_type\", \"event\");\n        envelope.put(\"message\", dataObj);\n        return envelope;\n    } catch (JSONException e) {\n        throw new RuntimeException(\"Can't construct a Mixpanel message\", e);\n    }\n}",
        "rpfc": "/**\n * Creates a message tracking an event, for consumption by MixpanelAPI\n * See:\n *\n *    https://help.mixpanel.com/hc/en-us/articles/360000857366-Guide-to-Mixpanel-Basics\n *\n * for a detailed discussion of event names, distinct ids, event properties, and how to use them\n * to get the most out of your metrics.\n *\n * @param distinctId a string uniquely identifying the individual cause associated with this event\n *           (for example, the user id of a signing-in user, or the hostname of a server)\n * @param eventName a human readable name for the event, for example \"Purchase\", or \"Threw Exception\"\n * @param properties a JSONObject associating properties with the event. These are useful\n *           for reporting and segmentation of events. It is often useful not only to include\n *           properties of the event itself (for example { 'Item Purchased' : 'Hat' } or\n *           { 'ExceptionType' : 'OutOfMemory' }), but also properties associated with the\n *           identified user (for example { 'MemberSince' : '2012-01-10' } or { 'TotalMemory' : '10TB' })\n * @return event message for consumption by MixpanelAPI\n */\npublic JSONObject event(String distinctId, String eventName, JSONObject properties) {\n    long time = System.currentTimeMillis();\n    // Nothing below should EVER throw a JSONException.\n    try {\n        JSONObject dataObj = new JSONObject();\n        dataObj.put(\"event\", eventName);\n        JSONObject propertiesObj = null;\n        if (properties == null) {\n            propertiesObj = new JSONObject();\n        } else {\n            propertiesObj = new JSONObject(properties.toString());\n        }\n        if (!propertiesObj.has(\"token\"))\n            propertiesObj.put(\"token\", mToken);\n        if (!propertiesObj.has(\"time\"))\n            propertiesObj.put(\"time\", time);\n        if (!propertiesObj.has(\"mp_lib\"))\n            propertiesObj.put(\"mp_lib\", \"jdk\");\n        if (distinctId != null)\n            propertiesObj.put(\"distinct_id\", distinctId);\n        dataObj.put(\"properties\", propertiesObj);\n        JSONObject envelope = new JSONObject();\n        envelope.put(\"envelope_version\", 1);\n        envelope.put(\"message_type\", \"event\");\n        envelope.put(\"message\", dataObj);\n        return envelope;\n    } catch (JSONException e) {\n        throw new RuntimeException(\"Can't construct a Mixpanel message\", e);\n    }\n}",
        "tuc": "public void testClientDelivery() {\n    ClientDelivery c = new ClientDelivery();\n    try {\n        c.addMessage(mSampleProps);\n        fail(\"addMessage did not throw\");\n    } catch (MixpanelMessageException e) {\n        // This is expected, we pass\n    }\n    try {\n        JSONObject event = mBuilder.event(\"a distinct id\", \"login\", mSampleProps);\n        c.addMessage(event);\n        JSONObject set = mBuilder.set(\"a distinct id\", mSampleProps);\n        c.addMessage(set);\n        JSONObject groupSet = mBuilder.groupSet(\"company\", \"Acme Inc.\", mSampleProps);\n        c.addMessage(groupSet);\n        Map<String, Long> increments = new HashMap<String, Long>();\n        increments.put(\"a key\", 24L);\n        JSONObject increment = mBuilder.increment(\"a distinct id\", increments);\n        c.addMessage(increment);\n    } catch (MixpanelMessageException e) {\n        fail(\"Threw exception on valid message\");\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "karussell___Jetwick",
        "commit": "14d0de6f5451a36e5df7cbed9bc53f86717c3ae2",
        "commit_message": "option to bulk or single index tweets\n",
        "p_path": "src/main/java/de/jetwick/es/ElasticTweetSearch.java",
        "t_path": "src/test/java/de/jetwick/es/ElasticTweetSearchTest.java",
        "p_name": "update",
        "t_name": "testaddoldtweetsifpersistent",
        "lpfc": "public void update(Collection<SolrTweet> tweets) {\n    try {\n        if (tweets.isEmpty())\n            return;\n        tweets = new SerialCommandExecutor(tweets).add(new TermCreateCommand()).execute();\n        BulkRequestBuilder brb = client.prepareBulk();\n        for (SolrTweet tw : tweets) {\n            String id = Long.toString(tw.getTwitterId());\n            XContentBuilder source = createDoc(tw);\n            brb.add(Requests.indexRequest(getIndexName()).type(getIndexType()).id(id).source(source));\n        }\n        if (brb.numberOfActions() > 0)\n            brb.execute().actionGet();\n    } catch (Exception e) {\n        logger.error(\"Exception while updating.\", e);\n    }\n}",
        "rpfc": "public void update(Collection<SolrTweet> tweets) {\n    try {\n        if (tweets.isEmpty())\n            return;\n        tweets = new SerialCommandExecutor(tweets).add(new TermCreateCommand()).execute();\n        boolean bulk = true;\n        if (bulk) {\n            BulkRequestBuilder brb = client.prepareBulk();\n            for (SolrTweet tw : tweets) {\n                String id = Long.toString(tw.getTwitterId());\n                XContentBuilder source = createDoc(tw);\n                brb.add(Requests.indexRequest(getIndexName()).type(getIndexType()).id(id).source(source));\n            }\n            if (brb.numberOfActions() > 0)\n                brb.execute().actionGet();\n        } else {\n            for (SolrTweet tw : tweets) {\n                String id = Long.toString(tw.getTwitterId());\n                feedDoc(id, createDoc(tw));\n            }\n        }\n    } catch (Exception e) {\n        logger.error(\"Exception while updating.\", e);\n    }\n}",
        "tuc": "@Test\npublic void testAddOldTweetsIfPersistent() throws SolrServerException {\n    SolrTweet tw = createTweet(2L, \"RT @userA: bla bli blu\", \"userB\");\n    Date dt = new MyDate().minusDays(2).toDate();\n    tw.setUpdatedAt(dt);\n    tw.setCreatedAt(dt);\n    assertEquals(1, twSearch.update(tw).size());\n    // testOverwriteTweetsIfPersistent\n    tw = createTweet(2L, \"totally new\", \"userB\");\n    dt = new MyDate().minusDays(2).toDate();\n    tw.setUpdatedAt(dt);\n    tw.setCreatedAt(dt);\n    assertEquals(1, twSearch.update(tw).size());\n    assertEquals(0, twSearch.search(\"bla\").size());\n    assertEquals(1, twSearch.search(\"new\").size());\n}",
        "label": 0
    },
    {
        "repo_name": "killbilling___recurly-java-library",
        "commit": "af7d25dff245056f1fc458c7b39f29e2de44d533",
        "commit_message": "now Recurly Ning lib support support JPY\n",
        "p_path": "src/main/java/com/ning/billing/recurly/model/RecurlyUnitCurrency.java",
        "t_path": "src/test/java/com/ning/billing/recurly/TestRecurlyClient.java",
        "p_name": "build",
        "t_name": "testaddons",
        "lpfc": "public static RecurlyUnitCurrency build(@Nullable final Object unitAmountInCents) {\n    if (RecurlyObject.isNull(unitAmountInCents)) {\n        return null;\n    }\n    if (unitAmountInCents instanceof RecurlyUnitCurrency) {\n        return (RecurlyUnitCurrency) unitAmountInCents;\n    }\n    final RecurlyUnitCurrency recurlyUnitCurrency = new RecurlyUnitCurrency();\n    if (unitAmountInCents instanceof Map) {\n        final Map amounts = (Map) unitAmountInCents;\n        recurlyUnitCurrency.setUnitAmountUSD(amounts.get(\"USD\"));\n        recurlyUnitCurrency.setUnitAmountAUD(amounts.get(\"AUD\"));\n        recurlyUnitCurrency.setUnitAmountCAD(amounts.get(\"CAD\"));\n        recurlyUnitCurrency.setUnitAmountEUR(amounts.get(\"EUR\"));\n        recurlyUnitCurrency.setUnitAmountGBP(amounts.get(\"GBP\"));\n        recurlyUnitCurrency.setUnitAmountCZK(amounts.get(\"CZK\"));\n        recurlyUnitCurrency.setUnitAmountDKK(amounts.get(\"DKK\"));\n        recurlyUnitCurrency.setUnitAmountHUF(amounts.get(\"HUF\"));\n        recurlyUnitCurrency.setUnitAmountNOK(amounts.get(\"NOK\"));\n        recurlyUnitCurrency.setUnitAmountNZD(amounts.get(\"NZD\"));\n        recurlyUnitCurrency.setUnitAmountPLN(amounts.get(\"PLN\"));\n        recurlyUnitCurrency.setUnitAmountSGD(amounts.get(\"SGD\"));\n        recurlyUnitCurrency.setUnitAmountSEK(amounts.get(\"SEK\"));\n        recurlyUnitCurrency.setUnitAmountCHF(amounts.get(\"CHF\"));\n        recurlyUnitCurrency.setUnitAmountZAR(amounts.get(\"ZAR\"));\n    }\n    return recurlyUnitCurrency;\n}",
        "rpfc": "public static RecurlyUnitCurrency build(@Nullable final Object unitAmountInCents) {\n    if (RecurlyObject.isNull(unitAmountInCents)) {\n        return null;\n    }\n    if (unitAmountInCents instanceof RecurlyUnitCurrency) {\n        return (RecurlyUnitCurrency) unitAmountInCents;\n    }\n    final RecurlyUnitCurrency recurlyUnitCurrency = new RecurlyUnitCurrency();\n    if (unitAmountInCents instanceof Map) {\n        final Map amounts = (Map) unitAmountInCents;\n        recurlyUnitCurrency.setUnitAmountUSD(amounts.get(\"USD\"));\n        recurlyUnitCurrency.setUnitAmountAUD(amounts.get(\"AUD\"));\n        recurlyUnitCurrency.setUnitAmountCAD(amounts.get(\"CAD\"));\n        recurlyUnitCurrency.setUnitAmountEUR(amounts.get(\"EUR\"));\n        recurlyUnitCurrency.setUnitAmountGBP(amounts.get(\"GBP\"));\n        recurlyUnitCurrency.setUnitAmountCZK(amounts.get(\"CZK\"));\n        recurlyUnitCurrency.setUnitAmountDKK(amounts.get(\"DKK\"));\n        recurlyUnitCurrency.setUnitAmountHUF(amounts.get(\"HUF\"));\n        recurlyUnitCurrency.setUnitAmountNOK(amounts.get(\"NOK\"));\n        recurlyUnitCurrency.setUnitAmountNZD(amounts.get(\"NZD\"));\n        recurlyUnitCurrency.setUnitAmountPLN(amounts.get(\"PLN\"));\n        recurlyUnitCurrency.setUnitAmountSGD(amounts.get(\"SGD\"));\n        recurlyUnitCurrency.setUnitAmountSEK(amounts.get(\"SEK\"));\n        recurlyUnitCurrency.setUnitAmountCHF(amounts.get(\"CHF\"));\n        recurlyUnitCurrency.setUnitAmountZAR(amounts.get(\"ZAR\"));\n        recurlyUnitCurrency.setUnitAmountJPY(amounts.get(\"JPY\"));\n    }\n    return recurlyUnitCurrency;\n}",
        "tuc": "@Test(groups = \"integration\")\npublic void testAddons() throws Exception {\n    // Create a Plan\n    final Plan planData = TestUtils.createRandomPlan();\n    final AddOn addOn = TestUtils.createRandomAddOn();\n    try {\n        // Create an AddOn\n        final Plan plan = recurlyClient.createPlan(planData);\n        AddOn addOnRecurly = recurlyClient.createPlanAddOn(plan.getPlanCode(), addOn);\n        // Test the creation\n        Assert.assertNotNull(addOnRecurly);\n        Assert.assertEquals(addOnRecurly.getAddOnCode(), addOn.getAddOnCode());\n        Assert.assertEquals(addOnRecurly.getName(), addOn.getName());\n        Assert.assertEquals(addOnRecurly.getUnitAmountInCents(), addOn.getUnitAmountInCents());\n        Assert.assertEquals(addOnRecurly.getDefaultQuantity(), addOn.getDefaultQuantity());\n        // Query for an AddOn\n        addOnRecurly = recurlyClient.getAddOn(plan.getPlanCode(), addOn.getAddOnCode());\n        // Check the 2 are the same\n        Assert.assertEquals(addOnRecurly.getAddOnCode(), addOn.getAddOnCode());\n        Assert.assertEquals(addOnRecurly.getName(), addOn.getName());\n        Assert.assertEquals(addOnRecurly.getDefaultQuantity(), addOn.getDefaultQuantity());\n        Assert.assertEquals(addOnRecurly.getDisplayQuantityOnHostedPage(), addOn.getDisplayQuantityOnHostedPage());\n        Assert.assertEquals(addOnRecurly.getUnitAmountInCents(), addOn.getUnitAmountInCents());\n        // Query for AddOns and Check again\n        AddOns addOns = recurlyClient.getAddOns(plan.getPlanCode());\n        Assert.assertEquals(addOns.size(), 1);\n        Assert.assertEquals(addOns.get(0).getAddOnCode(), addOn.getAddOnCode());\n        Assert.assertEquals(addOns.get(0).getName(), addOn.getName());\n        Assert.assertEquals(addOns.get(0).getDefaultQuantity(), addOn.getDefaultQuantity());\n        Assert.assertEquals(addOns.get(0).getDisplayQuantityOnHostedPage(), addOn.getDisplayQuantityOnHostedPage());\n        Assert.assertEquals(addOns.get(0).getUnitAmountInCents(), addOn.getUnitAmountInCents());\n    } finally {\n        // Delete an AddOn\n        recurlyClient.deleteAddOn(planData.getPlanCode(), addOn.getAddOnCode());\n        // Delete the plan\n        recurlyClient.deletePlan(planData.getPlanCode());\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "Nastel___remoraj",
        "commit": "36a01f2636366bc980611dcd137c651d094efaa3",
        "commit_message": "Merge branch 'master' of https://github.com/Nastel/remoraj\n",
        "p_path": "remora-core/src/main/java/com/jkoolcloud/remora/core/output/ChronicleOutput.java",
        "t_path": "remora-core/src/test/java/com/jkoolcloud/remora/core/output/ChronicleOutputTest.java",
        "p_name": "send",
        "t_name": "testrolling",
        "lpfc": "@Override\npublic void send(EntryDefinition entry) {\n    try {\n        appender.writeDocument(entry);\n    } catch (UnrecoverableTimeoutException e) {\n        logger.error(e);\n    }\n}",
        "rpfc": "@Override\npublic void send(EntryDefinition entry) {\n    queueWorkers.submit(new Runnable() {\n        @Override\n        public void run() {\n            try {\n                ((ChronicleAppenderThread) Thread.currentThread()).getAppender().writeDocument(entry);\n            } catch (Exception e) {\n                failCount.incrementAndGet();\n            }\n        }\n    });\n}",
        "tuc": "@Test\npublic void testRolling() throws InterruptedException {\n    ChronicleOutput output = new ChronicleOutput();\n    output.rollCycle = RollCycles.TEST_SECONDLY;\n    output.keepQueueRolls = 2;\n    File tempDir = Files.createTempDir();\n    tempDir.deleteOnExit();\n    System.out.println(tempDir.getAbsolutePath());\n    output.queuePath = tempDir.getPath();\n    output.init();\n    for (int i = 0; i <= 50; i++) {\n        output.send(new EntryDefinition(ChronicleOutputTest.class));\n        Thread.sleep(100);\n    }\n    // +1 = metadata; +1 = current\n    assertEquals(tempDir.list().length, output.keepQueueRolls.intValue() + 1 + 1);\n    output.shutdown();\n    // +1 = current after shutdown\n    assertEquals(tempDir.list().length, output.keepQueueRolls.intValue() + 1);\n    // queue\n    Arrays.asList(tempDir.listFiles()).forEach(file -> {\n        while (!file.delete()) {\n            // make sure it deletes\n        }\n    });\n    tempDir.delete();\n}",
        "label": 0
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "3ba5ae9310759273fdb521487f105b3edc9111e3",
        "commit_message": "feat(QiunetDatas): DbEntityBo \u52a0\u5165delete\u7684\u72b6\u6001. \u5904\u7406delete\u540e\u53ef\u80fd\u8fdb\u884cupdate\u7684\u95ee\u9898.\n",
        "p_path": "QiunetDatas/src/main/java/org/qiunet/data/db/loader/DbEntityBo.java",
        "t_path": "QiunetDatas/src/test/java/org/qiunet/data/db/TestDbDataSupport.java",
        "p_name": "update",
        "t_name": "testentity",
        "lpfc": "@Override\npublic void update() {\n    if (playerDataLoader == null) {\n        this.serialize();\n        getDo().update();\n        return;\n    }\n    if (entityStatus() == EntityStatus.INIT) {\n        throw new CustomException(\"Need insert first!\");\n    }\n    if (atomicStatus.compareAndSet(EntityStatus.NORMAL, EntityStatus.UPDATE)) {\n        playerDataLoader.cacheAsyncToDb.add(PlayerDataLoader.EntityOperate.UPDATE, this);\n    }\n}",
        "rpfc": "@Override\npublic void update() {\n    if (playerDataLoader == null) {\n        this.serialize();\n        getDo().update();\n        return;\n    }\n    if (entityStatus() == EntityStatus.INIT) {\n        throw new CustomException(\"Need insert first!\");\n    }\n    if (delete) {\n        throw new CustomException(\"Entity already deleted!!\");\n    }\n    if (atomicStatus.compareAndSet(EntityStatus.NORMAL, EntityStatus.UPDATE)) {\n        playerDataLoader.cacheAsyncToDb.add(PlayerDataLoader.EntityOperate.UPDATE, this);\n    }\n}",
        "tuc": "@Test\npublic void testEntity() {\n    String name = \"\u79cb\u9633\";\n    PlayerDo playerDo = new PlayerDo();\n    playerDo.setExp(1111111111111L);\n    playerDo.setName(\"\u79cb\u96331\");\n    playerDo.setLevel(10);\n    playerDo.setUid(playerDataLoader.getPlayerId());\n    PlayerBo playerBo = playerDataLoader.insertDo(playerDo);\n    playerBo.getDo().setName(name);\n    playerBo.getDo().setLevel(100);\n    playerBo.update();\n    PlayerBo bo = playerDataLoader.getData(PlayerBo.class);\n    Assert.assertEquals(bo.getDo().getName(), name);\n    Assert.assertEquals(bo.getDo().getLevel(), 100);\n    bo.delete();\n    bo = playerDataLoader.getData(PlayerBo.class);\n    Assert.assertNull(bo);\n}",
        "label": 0
    },
    {
        "repo_name": "SvenEwald___xmlbeam",
        "commit": "63ca74b2675c5b6741d335c0e516e562b8ae8ba3",
        "commit_message": "moving invocation handler methods to invocation classes",
        "p_path": "src/main/java/org/xmlbeam/ProjectionInvocationHandler.java",
        "t_path": "src/test/java/org/xmlbeam/refcards/FAQSnippets.java",
        "p_name": "invoke",
        "t_name": "uncamelcasetest",
        "lpfc": "@Override\npublic Object invoke(final Object proxy, final Method method, final Object[] args) throws Throwable {\n    final String resolvedXpath = applyParams(resolveXPath(args), method, args);\n    return invokeSetter(proxy, method, resolvedXpath, args);\n}",
        "rpfc": "@Override\npublic Object invoke(final Object proxy, final Method method, final Object[] args) throws Throwable {\n    final String resolvedXpath = applyParams(resolveXPath(args), method, args);\n    if (!ReflectionHelper.hasParameters(method)) {\n        throw new IllegalArgumentException(\"Method \" + method + \" was invoked as setter but has no parameter. Please add a parameter so this method could actually change the DOM.\");\n    }\n    if (method.getAnnotation(XBDocURL.class) != null) {\n        throw new IllegalArgumentException(\"Method \" + method + \" was invoked as setter but has a @\" + XBDocURL.class.getSimpleName() + \" annotation. Defining setters on external projections is not valid because there is no DOM attached.\");\n    }\n    final String pathToElement = resolvedXpath.replaceAll(\"\\\\[@\", \"[attribute::\").replaceAll(\"/?@.*\", \"\").replaceAll(\"\\\\[attribute::\", \"[@\");\n    final Document document = DOMHelper.getOwnerDocumentFor(node);\n    assert document != null;\n    final int findIndexOfValue = findIndexOfValue(method);\n    final Object valueToSet = args[findIndexOfValue];\n    final boolean isMultiValue = isMultiValue(method.getParameterTypes()[findIndexOfValue]);\n    if (\"/*\".equals(resolvedXpath)) {\n        if (isMultiValue) {\n            throw new IllegalArgumentException(\"Method \" + method + \" was invoked as setter changing the document root element, but tries to set multiple values.\");\n        }\n        return handeRootElementReplacement(proxy, method, document, valueToSet);\n    }\n    final boolean wildCardTarget = resolvedXpath.endsWith(\"/*\");\n    try {\n        final DuplexExpression duplexExpression = wildCardTarget ? new DuplexXPathParser().compile(resolvedXpath.substring(0, resolvedXpath.length() - 2)) : new DuplexXPathParser().compile(resolvedXpath);\n        if (duplexExpression.getExpressionType().isMustEvalAsString()) {\n            throw new XBPathException(\"Unwriteable xpath selector used \", method, resolvedXpath);\n        }\n        if (isMultiValue) {\n            if (duplexExpression.getExpressionType().equals(ExpressionType.ATTRIBUTE)) {\n                throw new IllegalArgumentException(\"Method \" + method + \" was invoked as setter changing some attribute, but was declared to set multiple values. I can not create multiple attributes for one path.\");\n            }\n            final Collection<?> collection2Set = valueToSet == null ? Collections.emptyList() : (valueToSet.getClass().isArray()) ? ReflectionHelper.array2ObjectList(valueToSet) : (Collection<?>) valueToSet;\n            if (wildCardTarget) {\n                final Element parentElement = (Element) duplexExpression.ensureExistence(node);\n                DOMHelper.removeAllChildren(parentElement);\n                int count = 0;\n                for (Object o : collection2Set) {\n                    if (o == null) {\n                        continue;\n                    }\n                    ++count;\n                    if (o instanceof Node) {\n                        DOMHelper.appendClone(parentElement, (Node) o);\n                        continue;\n                    }\n                    if (o instanceof InternalProjection) {\n                        DOMHelper.appendClone(parentElement, ((InternalProjection) o).getDOMBaseElement());\n                        continue;\n                    }\n                    throw new XBPathException(\"When using a wildcard target, the type to set must be a DOM Node or another projection. Otherwise I can not determine the element name.\", method, resolvedXpath);\n                }\n                return getProxyReturnValueForMethod(proxy, method, Integer.valueOf(count));\n            }\n            final Element parentElement = duplexExpression.ensureParentExistence(node);\n            duplexExpression.deleteAllMatchingChildren(parentElement);\n            int count = applyCollectionSetOnElement(collection2Set, parentElement, duplexExpression);\n            return getProxyReturnValueForMethod(proxy, method, Integer.valueOf(count));\n        }\n        if (duplexExpression.getExpressionType().equals(ExpressionType.ATTRIBUTE)) {\n            if (wildCardTarget) {\n                throw new XBPathException(\"Wildcards are not allowed when writing to an attribute. I need to know to which Element I should set the attribute\", method, resolvedXpath);\n            }\n            Attr attribute = (Attr) duplexExpression.ensureExistence(node);\n            if (valueToSet == null) {\n                attribute.getOwnerElement().removeAttributeNode(attribute);\n                return getProxyReturnValueForMethod(proxy, method, Integer.valueOf(1));\n            }\n            DOMHelper.setStringValue(attribute, valueToSet.toString());\n            return getProxyReturnValueForMethod(proxy, method, Integer.valueOf(1));\n        }\n        if ((valueToSet instanceof Node) || (valueToSet instanceof InternalProjection)) {\n            if (valueToSet instanceof Attr) {\n                if (wildCardTarget) {\n                    throw new XBPathException(\"Wildcards are not allowed when writing an attribute. I need to know to which Element I should set the attribute\", method, resolvedXpath);\n                }\n                Element parentNode = duplexExpression.ensureParentExistence(node);\n                if (((Attr) valueToSet).getNamespaceURI() != null) {\n                    parentNode.setAttributeNodeNS((Attr) valueToSet);\n                    return getProxyReturnValueForMethod(proxy, method, Integer.valueOf(1));\n                }\n                parentNode.setAttributeNode((Attr) valueToSet);\n                return getProxyReturnValueForMethod(proxy, method, Integer.valueOf(1));\n            }\n            final Element newNodeOrigin = valueToSet instanceof InternalProjection ? ((InternalProjection) valueToSet).getDOMBaseElement() : (Element) valueToSet;\n            final Element newNode = (Element) newNodeOrigin.cloneNode(true);\n            DOMHelper.ensureOwnership(document, newNode);\n            if (wildCardTarget) {\n                Element parentElement = (Element) duplexExpression.ensureExistence(node);\n                DOMHelper.removeAllChildren(parentElement);\n                parentElement.appendChild(newNode);\n                return getProxyReturnValueForMethod(proxy, method, Integer.valueOf(1));\n            }\n            Element previousElement = (Element) duplexExpression.ensureExistence(node);\n            DOMHelper.replaceElement(previousElement, newNode);\n            return getProxyReturnValueForMethod(proxy, method, Integer.valueOf(1));\n        }\n        final Element elementToChange = (Element) duplexExpression.ensureExistence(node);\n        if (valueToSet == null) {\n            DOMHelper.removeAllChildren(elementToChange);\n        } else {\n            elementToChange.setTextContent(valueToSet.toString());\n        }\n        return getProxyReturnValueForMethod(proxy, method, Integer.valueOf(1));\n    } catch (XBPathParsingException e) {\n        throw new XBPathException(e, method, pathToElement);\n    }\n}",
        "tuc": "@Test\npublic void UnCamelCaseTest() {\n    XBProjector projector = new XBProjector();\n    projector.config().setExternalizer(new ExternalizerAdapter() {\n        @Override\n        public String resolveXPath(String annotationValue, Method method, Object[] args) {\n            // Simplest conversion of camel case getter to xpath expression.\n            return method.getName().substring(3).replaceAll(\"[A-Z]\", \"/$0\");\n        }\n    });\n    List<String> departmentUsers = projector.projectXMLString(\"<Department><Users><Name>John Doe</Name><Name>Tommy Atkins</Name></Users></Department>\", ExampleProjection.class).getDepartmentUsersName();\n    assertTrue(departmentUsers.size() == 2);\n    assertEquals(\"John Doe\", departmentUsers.get(0));\n    assertEquals(\"Tommy Atkins\", departmentUsers.get(1));\n}",
        "label": 0
    },
    {
        "repo_name": "igniterealtime___tinder",
        "commit": "eb4ec66bd55b5e7b2a4e53c540d1313aaafdcf99",
        "commit_message": "TINDER-67: Exception messages caused by invalid input should include the offending value.\n",
        "p_path": "src/main/java/org/xmpp/packet/JID.java",
        "t_path": "src/test/java/org/xmpp/packet/NodePrepTest.java",
        "p_name": "nodeprep",
        "t_name": "testcachedresult",
        "lpfc": "public static String nodeprep(String node) {\n    if (node == null) {\n        return null;\n    }\n    final ValueWrapper<String> cachedResult = NODEPREP_CACHE.get(node);\n    final String answer;\n    if (cachedResult == null) {\n        try {\n            answer = Stringprep.nodeprep(node);\n            if (answer != null && answer.getBytes(\"UTF-8\").length > 1023) {\n                throw new IllegalArgumentException(\"Node cannot be larger \" + \"than 1023 bytes. Size is \" + answer.getBytes(\"UTF-8\").length + \" bytes.\");\n            }\n        } catch (UnsupportedEncodingException ex) {\n            throw new IllegalStateException(\"Unable to construct a JID node.\", ex);\n        } catch (Exception ex) {\n            NODEPREP_CACHE.put(node, new ValueWrapper<String>(Representation.ILLEGAL));\n            throw new IllegalArgumentException(\"The input is not a valid JID node: \" + node, ex);\n        }\n        NODEPREP_CACHE.put(answer, new ValueWrapper<String>(Representation.USE_KEY));\n        if (!node.equals(answer)) {\n            NODEPREP_CACHE.put(node, new ValueWrapper<String>(answer));\n        }\n    } else {\n        switch(cachedResult.getRepresentation()) {\n            case USE_KEY:\n                answer = node;\n                break;\n            case USE_VALUE:\n                answer = cachedResult.getValue();\n                break;\n            case ILLEGAL:\n                throw new IllegalArgumentException(\"The input is not a valid JID node: \" + node);\n            default:\n                throw new IllegalStateException(\"The implementation of JID#nodeprep(String) is broken.\");\n        }\n    }\n    return answer;\n}",
        "rpfc": "public static String nodeprep(String node) {\n    if (node == null) {\n        return null;\n    }\n    final ValueWrapper<String> cachedResult = NODEPREP_CACHE.get(node);\n    final String answer;\n    if (cachedResult == null) {\n        try {\n            answer = Stringprep.nodeprep(node);\n            if (answer != null && answer.getBytes(\"UTF-8\").length > 1023) {\n                throw new IllegalArgumentException(\"Node cannot be larger \" + \"than 1023 bytes (after nodeprepping). Size is \" + answer.getBytes(\"UTF-8\").length + \" bytes. Offending value: '\" + node + \"'\");\n            }\n        } catch (UnsupportedEncodingException ex) {\n            throw new IllegalStateException(\"Unable to construct a JID node. Offending value: '\" + node + \"'\", ex);\n        } catch (Exception ex) {\n            NODEPREP_CACHE.put(node, new ValueWrapper<String>(Representation.ILLEGAL));\n            throw new IllegalArgumentException(\"The input is not a valid JID node: \" + node, ex);\n        }\n        NODEPREP_CACHE.put(answer, new ValueWrapper<String>(Representation.USE_KEY));\n        if (!node.equals(answer)) {\n            NODEPREP_CACHE.put(node, new ValueWrapper<String>(answer));\n        }\n    } else {\n        switch(cachedResult.getRepresentation()) {\n            case USE_KEY:\n                answer = node;\n                break;\n            case USE_VALUE:\n                answer = cachedResult.getValue();\n                break;\n            case ILLEGAL:\n                throw new IllegalArgumentException(\"The input is not a valid JID node: \" + node);\n            default:\n                throw new IllegalStateException(\"The implementation of JID#nodeprep(String) is broken.\");\n        }\n    }\n    return answer;\n}",
        "tuc": "/**\n * Checks cache usage, by making sure that a subsequent request returns the\n * stringprepped answer, not the input data. Input data often equals the\n * prepped answer, which allows a bug like this to slip by easily.\n */\n@Test\npublic void testCachedResult() throws Exception {\n    // setup;\n    final String input = \"bword\\u2060joiner\";\n    // do magic\n    final String result1 = JID.nodeprep(input);\n    final String result2 = JID.nodeprep(input);\n    // verify\n    assertEquals(\"bwordjoiner\", result1);\n    assertEquals(result1, result2);\n}",
        "label": 0
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "134905efec7f666cbff58dcb632d239330dfec24",
        "commit_message": "\u6e05\u7a7a\u6570\u7ec4, \u5e76\u91ca\u653e\u5185\u5b58\n",
        "p_path": "QiunetUtils/src/main/java/org/qiunet/utils/classScanner/ScannerAllClassFile.java",
        "t_path": "QiunetUtils/src/test/java/org/qiunet/utils/classScanner/TestClassScanner.java",
        "p_name": "scanner",
        "t_name": "testclassscanner",
        "lpfc": "public void scanner() throws ClassNotFoundException {\n    if (scannerHanderList.isEmpty())\n        return;\n    for (String clazz : allclass) {\n        Class c = loader.loadClass(clazz);\n        for (IScannerHandler handler : scannerHanderList) {\n            if (handler.matchClazz(c))\n                handler.handler(c);\n        }\n    }\n    this.allclass.clear();\n}",
        "rpfc": "public void scanner() throws ClassNotFoundException {\n    if (scannerHanderList.isEmpty())\n        return;\n    for (String clazz : allclass) {\n        Class c = loader.loadClass(clazz);\n        for (IScannerHandler handler : scannerHanderList) {\n            if (handler.matchClazz(c))\n                handler.handler(c);\n        }\n    }\n    this.allclass = null;\n}",
        "tuc": "@Test\npublic void testClassScanner() {\n    ScannerAllClassFile scannerAllClassFile = new ScannerAllClassFile();\n    scannerAllClassFile.addScannerHandler(new ActionScannerHandler());\n    try {\n        scannerAllClassFile.scanner();\n    } catch (ClassNotFoundException e) {\n        e.printStackTrace();\n    }\n    Assert.assertNotNull(clazzName);\n    if (clazzName != null) {\n        Assert.assertEquals(\"PlayerHandler\", clazzName);\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "pkiraly___metadata-qa-api",
        "commit": "1bcae038cdc11393e1e208c0ea02a79717265599",
        "commit_message": "Implementing Sonar suggestions\n",
        "p_path": "src/main/java/de/gwdg/metadataqa/api/calculator/CalculatorFacade.java",
        "t_path": "src/test/java/de/gwdg/metadataqa/api/calculator/CalculatorFacadeTest.java",
        "p_name": "configure",
        "t_name": "testchanged",
        "lpfc": "public void configure() {\n    LOGGER.info(\"configure()\");\n    calculators = new ArrayList<>();\n    if (fieldExtractorEnabled) {\n        fieldExtractor = new FieldExtractor(schema);\n        calculators.add(fieldExtractor);\n    }\n    if (completenessMeasurementEnabled) {\n        completenessCalculator = new CompletenessCalculator(schema);\n        completenessCalculator.collectFields(completenessCollectFields);\n        completenessCalculator.setExistence(fieldExistenceMeasurementEnabled);\n        completenessCalculator.setCardinality(fieldCardinalityMeasurementEnabled);\n        calculators.add(completenessCalculator);\n    }\n    if (tfIdfMeasurementEnabled) {\n        tfidfCalculator = new TfIdfCalculator(schema);\n        if (solrConfiguration != null) {\n            tfidfCalculator.setSolrConfiguration(solrConfiguration);\n        } else {\n            throw new IllegalArgumentException(\"If TF-IDF measurement is enabled, Solr configuration should not be null.\");\n        }\n        tfidfCalculator.enableTermCollection(collectTfIdfTerms);\n        calculators.add(tfidfCalculator);\n    }\n    if (problemCatalogMeasurementEnabled && schema instanceof EdmSchema) {\n        var problemCatalog = new ProblemCatalog((EdmSchema) schema);\n        new LongSubject(problemCatalog);\n        new TitleAndDescriptionAreSame(problemCatalog);\n        new EmptyStrings(problemCatalog);\n        calculators.add(problemCatalog);\n    }\n    if (ruleCatalogMeasurementEnabled) {\n        calculators.add(new RuleCatalog(schema));\n    }\n    if (languageMeasurementEnabled) {\n        languageCalculator = new LanguageCalculator(schema);\n        calculators.add(languageCalculator);\n    }\n    if (multilingualSaturationMeasurementEnabled) {\n        multilingualSaturationCalculator = new MultilingualitySaturationCalculator(schema);\n        if (saturationExtendedResult) {\n            multilingualSaturationCalculator.setResultType(MultilingualitySaturationCalculator.ResultTypes.EXTENDED);\n        }\n        calculators.add(multilingualSaturationCalculator);\n    }\n    if (uniquenessMeasurementEnabled) {\n        if (solrClient == null && solrConfiguration == null) {\n            throw new IllegalArgumentException(\"If Uniqueness measurement is enabled, Solr configuration should not be null.\");\n        }\n        if (solrClient == null) {\n            solrClient = new DefaultSolrClient(solrConfiguration);\n        }\n        calculators.add(new UniquenessCalculator(solrClient, schema));\n    }\n}",
        "rpfc": "public void configure() {\n    LOGGER.info(\"configure()\");\n    calculators = new ArrayList<>();\n    addExtractor();\n    addCompleteness();\n    addTfIdfMeasurement();\n    addProblemCatalogMeasurement();\n    addRuleCatalogMeasurement();\n    addLanguageMeasurement();\n    addMultilingualSaturationMeasurement();\n    addUniquenessMeasurement();\n}",
        "tuc": "@Test\npublic void testChanged() {\n    CalculatorFacade calculator = new CalculatorFacade().setSchema(new EdmOaiPmhJsonSchema()).disableFieldExtractor();\n    assertFalse(calculator.isTfIdfMeasurementEnabled());\n    calculator.configure();\n    List<Calculator> calculators = calculator.getCalculators();\n    assertEquals(1, calculators.size());\n    calculator.enableTfIdfMeasurement();\n    calculator.configureSolr(\"localhost\", \"8983\", \"solr/europeana\");\n    calculator.conditionalConfiguration();\n    calculators = calculator.getCalculators();\n    assertEquals(2, calculators.size());\n    calculator.conditionalConfiguration();\n    calculators = calculator.getCalculators();\n    assertEquals(2, calculators.size());\n}",
        "label": 0
    },
    {
        "repo_name": "SvenEwald___xmlbeam",
        "commit": "db8f59ff72bad1163d5e67d74fc244e718d642be",
        "commit_message": "more refactorings",
        "p_path": "src/main/java/org/xmlbeam/ProjectionInvocationHandler.java",
        "t_path": "src/test/java/org/xmlbeam/refcards/FAQSnippets.java",
        "p_name": "invoke",
        "t_name": "uncamelcasetest",
        "lpfc": "@Override\npublic Object invoke(final Object proxy, final Method method, final Object[] args) throws Throwable {\n    return method.invoke(obj, args);\n}",
        "rpfc": "@Override\npublic Object invoke(final Object proxy, final Method method, final Object[] args) throws Throwable {\n    return this.method.invoke(obj, args);\n}",
        "tuc": "@Test\npublic void UnCamelCaseTest() {\n    XBProjector projector = new XBProjector();\n    projector.config().setExternalizer(new ExternalizerAdapter() {\n        @Override\n        public String resolveXPath(String annotationValue, Method method, Object[] args) {\n            // Simplest conversion of camel case getter to xpath expression.\n            return method.getName().substring(3).replaceAll(\"[A-Z]\", \"/$0\");\n        }\n    });\n    List<String> departmentUsers = projector.projectXMLString(\"<Department><Users><Name>John Doe</Name><Name>Tommy Atkins</Name></Users></Department>\", ExampleProjection.class).getDepartmentUsersName();\n    assertTrue(departmentUsers.size() == 2);\n    assertEquals(\"John Doe\", departmentUsers.get(0));\n    assertEquals(\"Tommy Atkins\", departmentUsers.get(1));\n}",
        "label": 0
    },
    {
        "repo_name": "SAP___fosstars-rating-core",
        "commit": "0d28e118ddbf158a10efe38d4828a629a3371b0c",
        "commit_message": "Fixes ConcurrentModificationException issue in GitHubDataFetcher\n\nFixes issue #206\n",
        "p_path": "src/main/java/com/sap/sgs/phosphor/fosstars/data/github/GitHubDataFetcher.java",
        "t_path": "src/test/java/com/sap/sgs/phosphor/fosstars/data/github/UsesSignedCommitTest.java",
        "p_name": "cleanup",
        "t_name": "testwhengettingrepositoryfails",
        "lpfc": "public void cleanup(CleanupStrategy strategy) {\n    Objects.requireNonNull(strategy, \"Hey! Cleanup strategy can't be null!\");\n    LOGGER.info(\"Cleaning up local repositories ...\");\n    BigInteger total = BigInteger.valueOf(0L);\n    for (Map.Entry<URL, LocalRepositoryInfo> entry : localRepositoriesInfo.entrySet()) {\n        LocalRepositoryInfo info = entry.getValue();\n        if (!Files.exists(info.path())) {\n            continue;\n        }\n        total = total.add(info.repositorySize());\n    }\n    for (Map.Entry<URL, LocalRepositoryInfo> entry : localRepositoriesInfo.entrySet()) {\n        URL url = entry.getKey();\n        LocalRepositoryInfo info = entry.getValue();\n        if (strategy.shouldBeDeleted(url, info, total)) {\n            try {\n                FileUtils.deleteDirectory(info.path().toFile());\n            } catch (IOException e) {\n                LOGGER.error(() -> String.format(\"Could not delete a local repository: %s\", info.path()), e);\n            }\n            localRepositoriesInfo.remove(url);\n        }\n    }\n}",
        "rpfc": "public void cleanup(CleanupStrategy strategy) {\n    Objects.requireNonNull(strategy, \"Hey! Cleanup strategy can't be null!\");\n    LOGGER.info(\"Cleaning up local repositories ...\");\n    BigInteger total = BigInteger.valueOf(0L);\n    for (Map.Entry<URL, LocalRepositoryInfo> entry : localRepositoriesInfo.entrySet()) {\n        LocalRepositoryInfo info = entry.getValue();\n        if (!Files.exists(info.path())) {\n            continue;\n        }\n        total = total.add(info.repositorySize());\n    }\n    Set<URL> toBeRemoved = new HashSet<>();\n    for (Map.Entry<URL, LocalRepositoryInfo> entry : localRepositoriesInfo.entrySet()) {\n        URL url = entry.getKey();\n        LocalRepositoryInfo info = entry.getValue();\n        if (strategy.shouldBeDeleted(url, info, total)) {\n            try {\n                FileUtils.deleteDirectory(info.path().toFile());\n            } catch (IOException e) {\n                LOGGER.error(() -> String.format(\"Could not delete a local repository: %s\", info.path()), e);\n            }\n            toBeRemoved.add(url);\n        }\n    }\n    toBeRemoved.forEach(localRepositoriesInfo::remove);\n}",
        "tuc": "@Test\npublic void testWhenGettingRepositoryFails() throws IOException {\n    final UsesSignedCommits provider = new UsesSignedCommits(fetcher);\n    LocalRepository repository = mock(LocalRepository.class);\n    when(repository.commitsWithin(any())).thenThrow(new IOException());\n    GitHubProject project = new GitHubProject(\"org\", \"test\");\n    fetcher.addForTesting(project, repository);\n    ValueHashSet values = new ValueHashSet();\n    assertEquals(0, values.size());\n    provider.update(project, values);\n    assertEquals(1, values.size());\n    assertTrue(values.has(USES_SIGNED_COMMITS));\n    assertTrue(values.of(USES_SIGNED_COMMITS).isPresent());\n    assertTrue(values.of(USES_SIGNED_COMMITS).get().isUnknown());\n}",
        "label": 0
    },
    {
        "repo_name": "mstritt___orbit-image-analysis",
        "commit": "8db994c9134cc2b899c7b7139687523a12501840",
        "commit_message": "Fix for multi-monitor weird display issue of F3 menu.\n",
        "p_path": "src/main/java/com/actelion/research/orbit/imageAnalysis/components/FeaturesAdminFrame.java",
        "t_path": "src/test/java/com/actelion/research/orbit/imageAnalysis/test/deeplearning/maskRCNN/TestCorpusCallosumSegment.java",
        "p_name": "initialize",
        "t_name": "testsegmentationannotationscustom",
        "lpfc": "private void initialize() {\n    this.setTitle(\"Feature Configuration (F3)\");\n    java.net.URL imgURL = this.getClass().getResource(OrbitImageAnalysis.LOGO_NAME);\n    if (imgURL != null) {\n        ImageIcon icon = new ImageIcon(imgURL);\n        this.setIconImage(icon.getImage());\n    }\n    setResizable(false);\n    setSize(new Dimension(frameWidth, frameHeight));\n    final JTabbedPane tabs = new JTabbedPane();\n    // classification\n    JPanel panelClassification = new JPanel();\n    panelClassification.setLayout(new FlowLayout(FlowLayout.LEFT, 20, 15));\n    JPanel panel = new JPanel(new GridLayout(1, 2));\n    JLabel lab = new JLabel(\"Structure size:\");\n    panel.add(lab);\n    tfWindowSize = new JTextField();\n    tfWindowSize.setText(Integer.toString(featureDescription.getWindowSize()));\n    tfWindowSize.setInputVerifier(new IntInputVerifier(2));\n    panel.add(tfWindowSize);\n    setCompBounds(panel, frameWidth - 50);\n    panelClassification.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Median filter radius (0-5):\");\n    panel.add(lab);\n    tfNumBlur = new JTextField();\n    tfNumBlur.setText(Integer.toString(featureDescription.getNumBlur()));\n    tfNumBlur.setInputVerifier(new IntInputVerifier(0, 0, 5));\n    panel.add(tfNumBlur);\n    setCompBounds(panel, frameWidth - 50);\n    panelClassification.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"RGB channels used:\");\n    panel.add(lab);\n    JPanel colPanel = new JPanel();\n    cbRed = new JCheckBox(\"red\", !featureDescription.isSkipRed());\n    cbGreen = new JCheckBox(\"green\", !featureDescription.isSkipGreen());\n    cbBlue = new JCheckBox(\"blue\", !featureDescription.isSkipBlue());\n    colPanel.add(cbRed);\n    colPanel.add(cbGreen);\n    colPanel.add(cbBlue);\n    panel.add(colPanel);\n    setCompBounds(panel, frameWidth - 50);\n    panelClassification.add(panel);\n    cbDeconvName = new JComboBox<>(Colour_Deconvolution.stainings);\n    cbDeconvName.setSelectedIndex(0);\n    if (featureDescription.getDeconvName() != null && featureDescription.getDeconvName().length() > 0 && (!featureDescription.getDeconvName().equals(Colour_Deconvolution.DECONV_NONE)))\n        setDeconvName(featureDescription.getDeconvName());\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Color Deconvolution Setup:\");\n    panel.add(lab);\n    panel.add(cbDeconvName);\n    setCompBounds(panel, frameWidth - 50);\n    panelClassification.add(panel);\n    deconvChannel0.setToolTipText(\"disable color deconvolution\");\n    deconvChannel1.setToolTipText(\"select primary staining\");\n    deconvChannel2.setToolTipText(\"select secondary staining\");\n    deconvChannel3.setToolTipText(\"select complementary color to staining 1 + staining 2\");\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Color Deconvolution Staining:\");\n    panel.add(lab);\n    JPanel radioBtnPanel = new JPanel();\n    radioBtnPanel.add(deconvChannel0);\n    radioBtnPanel.add(deconvChannel1);\n    radioBtnPanel.add(deconvChannel2);\n    radioBtnPanel.add(deconvChannel3);\n    ButtonGroup btnGroup = new ButtonGroup();\n    btnGroup.add(deconvChannel0);\n    btnGroup.add(deconvChannel1);\n    btnGroup.add(deconvChannel2);\n    btnGroup.add(deconvChannel3);\n    setDeconvChannel(featureDescription.getDeconvChannel());\n    panel.add(radioBtnPanel);\n    setCompBounds(panel, frameWidth - 50);\n    panelClassification.add(panel);\n    List<String> classList = new ArrayList<>();\n    //classList.add(\"<ALL>\");\n    StringBuilder selected = new StringBuilder();\n    OrbitModel model = OrbitImageAnalysis.getInstance().getModel();\n    if (model != null) {\n        OrbitModel classModel = model;\n        if (model.getSegmentationModel() != null)\n            classModel = model.getSegmentationModel();\n        int i = 0;\n        for (ClassShape cs : classModel.getClassShapes()) {\n            String cleanCS = cs.getName().replaceAll(\";\", \",\");\n            classList.add(cleanCS);\n            if (featureDescription.getFeatureClasses() != null) {\n                for (int fc : featureDescription.getFeatureClasses()) {\n                    if (i == fc) {\n                        selected.append(cleanCS).append(\"; \");\n                    }\n                }\n            }\n            i++;\n        }\n    }\n    cbFeatureClasses = new JComboCheckBox(classList);\n    cbFeatureClasses.setText(selected.toString());\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Classes for retrieving features/histograms:\");\n    panel.add(lab);\n    panel.add(cbFeatureClasses);\n    setCompBounds(panel, frameWidth - 50);\n    panelClassification.add(panel);\n    // fluo channels\n    if (featureDescription.getActiveFluoChannels() != null) {\n        setCbActiveFluoChannels(featureDescription.getActiveFluoChannels());\n    } else {\n        IOrbitImageMultiChannel multiChanImage = getOpenMultiChannelImage();\n        if (multiChanImage != null && multiChanImage.getChannelNames() != null && multiChanImage.getChannelNames().length > 0) {\n            setCbActiveFluoChannels(multiChanImage.getChannelNames());\n        }\n    }\n    if (cbFluoChannels == null)\n        cbFluoChannels = new JComboCheckBox();\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Active fluorescence channels:\");\n    panel.add(lab);\n    panel.add(cbFluoChannels);\n    setCompBounds(panel, frameWidth - 50);\n    panelClassification.add(panel);\n    tabs.add(\"Classification\", panelClassification);\n    // segmentation\n    JPanel panelSegmentation = new JPanel();\n    panelSegmentation.setLayout(new GridLayout(-1, 1));\n    final OrbitModel modelFin = model;\n    cbForSecondarySegmentationModel = new JCheckBox(\"Set Features for Secondary Segmentation\", featureDescription.isForSecondarySegmentationModel());\n    cbForSecondarySegmentationModel.setToolTipText(\"set the segmentation features for the secondary segmentation model\");\n    cbForSecondarySegmentationModel.addActionListener(e -> {\n        // attention: we use always the segmentation model for feature description!!! (this might go wrong...)\n        if (modelFin.getSegmentationModel() != null) {\n            FeatureDescription fd = modelFin.getSegmentationModel().getFeatureDescription();\n            if (cbForSecondarySegmentationModel.isSelected() && modelFin.getSecondarySegmentationModel() != null) {\n                fd = modelFin.getSecondarySegmentationModel().getFeatureDescription();\n            }\n            updateValues(fd);\n        }\n    });\n    setCompBounds(cbForSecondarySegmentationModel, frameWidth);\n    panelSegmentation.add(cbForSecondarySegmentationModel);\n    cbCytoplasmaSegmentation = new JCheckBox(\"Cytoplasma Segmentation\", featureDescription.isCytoplasmaSegmentation());\n    cbCytoplasmaSegmentation.setToolTipText(\"performs a cytoplasma segmentation based on a nuclei staining\");\n    setCompBounds(cbCytoplasmaSegmentation, frameWidth);\n    panelSegmentation.add(cbCytoplasmaSegmentation);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Minimum segmentation size:\");\n    panel.add(lab);\n    tfMinSegmentationSize = new JTextField();\n    tfMinSegmentationSize.setText(Integer.toString(featureDescription.getMinSegmentationSize()));\n    tfMinSegmentationSize.setInputVerifier(new IntInputVerifier(3, 1, 2000));\n    tfMinSegmentationSize.setToolTipText(\"the minimum area (in pixel) a segmented object must have\");\n    tfMinSegmentationSize.setColumns(4);\n    panel.add(tfMinSegmentationSize);\n    setCompBounds(panel, frameWidth - 50);\n    panelSegmentation.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Maximum segmentation length:\");\n    panel.add(lab);\n    tfMaxSegmentationLength = new IntegerTextField(featureDescription.getMaxSegmentationLength(), 500, 5, 20000);\n    tfMaxSegmentationLength.setText(Integer.toString(featureDescription.getMaxSegmentationLength()));\n    tfMaxSegmentationLength.setToolTipText(\"the maximum border path-length (in pixel) a segmented cell can have\");\n    tfMaxSegmentationLength.setHorizontalAlignment(JTextField.LEFT);\n    tfMaxSegmentationLength.setColumns(4);\n    panel.add(tfMaxSegmentationLength);\n    setCompBounds(panel, frameWidth - 50);\n    panelSegmentation.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Maximum open distance:\");\n    panel.add(lab);\n    tfMaxOpenDistance = new IntegerTextField(featureDescription.getMaxOpenDistance(), 3, 0, 100);\n    tfMaxOpenDistance.setText(Integer.toString(featureDescription.getMaxOpenDistance()));\n    tfMaxOpenDistance.setToolTipText(\"maximum distance (in pixel) between start and end of the path of an object\");\n    tfMaxOpenDistance.setHorizontalAlignment(JTextField.LEFT);\n    tfMaxOpenDistance.setColumns(4);\n    panel.add(tfMaxOpenDistance);\n    setCompBounds(panel, frameWidth - 50);\n    panelSegmentation.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Segmentation scale factor:\");\n    panel.add(lab);\n    tfSegmentationScale = new DoubleTextField(1, 1, 0.1, 10);\n    tfSegmentationScale.setHorizontalAlignment(JTextField.LEFT);\n    tfSegmentationScale.setText(featureDescription.getSegmentationScale() + \"\");\n    tfSegmentationScale.setColumns(4);\n    panel.add(tfSegmentationScale);\n    setCompBounds(panel, frameWidth - 50);\n    panelSegmentation.add(panel);\n    cbDisableWatershed = new JCheckBox(\"Disable object splitting\", featureDescription.isDisableWatershed());\n    cbDisableWatershed.setToolTipText(\"disable splitting of overlapping objects (do not apply watershed algorithm)\");\n    setCompBounds(cbDisableWatershed, frameWidth);\n    panelSegmentation.add(cbDisableWatershed);\n    cbDoCombineCrossTiles = new JCheckBox(\"Combine cross tile objects (slow)\", featureDescription.isCombineObjectsCrossTiles());\n    cbDoCombineCrossTiles.setToolTipText(\"combine objects across tiles\");\n    setCompBounds(cbDoCombineCrossTiles, frameWidth);\n    panelSegmentation.add(cbDoCombineCrossTiles);\n    cbFilterTileEdgeShapes = new JCheckBox(\"Discard tile border objects\", featureDescription.isFilterTileEdgeShapes());\n    cbFilterTileEdgeShapes.setToolTipText(\"discards objects at tile borders, e.g. objects which are divided by tiles\");\n    setCompBounds(cbFilterTileEdgeShapes, frameWidth);\n    panelSegmentation.add(cbFilterTileEdgeShapes);\n    panel = new JPanel(new GridLayout(1, 2));\n    cbMFS = new JCheckBox(\"Mumford-Shah segmentation (cell clusters):\", featureDescription.isMumfordShahSegmentation());\n    cbMFS.setToolTipText(\"enable mumford-shah segmentation (good for cell clusters)\");\n    cbMFS.addActionListener(e -> {\n        if (cbMFS.isSelected() && !cbDisableWatershed.isSelected()) {\n            cbDisableWatershed.setSelected(true);\n            JOptionPane.showMessageDialog(FeaturesAdminFrame.this, \"Mumford-Shah segmentation has its own object splitting algorithm, thus the additional object splitting has been disabled.\\n\" + \"However, you can enable it again in addition and try if the additional splitting (watershed algorithm) gives better results.\", \"Additional object splitting has been disabled\", JOptionPane.INFORMATION_MESSAGE);\n        }\n    });\n    panel.add(cbMFS);\n    JPanel mfsParamPanel = new JPanel();\n    mfsParamPanel.setBorder(BorderFactory.createEmptyBorder());\n    JPanel mfsSizePanel = new JPanel();\n    mfsSizePanel.setBorder(BorderFactory.createEmptyBorder());\n    JLabel mfsSizeLabel = new JLabel(\"Obj size [px]\");\n    mfsSizeLabel.setToolTipText(\"Expected size of objects in pixels. Smaller values will split objects more frequently.\");\n    tfMFSCellSize = new IntegerTextField(18, 18, 1, 1000);\n    tfMFSCellSize.setHorizontalAlignment(JTextField.LEFT);\n    tfMFSCellSize.setInt(featureDescription.getMumfordShahCellSize());\n    tfMFSCellSize.setColumns(3);\n    mfsSizePanel.add(mfsSizeLabel);\n    mfsSizePanel.add(tfMFSCellSize);\n    mfsParamPanel.add(mfsSizePanel);\n    JPanel mfsAlphaPanel = new JPanel();\n    mfsAlphaPanel.setBorder(BorderFactory.createEmptyBorder());\n    JLabel mfsAlphaLabel = new JLabel(\"Intens split [1-50]\");\n    mfsAlphaLabel.setToolTipText(\"Object splitting based on intensity. Smaller values will split objects more frequently.\\n\" + \"Recommended range: [1,50], allowed range: [1,255].\\n\" + \"This value corresponds to the alpha value of the mumford-shah functional which is responsible for the smoothing of the image.\\n\" + \"Higher values will smooth the image more and thus split objects less.\");\n    tfMFSAlpha = new IntegerTextField(5, 5, 1, 255);\n    tfMFSAlpha.setHorizontalAlignment(JTextField.LEFT);\n    tfMFSAlpha.setInt(featureDescription.getMumfordShahAlpha());\n    tfMFSAlpha.setColumns(3);\n    mfsAlphaPanel.add(mfsAlphaLabel);\n    mfsAlphaPanel.add(tfMFSAlpha);\n    mfsParamPanel.add(mfsAlphaPanel);\n    panel.add(mfsParamPanel);\n    setCompBounds(panel, frameWidth - 50);\n    panel.setPreferredSize(new Dimension((int) panel.getPreferredSize().getWidth(), (int) mfsAlphaPanel.getPreferredSize().getHeight()));\n    panelSegmentation.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Dilate:\");\n    panel.add(lab);\n    tfNumDilate = new IntegerTextField(featureDescription.getNumDilate(), 10, 0, 10);\n    tfNumDilate.setText(Integer.toString(featureDescription.getNumDilate()));\n    tfNumDilate.setToolTipText(\"dilate foreground before object segmentation\");\n    tfNumDilate.setHorizontalAlignment(JTextField.LEFT);\n    tfNumDilate.setColumns(4);\n    panel.add(tfNumDilate);\n    setCompBounds(panel, frameWidth - 50);\n    panelSegmentation.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Erode:\");\n    panel.add(lab);\n    tfNumErode = new IntegerTextField(featureDescription.getNumErode(), 10, 0, 10);\n    tfNumErode.setText(Integer.toString(featureDescription.getNumErode()));\n    tfNumErode.setToolTipText(\"erode foreground before object segmentation\");\n    tfNumErode.setHorizontalAlignment(JTextField.LEFT);\n    tfNumErode.setColumns(4);\n    panel.add(tfNumErode);\n    setCompBounds(panel, frameWidth - 50);\n    panelSegmentation.add(panel);\n    cbDilateBeforeErode = new JCheckBox(\"Dilate before erode\", featureDescription.isDilateBeforeErode());\n    cbDilateBeforeErode.setToolTipText(\"otherwise erode before dilate\");\n    setCompBounds(cbDilateBeforeErode, frameWidth);\n    panelSegmentation.add(cbDilateBeforeErode);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Despeckle: [0-50]\");\n    panel.add(lab);\n    tfRemoveOutliers = new IntegerTextField(featureDescription.getRemoveOutliers(), 0, 0, 50);\n    tfRemoveOutliers.setText(Integer.toString(featureDescription.getRemoveOutliers()));\n    tfRemoveOutliers.setToolTipText(\"despeckle foerground before object segmentation (remove outliers)\");\n    tfRemoveOutliers.setHorizontalAlignment(JTextField.LEFT);\n    tfRemoveOutliers.setColumns(4);\n    panel.add(tfRemoveOutliers);\n    setCompBounds(panel, frameWidth - 50);\n    panelSegmentation.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Smooth objects (GraphCut): [0-100]\");\n    panel.add(lab);\n    tfGraphCut = new DoubleTextField(featureDescription.getGraphCut(), 10, 0, 100);\n    tfGraphCut.setText(Double.toString(featureDescription.getGraphCut()));\n    tfGraphCut.setToolTipText(\"smooth objects and fill holes\");\n    tfGraphCut.setHorizontalAlignment(JTextField.LEFT);\n    tfGraphCut.setColumns(4);\n    panel.add(tfGraphCut);\n    setCompBounds(panel, frameWidth - 50);\n    panelSegmentation.add(panel);\n    // large object detection\n    cbNerveDetectionMode = new JCheckBox(\"Nerve Detection Mode\", featureDescription.isDeactivateWatershed());\n    cbNerveDetectionMode.setToolTipText(\"activate for nerve detection (large object detection)\");\n    setCompBounds(cbNerveDetectionMode, frameWidth);\n    panelSegmentation.add(cbNerveDetectionMode);\n    JScrollPane segScrollPane = new JScrollPane(panelSegmentation, ScrollPaneConstants.VERTICAL_SCROLLBAR_AS_NEEDED, ScrollPaneConstants.HORIZONTAL_SCROLLBAR_NEVER);\n    tabs.add(\"Segmentation\", segScrollPane);\n    // deep learning\n    JPanel panelDeepLearning = new JPanel();\n    panelDeepLearning.setLayout(new FlowLayout(FlowLayout.LEFT, 20, 15));\n    // large object detection\n    cbDeepLearning = new JCheckBox(\"Deep Learning Segmentation\", featureDescription.isDeepLearningSegmentation());\n    cbDeepLearning.setToolTipText(\"Enable deep learning instance segmentation. Select a segmentation model before using it.\");\n    setCompBounds(cbDeepLearning, frameWidth);\n    panelDeepLearning.add(cbDeepLearning);\n    panel = new JPanel(new GridLayout(2, 2));\n    lab = new JLabel(\"Deep Learning Model Path or URL:\");\n    panel.add(lab);\n    JPanel pathPanel = new JPanel(new FlowLayout());\n    tfDeepLearningModelPath = new JTextField(featureDescription.getDeepLearningModelPath(), 15);\n    tfDeepLearningModelPath.setToolTipText(\"can be a full path to a model file or a URL\");\n    tfDeepLearningModelPath.setHorizontalAlignment(JTextField.LEFT);\n    pathPanel.add(tfDeepLearningModelPath);\n    //JPanel dlSettings = new JPanel(new FlowLayout());\n    //        ArrayList<AbstractSegment<? extends AbstractDetections<? extends AbstractDetection>,? extends AbstractSegmentationSettings>>\n    //                dLSegmentMethods = new ArrayList<>();\n    final JButton fileSelectBtn = new JButton(\"browse\");\n    fileSelectBtn.addActionListener(e -> {\n        JFileChooser fileChooser = new JFileChooser();\n        FileNameExtensionFilter filter = new FileNameExtensionFilter(\"DL Segmentation Models (*.pb)\", \"pb\");\n        fileChooser.setFileFilter(filter);\n        fileChooser.setDialogType(JFileChooser.OPEN_DIALOG);\n        String dir = prefs.get(\"OrbitImageAnalysis.OpenDeepLearningModelPath\", null);\n        if (dir != null) {\n            File cd = new File(dir);\n            fileChooser.setCurrentDirectory(cd);\n        }\n        int returnVal = fileChooser.showSaveDialog(FeaturesAdminFrame.this);\n        if (returnVal == JFileChooser.APPROVE_OPTION) {\n            prefs.put(\"OrbitImageAnalysis.OpenDeepLearningModelPath\", fileChooser.getCurrentDirectory().getAbsolutePath());\n            String fn = fileChooser.getSelectedFile().getAbsolutePath();\n            File file = new File(fn);\n            if (file.isDirectory())\n                return;\n            tfDeepLearningModelPath.setText(file.getAbsolutePath());\n        }\n    });\n    pathPanel.add(fileSelectBtn);\n    panel.add(pathPanel);\n    lab = new JLabel(\"Predefined Model\");\n    panel.add(lab);\n    //select rdf.* from RAW_DATA_FILE rdf where rdf.FILENAME like '%.pb';\n    // http://ares:8080/orbit/rdf?orbitID=19340932&download=true\n    MaskRCNNSegmentationSettings nucleiSettings = new MaskRCNNSegmentationSettings(\"Nuclei\", \"http://ares:8080/orbit/rdf?orbitID=19340900\", \"C:\\\\Users\\\\fullejo1\\\\Downloads\\\\deepretina_final.pb\", 512, 512, 0.5f, 512, 28, 28, 2, \"NucleiS\", false, MaskRCNNSegmentationSettings.PostProcessMethod.STANDARD);\n    MaskRCNNSegmentationSettings insulinSettings = new MaskRCNNSegmentationSettings(\"Pancreas Islets\", \"http://ares:8080/orbit/rdf?orbitID=19340903\", \"C:\\\\Users\\\\fullejo1\\\\Downloads\\\\insulin_009.pb\", 512, 512, 16f, 10, 56, 56, 5, \"IsletS\", true, MaskRCNNSegmentationSettings.PostProcessMethod.STANDARD);\n    ArrayList<Color> colors = new ArrayList<>(Arrays.asList(Color.BLACK, Color.RED, Color.GREEN, Color.BLUE, Color.YELLOW));\n    ArrayList<String> classNames = new ArrayList<>(Arrays.asList(\"Background\", \"g0\", \"g1\", \"g2\", \"g3\"));\n    try {\n        insulinSettings.setCustomClassNames(colors, classNames);\n    } catch (Exception e) {\n        e.printStackTrace();\n    }\n    DLR101SegmentationSettings glomeruliSettings = new DLR101SegmentationSettings(\"Glomeruli\", \"http://ares:8080/orbit/rdf?orbitID=23539963\", \"C:\\\\Users\\\\fullejo1\\\\Downloads\\\\glomeruli-410k.pb\", 512, 512, 2, \"Glomeruli\", true);\n    // Since this is a two-step method it needs a different approach...\n    //MaskRCNNSegment corpus_callosum = new MaskRCNNSegment(new File(\"D:/deeplearning/insulin/models/insulin_009.pb\"), MaskRCNNSegment.PostProcessMethod.STANDARD);\n    //        dLSegmentMethods.add(nuclei);\n    //        dLSegmentMethods.add(insulin);\n    //        dLSegmentMethods.add(glomeruli);\n    AbstractSegmentationSettings<?>[] dLSegmentArray = new AbstractSegmentationSettings<?>[] { nucleiSettings, insulinSettings, glomeruliSettings };\n    dlSegmentMethodsModel = new DLSegmentModelComboBoxModel(dLSegmentArray);\n    dLMethodComboBox = new JComboBox<>(dlSegmentMethodsModel);\n    panel.add(dLMethodComboBox);\n    // TODO: Add a checkbox (Store annotations).\n    setCompBounds(panel, frameWidth - 50);\n    panelDeepLearning.add(panel);\n    tabs.add(\"Deep Learning\", panelDeepLearning);\n    // roi\n    JPanel panelROI = new JPanel();\n    panelROI.setLayout(new FlowLayout(FlowLayout.LEFT, 20, 15));\n    List<AnnotationGroupLabel> annotationGroups = new ArrayList<>(12);\n    annotationGroups.add(new AnnotationGroupLabel(-1, \"Ignore\"));\n    annotationGroups.add(new AnnotationGroupLabel(0, \"All Groups\"));\n    for (int g = 1; g < (OrbitUtils.ANNOTATION_GROUPS + 1); g++) annotationGroups.add(new AnnotationGroupLabel(g, \"Group \" + g));\n    cbAnnotationROI = new JComboBox<>(annotationGroups.toArray(new AnnotationGroupLabel[0]));\n    cbAnnotationROI.setSelectedIndex(0);\n    // +1 because 'ignore' is -1\n    int groupIdx = OrbitImageAnalysis.getInstance().getModel().getAnnotationGroup() + 1;\n    if (cbAnnotationROI.getItemCount() > groupIdx)\n        cbAnnotationROI.setSelectedIndex(groupIdx);\n    cbAnnotationROI.setToolTipText(\"Use annotations (excl/incl/ROI) to build the ROI?\");\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Use annotations as ROI:\");\n    panel.add(lab);\n    panel.add(cbAnnotationROI);\n    setCompBounds(panel, frameWidth - 50);\n    panelROI.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Fixed circular ROI (pixel):\");\n    panel.add(lab);\n    tfFixedROI = new JTextField();\n    tfFixedROI.setText(Integer.toString(OrbitImageAnalysis.getInstance().getModel().getFixedCircularROI()));\n    tfFixedROI.setInputVerifier(new IntInputVerifier(0));\n    panel.add(tfFixedROI);\n    setCompBounds(panel, frameWidth - 50);\n    panelROI.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"ROI offset X:\");\n    panel.add(lab);\n    tfROIX = new JTextField();\n    tfROIX.setText(Integer.toString(OrbitImageAnalysis.getInstance().getModel().getFixedROIOffsetX()));\n    tfROIX.setInputVerifier(new IntInputVerifier(0));\n    panel.add(tfROIX);\n    setCompBounds(panel, frameWidth - 50);\n    panelROI.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"ROI offset Y:\");\n    panel.add(lab);\n    tfROIY = new JTextField();\n    tfROIY.setText(Integer.toString(OrbitImageAnalysis.getInstance().getModel().getFixedROIOffsetY()));\n    tfROIY.setInputVerifier(new IntInputVerifier(0));\n    panel.add(tfROIY);\n    setCompBounds(panel, frameWidth - 50);\n    panelROI.add(panel);\n    tabs.add(\"ROI\", panelROI);\n    // image adjustments\n    JPanel panelImageAdjustments = new JPanel();\n    panelImageAdjustments.setLayout(new FlowLayout(FlowLayout.LEFT, 20, 15));\n    cbUseImageAdjustments = new JCheckBox(\"Use image adjustments *\", featureDescription.isUseImageAdjustments());\n    cbUseImageAdjustments.setToolTipText(\"load image adjustments from database before classification (brightness, contrast, gamma) - use with care!\");\n    setCompBounds(cbUseImageAdjustments, frameWidth);\n    panelImageAdjustments.add(cbUseImageAdjustments);\n    JLabel imageAdjustmentWarning = new JLabel(\"<html><body>* If activated, image adjustments like gamma, brightness \" + \"and contrast from the image<br/>adjustment panel are taken into account.\" + \"<br/>Please use it with care, because it will influence the quantification results,<br/>\" + \"thus you might get an unwanted bias.\" + \"<br/>If you really want to use it, please double-check that the image adjustments are saved.</body></html>\");\n    panelImageAdjustments.add(imageAdjustmentWarning);\n    tabs.add(\"Image Adjustments\", panelImageAdjustments);\n    // layout\n    setLayout(new BorderLayout());\n    add(tabs, BorderLayout.CENTER);\n    btnOK = new JButton(\"OK\");\n    add(btnOK, BorderLayout.SOUTH);\n    tabs.setSelectedIndex(FeaturesAdminFrame.selectedTab);\n    tabs.addChangeListener(e -> {\n        logger.trace(\"selected tab: \" + tabs.getSelectedIndex());\n        FeaturesAdminFrame.selectedTab = tabs.getSelectedIndex();\n    });\n    addActionListeners();\n    Toolkit toolkit = getToolkit();\n    Dimension size = toolkit.getScreenSize();\n    setLocation((size.width - getWidth()) / 2, (size.height - getHeight()) / 2);\n}",
        "rpfc": "private void initialize() {\n    this.setTitle(\"Feature Configuration (F3)\");\n    java.net.URL imgURL = this.getClass().getResource(OrbitImageAnalysis.LOGO_NAME);\n    if (imgURL != null) {\n        ImageIcon icon = new ImageIcon(imgURL);\n        this.setIconImage(icon.getImage());\n    }\n    setResizable(false);\n    setSize(new Dimension(frameWidth, frameHeight));\n    final JTabbedPane tabs = new JTabbedPane();\n    // classification\n    JPanel panelClassification = new JPanel();\n    panelClassification.setLayout(new FlowLayout(FlowLayout.LEFT, 20, 15));\n    JPanel panel = new JPanel(new GridLayout(1, 2));\n    JLabel lab = new JLabel(\"Structure size:\");\n    panel.add(lab);\n    tfWindowSize = new JTextField();\n    tfWindowSize.setText(Integer.toString(featureDescription.getWindowSize()));\n    tfWindowSize.setInputVerifier(new IntInputVerifier(2));\n    panel.add(tfWindowSize);\n    setCompBounds(panel, frameWidth - 50);\n    panelClassification.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Median filter radius (0-5):\");\n    panel.add(lab);\n    tfNumBlur = new JTextField();\n    tfNumBlur.setText(Integer.toString(featureDescription.getNumBlur()));\n    tfNumBlur.setInputVerifier(new IntInputVerifier(0, 0, 5));\n    panel.add(tfNumBlur);\n    setCompBounds(panel, frameWidth - 50);\n    panelClassification.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"RGB channels used:\");\n    panel.add(lab);\n    JPanel colPanel = new JPanel();\n    cbRed = new JCheckBox(\"red\", !featureDescription.isSkipRed());\n    cbGreen = new JCheckBox(\"green\", !featureDescription.isSkipGreen());\n    cbBlue = new JCheckBox(\"blue\", !featureDescription.isSkipBlue());\n    colPanel.add(cbRed);\n    colPanel.add(cbGreen);\n    colPanel.add(cbBlue);\n    panel.add(colPanel);\n    setCompBounds(panel, frameWidth - 50);\n    panelClassification.add(panel);\n    cbDeconvName = new JComboBox<>(Colour_Deconvolution.stainings);\n    cbDeconvName.setSelectedIndex(0);\n    if (featureDescription.getDeconvName() != null && featureDescription.getDeconvName().length() > 0 && (!featureDescription.getDeconvName().equals(Colour_Deconvolution.DECONV_NONE)))\n        setDeconvName(featureDescription.getDeconvName());\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Color Deconvolution Setup:\");\n    panel.add(lab);\n    panel.add(cbDeconvName);\n    setCompBounds(panel, frameWidth - 50);\n    panelClassification.add(panel);\n    deconvChannel0.setToolTipText(\"disable color deconvolution\");\n    deconvChannel1.setToolTipText(\"select primary staining\");\n    deconvChannel2.setToolTipText(\"select secondary staining\");\n    deconvChannel3.setToolTipText(\"select complementary color to staining 1 + staining 2\");\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Color Deconvolution Staining:\");\n    panel.add(lab);\n    JPanel radioBtnPanel = new JPanel();\n    radioBtnPanel.add(deconvChannel0);\n    radioBtnPanel.add(deconvChannel1);\n    radioBtnPanel.add(deconvChannel2);\n    radioBtnPanel.add(deconvChannel3);\n    ButtonGroup btnGroup = new ButtonGroup();\n    btnGroup.add(deconvChannel0);\n    btnGroup.add(deconvChannel1);\n    btnGroup.add(deconvChannel2);\n    btnGroup.add(deconvChannel3);\n    setDeconvChannel(featureDescription.getDeconvChannel());\n    panel.add(radioBtnPanel);\n    setCompBounds(panel, frameWidth - 50);\n    panelClassification.add(panel);\n    List<String> classList = new ArrayList<>();\n    //classList.add(\"<ALL>\");\n    StringBuilder selected = new StringBuilder();\n    OrbitModel model = OrbitImageAnalysis.getInstance().getModel();\n    if (model != null) {\n        OrbitModel classModel = model;\n        if (model.getSegmentationModel() != null)\n            classModel = model.getSegmentationModel();\n        int i = 0;\n        for (ClassShape cs : classModel.getClassShapes()) {\n            String cleanCS = cs.getName().replaceAll(\";\", \",\");\n            classList.add(cleanCS);\n            if (featureDescription.getFeatureClasses() != null) {\n                for (int fc : featureDescription.getFeatureClasses()) {\n                    if (i == fc) {\n                        selected.append(cleanCS).append(\"; \");\n                    }\n                }\n            }\n            i++;\n        }\n    }\n    cbFeatureClasses = new JComboCheckBox(classList);\n    cbFeatureClasses.setText(selected.toString());\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Classes for retrieving features/histograms:\");\n    panel.add(lab);\n    panel.add(cbFeatureClasses);\n    setCompBounds(panel, frameWidth - 50);\n    panelClassification.add(panel);\n    // fluo channels\n    if (featureDescription.getActiveFluoChannels() != null) {\n        setCbActiveFluoChannels(featureDescription.getActiveFluoChannels());\n    } else {\n        IOrbitImageMultiChannel multiChanImage = getOpenMultiChannelImage();\n        if (multiChanImage != null && multiChanImage.getChannelNames() != null && multiChanImage.getChannelNames().length > 0) {\n            setCbActiveFluoChannels(multiChanImage.getChannelNames());\n        }\n    }\n    if (cbFluoChannels == null)\n        cbFluoChannels = new JComboCheckBox();\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Active fluorescence channels:\");\n    panel.add(lab);\n    panel.add(cbFluoChannels);\n    setCompBounds(panel, frameWidth - 50);\n    panelClassification.add(panel);\n    tabs.add(\"Classification\", panelClassification);\n    // segmentation\n    JPanel panelSegmentation = new JPanel();\n    panelSegmentation.setLayout(new GridLayout(-1, 1));\n    final OrbitModel modelFin = model;\n    cbForSecondarySegmentationModel = new JCheckBox(\"Set Features for Secondary Segmentation\", featureDescription.isForSecondarySegmentationModel());\n    cbForSecondarySegmentationModel.setToolTipText(\"set the segmentation features for the secondary segmentation model\");\n    cbForSecondarySegmentationModel.addActionListener(e -> {\n        // attention: we use always the segmentation model for feature description!!! (this might go wrong...)\n        if (modelFin.getSegmentationModel() != null) {\n            FeatureDescription fd = modelFin.getSegmentationModel().getFeatureDescription();\n            if (cbForSecondarySegmentationModel.isSelected() && modelFin.getSecondarySegmentationModel() != null) {\n                fd = modelFin.getSecondarySegmentationModel().getFeatureDescription();\n            }\n            updateValues(fd);\n        }\n    });\n    setCompBounds(cbForSecondarySegmentationModel, frameWidth);\n    panelSegmentation.add(cbForSecondarySegmentationModel);\n    cbCytoplasmaSegmentation = new JCheckBox(\"Cytoplasma Segmentation\", featureDescription.isCytoplasmaSegmentation());\n    cbCytoplasmaSegmentation.setToolTipText(\"performs a cytoplasma segmentation based on a nuclei staining\");\n    setCompBounds(cbCytoplasmaSegmentation, frameWidth);\n    panelSegmentation.add(cbCytoplasmaSegmentation);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Minimum segmentation size:\");\n    panel.add(lab);\n    tfMinSegmentationSize = new JTextField();\n    tfMinSegmentationSize.setText(Integer.toString(featureDescription.getMinSegmentationSize()));\n    tfMinSegmentationSize.setInputVerifier(new IntInputVerifier(3, 1, 2000));\n    tfMinSegmentationSize.setToolTipText(\"the minimum area (in pixel) a segmented object must have\");\n    tfMinSegmentationSize.setColumns(4);\n    panel.add(tfMinSegmentationSize);\n    setCompBounds(panel, frameWidth - 50);\n    panelSegmentation.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Maximum segmentation length:\");\n    panel.add(lab);\n    tfMaxSegmentationLength = new IntegerTextField(featureDescription.getMaxSegmentationLength(), 500, 5, 20000);\n    tfMaxSegmentationLength.setText(Integer.toString(featureDescription.getMaxSegmentationLength()));\n    tfMaxSegmentationLength.setToolTipText(\"the maximum border path-length (in pixel) a segmented cell can have\");\n    tfMaxSegmentationLength.setHorizontalAlignment(JTextField.LEFT);\n    tfMaxSegmentationLength.setColumns(4);\n    panel.add(tfMaxSegmentationLength);\n    setCompBounds(panel, frameWidth - 50);\n    panelSegmentation.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Maximum open distance:\");\n    panel.add(lab);\n    tfMaxOpenDistance = new IntegerTextField(featureDescription.getMaxOpenDistance(), 3, 0, 100);\n    tfMaxOpenDistance.setText(Integer.toString(featureDescription.getMaxOpenDistance()));\n    tfMaxOpenDistance.setToolTipText(\"maximum distance (in pixel) between start and end of the path of an object\");\n    tfMaxOpenDistance.setHorizontalAlignment(JTextField.LEFT);\n    tfMaxOpenDistance.setColumns(4);\n    panel.add(tfMaxOpenDistance);\n    setCompBounds(panel, frameWidth - 50);\n    panelSegmentation.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Segmentation scale factor:\");\n    panel.add(lab);\n    tfSegmentationScale = new DoubleTextField(1, 1, 0.1, 10);\n    tfSegmentationScale.setHorizontalAlignment(JTextField.LEFT);\n    tfSegmentationScale.setText(featureDescription.getSegmentationScale() + \"\");\n    tfSegmentationScale.setColumns(4);\n    panel.add(tfSegmentationScale);\n    setCompBounds(panel, frameWidth - 50);\n    panelSegmentation.add(panel);\n    cbDisableWatershed = new JCheckBox(\"Disable object splitting\", featureDescription.isDisableWatershed());\n    cbDisableWatershed.setToolTipText(\"disable splitting of overlapping objects (do not apply watershed algorithm)\");\n    setCompBounds(cbDisableWatershed, frameWidth);\n    panelSegmentation.add(cbDisableWatershed);\n    cbDoCombineCrossTiles = new JCheckBox(\"Combine cross tile objects (slow)\", featureDescription.isCombineObjectsCrossTiles());\n    cbDoCombineCrossTiles.setToolTipText(\"combine objects across tiles\");\n    setCompBounds(cbDoCombineCrossTiles, frameWidth);\n    panelSegmentation.add(cbDoCombineCrossTiles);\n    cbFilterTileEdgeShapes = new JCheckBox(\"Discard tile border objects\", featureDescription.isFilterTileEdgeShapes());\n    cbFilterTileEdgeShapes.setToolTipText(\"discards objects at tile borders, e.g. objects which are divided by tiles\");\n    setCompBounds(cbFilterTileEdgeShapes, frameWidth);\n    panelSegmentation.add(cbFilterTileEdgeShapes);\n    panel = new JPanel(new GridLayout(1, 2));\n    cbMFS = new JCheckBox(\"Mumford-Shah segmentation (cell clusters):\", featureDescription.isMumfordShahSegmentation());\n    cbMFS.setToolTipText(\"enable mumford-shah segmentation (good for cell clusters)\");\n    cbMFS.addActionListener(e -> {\n        if (cbMFS.isSelected() && !cbDisableWatershed.isSelected()) {\n            cbDisableWatershed.setSelected(true);\n            JOptionPane.showMessageDialog(FeaturesAdminFrame.this, \"Mumford-Shah segmentation has its own object splitting algorithm, thus the additional object splitting has been disabled.\\n\" + \"However, you can enable it again in addition and try if the additional splitting (watershed algorithm) gives better results.\", \"Additional object splitting has been disabled\", JOptionPane.INFORMATION_MESSAGE);\n        }\n    });\n    panel.add(cbMFS);\n    JPanel mfsParamPanel = new JPanel();\n    mfsParamPanel.setBorder(BorderFactory.createEmptyBorder());\n    JPanel mfsSizePanel = new JPanel();\n    mfsSizePanel.setBorder(BorderFactory.createEmptyBorder());\n    JLabel mfsSizeLabel = new JLabel(\"Obj size [px]\");\n    mfsSizeLabel.setToolTipText(\"Expected size of objects in pixels. Smaller values will split objects more frequently.\");\n    tfMFSCellSize = new IntegerTextField(18, 18, 1, 1000);\n    tfMFSCellSize.setHorizontalAlignment(JTextField.LEFT);\n    tfMFSCellSize.setInt(featureDescription.getMumfordShahCellSize());\n    tfMFSCellSize.setColumns(3);\n    mfsSizePanel.add(mfsSizeLabel);\n    mfsSizePanel.add(tfMFSCellSize);\n    mfsParamPanel.add(mfsSizePanel);\n    JPanel mfsAlphaPanel = new JPanel();\n    mfsAlphaPanel.setBorder(BorderFactory.createEmptyBorder());\n    JLabel mfsAlphaLabel = new JLabel(\"Intens split [1-50]\");\n    mfsAlphaLabel.setToolTipText(\"Object splitting based on intensity. Smaller values will split objects more frequently.\\n\" + \"Recommended range: [1,50], allowed range: [1,255].\\n\" + \"This value corresponds to the alpha value of the mumford-shah functional which is responsible for the smoothing of the image.\\n\" + \"Higher values will smooth the image more and thus split objects less.\");\n    tfMFSAlpha = new IntegerTextField(5, 5, 1, 255);\n    tfMFSAlpha.setHorizontalAlignment(JTextField.LEFT);\n    tfMFSAlpha.setInt(featureDescription.getMumfordShahAlpha());\n    tfMFSAlpha.setColumns(3);\n    mfsAlphaPanel.add(mfsAlphaLabel);\n    mfsAlphaPanel.add(tfMFSAlpha);\n    mfsParamPanel.add(mfsAlphaPanel);\n    panel.add(mfsParamPanel);\n    setCompBounds(panel, frameWidth - 50);\n    panel.setPreferredSize(new Dimension((int) panel.getPreferredSize().getWidth(), (int) mfsAlphaPanel.getPreferredSize().getHeight()));\n    panelSegmentation.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Dilate:\");\n    panel.add(lab);\n    tfNumDilate = new IntegerTextField(featureDescription.getNumDilate(), 10, 0, 10);\n    tfNumDilate.setText(Integer.toString(featureDescription.getNumDilate()));\n    tfNumDilate.setToolTipText(\"dilate foreground before object segmentation\");\n    tfNumDilate.setHorizontalAlignment(JTextField.LEFT);\n    tfNumDilate.setColumns(4);\n    panel.add(tfNumDilate);\n    setCompBounds(panel, frameWidth - 50);\n    panelSegmentation.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Erode:\");\n    panel.add(lab);\n    tfNumErode = new IntegerTextField(featureDescription.getNumErode(), 10, 0, 10);\n    tfNumErode.setText(Integer.toString(featureDescription.getNumErode()));\n    tfNumErode.setToolTipText(\"erode foreground before object segmentation\");\n    tfNumErode.setHorizontalAlignment(JTextField.LEFT);\n    tfNumErode.setColumns(4);\n    panel.add(tfNumErode);\n    setCompBounds(panel, frameWidth - 50);\n    panelSegmentation.add(panel);\n    cbDilateBeforeErode = new JCheckBox(\"Dilate before erode\", featureDescription.isDilateBeforeErode());\n    cbDilateBeforeErode.setToolTipText(\"otherwise erode before dilate\");\n    setCompBounds(cbDilateBeforeErode, frameWidth);\n    panelSegmentation.add(cbDilateBeforeErode);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Despeckle: [0-50]\");\n    panel.add(lab);\n    tfRemoveOutliers = new IntegerTextField(featureDescription.getRemoveOutliers(), 0, 0, 50);\n    tfRemoveOutliers.setText(Integer.toString(featureDescription.getRemoveOutliers()));\n    tfRemoveOutliers.setToolTipText(\"despeckle foerground before object segmentation (remove outliers)\");\n    tfRemoveOutliers.setHorizontalAlignment(JTextField.LEFT);\n    tfRemoveOutliers.setColumns(4);\n    panel.add(tfRemoveOutliers);\n    setCompBounds(panel, frameWidth - 50);\n    panelSegmentation.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Smooth objects (GraphCut): [0-100]\");\n    panel.add(lab);\n    tfGraphCut = new DoubleTextField(featureDescription.getGraphCut(), 10, 0, 100);\n    tfGraphCut.setText(Double.toString(featureDescription.getGraphCut()));\n    tfGraphCut.setToolTipText(\"smooth objects and fill holes\");\n    tfGraphCut.setHorizontalAlignment(JTextField.LEFT);\n    tfGraphCut.setColumns(4);\n    panel.add(tfGraphCut);\n    setCompBounds(panel, frameWidth - 50);\n    panelSegmentation.add(panel);\n    // large object detection\n    cbNerveDetectionMode = new JCheckBox(\"Nerve Detection Mode\", featureDescription.isDeactivateWatershed());\n    cbNerveDetectionMode.setToolTipText(\"activate for nerve detection (large object detection)\");\n    setCompBounds(cbNerveDetectionMode, frameWidth);\n    panelSegmentation.add(cbNerveDetectionMode);\n    JScrollPane segScrollPane = new JScrollPane(panelSegmentation, ScrollPaneConstants.VERTICAL_SCROLLBAR_AS_NEEDED, ScrollPaneConstants.HORIZONTAL_SCROLLBAR_NEVER);\n    tabs.add(\"Segmentation\", segScrollPane);\n    // deep learning\n    JPanel panelDeepLearning = new JPanel();\n    panelDeepLearning.setLayout(new FlowLayout(FlowLayout.LEFT, 20, 15));\n    // large object detection\n    cbDeepLearning = new JCheckBox(\"Deep Learning Segmentation\", featureDescription.isDeepLearningSegmentation());\n    cbDeepLearning.setToolTipText(\"Enable deep learning instance segmentation. Select a segmentation model before using it.\");\n    setCompBounds(cbDeepLearning, frameWidth);\n    panelDeepLearning.add(cbDeepLearning);\n    panel = new JPanel(new GridLayout(2, 2));\n    lab = new JLabel(\"Deep Learning Model Path or URL:\");\n    panel.add(lab);\n    JPanel pathPanel = new JPanel(new FlowLayout());\n    tfDeepLearningModelPath = new JTextField(featureDescription.getDeepLearningModelPath(), 15);\n    tfDeepLearningModelPath.setToolTipText(\"can be a full path to a model file or a URL\");\n    tfDeepLearningModelPath.setHorizontalAlignment(JTextField.LEFT);\n    pathPanel.add(tfDeepLearningModelPath);\n    //JPanel dlSettings = new JPanel(new FlowLayout());\n    //        ArrayList<AbstractSegment<? extends AbstractDetections<? extends AbstractDetection>,? extends AbstractSegmentationSettings>>\n    //                dLSegmentMethods = new ArrayList<>();\n    final JButton fileSelectBtn = new JButton(\"browse\");\n    fileSelectBtn.addActionListener(e -> {\n        JFileChooser fileChooser = new JFileChooser();\n        FileNameExtensionFilter filter = new FileNameExtensionFilter(\"DL Segmentation Models (*.pb)\", \"pb\");\n        fileChooser.setFileFilter(filter);\n        fileChooser.setDialogType(JFileChooser.OPEN_DIALOG);\n        String dir = prefs.get(\"OrbitImageAnalysis.OpenDeepLearningModelPath\", null);\n        if (dir != null) {\n            File cd = new File(dir);\n            fileChooser.setCurrentDirectory(cd);\n        }\n        int returnVal = fileChooser.showSaveDialog(FeaturesAdminFrame.this);\n        if (returnVal == JFileChooser.APPROVE_OPTION) {\n            prefs.put(\"OrbitImageAnalysis.OpenDeepLearningModelPath\", fileChooser.getCurrentDirectory().getAbsolutePath());\n            String fn = fileChooser.getSelectedFile().getAbsolutePath();\n            File file = new File(fn);\n            if (file.isDirectory())\n                return;\n            tfDeepLearningModelPath.setText(file.getAbsolutePath());\n        }\n    });\n    pathPanel.add(fileSelectBtn);\n    panel.add(pathPanel);\n    lab = new JLabel(\"Predefined Model\");\n    panel.add(lab);\n    //select rdf.* from RAW_DATA_FILE rdf where rdf.FILENAME like '%.pb';\n    // http://ares:8080/orbit/rdf?orbitID=19340932&download=true\n    MaskRCNNSegmentationSettings nucleiSettings = new MaskRCNNSegmentationSettings(\"Nuclei\", \"http://ares:8080/orbit/rdf?orbitID=19340900\", \"C:\\\\Users\\\\fullejo1\\\\Downloads\\\\deepretina_final.pb\", 512, 512, 0.5f, 512, 28, 28, 2, \"NucleiS\", true, MaskRCNNSegmentationSettings.PostProcessMethod.STANDARD);\n    MaskRCNNSegmentationSettings insulinSettings = new MaskRCNNSegmentationSettings(\"Pancreas Islets\", \"http://ares:8080/orbit/rdf?orbitID=19340903\", \"C:\\\\Users\\\\fullejo1\\\\Downloads\\\\insulin_009.pb\", 512, 512, 16f, 10, 56, 56, 5, \"IsletS\", true, MaskRCNNSegmentationSettings.PostProcessMethod.STANDARD);\n    ArrayList<Color> colors = new ArrayList<>(Arrays.asList(Color.BLACK, Color.RED, Color.GREEN, Color.BLUE, Color.YELLOW));\n    ArrayList<String> classNames = new ArrayList<>(Arrays.asList(\"Background\", \"g0\", \"g1\", \"g2\", \"g3\"));\n    try {\n        insulinSettings.setCustomClassNames(colors, classNames);\n    } catch (Exception e) {\n        e.printStackTrace();\n    }\n    DLR101SegmentationSettings glomeruliSettings = new DLR101SegmentationSettings(\"Glomeruli\", \"http://ares:8080/orbit/rdf?orbitID=23539963\", \"C:\\\\Users\\\\fullejo1\\\\Downloads\\\\glomeruli-410k.pb\", 512, 512, 2, \"Glomeruli\", true);\n    // Since this is a two-step method it needs a different approach...\n    //MaskRCNNSegment corpus_callosum = new MaskRCNNSegment(new File(\"D:/deeplearning/insulin/models/insulin_009.pb\"), MaskRCNNSegment.PostProcessMethod.STANDARD);\n    //        dLSegmentMethods.add(nuclei);\n    //        dLSegmentMethods.add(insulin);\n    //        dLSegmentMethods.add(glomeruli);\n    AbstractSegmentationSettings<?>[] dLSegmentArray = new AbstractSegmentationSettings<?>[] { nucleiSettings, insulinSettings, glomeruliSettings };\n    dlSegmentMethodsModel = new DLSegmentModelComboBoxModel(dLSegmentArray);\n    dLMethodComboBox = new JComboBox<>(dlSegmentMethodsModel);\n    panel.add(dLMethodComboBox);\n    // TODO: Add a checkbox (Store annotations).\n    setCompBounds(panel, frameWidth - 50);\n    panelDeepLearning.add(panel);\n    tabs.add(\"Deep Learning\", panelDeepLearning);\n    // roi\n    JPanel panelROI = new JPanel();\n    panelROI.setLayout(new FlowLayout(FlowLayout.LEFT, 20, 15));\n    List<AnnotationGroupLabel> annotationGroups = new ArrayList<>(12);\n    annotationGroups.add(new AnnotationGroupLabel(-1, \"Ignore\"));\n    annotationGroups.add(new AnnotationGroupLabel(0, \"All Groups\"));\n    for (int g = 1; g < (OrbitUtils.ANNOTATION_GROUPS + 1); g++) annotationGroups.add(new AnnotationGroupLabel(g, \"Group \" + g));\n    cbAnnotationROI = new JComboBox<>(annotationGroups.toArray(new AnnotationGroupLabel[0]));\n    cbAnnotationROI.setSelectedIndex(0);\n    // +1 because 'ignore' is -1\n    int groupIdx = OrbitImageAnalysis.getInstance().getModel().getAnnotationGroup() + 1;\n    if (cbAnnotationROI.getItemCount() > groupIdx)\n        cbAnnotationROI.setSelectedIndex(groupIdx);\n    cbAnnotationROI.setToolTipText(\"Use annotations (excl/incl/ROI) to build the ROI?\");\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Use annotations as ROI:\");\n    panel.add(lab);\n    panel.add(cbAnnotationROI);\n    setCompBounds(panel, frameWidth - 50);\n    panelROI.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"Fixed circular ROI (pixel):\");\n    panel.add(lab);\n    tfFixedROI = new JTextField();\n    tfFixedROI.setText(Integer.toString(OrbitImageAnalysis.getInstance().getModel().getFixedCircularROI()));\n    tfFixedROI.setInputVerifier(new IntInputVerifier(0));\n    panel.add(tfFixedROI);\n    setCompBounds(panel, frameWidth - 50);\n    panelROI.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"ROI offset X:\");\n    panel.add(lab);\n    tfROIX = new JTextField();\n    tfROIX.setText(Integer.toString(OrbitImageAnalysis.getInstance().getModel().getFixedROIOffsetX()));\n    tfROIX.setInputVerifier(new IntInputVerifier(0));\n    panel.add(tfROIX);\n    setCompBounds(panel, frameWidth - 50);\n    panelROI.add(panel);\n    panel = new JPanel(new GridLayout(1, 2));\n    lab = new JLabel(\"ROI offset Y:\");\n    panel.add(lab);\n    tfROIY = new JTextField();\n    tfROIY.setText(Integer.toString(OrbitImageAnalysis.getInstance().getModel().getFixedROIOffsetY()));\n    tfROIY.setInputVerifier(new IntInputVerifier(0));\n    panel.add(tfROIY);\n    setCompBounds(panel, frameWidth - 50);\n    panelROI.add(panel);\n    tabs.add(\"ROI\", panelROI);\n    // image adjustments\n    JPanel panelImageAdjustments = new JPanel();\n    panelImageAdjustments.setLayout(new FlowLayout(FlowLayout.LEFT, 20, 15));\n    cbUseImageAdjustments = new JCheckBox(\"Use image adjustments *\", featureDescription.isUseImageAdjustments());\n    cbUseImageAdjustments.setToolTipText(\"load image adjustments from database before classification (brightness, contrast, gamma) - use with care!\");\n    setCompBounds(cbUseImageAdjustments, frameWidth);\n    panelImageAdjustments.add(cbUseImageAdjustments);\n    JLabel imageAdjustmentWarning = new JLabel(\"<html><body>* If activated, image adjustments like gamma, brightness \" + \"and contrast from the image<br/>adjustment panel are taken into account.\" + \"<br/>Please use it with care, because it will influence the quantification results,<br/>\" + \"thus you might get an unwanted bias.\" + \"<br/>If you really want to use it, please double-check that the image adjustments are saved.</body></html>\");\n    panelImageAdjustments.add(imageAdjustmentWarning);\n    tabs.add(\"Image Adjustments\", panelImageAdjustments);\n    // layout\n    setLayout(new BorderLayout());\n    add(tabs, BorderLayout.CENTER);\n    btnOK = new JButton(\"OK\");\n    add(btnOK, BorderLayout.SOUTH);\n    tabs.setSelectedIndex(FeaturesAdminFrame.selectedTab);\n    tabs.addChangeListener(e -> {\n        logger.trace(\"selected tab: \" + tabs.getSelectedIndex());\n        FeaturesAdminFrame.selectedTab = tabs.getSelectedIndex();\n    });\n    addActionListeners();\n    //        Toolkit toolkit = getToolkit();\n    //        Dimension size = toolkit.getScreenSize();\n    //setLocation((size.width - getWidth()) / 2, (size.height - getHeight()) / 2);\n    setLocationRelativeTo(OrbitImageAnalysis.getInstance());\n    //        Toolkit toolkit = getToolkit();\n    //        Dimension size = toolkit.getScreenSize();\n}",
        "tuc": "@Test\npublic void testSegmentationAnnotationsCustom() throws Exception {\n    // Test image (Orbit/RDF ID).\n    int[] images = { 19340922 };\n    // Do a 'low-res' segmentation.\n    RawDataFile rdf = DALConfig.getImageProvider().LoadRawDataFile(images[0]);\n    RecognitionFrame rf = new RecognitionFrame(rdf);\n    // Whole slide image containing a brain (at top-level of image pyramid - lowest resolution).\n    BufferedImage smallImage = rf.bimg.getMipMaps()[rf.bimg.getMipMaps().length - 1].getImage().getAsBufferedImage();\n    // Size of image to use for brain detection.\n    Point brainImgDims = new Point(512, 512);\n    // Size of image to use for Corpus Callosum detection.\n    Point ccImgDims = new Point(512, 512);\n    // Load MaskRCNN models.\n    File maskRCNNBrainModel = new File(\"D:/deeplearning/corpus_callosum/finalbrainDetect2.pb\");\n    File maskRCNNCorpusCallosumModel = new File(\"D:/deeplearning/corpus_callosum/finalbrain15-56b.pb\");\n    // Setup the brain detector.\n    MaskRCNNSegmentationSettings brainSettings = new MaskRCNNSegmentationSettings(\"Brain\", \"http://ares:8080/orbit/rdf?orbitID=19340900\", null, brainImgDims.x, brainImgDims.y, 1f, 1, 28, 28, 2, \"Brain\", false, MaskRCNNSegmentationSettings.PostProcessMethod.CUSTOM);\n    MaskRCNNSegment brainModel = new MaskRCNNSegment(brainSettings);\n    // Resize the whole slide image to low-res for the MaskRCNN brain detection.\n    BufferedImage image512 = DLHelpers.resize(smallImage, brainSettings.getImageWidth(), brainSettings.getImageHeight());\n    Tensor<Float> input = DLHelpers.convertBufferedImageToTensor(image512, brainSettings.getImageWidth(), brainSettings.getImageHeight());\n    // Detect, and process the detected brain.\n    MaskRCNNRawDetections rawBrain = brainModel.getMaskRCNNRawDetections(input);\n    MaskRCNNDetections brainz = brainModel.processDetections(brainSettings.getImageWidth(), brainSettings.getImageHeight(), rawBrain);\n    // Should only find one brain.\n    assertEquals(brainz.getDetections().size(), 1);\n    // Bounding box that covers the brain.\n    Rectangle brainBB = brainz.getBoundingBoxes().get(0);\n    // Scaling factor to rescale the bounding box to the whole slide image.\n    float xScale = (float) smallImage.getWidth() / (float) brainImgDims.x;\n    float yScale = (float) smallImage.getHeight() / (float) brainImgDims.y;\n    // Create a border padding around the bounding box,\n    // rescale it so the bounding box is the same scale as the whole slide image.\n    // and then chop out a sub-image that covers the brain from the whole slide image.\n    int pad = 20;\n    Rectangle brainBBPadded = new Rectangle(brainBB.x - pad, brainBB.y - pad, brainBB.width + pad * 2, brainBB.height + pad * 2);\n    Rectangle bbScaled = new Rectangle((int) ((brainBBPadded.x) * xScale), (int) ((brainBBPadded.y) * yScale), (int) ((brainBBPadded.width) * xScale), (int) ((brainBBPadded.height) * yScale));\n    bbScaled = new Rectangle(smallImage.getMinX(), smallImage.getMinY(), smallImage.getWidth(), smallImage.getHeight()).intersection(bbScaled);\n    BufferedImage brainImg = smallImage.getSubimage(bbScaled.x, bbScaled.y, (int) bbScaled.getWidth(), (int) bbScaled.getHeight());\n    brainImg = DLHelpers.resize(brainImg, ccImgDims.x, ccImgDims.y);\n    // Determine the rescaling factor for the area of interest (brain + padding) compared to the desired size\n    // of the image for detecting the Corpus Callosum.\n    float brainScaleX = (float) brainBBPadded.width / (float) ccImgDims.x;\n    float brainScaleY = (float) brainBBPadded.height / (float) ccImgDims.y;\n    // Settings for Corpus Callosum detection.\n    MaskRCNNSegmentationSettings corpusCallosumSettings = new MaskRCNNSegmentationSettings(\"Corpus Callosum\", \"http://ares:8080/orbit/rdf?orbitID=19340900\", null, ccImgDims.x, ccImgDims.y, brainScaleX, brainScaleY, 1, 56, 56, 2, \"Corpus_Callosum\", false, MaskRCNNSegmentationSettings.PostProcessMethod.CUSTOM);\n    // Setup the Corpus Callosum segmentation model.\n    MaskRCNNSegment ccModel = new MaskRCNNSegment(corpusCallosumSettings);\n    // Create tensor from brain image.\n    Tensor<Float> input2 = DLHelpers.convertBufferedImageToTensor(brainImg, corpusCallosumSettings.getImageWidth(), corpusCallosumSettings.getImageHeight());\n    // Apply Corpus Callosum Model\n    MaskRCNNRawDetections rawCC = ccModel.getMaskRCNNRawDetections(input2);\n    // Scaling factor for the whole slide image relative to the brain image used for Corpus Callosum detection.\n    float imageScaleX = (float) rf.bimg.getWidth() / (float) brainImgDims.x;\n    float imageScaleY = (float) rf.bimg.getHeight() / (float) brainImgDims.y;\n    // Extract the detections for annotations.\n    // TODO: This method is currently being abused... (point being used for scaling, not translation...)\n    Point2D scaleFactor = new Point2D.Float(imageScaleX, imageScaleY);\n    MaskRCNNDetections cc = ccModel.processDetections(corpusCallosumSettings.getImageWidth(), corpusCallosumSettings.getImageHeight(), rawCC, brainBBPadded, scaleFactor, new Point(0, 0));\n    // Store annotations.\n    //ccModel.storeShapes(cc, corpusCallosumSettings, 19340922, \"AutomatedAnnotation\");\n    // Expect one Corpus Callosum\n    assertEquals(cc.getDetections().size(), 1);\n    // Expect Bounding Box:\n    assertEquals(cc.getDetections().get(0).getBoundingBox().getBounds(), new Rectangle(20787, 10238, 3173, 18283));\n}",
        "label": 0
    },
    {
        "repo_name": "ncats___molvec",
        "commit": "db0f0acef17e8b3f8de02482723a21afa96d0597",
        "commit_message": "a litle more precise about heuristic for dash line final rescue\n",
        "p_path": "src/main/java/tripod/molvec/algo/StructureImageExtractor.java",
        "t_path": "src/test/java/tripod/molvec/algo/RegressionTest.java",
        "p_name": "load",
        "t_name": "test1",
        "lpfc": "private void load(Bitmap aBitMap) throws Exception {\n    ctabRaw.clear();\n    ocrAttempt.clear();\n    bitmap = aBitMap;\n    SCOCR[] socr = new SCOCR[] { OCR_DEFAULT.orElse(OCR_BACKUP, OCRcutoffCosine) };\n    double[] maxBondLength = new double[] { INITIAL_MAX_BOND_LENGTH };\n    thin = bitmap.thin();\n    {\n        List<int[]> hollow = thin.findHollowPoints();\n        if (hollow.size() > 0.002 * thin.fractionPixelsOn() * thin.width() * thin.height()) {\n            bitmap = new Bitmap.BitmapBuilder(bitmap).boxBlur(1).threshold(2).build();\n            thin = bitmap.thin();\n        }\n    }\n    polygons = bitmap.connectedComponents(Bitmap.Bbox.DoublePolygon);\n    boolean isLarge = false;\n    if (!polygons.isEmpty()) {\n        isLarge = polygons.size() > 4000;\n    }\n    if (isLarge) {\n        throw new IllegalStateException(\"Cannot support images with over 4000 polygons at this time\");\n    }\n    Set<Shape> likelyOCR = Collections.synchronizedSet(new LinkedHashSet<Shape>());\n    Set<Shape> likelyOCRNumbers = Collections.synchronizedSet(new LinkedHashSet<Shape>());\n    Set<Shape> likelyOCRNonBond = Collections.synchronizedSet(new LinkedHashSet<Shape>());\n    Set<Shape> likelyOCRAll = Collections.synchronizedSet(new LinkedHashSet<Shape>());\n    Set<Shape> likelyOCRIgnore = Collections.synchronizedSet(new LinkedHashSet<Shape>());\n    List<Shape> ocrRescues = new ArrayList<Shape>();\n    Set<Shape> verticalShapes = Collections.synchronizedSet(new LinkedHashSet<Shape>());\n    processOCR(socr[0], polygons, bitmap, thin, (s, potential) -> {\n        ocrAttempt.put(s, potential);\n        if (potential.stream().filter(e -> e.v().doubleValue() > OCRcutoffCosine).findAny().isPresent()) {\n            CharType ct = OCRIsLikely(potential.get(0));\n            if (ct.equals(CharType.ChemLikely)) {\n                likelyOCR.add(s);\n                likelyOCRNonBond.add(s);\n            } else if (ct.equals(CharType.NumericLikely)) {\n                likelyOCRNonBond.add(s);\n                likelyOCRNumbers.add(s);\n            } else if (ct.equals(CharType.VerticalBondLikely)) {\n                verticalShapes.add(s);\n            }\n            likelyOCRAll.add(s);\n        }\n    });\n    double averageHeightOCR1 = likelyOCR.stream().map(Shape::getBounds2D).filter(Objects::nonNull).mapToDouble(Rectangle2D::getHeight).filter(Objects::nonNull).average().orElse(10000);\n    verticalShapes.stream().filter(s -> s.getBounds2D().getHeight() > averageHeightOCR1 * 1.5).forEach(s -> {\n        likelyOCR.remove(s);\n        likelyOCRNumbers.remove(s);\n        likelyOCRAll.remove(s);\n    });\n    List<Shape> circles = polygons.stream().filter(p -> !likelyOCRAll.contains(p)).map(s -> Tuple.of(s, GeomUtil.getCircleLikeScore(s))).filter(t -> t.v() > 0.9).map(t -> t.k()).map(s -> GeomUtil.growShape(s, 2)).collect(Collectors.toList());\n    lines = GeomUtil.asLines(thin.segments()).stream().filter(l -> !circles.stream().filter(s -> s.contains(l.getP1()) || s.contains(l.getP1())).findFirst().isPresent()).map(l -> GeomUtil.LineWrapper.of(l)).collect(Collectors.toList());\n    ctabRaw.clear();\n    boolean[] foundNewOCR = new boolean[] { true };\n    int repeats = 0;\n    List<Shape> realRescueOCRCandidates = Collections.synchronizedList(new ArrayList<>());\n    double[] averageHeightOCRFinal = new double[] { 0 };\n    double[] averageWidthOCRFinal = new double[] { 0 };\n    List<Point2D> intersectionNodes = new ArrayList<>();\n    if (PRE_RESCUE_OCR) {\n        rescueOCR(lines, polygons, likelyOCR, socr[0], (s, potential) -> {\n            String ss = potential.get(0).k().toString();\n            if (ss.equalsIgnoreCase(\"C\")) {\n                return;\n            }\n            Point2D cent = GeomUtil.findCenterOfShape(s);\n            boolean cont = likelyOCRAll.stream().filter(s1 -> s1.contains(cent)).findAny().isPresent();\n            if (cont)\n                return;\n            ocrAttempt.put(s, potential);\n            CharType ct = OCRIsLikely(potential.get(0));\n            if (ct.equals(CharType.ChemLikely)) {\n                likelyOCR.add(s);\n                likelyOCRNonBond.add(s);\n            } else if (ct.equals(CharType.NumericLikely)) {\n                likelyOCRNonBond.add(s);\n                likelyOCRNumbers.add(s);\n            }\n            likelyOCRAll.add(s);\n        });\n    }\n    double[] ignoreTooSmall = new double[] { 0.0 };\n    while (foundNewOCR[0] && repeats < MAX_OCR_FULL_REPEATS) {\n        if (Thread.interrupted())\n            throw new InterruptedException();\n        repeats++;\n        foundNewOCR[0] = false;\n        intersectionNodes.clear();\n        double averageLargestOCR = likelyOCR.stream().map(s -> GeomUtil.getPairOfFarthestPoints(s)).filter(p -> p != null && p.length == 2).mapToDouble(p -> p[0].distance(p[1])).average().orElse(0);\n        double averageAreaOCR = likelyOCR.stream().mapToDouble(s -> GeomUtil.area(s)).average().orElse(0);\n        double averageWidthOCR = likelyOCR.stream().map(Shape::getBounds2D).filter(Objects::nonNull).mapToDouble(Rectangle2D::getWidth).filter(Objects::nonNull).average().orElse(0);\n        double averageHeightOCR = likelyOCR.stream().map(Shape::getBounds2D).filter(Objects::nonNull).mapToDouble(Rectangle2D::getHeight).filter(Objects::nonNull).average().orElse(0);\n        double averageWidthNumberOCR = likelyOCRNumbers.stream().map(Shape::getBounds2D).filter(Objects::nonNull).mapToDouble(Rectangle2D::getWidth).filter(Objects::nonNull).average().orElse(0);\n        double averageHeightNumberOCR = likelyOCRNumbers.stream().map(Shape::getBounds2D).filter(Objects::nonNull).mapToDouble(Rectangle2D::getHeight).filter(Objects::nonNull).average().orElse(0);\n        averageHeightOCRFinal[0] = averageHeightOCR;\n        averageWidthOCRFinal[0] = averageWidthOCR;\n        likelyOCRAll.retainAll(likelyOCRAll.stream().filter(Objects::nonNull).map(s -> Tuple.of(s, GeomUtil.getPairOfFarthestPoints(s))).filter(t -> t.v()[0].distance(t.v()[1]) > averageLargestOCR * MIN_LONGEST_WIDTH_RATIO_FOR_OCR_TO_AVERAGE).map(t -> t.k()).collect(Collectors.toList()));\n        likelyOCR.retainAll(likelyOCR.stream().map(s -> Tuple.of(s, GeomUtil.getPairOfFarthestPoints(s))).filter(t -> t.v()[0].distance(t.v()[1]) > averageLargestOCR * MIN_LONGEST_WIDTH_RATIO_FOR_OCR_TO_AVERAGE).map(t -> t.k()).collect(Collectors.toList()));\n        Predicate<Line2D> isInOCRShape = (l) -> {\n            if (likelyOCR.isEmpty())\n                return false;\n            Tuple<Shape, Double> shape1 = GeomUtil.findClosestShapeTo(likelyOCRNonBond, l.getP1());\n            if (shape1.v() > OCR_TO_BOND_MAX_DISTANCE) {\n                return false;\n            }\n            Tuple<Shape, Double> shape2 = GeomUtil.findClosestShapeTo(likelyOCRNonBond, l.getP2());\n            if (shape2.v() > OCR_TO_BOND_MAX_DISTANCE) {\n                return false;\n            }\n            if (shape1.k() == shape2.k()) {\n                return true;\n            }\n            boolean anyOutside = GeomUtil.getLinesNotInside(l, Arrays.asList(shape1.k(), shape2.k())).stream().filter(l1 -> l1 != null).filter(GeomUtil.longerThan(1)).findAny().isPresent();\n            if (!anyOutside)\n                return true;\n            return false;\n        };\n        Predicate<Line2D> tryToMerge = isInOCRShape.negate().and((l) -> {\n            return true;\n        });\n        List<Line2D> useLines = lines.stream().filter(t -> !likelyOCRIgnore.stream().filter(s -> s.contains(t.centerPoint())).findAny().isPresent()).map(l -> l.getLine()).collect(Collectors.toList());\n        List<LineWrapper> smallLines = useLines.stream().filter(tryToMerge).map(l -> LineWrapper.of(l)).collect(Collectors.toList());\n        List<Line2D> bigLines = useLines.stream().filter(tryToMerge.negate()).collect(Collectors.toList());\n        smallLines = bitmap.combineLines(smallLines, MAX_DISTANCE_FOR_STITCHING_SMALL_SEGMENTS, MAX_TOLERANCE_FOR_STITCHING_SMALL_SEGMENTS_FULL, MAX_POINT_DISTANCE_TO_BE_PART_OF_MULTI_NODE, MAX_ANGLE_FOR_JOINING_SEGMENTS, MIN_SIZE_FOR_ANGLE_COMPARE_JOINING_SEGMENTS);\n        smallLines = thin.combineLines(smallLines, MAX_DISTANCE_FOR_STITCHING_SMALL_SEGMENTS, MAX_TOLERANCE_FOR_STITCHING_SMALL_SEGMENTS_THIN, MAX_POINT_DISTANCE_TO_BE_PART_OF_MULTI_NODE, MAX_ANGLE_FOR_JOINING_SEGMENTS, MIN_SIZE_FOR_ANGLE_COMPARE_JOINING_SEGMENTS);\n        List<Line2D> removedTinyLines = smallLines.stream().map(l -> l.getLine()).filter(GeomUtil.longerThan(MAX_DISTANCE_FOR_STITCHING_SMALL_SEGMENTS).negate()).collect(Collectors.toList());\n        List<Point2D> removedTinyVertices = removedTinyLines.stream().flatMap(l -> Stream.of(l.getP1(), l.getP2())).collect(Collectors.toList());\n        smallLines = smallLines.stream().filter(l -> l.length() > MAX_DISTANCE_FOR_STITCHING_SMALL_SEGMENTS).collect(Collectors.toList());\n        List<Point2D> verts = smallLines.stream().flatMap(l -> l.streamPoints()).collect(Collectors.toList());\n        double[] lDistOCRToLine = likelyOCR.stream().map(s -> Tuple.of(s, GeomUtil.findCenterOfShape(s))).map(Tuple.vmap(p -> Tuple.of(p, GeomUtil.findClosestPoint(verts, p)))).map(Tuple.vmap(t -> t.k().distance(t.v()))).mapToDouble(t -> t.v()).sorted().toArray();\n        List<Shape> extendableOCR = likelyOCR.stream().map(s -> Tuple.of(s, ocrAttempt.get(s))).filter(t -> t.v() != null).filter(t -> t.v().size() > 0).map(Tuple.vmap(l -> l.get(0).k().toString())).filter(t -> !t.v().equals(\"H\")).map(t -> t.k()).collect(Collectors.toList());\n        OptionalDouble avgDistOCRToLine = Optional.of(0).filter(d -> lDistOCRToLine.length > 0).map(d -> lDistOCRToLine[lDistOCRToLine.length / 2]).map(d -> OptionalDouble.of(d)).orElse(OptionalDouble.empty());\n        linesJoined = Stream.concat(bigLines.stream().map(l -> LineWrapper.of(l)), smallLines.stream()).collect(Collectors.toList());\n        double largestBond = smallLines.stream().mapToDouble(l -> l.length()).max().orElse(0);\n        double averageLine = smallLines.stream().mapToDouble(l -> l.length()).filter(d -> d > ignoreTooSmall[0]).average().orElse(0);\n        if (largestBond > 2.0 * averageLine) {\n            largestBond = 1.4 * averageLine;\n        }\n        List<List<LineWrapper>> preprocess = GeomUtil.reduceMultiBonds(Arrays.asList(linesJoined), MAX_ANGLE_FOR_PARALLEL, MAX_DISTANCE_TO_MERGE_PARALLEL_LINES, MIN_PROJECTION_RATIO_FOR_HIGH_ORDER_BONDS, 0, MAX_DELTA_LENGTH_FOR_STITCHING_LINES_ON_BOND_ORDER_CALC, (l) -> {\n        }).stream().map(t -> t.k()).map(t -> LineWrapper.of(t)).map(t -> Tuple.of(t, likelyOCR.stream().filter(s -> s.contains(t.centerPoint())).findAny().isPresent())).collect(Collectors.groupingBy(t -> t.v())).values().stream().map(tl -> tl.stream().map(t -> t.k()).collect(Collectors.toList())).collect(Collectors.toList());\n        List<LineWrapper> rejBondOrderLines = new ArrayList<>();\n        linesOrder = GeomUtil.reduceMultiBonds(preprocess, MAX_ANGLE_FOR_PARALLEL, largestBond / 3, MIN_PROJECTION_RATIO_FOR_HIGH_ORDER_BONDS, MIN_BIGGER_PROJECTION_RATIO_FOR_HIGH_ORDER_BONDS, MAX_DELTA_LENGTH_FOR_STITCHING_LINES_ON_BOND_ORDER_CALC, (rejLine -> rejBondOrderLines.add(LineWrapper.of(rejLine))));\n        List<Shape> growLines = linesOrder.stream().map(t -> t.k()).map(l -> GeomUtil.growLine(l, 5)).collect(Collectors.toList());\n        List<Shape> rescueOCRCandidates = new ArrayList<>();\n        List<Shape> connectedComponents = polygons.stream().map(s -> GeomUtil.growShape(s, 2)).collect(Collectors.toList());\n        int reps = 0;\n        boolean tooLongBond = true;\n        double maxRatioInitial = 0.5;\n        double maxTotalRatioInitial = 1.4;\n        double maxRatio = 0.5;\n        double maxTotalRatio = 1.4;\n        List<LineWrapper> dottedLines = new ArrayList<>();\n        while (tooLongBond) {\n            if (Thread.interrupted())\n                throw new InterruptedException();\n            rescueOCRCandidates.clear();\n            List<Tuple<Line2D, Integer>> linesOrderRestricted = linesOrder.stream().filter(t -> {\n                Line2D l = t.k();\n                return isInOCRShape.negate().test(l);\n            }).collect(Collectors.toList());\n            ctab = GeomUtil.getConnectionTable(linesOrderRestricted, extendableOCR, maxRatioForIntersection, maxCandidateRatioForIntersection, maxPerLineDistanceRatioForIntersection, minPerLineDistanceRatioForIntersection, maxCandidateRatioForIntersectionWithNeighbor, GeomUtil.longerThan(maxBondLength[0]).negate()).mergeNodesCloserThan(MAX_DISTANCE_BEFORE_MERGING_NODES);\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            ctab.getEdgesWhichMightBeWiggleLines().forEach(t -> {\n                realRescueOCRCandidates.add(t.k().getLine());\n                Point2D p1 = t.k().getLine().getP1();\n                Point2D p2 = t.k().getLine().getP2();\n                List<Node> rnodes = new ArrayList<>();\n                List<Node> lnodes = new ArrayList<>();\n                t.v().stream().flatMap(e -> e.streamNodes()).distinct().forEach(n -> {\n                    if (n.getPoint().distance(p1) < n.getPoint().distance(p2)) {\n                        n.setPoint(p1);\n                        rnodes.add(n);\n                    } else {\n                        n.setPoint(p2);\n                        lnodes.add(n);\n                    }\n                });\n                if (rnodes.size() > 0 && lnodes.size() > 0) {\n                    Edge e = ctab.addEdge(rnodes.get(0).getIndex(), lnodes.get(0).getIndex(), 1);\n                    e.setDashed(true);\n                    foundNewOCR[0] = true;\n                }\n            });\n            ctab.mergeNodesCloserThan(MAX_DISTANCE_BEFORE_MERGING_NODES);\n            ctab.standardCleanEdges();\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            List<Double> allDashLengths = new ArrayList<>();\n            ctab.getEdgesWhichMightBeDottedLines().stream().map(e -> Tuple.of(e, GeomUtil.vertices(e.stream().map(e1 -> e1.getLine()).collect(Collectors.toList())))).map(Tuple.vmap(vts -> GeomUtil.getPairOfFarthestPoints(vts))).forEach(s -> {\n                List<Node> rnodes = new ArrayList<>();\n                List<Node> lnodes = new ArrayList<>();\n                s.k().stream().peek(e -> allDashLengths.add(e.getEdgeLength())).flatMap(e -> e.streamNodes()).distinct().forEach(n -> {\n                    if (n.getPoint().distance(s.v()[0]) < n.getPoint().distance(s.v()[1])) {\n                        n.setPoint(s.v()[0]);\n                        rnodes.add(n);\n                    } else {\n                        n.setPoint(s.v()[1]);\n                        lnodes.add(n);\n                    }\n                });\n                dottedLines.add(LineWrapper.of(new Line2D.Double(s.v()[0], s.v()[1])));\n                if (rnodes.size() > 0 && lnodes.size() > 0) {\n                    Edge e = ctab.addEdge(rnodes.get(0).getIndex(), lnodes.get(0).getIndex(), 1);\n                    e.setDashed(true);\n                    foundNewOCR[0] = true;\n                }\n            });\n            ctab.standardCleanEdges();\n            ctab.mergeNodesCloserThan(MAX_DISTANCE_BEFORE_MERGING_NODES);\n            double avgDot = allDashLengths.stream().mapToDouble(d -> d).average().orElse(2);\n            if (foundNewOCR[0] && avgDot > ignoreTooSmall[0]) {\n                ignoreTooSmall[0] = avgDot * 1.1;\n                break;\n            } else {\n                foundNewOCR[0] = false;\n            }\n            for (Shape s : likelyOCR) {\n                ctab.mergeAllNodesInsideCenter(s, OCR_TO_BOND_MAX_DISTANCE);\n            }\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            List<List<Node>> newNodesForMerge = new ArrayList<>();\n            Function<List<Node>, Point2D> bestIntersectionPoint = (nl) -> {\n                Point2D center = GeomUtil.findCenterOfVertices(nl.stream().map(n -> n.getPoint()).collect(Collectors.toList()));\n                double radSq = Math.pow(ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE, 2);\n                List<Edge> el = nl.stream().flatMap(n -> n.getEdges().stream()).collect(Collectors.toList());\n                List<Point2D> intersections = GeomUtil.eachCombination(el).flatMap(t -> {\n                    if (t.k() == t.v())\n                        return Stream.of(t.k().getPoint1(), t.k().getPoint2());\n                    return Stream.of(GeomUtil.intersection(t.k().getLine(), t.v().getLine()));\n                }).filter(p -> p != null).filter(p -> p.distanceSq(center) < radSq).collect(Collectors.toList());\n                if (!intersections.isEmpty()) {\n                    return GeomUtil.findCenterMostPoint(intersections);\n                } else {\n                    return center;\n                }\n            };\n            List<Point2D> mergedPoints = new ArrayList<Point2D>();\n            ctab.mergeNodesCloserThan(ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE_INITIAL, (nl) -> {\n                Point2D[] far = GeomUtil.getPairOfFarthestPoints(nl.stream().map(n -> n.getPoint()).collect(Collectors.toList()));\n                if (nl.size() == 2 && far[0].distance(far[1]) > ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE) {\n                    return null;\n                }\n                if (far[0].distance(far[1]) > 0.9 * ctab.getAverageBondLength()) {\n                    List<Node> group1 = new ArrayList<>();\n                    List<Node> group2 = new ArrayList<>();\n                    nl.forEach(n -> {\n                        if (n.getPoint().distance(far[0]) < n.getPoint().distance(far[1])) {\n                            group1.add(n);\n                        } else {\n                            group2.add(n);\n                        }\n                    });\n                    newNodesForMerge.add(group1);\n                    newNodesForMerge.add(group2);\n                    return null;\n                }\n                Point2D cpt = GeomUtil.findCenterOfVertices(nl.stream().map(n -> n.getPoint()).collect(Collectors.toList()));\n                List<Point2D> missingPoints = removedTinyVertices.stream().filter(pt -> pt.distance(cpt) < ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE).collect(Collectors.toList());\n                missingPoints.addAll(nl.stream().map(n -> n.getPoint()).collect(Collectors.toList()));\n                if (missingPoints.size() > 3) {\n                    Shape candidate = GeomUtil.convexHull2(missingPoints.stream().toArray(i -> new Point2D[i]));\n                    if (GeomUtil.area(candidate) > 0.5 * averageAreaOCR) {\n                        Point2D center = GeomUtil.findCenterOfVertices(missingPoints);\n                        ;\n                        candidate = GeomUtil.growShape(candidate, 4);\n                        rescueOCRCandidates.add(candidate);\n                        return center;\n                    }\n                }\n                mergedPoints.addAll(missingPoints);\n                return bestIntersectionPoint.apply(nl);\n            });\n            newNodesForMerge.forEach(ln -> {\n                Point2D center = bestIntersectionPoint.apply(ln);\n                ctab.mergeNodes(ln.stream().map(n -> n.getIndex()).collect(Collectors.toList()), (ll) -> {\n                    return center;\n                });\n            });\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            ctab.removeOrphanNodes();\n            ctab.standardCleanEdges();\n            ctab.mergeNodesCloserThan(ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE, (nl) -> {\n                Point2D cpt = GeomUtil.findCenterOfVertices(nl.stream().map(n -> n.getPoint()).collect(Collectors.toList()));\n                List<Point2D> missingPoints = removedTinyVertices.stream().filter(pt -> pt.distance(cpt) < ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE).collect(Collectors.toList());\n                missingPoints.addAll(nl.stream().map(n -> n.getPoint()).collect(Collectors.toList()));\n                if (missingPoints.size() > 3) {\n                    Shape candidate = GeomUtil.convexHull2(missingPoints.stream().toArray(i -> new Point2D[i]));\n                    double area = GeomUtil.area(candidate);\n                    if (area > 0.5 * averageAreaOCR) {\n                        candidate = GeomUtil.growShape(candidate, 4);\n                        rescueOCRCandidates.add(candidate);\n                        return GeomUtil.findCenterOfVertices(missingPoints);\n                    }\n                }\n                mergedPoints.addAll(missingPoints);\n                return bestIntersectionPoint.apply(nl);\n            });\n            GeomUtil.groupThings(mergedPoints, (tp) -> {\n                Point2D p1 = tp.k();\n                Point2D p2 = tp.v();\n                if (p1.distance(p2) < ctab.getAverageBondLength() * 0.6) {\n                    return connectedComponents.stream().filter(s -> s.contains(p1) && s.contains(p2)).findAny().isPresent();\n                }\n                return false;\n            }).forEach(ll -> {\n                Point2D[] pts = ll.toArray(new Point2D[0]);\n                if (pts.length > 3) {\n                    Shape candidate = GeomUtil.convexHull2(pts);\n                    if (GeomUtil.area(candidate) > 0.5 * averageAreaOCR) {\n                        candidate = GeomUtil.growShape(candidate, 4);\n                        rescueOCRCandidates.add(candidate);\n                    }\n                }\n            });\n            ctab.removeOrphanNodes();\n            ctab.standardCleanEdges();\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            Set<Edge> splitEdges = new HashSet<Edge>();\n            ctab.createNodesOnIntersectingLines(2, elist -> {\n                splitEdges.addAll(elist);\n                return true;\n            }, (nn) -> {\n                intersectionNodes.add(nn.getPoint());\n            });\n            List<Shape> expectedLineZones = Stream.concat(growLines.stream(), likelyOCR.stream()).collect(Collectors.toList());\n            ctab.getEdges().stream().filter(e -> splitEdges.contains(e)).collect(Collectors.toList()).forEach(e -> {\n                Line2D ll = e.getLine();\n                double totLen = GeomUtil.getLinesNotInside(ll, expectedLineZones).stream().mapToDouble(l -> GeomUtil.length(l)).sum();\n                if (totLen > GeomUtil.length(ll) * 0.8) {\n                    ctab.removeEdge(e);\n                }\n            });\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            ctab.mergeFilteredNodesCloserThan(ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE_AFTER_SPLIT, n -> true);\n            ctab.standardCleanEdges();\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            if (avgDistOCRToLine.isPresent()) {\n                double nmaxRatio = (avgDistOCRToLine.getAsDouble()) / ctab.getAverageBondLength();\n                if (nmaxRatio > maxRatioInitial) {\n                    maxRatio = nmaxRatio;\n                    double maxlen = ctab.getEdges().stream().mapToDouble(e -> e.getEdgeLength()).max().orElse(1);\n                    maxlen = Math.max(maxlen, averageWidthOCR);\n                    maxTotalRatio = Math.max(maxTotalRatioInitial, maxlen / ctab.getAverageBondLength());\n                }\n            }\n            ctab.mergeNodesExtendingTo(likelyOCR, maxRatio, maxTotalRatio);\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            ctab.removeOrphanNodes();\n            ctab.mergeFilteredNodesCloserThan(ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE_AFTER_SPLIT, n -> true);\n            ctab.standardCleanEdges();\n            ctab.mergeFilteredNodesCloserThan(ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE, n -> {\n                if (intersectionNodes.stream().filter(p -> p.distance(n.getPoint()) < 0.01 * ctab.getAverageBondLength()).findAny().isPresent()) {\n                    if (n.getEdgeCount() == 4)\n                        return false;\n                    return true;\n                }\n                return true;\n            });\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            for (Shape s : likelyOCR) {\n                ctab.mergeAllNodesInsideCenter(s, OCR_TO_BOND_MAX_DISTANCE);\n            }\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            ctab.makeMissingNodesForShapes(likelyOCR, MAX_BOND_TO_AVG_BOND_RATIO_FOR_NOVEL, MIN_BOND_TO_AVG_BOND_RATIO_FOR_NOVEL);\n            Set<Node> toRemove = new HashSet<Node>();\n            Set<Edge> toRemoveEdges = new HashSet<Edge>();\n            ctab.makeMissingBondsToNeighbors(bitmap, MAX_BOND_TO_AVG_BOND_RATIO_FOR_NOVEL, MAX_TOLERANCE_FOR_DASH_BONDS, likelyOCR, OCR_TO_BOND_MAX_DISTANCE, (t) -> {\n                Edge e = t.v();\n                Node n1 = e.getRealNode1();\n                Node n2 = e.getRealNode2();\n                List<Edge> existingEdges1 = n1.getEdges();\n                List<Edge> existingEdges2 = n2.getEdges();\n                Set<Node> n1Neigh = existingEdges1.stream().flatMap(ne -> Stream.of(ne.getRealNode1(), ne.getRealNode2())).filter(n -> !n.equals(n1)).collect(Collectors.toSet());\n                Set<Node> n2Neigh = existingEdges2.stream().flatMap(ne -> Stream.of(ne.getRealNode1(), ne.getRealNode2())).filter(n -> !n.equals(n2)).collect(Collectors.toSet());\n                List<Node> commonNeigh = n1Neigh.stream().filter(nn -> n2Neigh.contains(nn)).collect(Collectors.toList());\n                boolean alreadyExists = false;\n                if (!commonNeigh.isEmpty()) {\n                    for (Node cn : commonNeigh) {\n                        Point2D cp = cn.getPoint();\n                        double distance1 = n1.getPoint().distance(cp);\n                        double distance2 = n2.getPoint().distance(cp);\n                        double sumd = distance1 + distance2;\n                        double ddelta = Math.abs(sumd - e.getEdgeLength());\n                        List<Edge> edges = cn.getEdges();\n                        if (edges.size() == 2) {\n                            if (ddelta < MAX_DELTA_LENGTH_FOR_STITCHING_LINES_ON_BOND_ORDER_CALC) {\n                                if (!toRemove.contains(n1) && !toRemove.contains(n2)) {\n                                    toRemove.add(cn);\n                                }\n                                double o2 = edges.stream().map(et -> Tuple.of(et, et.getEdgeLength())).mapToDouble(e1 -> (e1.k().getOrder() * e1.v())).sum();\n                                int o = (int) Math.round(((o2 / sumd) + 0.05));\n                                t.v().setOrder(o);\n                            }\n                            if (!edges.stream().anyMatch(e2 -> e2.getDashed())) {\n                                alreadyExists = true;\n                            }\n                        } else if (edges.size() == 4) {\n                            boolean isIntersection = intersectionNodes.stream().filter(in -> in.distance(cp) < 2).findAny().isPresent();\n                            if (isIntersection && Math.abs(sumd - t.v().getEdgeLength()) < ctab.getAverageBondLength() * 0.05) {\n                                toRemoveEdges.add(t.v());\n                            }\n                        }\n                    }\n                }\n                if (t.k() > MAX_TOLERANCE_FOR_SINGLE_BONDS) {\n                    if (!alreadyExists) {\n                        t.v().setDashed(true);\n                    }\n                }\n            });\n            toRemoveEdges.forEach(e -> ctab.removeEdge(e));\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            toRemove.forEach(n -> ctab.removeNodeAndEdges(n));\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            double avgBondLength = ctab.getAverageBondLength();\n            maxBondLength[0] = avgBondLength * MAX_BOND_TO_AVG_BOND_RATIO_TO_KEEP;\n            Predicate<Line2D> longerThanMax = GeomUtil.longerThan(maxBondLength[0]);\n            tooLongBond = ctab.getEdges().stream().filter(e -> longerThanMax.test(e.getLine())).findAny().isPresent();\n            if (tooLongBond) {\n                reps++;\n            }\n            if (reps > MAX_REPS)\n                break;\n        }\n        AtomicBoolean anyOtherIntersections = new AtomicBoolean(false);\n        ctab.createNodesOnIntersectingLines(3, elist -> {\n            long longEnoughBonds = elist.stream().filter(e -> e.getEdgeLength() > MAX_BOND_TO_AVG_BOND_RATIO_FOR_INTERSECTION * ctab.getAverageBondLength()).count();\n            if (longEnoughBonds < 3)\n                return false;\n            anyOtherIntersections.set(true);\n            return true;\n        }, (nn) -> {\n            intersectionNodes.add(nn.getPoint());\n        });\n        if (anyOtherIntersections.get()) {\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            ctab.mergeNodesCloserThan(ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE);\n            ctab.standardCleanEdges();\n        }\n        double shortestRealBondRatio = .3;\n        ctab.fixBondOrders(likelyOCR, shortestRealBondRatio, e -> {\n            e.setOrder(1);\n        });\n        double avgbond = ctab.getAverageBondLength();\n        double SEED_BOND_RATIO_FOR_OCR_WIDTH = 0.0;\n        double SEED_BOND_RATIO_FOR_OCR_WIDTH_FOR_CENTROID = 0.5;\n        List<Node> unmatchedNodes = ctab.getNodesNotInShapes(likelyOCR, OCR_TO_BOND_MAX_DISTANCE + avgbond * SEED_BOND_RATIO_FOR_OCR_WIDTH);\n        List<LineWrapper> verticesJl = linesJoined.stream().collect(Collectors.toList());\n        List<Point2D> verticesJ = verticesJl.stream().flatMap(l -> l.streamPoints()).collect(Collectors.toList());\n        List<Shape> toAddAllOCR = new ArrayList<Shape>();\n        Map<Shape, List<Tuple<Character, Number>>> gotCache = new HashMap<>();\n        unmatchedNodes.forEach(n -> {\n            Point2D cpt = n.getPoint();\n            Point2D centerRescue = rescueOCRCandidates.stream().filter(sc -> sc.contains(n.getPoint())).map(sc -> Tuple.of(sc, GeomUtil.area(sc)).withVComparator()).sorted(Comparator.reverseOrder()).map(t -> t.k()).findAny().map(sc -> GeomUtil.findCenterOfShape(sc)).orElse(null);\n            int numEdges = n.getEdges().size();\n            Shape[] area = new Shape[] { null };\n            Shape nshape = null;\n            double radius = Math.max(avgbond * SEED_BOND_RATIO_FOR_OCR_WIDTH_FOR_CENTROID, averageLargestOCR / 2);\n            List<Point2D> allVertices = verticesJ;\n            if (centerRescue != null) {\n                cpt = centerRescue;\n            }\n            boolean keep = true;\n            for (int i = 0; i < 3; i++) {\n                keep = true;\n                area[0] = GeomUtil.makeShapeAround(cpt, radius);\n                List<Point2D> insideVertices = allVertices.stream().filter(v -> area[0].contains(v)).collect(Collectors.toList());\n                if (insideVertices.size() <= numEdges + 1) {\n                    keep = false;\n                }\n                List<Point2D> insideVertices2 = allVertices.stream().filter(v -> area[0].contains(v)).collect(Collectors.toList());\n                Point2D center = GeomUtil.findCenterOfVertices(insideVertices2);\n                ;\n                double distanceMean = insideVertices2.stream().mapToDouble(pt -> center.distance(pt)).average().orElse(0);\n                double distanceVar = insideVertices2.stream().mapToDouble(pt -> Math.pow(distanceMean - center.distance(pt), 2)).sum();\n                double distanceSTDEV = Math.sqrt(distanceVar / (insideVertices2.size() - 1));\n                List<Point2D> realMissing = insideVertices2.stream().filter(pt -> center.distance(pt) < distanceMean + distanceSTDEV * 2.5).collect(Collectors.toList());\n                nshape = GeomUtil.convexHull2(realMissing.toArray(new Point2D[0]));\n                Point2D[] far = GeomUtil.getPairOfFarthestPoints(nshape);\n                double arean = GeomUtil.area(nshape);\n                double r = 0;\n                if (far != null) {\n                    r = far[0].distance(far[1]);\n                }\n                if (r < averageLargestOCR * MIN_LONGEST_WIDTH_RATIO_FOR_OCR_TO_AVERAGE) {\n                    keep = false;\n                }\n                if (arean < GeomUtil.area(nshape.getBounds2D()) * MIN_AREA_RATIO_FOR_HULL_TO_BBOX_OCR) {\n                    keep = false;\n                }\n                if (GeomUtil.area(nshape.getBounds2D()) > avgbond * avgbond * 0.5) {\n                    keep = false;\n                }\n                if (GeomUtil.area(nshape.getBounds2D()) < averageAreaOCR * MIN_AREA_RATIO_FOR_OCR_TO_AVERAGE) {\n                    keep = false;\n                }\n                if (GeomUtil.area(nshape.getBounds2D()) > averageAreaOCR * MAX_AREA_RATIO_FOR_OCR_TO_AVERAGE) {\n                    keep = false;\n                }\n                radius = Math.max(averageLargestOCR / 2, r / 2);\n                cpt = GeomUtil.findCenterOfShape(nshape);\n            }\n            if (keep) {\n                Bitmap nmap = bitmap.getLazyCrop(nshape);\n                Bitmap nthinmap = thin.getLazyCrop(nshape);\n                if (nmap != null && nthinmap != null) {\n                    nshape = GeomUtil.growShape(nshape, 2);\n                    nmap = bitmap.crop(nshape);\n                    List<Shape> slist = nmap.connectedComponents(Bitmap.Bbox.DoublePolygon);\n                    Shape bshape = slist.stream().map(s -> Tuple.of(s, s.getBounds2D().getWidth() * s.getBounds2D().getHeight()).withVComparator()).max(CompareUtil.naturalOrder()).map(t -> t.k()).orElse(nshape);\n                    Rectangle2D rect1 = nshape.getBounds2D();\n                    AffineTransform at = new AffineTransform();\n                    at.translate(rect1.getX(), rect1.getY());\n                    nshape = at.createTransformedShape(bshape).getBounds2D();\n                    nmap = bitmap.getLazyCrop(nshape);\n                    nthinmap = thin.getLazyCrop(nshape);\n                    if (GeomUtil.area(nshape) < averageAreaOCR * MIN_AREA_RATIO_FOR_OCR_TO_AVERAGE) {\n                        return;\n                    }\n                    if (nmap != null && nthinmap != null) {\n                        processOCRShape(socr[0], nshape, bitmap, thin, (s, potential) -> {\n                            if (potential.get(0).v().doubleValue() > OCRcutoffCosineRescue) {\n                                String st = potential.get(0).k().toString();\n                                if (BranchNode.interpretOCRStringAsAtom2(st) != null) {\n                                    toAddAllOCR.add(s);\n                                    gotCache.put(s, potential);\n                                }\n                            }\n                        });\n                    }\n                }\n            }\n        });\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        GeomUtil.mergeOverlappingShapes(toAddAllOCR, 0.75).forEach(nshape -> {\n            boolean sigOverlap = likelyOCRAll.stream().map(s -> Tuple.of(s, GeomUtil.getIntersectionShape(nshape, s))).filter(os -> os.v().isPresent()).map(Tuple.vmap(os -> os.get())).map(Tuple.vmap(s -> GeomUtil.area(s))).map(Tuple.kmap(s -> GeomUtil.area(s))).mapToDouble(t -> t.v() / t.k()).filter(areaFraction -> areaFraction > 0.0).findAny().isPresent();\n            if (sigOverlap) {\n                return;\n            }\n            List<Tuple<Character, Number>> matches = gotCache.getOrDefault(nshape, new ArrayList<>());\n            if (matches.isEmpty()) {\n                Bitmap nmap = bitmap.getLazyCrop(nshape);\n                Bitmap nthinmap = thin.getLazyCrop(nshape);\n                if (nmap != null && nthinmap != null) {\n                    processOCRShape(socr[0], nshape, bitmap, thin, (s, potential) -> {\n                        matches.addAll(potential);\n                    });\n                }\n            }\n            ocrAttempt.put(nshape, matches);\n            if (matches.get(0).v().doubleValue() > OCRcutoffCosineRescue) {\n                CharType ct = OCRIsLikely(matches.get(0));\n                if (ct.equals(CharType.ChemLikely)) {\n                    likelyOCR.add(nshape);\n                    likelyOCRNonBond.add(nshape);\n                } else if (ct.equals(CharType.NumericLikely)) {\n                    likelyOCRNonBond.add(nshape);\n                    likelyOCRNumbers.add(nshape);\n                }\n                likelyOCRAll.add(nshape);\n                ocrRescues.add(nshape);\n                foundNewOCR[0] = true;\n            }\n        });\n        ctab.mergeNodesExtendingTo(likelyOCR, maxRatio, maxTotalRatio);\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        double cosThetaOCRShape = Math.cos(MAX_THETA_FOR_OCR_SEPERATION);\n        List<Shape> growLikelyOCRNonBond = likelyOCRNonBond.stream().map(s -> GeomUtil.growShape(s, 2)).collect(Collectors.toList());\n        List<Shape> growLikelyOCR = likelyOCR.stream().map(s -> GeomUtil.growShape(s, 2)).collect(Collectors.toList());\n        List<Shape> maybeDash = polygons.stream().filter(s -> GeomUtil.area(s) < averageAreaOCR).filter(s -> !likelyOCR.contains(s)).map(s -> Tuple.of(s, GeomUtil.findCenterOfShape(s))).filter(st -> !growLikelyOCR.stream().filter(g -> g.contains(st.v())).findFirst().isPresent()).map(t -> t.k()).map(t -> Tuple.of(t, GeomUtil.findLongestSplittingLine(t))).filter(t -> {\n            if (t.v() != null) {\n                return t.v().length() < ctab.getAverageBondLength() * 0.5;\n            }\n            return true;\n        }).map(t -> t.k()).collect(Collectors.toList());\n        List<Shape> maybeDashCollection = GeomUtil.groupThings(maybeDash, t -> {\n            Shape s1 = t.k();\n            Shape s2 = t.v();\n            Point2D p1 = GeomUtil.findCenterOfShape(s1);\n            Point2D p2 = GeomUtil.findCenterOfShape(s2);\n            return p1.distance(p2) < ctab.getAverageBondLength() / 3;\n        }).stream().filter(sl -> sl.size() >= 3).map(l -> {\n            LineWrapper splitting = GeomUtil.findLongestSplittingLine(l.stream().collect(GeomUtil.joined()));\n            return l.stream().map(l1 -> Tuple.of(l1, GeomUtil.findLongestSplittingLine(l1))).filter(l2 -> splitting.cosTheta(l2.v()) < Math.cos(45 * Math.PI / 180)).map(t -> t.k()).collect(Collectors.toList());\n        }).map(l -> l.stream().collect(GeomUtil.joined())).filter(b -> b != null).filter(bshape -> {\n            Point2D[] pts = GeomUtil.getPairOfFarthestPoints(bshape);\n            double dist = pts[0].distance(pts[1]);\n            if (dist < ctab.getAverageBondLength() * 1.3 && dist > ctab.getAverageBondLength() * 0.6) {\n                return true;\n            } else {\n                return false;\n            }\n        }).collect(Collectors.toList());\n        likelyOCRAll.stream().filter(p -> maybeDashCollection.stream().filter(ds -> ds.contains(GeomUtil.findCenterOfShape(p))).findAny().isPresent()).collect(Collectors.toList()).forEach(ocs -> {\n            likelyOCRAll.remove(ocs);\n            likelyOCR.remove(ocs);\n            likelyOCRNumbers.remove(ocs);\n            likelyOCRNonBond.remove(ocs);\n        });\n        double wid = averageWidthOCR;\n        List<List<Shape>> ocrGroupList = GeomUtil.groupShapesIfClosestPointsMatchCriteria(likelyOCRAll, t -> {\n            Point2D[] pts = t.v();\n            Shape[] shapes = t.k();\n            double dist = pts[0].distanceSq(pts[1]);\n            double cutoff = Math.max(ctab.getAverageBondLength() * MAX_BOND_RATIO_FOR_OCR_CHAR_SPACING, wid);\n            if (dist > cutoff * cutoff) {\n                return false;\n            }\n            List<Tuple<Character, Number>> attempt0 = ocrAttempt.get(shapes[0]);\n            List<Tuple<Character, Number>> attempt1 = ocrAttempt.get(shapes[1]);\n            String v1 = (attempt0 == null || attempt0.isEmpty()) ? \"\" : attempt0.get(0).k().toString();\n            String v2 = (attempt1 == null || attempt1.isEmpty()) ? \"\" : attempt1.get(0).k().toString();\n            if (v1.equals(\"\\\\\") || v1.equals(\"/\") || v2.equals(\"\\\\\") || v2.equals(\"/\")) {\n                return false;\n            }\n            double s1y1 = shapes[0].getBounds2D().getMinY();\n            double s1y2 = shapes[0].getBounds2D().getMaxY();\n            double s2y1 = shapes[1].getBounds2D().getMinY();\n            double s2y2 = shapes[1].getBounds2D().getMaxY();\n            if (!((s1y1 >= s2y1 && s1y1 <= s2y2) || (s1y2 >= s2y1 && s1y2 <= s2y2) || ((s1y2 + s1y1) / 2 >= s2y1 && (s1y2 + s1y1) / 2 <= s2y2))) {\n                return false;\n            }\n            double pvoverlapT = Math.max(s1y2, s2y2) - Math.min(s1y1, s2y1);\n            double pvoverlapI = Math.min(s1y2, s2y2) - Math.max(s1y1, s2y1);\n            if (pvoverlapI < pvoverlapT * 0.4) {\n                if (likelyOCR.contains(shapes[0]) && likelyOCR.contains(shapes[1])) {\n                    if (!v1.equalsIgnoreCase(\"S\") && !v2.equalsIgnoreCase(\"S\")) {\n                        return false;\n                    }\n                }\n            }\n            Point2D cs1 = GeomUtil.findCenterOfShape(shapes[0]);\n            Point2D cs2 = GeomUtil.findCenterOfShape(shapes[1]);\n            LineWrapper cenLine = LineWrapper.of(new Line2D.Double(cs1, cs2));\n            double[] vec = cenLine.vector();\n            double cosTheta = Math.abs(vec[0] * cenLine.recipLength());\n            if (cosTheta > cosThetaOCRShape) {\n                return true;\n            }\n            return false;\n        });\n        bestGuessOCR.clear();\n        boolean areLikelyNumbers = (likelyOCRNumbers.size() >= 4);\n        ocrGroupList.stream().forEach(g -> {\n            List<Shape> sorted = g.stream().map(s -> Tuple.of(s, s)).map(Tuple.vmap(s -> s.getBounds2D().getMinX())).map(t -> t.withVComparator()).sorted().map(t -> t.k()).collect(Collectors.toList());\n            String soFar = \"\";\n            Shape making = null;\n            List<Tuple<Shape, Tuple<List<Shape>, String>>> toAdd = new ArrayList<>();\n            List<Shape> lsofar = new ArrayList<>();\n            for (Shape s : sorted) {\n                lsofar.add(s);\n                List<Tuple<Character, Number>> list = ocrAttempt.get(s);\n                String v = (list == null || list.isEmpty()) ? \"\" : list.get(0).k().toString();\n                if (s.getBounds2D().getHeight() < averageHeightOCR * 0.8 || (s.getBounds2D().getHeight() <= averageHeightNumberOCR * 1.1 && areLikelyNumbers)) {\n                    if (v.equalsIgnoreCase(\"S\")) {\n                        Tuple<Character, Number> tc = list.stream().filter(c -> c.k().toString().equals(\"3\") || c.k().toString().equals(\"8\")).findFirst().orElse(null);\n                        if (tc != null) {\n                            if (tc.v().doubleValue() > OCRcutoffCosineRescue) {\n                                v = tc.k().toString();\n                            }\n                        }\n                    }\n                }\n                if (v.equals(\"?\")) {\n                    v = \"N+\";\n                }\n                if (v.equals(\"-\")) {\n                    if (!soFar.equals(\"t\")) {\n                        if (making != null) {\n                            toAdd.add(Tuple.of(making, Tuple.of(lsofar, soFar)));\n                            lsofar.remove(lsofar.size() - 1);\n                            lsofar = new ArrayList<>();\n                        }\n                        soFar = \"\";\n                        making = null;\n                        continue;\n                    }\n                }\n                if (making == null) {\n                    making = s;\n                } else {\n                    making = GeomUtil.add(making, s);\n                }\n                soFar += v;\n            }\n            if (making != null) {\n                toAdd.add(Tuple.of(making, Tuple.of(lsofar, soFar)));\n            }\n            Map<String, List<String>> dontMerge = new HashMap<>();\n            dontMerge.put(\"OO\", Arrays.asList(\"O\", \"O\"));\n            dontMerge.put(\"Oo\", Arrays.asList(\"O\", \"o\"));\n            dontMerge.put(\"oO\", Arrays.asList(\"o\", \"O\"));\n            dontMerge.put(\"oo\", Arrays.asList(\"o\", \"o\"));\n            dontMerge.put(\"OF\", Arrays.asList(\"O\", \"F\"));\n            dontMerge.put(\"oF\", Arrays.asList(\"o\", \"F\"));\n            dontMerge.put(\"Fo\", Arrays.asList(\"F\", \"o\"));\n            dontMerge.put(\"FO\", Arrays.asList(\"F\", \"O\"));\n            dontMerge.put(\"FF\", Arrays.asList(\"F\", \"F\"));\n            dontMerge.put(\"CH3CH3\", Arrays.asList(\"CH3\", \"CH3\"));\n            dontMerge.put(\"cH3cH3\", Arrays.asList(\"cH3\", \"cH3\"));\n            dontMerge.put(\"HOOH\", Arrays.asList(\"HO\", \"OH\"));\n            dontMerge.put(\"OHOH\", Arrays.asList(\"OH\", \"OH\"));\n            dontMerge.put(\"OHHO\", Arrays.asList(\"OH\", \"HO\"));\n            dontMerge.put(\"BrBr\", Arrays.asList(\"Br\", \"Br\"));\n            for (Tuple<Shape, Tuple<List<Shape>, String>> tt : toAdd) {\n                boolean removeBad = false;\n                String val = tt.v().v();\n                List<Shape> contains = tt.v().k();\n                Shape parent = tt.k();\n                if (val.contains(\"~\")) {\n                    if (val.equals(\"~\")) {\n                        val = \"Cl\";\n                    } else {\n                        val = val.replace(\"~\", \"O\");\n                    }\n                }\n                if (val.contains(\"$\")) {\n                    val = val.replace(\"$\", \"O2\");\n                }\n                if (val.contains(\"!\")) {\n                    val = val.replace(\"!\", \"H3\");\n                }\n                if (val.contains(\"`\")) {\n                    val = val.replace(\"`\", \"HO\");\n                }\n                if (val.contains(\"%\")) {\n                    val = val.replace(\"%\", \"OC\");\n                }\n                if (val.matches(\"[cC][h][1ilt][r][a][1ilt]\")) {\n                    removeBad = true;\n                }\n                if (val.equals(\"IN\") || val.equals(\"tN\") || val.equals(\"lN\")) {\n                    bestGuessOCR.put(contains.get(1), \"N\");\n                    continue;\n                }\n                if (dontMerge.containsKey(val)) {\n                    List<String> keepAs = dontMerge.get(val);\n                    int findex = 0;\n                    for (int i = 0; i < keepAs.size(); i++) {\n                        String keep = keepAs.get(i);\n                        String g1 = val.substring(findex, findex + keep.length());\n                        Shape parts = contains.stream().skip(findex).limit(keep.length()).collect(GeomUtil.joined());\n                        findex = findex + keep.length();\n                        if (keep.equals(g1)) {\n                            bestGuessOCR.put(parts, keep);\n                        }\n                    }\n                    continue;\n                }\n                if (val.contains(\"F8\")) {\n                    val = val.replace(\"F8\", \"F3\");\n                }\n                if (val.matches(\"[cC]H[Ss]\")) {\n                    val = val.replaceAll(\"[cC]H[Ss]\", \"CH3\");\n                }\n                if (val.matches(\".*[cC][tlI][Ss].*\")) {\n                    val = val.replaceAll(\"[cC][tlI][Ss]\", \"Cl3\");\n                }\n                if (val.matches(\".*[cC][tlI][8].*\")) {\n                    val = val.replaceAll(\"[cC][tlI][8]\", \"Cl3\");\n                }\n                BranchNode bn = BranchNode.interpretOCRStringAsAtom2(val);\n                if (val.length() > 5) {\n                    if (bn == null) {\n                        removeBad = true;\n                    }\n                }\n                if (parent.getBounds2D().getHeight() <= averageHeightNumberOCR * 1.1 && areLikelyNumbers) {\n                    if (val.matches(\"[0-9t][0-9t]*\")) {\n                        val = \"#\" + val.replace(\"t\", \"1\");\n                        removeBad = true;\n                    }\n                }\n                if (bn == null || !bn.isRealNode()) {\n                    if (REMOVE_NONSENSE_OCR_LINES) {\n                        removeBad = true;\n                    }\n                }\n                if (removeBad) {\n                    contains.stream().forEach(s2 -> {\n                        likelyOCR.remove(s2);\n                        likelyOCRNumbers.remove(s2);\n                        likelyOCRNonBond.remove(s2);\n                        likelyOCRAll.remove(s2);\n                    });\n                    foundNewOCR[0] = true;\n                    likelyOCRIgnore.add(GeomUtil.growShape(parent, 2));\n                    likelyOCR.removeAll(ocrRescues);\n                    likelyOCRNumbers.removeAll(ocrRescues);\n                    likelyOCRNonBond.removeAll(ocrRescues);\n                    likelyOCRAll.removeAll(ocrRescues);\n                }\n                bestGuessOCR.put(parent, val);\n            }\n        });\n        bestGuessOCR.entrySet().stream().map(Tuple::of).filter(t -> t.v().equals(\"H\")).collect(Collectors.toList()).stream().forEach(t -> {\n            double cutoff = Math.max(ctab.getAverageBondLength() * MAX_BOND_RATIO_FOR_OCR_CHAR_SPACING, averageWidthOCR);\n            Tuple<Shape, String> toConnect = bestGuessOCR.entrySet().stream().map(Tuple::of).filter(t1 -> t1.k() != t.k()).filter(t1 -> t1.v().equals(\"N\") || t1.v().equals(\"Nt\") || t1.v().equals(\"N+\") || t1.v().equals(\"NI\") || t1.v().equals(\"Nl\") || t1.v().equals(\"O\") || t1.v().equals(\"S\")).filter(t1 -> GeomUtil.distance(t.k(), t1.k()) < cutoff).filter(t1 -> (Math.abs(t1.k().getBounds2D().getMinX() - t.k().getBounds2D().getMinX()) < cutoff / 3.0)).filter(t1 -> (Math.abs(t1.k().getBounds2D().getMaxX() - t.k().getBounds2D().getMaxX()) < cutoff / 1.5)).findFirst().orElse(null);\n            ;\n            if (toConnect != null) {\n                Shape nshape = GeomUtil.add(t.k(), toConnect.k());\n                String old = toConnect.v();\n                if (old.contains(\"N\") && !old.equals(\"N+\"))\n                    old = \"N\";\n                String nstring = old + t.v();\n                bestGuessOCR.put(nshape, nstring);\n                bestGuessOCR.remove(t.k());\n                bestGuessOCR.remove(toConnect.k());\n            }\n        });\n        bestGuessOCR.entrySet().stream().map(Tuple::of).filter(t -> t.v().equals(\"O2\") || t.v().equalsIgnoreCase(\"Boc\")).collect(Collectors.toList()).stream().forEach(t -> {\n            double cutoff = Math.max(ctab.getAverageBondLength() * MAX_BOND_RATIO_FOR_OCR_CHAR_SPACING, averageWidthOCR);\n            Tuple<Shape, String> toConnect = bestGuessOCR.entrySet().stream().map(Tuple::of).filter(t1 -> t1.k() != t.k()).filter(t1 -> t1.v().equals(\"S\") || t1.v().equals(\"N\")).filter(t1 -> GeomUtil.distance(t.k(), t1.k()) < cutoff).filter(t1 -> (Math.abs(t1.k().getBounds2D().getMinX() - t.k().getBounds2D().getMinX()) < cutoff / 3.0)).findFirst().orElse(null);\n            ;\n            if (toConnect != null) {\n                Shape nshape = GeomUtil.add(t.k(), toConnect.k());\n                String old = toConnect.v();\n                String nstring = old + t.v();\n                bestGuessOCR.put(nshape, nstring);\n                bestGuessOCR.remove(t.k());\n                bestGuessOCR.remove(toConnect.k());\n            }\n        });\n        ctab.standardCleanEdges();\n        List<Shape> ocrMeaningful = bestGuessOCR.keySet().stream().peek(t -> System.out.println(bestGuessOCR.get(t))).filter(s -> BranchNode.interpretOCRStringAsAtom2(bestGuessOCR.get(s)) != null).collect(Collectors.toList());\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        Set<Node> alreadyFixedNodes = new HashSet<Node>();\n        bestGuessOCR.entrySet().stream().map(Tuple::of).map(Tuple.vmap(s -> Tuple.of(s, (s.equals(\"H\")) ? 1 : 0).withVComparator())).map(t -> t.withVComparator()).sorted().filter(t -> !t.v().k().startsWith(\"#\")).map(Tuple.vmap(t -> t.k())).collect(Collectors.toList()).forEach(shapeString -> {\n            Shape s = shapeString.k();\n            String sym = shapeString.v();\n            Point2D cen = GeomUtil.findCenterOfShape(s);\n            Point2D centert = cen;\n            BranchNode actual1;\n            if (sym.equals(\"I\")) {\n                List<Node> ln = ctab.getNodesInsideShape(s, 2);\n                if (ln.isEmpty())\n                    return;\n                boolean isLinker = ln.stream().filter(n -> n.getEdgeCount() > 1).findAny().isPresent();\n                if (isLinker)\n                    return;\n                boolean tooClose = ctab.getNodes().stream().filter(n -> !ln.contains(n)).filter(n -> n.getPoint().distance(cen) < ctab.getAverageBondLength() * 0.8).findAny().isPresent();\n                if (tooClose)\n                    return;\n                boolean doublePoss = rejBondOrderLines.stream().filter(lw -> lw.growLine(ctab.getAverageBondLength() * 0.1).contains(cen)).findAny().isPresent();\n                if (doublePoss)\n                    return;\n                if (s.getBounds2D().getHeight() > ctab.getAverageBondLength() * 0.7) {\n                    return;\n                }\n                actual1 = new BranchNode(\"I\");\n                bestGuessOCR.put(s, \"t\");\n            } else {\n                actual1 = BranchNode.interpretOCRStringAsAtom2(sym);\n            }\n            BranchNode actual = actual1;\n            if (actual != null && actual.isRealNode()) {\n                if (sym.length() > 1) {\n                    List<Line2D> externalLines = ctab.getAllEdgesEntering(s, MAX_BOND_RATIO_FOR_MERGING_TO_OCR * ctab.getAverageBondLength()).stream().map(t -> t.k().getLine()).collect(Collectors.toList());\n                    if (externalLines.size() == 1) {\n                        Line2D exl = externalLines.get(0);\n                        Point2D tc = centert;\n                        Point2D cnew = likelyOCR.stream().map(s1 -> GeomUtil.findCenterOfShape(s1)).filter(spt -> s.contains(spt)).map(cpt -> GeomUtil.projectPointOntoLineWithRejection(exl, cpt)).map(Tuple.vmap(d -> Math.abs(d))).map(t -> t.withVComparator()).min(Comparator.naturalOrder()).map(t -> t.k()).orElseGet(() -> {\n                            return GeomUtil.projectPointOntoLine(externalLines.get(0), tc);\n                        });\n                        if (s.contains(cnew)) {\n                            centert = cnew;\n                        }\n                    } else {\n                        List<Point2D> intersections = GeomUtil.eachCombination(externalLines).map(t -> GeomUtil.intersection(t.k(), t.v())).filter(p -> p != null).filter(p -> s.contains(p)).collect(Collectors.toList());\n                        if (intersections.size() == 1) {\n                            centert = intersections.get(0);\n                        } else if (intersections.size() > 1) {\n                            centert = GeomUtil.findCenterOfVertices(intersections);\n                        }\n                    }\n                    if (!s.contains(centert)) {\n                        centert = GeomUtil.findCenterOfShape(s);\n                    }\n                }\n                Point2D center = centert;\n                ctab.mergeAllNodesInside(s, MAX_BOND_RATIO_FOR_MERGING_TO_OCR * ctab.getAverageBondLength(), (n) -> {\n                    if (sym.equals(\"H\")) {\n                        if (GeomUtil.findClosestShapeTo(ocrMeaningful, n.getPoint()).k() != s) {\n                            return false;\n                        }\n                    }\n                    boolean contains = s.contains(n.getPoint());\n                    if (!contains) {\n                        if (actual.isTerminal() && n.getEdgeCount() > 1) {\n                            long cc = n.getNeighborNodes().stream().map(t -> t.k()).filter(nn -> GeomUtil.distanceTo(s, nn.getPoint()) < 2).count();\n                            if (cc == 0)\n                                return false;\n                        }\n                        if (n.getEdgeCount() > 1) {\n                            boolean nhas = n.getNeighborNodes().stream().map(n1 -> n1.k().getPoint()).filter(p -> s.contains(p)).findAny().isPresent();\n                            if (!nhas) {\n                                boolean edgeHas = n.getEdges().stream().map(e -> e.getLine()).map(l -> GeomUtil.getLineInside(l, s)).filter(l -> l.isPresent()).findAny().isPresent();\n                                if (!edgeHas) {\n                                    double avgNDist = GeomUtil.eachCombination(n.getNeighborNodes().stream().map(n1 -> n1.k().getPoint()).collect(Collectors.toList())).map(t -> new Line2D.Double(t.k(), t.v())).mapToDouble(l -> GeomUtil.length(l)).average().orElseGet(() -> ctab.getAverageBondLength());\n                                    if (avgNDist > ctab.getAverageBondLength()) {\n                                        if (!sym.equals(\"H\")) {\n                                            Tuple<Shape, Double> bs = GeomUtil.findClosestShapeTo(likelyOCR, n.getPoint());\n                                            if (bs != null) {\n                                                char tt = Optional.ofNullable(ocrAttempt.get(bs.k())).map(l -> l.get(0).k()).orElse(null);\n                                                if (tt == 'H') {\n                                                    return false;\n                                                }\n                                            }\n                                        }\n                                    }\n                                }\n                            }\n                        }\n                    }\n                    if (alreadyFixedNodes.contains(n))\n                        return false;\n                    if (n.getEdgeCount() == 0)\n                        return false;\n                    return true;\n                }, (l) -> {\n                    boolean matchesOthers = l.stream().map(pt -> GeomUtil.findClosestShapeTo(ocrMeaningful, pt).k()).filter(sb -> (sb != s)).findAny().isPresent();\n                    if (!matchesOthers) {\n                        return center;\n                    } else {\n                        return GeomUtil.findCenterMostPoint(l);\n                    }\n                });\n                List<Node> mergedNodes = ctab.getAllNodesInsideShape(s, MAX_BOND_RATIO_FOR_MERGING_TO_OCR * ctab.getAverageBondLength());\n                alreadyFixedNodes.addAll(mergedNodes);\n            }\n        });\n        ctab.standardCleanEdges();\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        List<Tuple<Line2D, Point2D>> lj = Stream.concat(removedTinyLines.stream(), linesJoined.stream().map(l -> l.getLine())).flatMap(l -> GeomUtil.getLinesNotInside(l, growLikelyOCRNonBond).stream()).map(l -> Tuple.of(l, GeomUtil.findCenterOfShape(l))).collect(Collectors.toList());\n        Set<Line2D> taken = new HashSet<Line2D>();\n        List<Tuple<List<Line2D>, Tuple<Node, Node>>> edgesToMake = new ArrayList<>();\n        GeomUtil.groupShapesIfClosestPointsMatchCriteria(likelyOCR, (t) -> {\n            Point2D[] pts = t.v();\n            if (pts[0].distance(pts[1]) < ctab.getAverageBondLength() * .9) {\n                return true;\n            }\n            return false;\n        }).stream().filter(ls -> ls.size() >= 2).flatMap(ls -> {\n            return GeomUtil.eachCombination(ls).filter(t -> GeomUtil.distance(t.k(), t.v()) < ctab.getAverageBondLength() * 1.2).map(t -> {\n                return Tuple.of(t, GeomUtil.add(GeomUtil.growShape(t.k(), 1), GeomUtil.growShape(t.v(), 1)));\n            }).map(Tuple.vmap(s -> Tuple.of(s, GeomUtil.area(s)).withVComparator())).map(t -> t.withVComparator()).sorted().map(Tuple.vmap(st -> st.k())).map(t -> {\n                Shape cshape = t.v();\n                Line2D makeLine = new Line2D.Double(GeomUtil.findCenterOfShape(t.k().k()), GeomUtil.findCenterOfShape(t.k().v()));\n                List<Line2D> opl = lj.stream().filter(l1 -> cshape.contains(l1.v())).map(t1 -> t1.k()).filter(l -> GeomUtil.cosTheta(l, makeLine) > Math.cos(25 * Math.PI / 180)).collect(Collectors.toList());\n                return Tuple.of(t.k(), opl);\n            });\n        }).forEach(lst -> {\n            List<Node> nodes = Stream.of(lst.k().k(), lst.k().v()).map(s -> ctab.getNodesInsideShape(s, 2)).flatMap(nds -> nds.stream()).distinct().collect(Collectors.toList());\n            if (nodes.size() == 2) {\n                Node n1 = nodes.get(0);\n                Node n2 = nodes.get(1);\n                Edge alreadyEdge = ctab.getEdgeBetweenNodes(n1, n2).orElse(null);\n                boolean haspossibleLine = !lst.v().isEmpty();\n                boolean already = (alreadyEdge != null);\n                if (!already && haspossibleLine) {\n                    edgesToMake.add(Tuple.of(lst.v(), Tuple.of(nodes.get(0), nodes.get(1))));\n                } else if (!already && !haspossibleLine) {\n                } else if (already && !haspossibleLine) {\n                    ctab.removeEdge(alreadyEdge);\n                } else {\n                    taken.addAll(lst.v());\n                    int order = GeomUtil.groupThings(lst.v(), tlines -> {\n                        Line2D l1 = tlines.k();\n                        Line2D l2 = tlines.v();\n                        if (GeomUtil.cosTheta(l1, l2) > Math.cos(10 * Math.PI / 180)) {\n                            return true;\n                        }\n                        return false;\n                    }).stream().map(l -> GeomUtil.getLineOffsetsToLongestLine(l)).map(l -> {\n                        OptionalDouble opdoff = l.stream().filter(t -> Math.abs(t.v()) > ctab.getAverageBondLength() * 0.1).mapToDouble(t -> t.v()).min();\n                        if (!opdoff.isPresent()) {\n                            return l.stream().map(l1 -> l1.k()).limit(1).collect(Collectors.toList());\n                        }\n                        double doff = opdoff.getAsDouble();\n                        List<Line2D> nlines = l.stream().map(Tuple.vmap(d -> (int) Math.round(d / doff))).map(t -> t.swap()).collect(Tuple.toGroupedMap()).entrySet().stream().map(Tuple::of).map(Tuple.vmap(v1 -> GeomUtil.getPairOfFarthestPoints(GeomUtil.vertices(v1)))).map(Tuple.vmap(v1 -> new Line2D.Double(v1[0], v1[1]))).map(t -> t.v()).collect(Collectors.toList());\n                        return nlines;\n                    }).mapToInt(ll -> ll.size()).max().getAsInt();\n                    if (order > alreadyEdge.getOrder()) {\n                        alreadyEdge.setOrder(order);\n                    }\n                }\n            }\n        });\n        edgesToMake.stream().filter(t -> {\n            return taken.addAll(t.k());\n        }).forEach(t -> {\n            List<Edge> crossingEdges = ctab.getBondsThatCross(t.v().k(), t.v().v());\n            if (!crossingEdges.isEmpty())\n                return;\n            Line2D nline = new Line2D.Double(t.v().k().getPoint(), t.v().v().getPoint());\n            List<Line2D> k = t.k().stream().filter(l -> GeomUtil.cosTheta(nline, l) > Math.cos(45 * Math.PI / 180)).collect(Collectors.toList());\n            if (k.isEmpty())\n                return;\n            int order = GeomUtil.groupThings(k, tlines -> {\n                Line2D l1 = tlines.k();\n                Line2D l2 = tlines.v();\n                if (GeomUtil.cosTheta(l1, l2) > Math.cos(10 * Math.PI / 180)) {\n                    return true;\n                }\n                return false;\n            }).stream().map(l -> GeomUtil.getLineOffsetsToLongestLine(l)).map(l -> {\n                OptionalDouble opdoff = l.stream().filter(tb -> Math.abs(tb.v()) > ctab.getAverageBondLength() * 0.1).mapToDouble(tb -> tb.v()).min();\n                if (!opdoff.isPresent()) {\n                    return l.stream().map(l1 -> l1.k()).limit(1).collect(Collectors.toList());\n                }\n                double doff = opdoff.getAsDouble();\n                List<Line2D> nlines = l.stream().map(Tuple.vmap(d -> (int) Math.round(d / doff))).map(tb -> tb.swap()).collect(Tuple.toGroupedMap()).entrySet().stream().map(Tuple::of).map(Tuple.vmap(v1 -> GeomUtil.getPairOfFarthestPoints(GeomUtil.vertices(v1)))).map(Tuple.vmap(v1 -> new Line2D.Double(v1[0], v1[1]))).map(tb -> tb.v()).collect(Collectors.toList());\n                return nlines;\n            }).mapToInt(ll -> ll.size()).max().getAsInt();\n            ctab.addEdge(t.v().k().getIndex(), t.v().v().getIndex(), order);\n        });\n        ctab.standardCleanEdges();\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        ctab.makeMissingBondsToNeighbors(bitmap, MAX_BOND_TO_AVG_BOND_RATIO_FOR_NOVEL, MAX_TOLERANCE_FOR_SINGLE_BONDS, likelyOCR, OCR_TO_BOND_MAX_DISTANCE, (t) -> {\n            if (t.k() > MAX_TOLERANCE_FOR_SINGLE_BONDS) {\n                t.v().setDashed(true);\n            }\n        });\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        {\n            double avgL = ctab.getAverageBondLength();\n            Set<Edge> skip = new HashSet<Edge>();\n            ctab.getEdges().stream().map(l -> Tuple.of(l, LineWrapper.of(l.getLine())).withVComparator()).filter(e -> e.v().length() > avgL).sorted(Comparator.reverseOrder()).map(t -> t.k()).filter(t -> !skip.contains(t)).forEach(e -> {\n                Node n1 = e.getRealNode1();\n                Node n2 = e.getRealNode2();\n                List<KEqualityTuple<Node, Edge>> neigh1 = n1.getNeighborNodes();\n                List<KEqualityTuple<Node, Edge>> neigh2 = n2.getNeighborNodes();\n                List<KEqualityTuple<Node, Edge>> things1 = neigh1.stream().filter(ne -> neigh2.contains(ne)).collect(Collectors.toList());\n                List<KEqualityTuple<Node, Edge>> things2 = neigh2.stream().filter(ne -> neigh1.contains(ne)).collect(Collectors.toList());\n                List<KEqualityTuple<Node, Edge>> things = Stream.concat(things1.stream(), things2.stream()).collect(Collectors.toList());\n                if (things.size() > 0) {\n                    Point2D p1 = n1.getPoint();\n                    Point2D p2 = n2.getPoint();\n                    Node n3 = things.get(0).k();\n                    Point2D p3 = n3.getPoint();\n                    if (n3.getEdgeCount() == 4 && intersectionNodes.stream().filter(in -> in.distance(p3) < 2).findAny().isPresent()) {\n                        things.stream().map(t -> t.v()).forEach(e2 -> {\n                            skip.add(e2);\n                        });\n                        return;\n                    }\n                    if (n1.getEdgeCount() == 4 && intersectionNodes.stream().filter(in -> in.distance(p1) < 2).findAny().isPresent()) {\n                        things.stream().map(t -> t.v()).forEach(e2 -> {\n                            skip.add(e2);\n                        });\n                        return;\n                    }\n                    if (n2.getEdgeCount() == 4 && intersectionNodes.stream().filter(in -> in.distance(p2) < 2).findAny().isPresent()) {\n                        things.stream().map(t -> t.v()).forEach(e2 -> {\n                            skip.add(e2);\n                        });\n                        return;\n                    }\n                    Edge oedge1 = things.get(0).v();\n                    Edge oedge2 = things.get(1).v();\n                    double tarea = Math.abs(GeomUtil.areaTriangle(p1, p2, p3));\n                    double expected = Math.sqrt(3) / 4 * Math.pow(e.getEdgeLength(), 2);\n                    if (tarea < expected * 0.5) {\n                        boolean removeLong = true;\n                        if ((e.getEdgeLength() < avgL * 1.8) && (!e.getDashed() && (oedge1.getDashed() && oedge2.getDashed()) || (oedge1.getDashed() || oedge2.getDashed() && e.getOrder() > 1))) {\n                            removeLong = false;\n                        } else {\n                            if (oedge1.getEdgeLength() < avgL * 0.7 && oedge2.getEdgeLength() < avgL * 0.7) {\n                                removeLong = false;\n                            } else {\n                                removeLong = true;\n                            }\n                        }\n                        if (removeLong) {\n                            ctab.removeEdge(e);\n                        } else {\n                            ctab.removeEdge(oedge1);\n                            ctab.removeEdge(oedge2);\n                        }\n                    } else {\n                        things.stream().map(t -> t.v()).forEach(e2 -> {\n                            skip.add(e2);\n                        });\n                    }\n                }\n            });\n        }\n        ctab.removeOrphanNodes();\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        {\n            ctab.getDashLikeScoreForAllEdges(bitmap, likelyOCR).forEach(t -> {\n                if (t.v() < MIN_ST_DEV_FOR_KEEPING_DASHED_LINES && t.k().getDashed()) {\n                    t.k().setDashed(false);\n                    double tol = ctab.getToleranceForEdge(t.k(), bitmap, likelyOCR);\n                    if (tol > MAX_TOLERANCE_FOR_DASH_BONDS) {\n                        ctab.removeEdge(t.k());\n                    }\n                }\n            });\n        }\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        double fbondlength = ctab.getAverageBondLength();\n        ctab.getEdges().stream().filter(e -> e.getOrder() == 3).collect(Collectors.toList()).forEach(e -> {\n            LineWrapper lb = LineWrapper.of(e.getLine());\n            double len = e.getEdgeLength();\n            int c = ctab.getEdges().size();\n            double otherBondAverage = (c * ctab.getAverageBondLength() - len) / (c - 1);\n            int n = (int) Math.round(len / otherBondAverage);\n            if (n > 1) {\n                Node n1 = e.getRealNode1();\n                Node n2 = e.getRealNode2();\n                Shape bigLineShape = lb.growLine(len / 3);\n                Point2D apt = rejBondOrderLines.stream().filter(l -> l.length() > ctab.getAverageBondLength() * 0.5).filter(p -> bigLineShape.contains(p.centerPoint())).map(p -> lb.projectPointOntoLine(p.centerPoint())).collect(GeomUtil.averagePoint());\n                List<Point2D> pts = lb.splitIntoNPieces(n);\n                Node pnode = n1;\n                Edge closestEdge = null;\n                double closestD = 999999;\n                for (int i = 1; i < pts.size(); i++) {\n                    Node nn = null;\n                    if (i < pts.size() - 1) {\n                        Point2D np = pts.get(i);\n                        nn = ctab.addNode(np);\n                    } else {\n                        nn = n2;\n                    }\n                    Edge ne = ctab.addEdge(pnode.getIndex(), nn.getIndex(), 1);\n                    Point2D cpt = Stream.of(ne.getPoint1(), ne.getPoint2()).collect(GeomUtil.averagePoint());\n                    double dpt = cpt.distance(apt);\n                    if (dpt < closestD) {\n                        closestD = dpt;\n                        closestEdge = ne;\n                    }\n                    pnode = nn;\n                }\n                if (closestEdge != null) {\n                    closestEdge.setOrder(3);\n                    ctab.removeEdge(e);\n                }\n            }\n        });\n        List<Shape> appliedOCR = new ArrayList<Shape>();\n        for (Shape s : bestGuessOCR.keySet()) {\n            String sym = bestGuessOCR.get(s);\n            BranchNode actual = BranchNode.interpretOCRStringAsAtom2(sym);\n            if (actual != null && actual.isRealNode()) {\n                if (!actual.isLinkable()) {\n                    for (Node n : ctab.getAllNodesInsideShape(s, 0.1)) {\n                        ctab.removeNodeAndEdges(n);\n                    }\n                    continue;\n                }\n                appliedOCR.add(s);\n                List<Node> nlist = ctab.getNodesInsideShape(s, 0.1).stream().filter(n -> !n.isInvented()).collect(Collectors.toList());\n                if (actual.getSymbol().equals(\"I\") && !nlist.isEmpty() && nlist.get(0).getEdgeCount() > 1) {\n                    continue;\n                }\n                if (nlist.size() > 1) {\n                    Point2D np = nlist.get(0).getPoint();\n                    ctab.mergeAllNodesInside(s, 0.1, (n) -> !n.isInvented(), (l) -> np);\n                    ctab.standardCleanEdges();\n                    nlist = ctab.getNodesInsideShape(s, 0.1).stream().filter(n -> !n.isInvented()).collect(Collectors.toList());\n                }\n                nlist.forEach(n -> {\n                    n.setSymbol(actual.getSymbol());\n                });\n                if (nlist.size() == 1) {\n                    Node pnode = nlist.get(0);\n                    pnode.setCharge(actual.getCharge());\n                    Point2D ppoint = pnode.getPoint();\n                    Node lneigh = null;\n                    Node rneigh = null;\n                    if (pnode.getEdgeCount() >= 2) {\n                        lneigh = pnode.getNeighborNodes().stream().map(n -> n.k()).map(n -> Tuple.of(n, n.getPoint().getX())).map(t -> t.withVComparator()).min(Comparator.naturalOrder()).map(t -> t.k()).orElse(null);\n                        rneigh = pnode.getNeighborNodes().stream().map(n -> n.k()).map(n -> Tuple.of(n, n.getPoint().getX())).map(t -> t.withVComparator()).max(Comparator.naturalOrder()).map(t -> t.k()).orElse(null);\n                    }\n                    if (actual.hasChildren()) {\n                        actual.generateCoordinates();\n                        AffineTransform at = new AffineTransform();\n                        at.translate(ppoint.getX(), ppoint.getY());\n                        at.scale(fbondlength, fbondlength);\n                        if (pnode.getEdges().size() > 0) {\n                            Point2D otherPoint = pnode.getEdges().stream().map(e -> e.getOtherNode(pnode)).map(n -> n.getPoint()).collect(GeomUtil.averagePoint());\n                            double ang = GeomUtil.angle(ppoint, otherPoint);\n                            at.rotate(ang + Math.PI);\n                        }\n                        actual.applyTransform(at);\n                        Map<BranchNode, Node> parentNodes = new HashMap<BranchNode, Node>();\n                        parentNodes.put(actual, pnode);\n                        actual.forEachBranchNode((parN, curN) -> {\n                            if (parN == null)\n                                return;\n                            Node mpnode = pnode;\n                            if (parN != null) {\n                                mpnode = parentNodes.get(parN);\n                            }\n                            Node n = ctab.addNode(curN.getSuggestedPoint()).setSymbol(curN.getSymbol()).setCharge(curN.getCharge()).setInvented(true);\n                            Edge e = ctab.addEdge(mpnode.getIndex(), n.getIndex(), curN.getOrderToParent());\n                            if (curN.getWedgeType() == 1) {\n                                e.setWedge(true);\n                            } else if (curN.getWedgeType() == -1) {\n                                e.setDashed(true);\n                            }\n                            curN.getRingBond().ifPresent(t -> {\n                                Node rn = parentNodes.get(t.k());\n                                ctab.addEdge(rn.getIndex(), n.getIndex(), t.v());\n                            });\n                            parentNodes.put(curN, n);\n                        });\n                        if (actual.canBeChain()) {\n                            if (lneigh != null && rneigh != null) {\n                                Node l = lneigh;\n                                Node r = rneigh;\n                                Node nl = parentNodes.get(actual.getLeftBranchNode());\n                                Node nr = parentNodes.get(actual.getRightBranchNode());\n                                pnode.getNeighborNodes().stream().filter(t -> t.k() == l || t.k() == r).collect(Collectors.toList()).forEach(t -> {\n                                    Edge e = t.v();\n                                    Node on = t.k();\n                                    Node nn = (on == l) ? nr : nl;\n                                    ctab.removeEdge(e);\n                                    ctab.addEdge(on.getIndex(), nn.getIndex(), e.getOrder());\n                                });\n                                Line2D oldLine = new Line2D.Double(nl.getPoint(), nr.getPoint());\n                                Point2D[] far = GeomUtil.getPairOfFarthestPoints(s);\n                                double minx = Math.min(far[0].getX(), far[1].getX());\n                                double maxx = Math.max(far[0].getX(), far[1].getX());\n                                Line2D newLine = new Line2D.Double(maxx - averageWidthOCR / 2, pnode.getPoint().getY(), minx + averageWidthOCR / 2, pnode.getPoint().getY());\n                                Point2D navg = Stream.of(l.getPoint(), r.getPoint()).collect(GeomUtil.averagePoint());\n                                boolean flip = false;\n                                if (navg.getY() > newLine.getY1()) {\n                                    flip = true;\n                                }\n                                AffineTransform att = GeomUtil.getTransformFromLineToLine(oldLine, newLine, flip);\n                                parentNodes.values().stream().forEach(pn -> {\n                                    pn.setPoint(att.transform(pn.getPoint(), null));\n                                });\n                            }\n                        }\n                    }\n                }\n            }\n        }\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        if (true) {\n            ConnectionTable biggestSection = ctab.getDisconnectedComponents().stream().map(ct -> Tuple.of(ct, GeomUtil.area(ct.getConvexHull())).withVComparator()).max(Comparator.naturalOrder()).map(t -> t.k()).orElse(null);\n            if (biggestSection != null) {\n                if (biggestSection.getAverageBondLength() < 20) {\n                    bitmap = new Bitmap.BitmapBuilder(bitmap).scale(2).gaussBlur(1).threshold(6).build();\n                    load(bitmap);\n                    return;\n                }\n            }\n        }\n        if (ctab.getDisconnectedComponents().size() > 1) {\n            Tuple<ConnectionTable, Shape> ctshape = ctab.getDisconnectedComponents().stream().map(ct -> Tuple.of(ct, ct.getConvexHull())).map(Tuple.vmap(h -> Tuple.of(h, -GeomUtil.area(h)).withVComparator())).map(t -> t.withVComparator()).sorted().limit(1).map(Tuple.vmap(t -> t.k())).findFirst().orElse(null);\n            if (ctshape != null) {\n                double bestbond = ctshape.k().getAverageBondLength();\n                List<ConnectionTable> ctabs = ctab.getDisconnectedComponents();\n                Shape crop = ctabs.stream().filter(ct -> ct.getAverageBondLength() < 0.7 * bestbond || ct.getAverageBondLength() > 1.33 * bestbond).map(ct -> ct.getAreaAround(bestbond)).collect(GeomUtil.union()).orElse(null);\n                Shape keep = ctabs.stream().filter(ct -> ct.getAverageBondLength() >= 0.7 * bestbond && ct.getAverageBondLength() <= 1.33 * bestbond).map(ct -> ct.getAreaAround(bestbond)).collect(GeomUtil.union()).orElse(null);\n                if (crop != null && keep != null) {\n                    polygons.stream().filter(s -> !likelyOCR.contains(s)).map(s -> Tuple.of(s, GeomUtil.findCenterOfShape(s))).filter(t -> !keep.contains(t.v())).filter(t -> crop.contains(t.v())).map(t -> t.k()).forEach(p -> {\n                        likelyOCR.remove(p);\n                        likelyOCRNumbers.remove(p);\n                        likelyOCRNonBond.remove(p);\n                        likelyOCRAll.remove(p);\n                        likelyOCRIgnore.add(GeomUtil.growShapeNPoly(p, 2, 16));\n                        foundNewOCR[0] = true;\n                    });\n                }\n            }\n        }\n        if (foundNewOCR[0] && repeats < MAX_OCR_FULL_REPEATS) {\n            System.out.println(\"Starting over...\");\n            continue;\n        }\n        Set<Node> toRemove = new HashSet<Node>();\n        do {\n            toRemove.clear();\n            ctab.getNodesNotInShapes(appliedOCR, 0).stream().map(n -> Tuple.of(n, n.getNeighborNodes())).filter(t -> t.v().size() == 2).filter(t -> t.v().get(0).v().getEdgeLength() < ctab.getAverageBondLength()).filter(t -> t.v().get(1).v().getEdgeLength() < ctab.getAverageBondLength()).filter(t -> t.v().get(0).v().getOrder() == 1).filter(t -> t.v().get(1).v().getOrder() == 1).collect(Collectors.toList()).forEach(t -> {\n                Node n1 = t.k();\n                Node n2 = t.v().get(0).k();\n                Node n3 = t.v().get(1).k();\n                if (toRemove.contains(n2) || toRemove.contains(n3))\n                    return;\n                double d1 = n1.distanceTo(n2) + n1.distanceTo(n3);\n                double d2 = n2.distanceTo(n3);\n                if (d2 / d1 > .95) {\n                    Line2D longLine = new Line2D.Double(n2.getPoint(), n3.getPoint());\n                    Point2D np = GeomUtil.projectPointOntoLine(longLine, n1.getPoint());\n                    if (np.distance(n1.getPoint()) < 3) {\n                        ctab.addEdge(n2.getIndex(), n3.getIndex(), 1);\n                        toRemove.add(n1);\n                    }\n                }\n            });\n            toRemove.forEach(n -> ctab.removeNodeAndEdges(n));\n            ctab.standardCleanEdges();\n        } while (!toRemove.isEmpty());\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        ctab.getNodes().stream().filter(n -> n.getEdgeCount() >= 2).map(n -> {\n            return Tuple.of(n, GeomUtil.eachCombination(n.getEdges()).map(t -> {\n                if (t.k().getEdgeLength() > t.v().getEdgeLength()) {\n                    t = t.swap();\n                }\n                return t;\n            }).filter(t -> t.v().getEdgeLength() >= ctab.getAverageBondLength()).collect(Collectors.toList()));\n        }).filter(ed -> !ed.v().isEmpty()).collect(Collectors.toList()).forEach(te -> {\n            Node n = te.k();\n            te.v().forEach(t -> {\n                Node tnode = t.k().getOtherNode(n);\n                Node otherNode = t.v().getOtherNode(n);\n                Point2D ppnt = GeomUtil.projectPointOntoLine(t.v().getLine(), tnode.getPoint());\n                if (ppnt.distance(tnode.getPoint()) < 0.1 * ctab.getAverageBondLength()) {\n                    double sd1 = ppnt.distance(otherNode.getPoint());\n                    if (sd1 < t.v().getEdgeLength()) {\n                        tnode.setPoint(ppnt);\n                        ctab.addEdge(tnode.getIndex(), t.v().getOtherNode(n).getIndex(), t.v().getOrder());\n                        ctab.removeEdge(t.v());\n                    }\n                }\n            });\n        });\n        ctab.standardCleanEdges();\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        ctab.getEdges().stream().filter(e -> e.getEdgeLength() < ctab.getAverageBondLength() * 0.55).collect(Collectors.toList()).forEach(e -> {\n            Node n1 = e.getRealNode1();\n            Node n2 = e.getRealNode2();\n            boolean wasIntersection = intersectionNodes.stream().filter(p -> p.distance(n1.getPoint()) < ctab.getAverageBondLength() * 0.2 || p.distance(n2.getPoint()) < ctab.getAverageBondLength() * 0.2).findAny().isPresent();\n            if (wasIntersection) {\n                return;\n            }\n            List<Node> neigh1 = n1.getEdges().stream().filter(e1 -> e1 != e).map(e1 -> e1.getOtherNode(n1)).collect(Collectors.toList());\n            List<Node> neigh2 = n2.getEdges().stream().filter(e1 -> e1 != e).map(e1 -> e1.getOtherNode(n2)).collect(Collectors.toList());\n            if (neigh1.isEmpty() || neigh2.isEmpty())\n                return;\n            double avgBondLengthIfN1Merge = neigh2.stream().mapToDouble(nn -> nn.distanceTo(n1)).average().orElse(0);\n            double avgBondLengthIfN2Merge = neigh1.stream().mapToDouble(nn -> nn.distanceTo(n2)).average().orElse(0);\n            double maxBondLengthIfN1Merge = neigh2.stream().mapToDouble(nn -> nn.distanceTo(n1)).max().orElse(0);\n            double maxBondLengthIfN2Merge = neigh1.stream().mapToDouble(nn -> nn.distanceTo(n2)).max().orElse(0);\n            double minBondLengthIfN1Merge = neigh2.stream().mapToDouble(nn -> nn.distanceTo(n1)).min().orElse(0);\n            double minBondLengthIfN2Merge = neigh1.stream().mapToDouble(nn -> nn.distanceTo(n2)).min().orElse(0);\n            Predicate<Double> isReasonable = d -> {\n                return d < ctab.getAverageBondLength() * 1.3 && d > ctab.getAverageBondLength() * 0.7;\n            };\n            boolean n1Merge = false;\n            boolean n2Merge = false;\n            if (isReasonable.test(avgBondLengthIfN1Merge) && isReasonable.test(maxBondLengthIfN1Merge) && isReasonable.test(minBondLengthIfN1Merge)) {\n                n1Merge = true;\n            }\n            if (isReasonable.test(avgBondLengthIfN2Merge) && isReasonable.test(maxBondLengthIfN2Merge) && isReasonable.test(minBondLengthIfN2Merge)) {\n                n2Merge = true;\n            }\n            if (!n1Merge && !n2Merge)\n                return;\n            if (n1Merge && !n2Merge) {\n                Point2D mp = n1.getPoint();\n                ctab.mergeNodes(Stream.of(n1.getIndex(), n2.getIndex()).collect(Collectors.toList()), (l) -> mp);\n            } else if (n1Merge && !n2Merge) {\n                Point2D mp = n2.getPoint();\n                ctab.mergeNodes(Stream.of(n1.getIndex(), n2.getIndex()).collect(Collectors.toList()), (l) -> mp);\n            } else {\n                ctab.mergeNodesAverage(n1.getIndex(), n2.getIndex());\n            }\n        });\n        ctab.standardCleanEdges();\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        List<Node> toRemoveNodesCage = new ArrayList<>();\n        ctab.getNodes().stream().filter(n -> n.getEdgeCount() == 4).filter(n -> n.getSymbol().equals(\"C\")).filter(n -> !n.isInvented()).filter(n -> intersectionNodes.stream().filter(p -> p.distance(n.getPoint()) < ctab.getAverageBondLength() * 0.1).findAny().isPresent()).forEach(n -> {\n            if (n.isInRing(7)) {\n                List<Node> neigh = n.getNeighborNodes().stream().map(t -> t.k()).collect(Collectors.toList());\n                if (neigh.stream().filter(nn -> toRemoveNodesCage.contains(nn)).findAny().isPresent()) {\n                    return;\n                }\n                List<List<Node>> pairs = GeomUtil.groupThings(neigh, (t1) -> {\n                    Line2D nline = new Line2D.Double(t1.k().getPoint(), t1.v().getPoint());\n                    Point2D pp = GeomUtil.projectPointOntoLine(nline, n.getPoint());\n                    if (pp.distance(n.getPoint()) < ctab.getAverageBondLength() * 0.02) {\n                        return true;\n                    }\n                    return false;\n                });\n                if (pairs.size() == 2) {\n                    List<Node> npair1 = pairs.get(0);\n                    List<Node> npair2 = pairs.get(1);\n                    if (npair1.size() == 2 && npair2.size() == 2) {\n                        Line2D l1p = new Line2D.Double(npair1.get(0).getPoint(), npair1.get(1).getPoint());\n                        Line2D l2p = new Line2D.Double(npair2.get(0).getPoint(), npair2.get(1).getPoint());\n                        Shape l1s = GeomUtil.growLine(l1p, ctab.getAverageBondLength() * 0.2);\n                        Shape l2s = GeomUtil.growLine(l2p, ctab.getAverageBondLength() * 0.2);\n                        boolean l1Has = lj.stream().filter(t -> l1s.contains(t.v())).filter(t -> GeomUtil.cosTheta(t.k(), l1p) > 0.6).map(l -> l.k()).map(l -> GeomUtil.growLine(l, ctab.getAverageBondLength() * 0.2)).filter(ls -> ls.contains(n.getPoint())).findFirst().isPresent();\n                        boolean l2Has = lj.stream().filter(t -> l2s.contains(t.v())).filter(t -> GeomUtil.cosTheta(t.k(), l2p) > 0.6).map(l -> l.k()).map(l -> GeomUtil.growLine(l, ctab.getAverageBondLength() * 0.2)).filter(ls -> ls.contains(n.getPoint())).findFirst().isPresent();\n                        boolean doit = false;\n                        if ((l1Has && !l2Has) || (l2Has && !l1Has)) {\n                            doit = true;\n                        } else {\n                            double averageOtherBondLength = ctab.getEdges().stream().filter(e2 -> !e2.hasNode(n)).mapToDouble(e2 -> e2.getEdgeLength()).average().orElse(ctab.getAverageBondLength());\n                            double averageCurrentBondLength = n.getEdges().stream().mapToDouble(e -> e.getEdgeLength()).average().orElse(0);\n                            double averageNewBondLength = 0.5 * (npair1.get(0).distanceTo(npair1.get(1)) + npair2.get(0).distanceTo(npair2.get(1)));\n                            if (Math.abs(averageNewBondLength - averageOtherBondLength) < Math.abs(averageCurrentBondLength - averageOtherBondLength)) {\n                                doit = true;\n                            }\n                        }\n                        if (doit) {\n                            ctab.addEdge(npair1.get(0).getIndex(), npair1.get(1).getIndex(), 1);\n                            ctab.addEdge(npair2.get(0).getIndex(), npair2.get(1).getIndex(), 1);\n                            toRemoveNodesCage.add(n);\n                        }\n                    }\n                }\n            }\n        });\n        for (Node r : toRemoveNodesCage) {\n            ctab.removeNodeAndEdges(r);\n        }\n        toRemoveNodesCage.clear();\n        ctab.getNodes().stream().filter(n -> n.getEdgeCount() == 2).filter(n -> n.getSymbol().equals(\"C\")).filter(n -> !n.isInvented()).filter(n -> n.getEdges().stream().filter(e -> e.getOrder() == 1).count() == 2).filter(n -> Optional.ofNullable(GeomUtil.findClosestShapeTo(likelyOCR, n.getPoint())).map(t -> t.v()).orElse(100.0) > ctab.getAverageBondLength() * 0.2).forEach(n -> {\n            List<Tuple<Node, Node>> tn = GeomUtil.eachCombination(n.getNeighborNodes()).filter(t -> {\n                Node n1 = t.k().k();\n                Node n2 = t.v().k();\n                Line2D l = new Line2D.Double(n1.getPoint(), n2.getPoint());\n                Point2D pp = GeomUtil.projectPointOntoLine(l, n.getPoint());\n                if (pp.distance(n.getPoint()) < ctab.getAverageBondLength() * 0.1) {\n                    return true;\n                }\n                return false;\n            }).map(t -> Tuple.of(t.k().k(), t.v().k())).collect(Collectors.toList());\n            if (tn.size() == 1) {\n                toRemoveNodesCage.add(n);\n                ctab.addEdge(tn.get(0).k().getIndex(), tn.get(0).v().getIndex(), 1);\n            }\n        });\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        toRemoveNodesCage.forEach(n -> {\n            ctab.removeNodeAndEdges(n);\n        });\n        ctab.standardCleanEdges();\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        ctab.removeOrphanNodes();\n        List<Shape> singleBondInfluenceAreas = ctab.getEdges().stream().filter(e -> e.getOrder() == 1).filter(e -> !e.isInventedBond()).map(e -> Tuple.of(e, GeomUtil.growLine(e.getLine(), e.getEdgeLength() / 5))).map(t -> t.v()).collect(Collectors.toList());\n        Set<Edge> wasDouble = new HashSet<Edge>();\n        ctab.getEdges().stream().filter(e -> e.getOrder() == 2).filter(e -> !e.isInventedBond()).filter(e -> e.getRealNode1().getSymbol().equals(\"C\") && e.getRealNode2().getSymbol().equals(\"C\")).peek(e -> wasDouble.add(e)).collect(Collectors.toList()).forEach(e -> {\n            Line2D lb = e.getLine();\n            Point2D apnt = Stream.of(lb.getP1(), lb.getP2()).collect(GeomUtil.averagePoint());\n            List<Point2D> possibleOtherDoubleBonds = rejBondOrderLines.stream().filter(l -> l.length() > ctab.getAverageBondLength() * 0.4).map(l -> l.centerPoint()).filter(p -> p.distance(apnt) < ctab.getAverageBondLength() * 0.8).collect(Collectors.toList());\n            if (possibleOtherDoubleBonds.isEmpty()) {\n                e.getNeighborEdges().stream().filter(e2 -> GeomUtil.cosTheta(e.getLine(), e2.getLine()) > Math.cos(2 * Math.PI / 180)).filter(e2 -> wasDouble.contains(e2)).findAny().ifPresent(eo -> {\n                    e.setOrder(1);\n                });\n                return;\n            }\n            boolean couldBeAnother = possibleOtherDoubleBonds.stream().flatMap(p -> singleBondInfluenceAreas.stream().filter(s -> s.contains(p))).findAny().isPresent();\n            if (couldBeAnother) {\n                e.setOrder(1);\n            }\n        });\n        ctab.getEdges().stream().filter(e -> e.getOrder() == 3).filter(e -> !e.isInventedBond()).filter(e -> e.getRealNode1().getSymbol().equals(\"C\") && e.getRealNode2().getSymbol().equals(\"C\")).collect(Collectors.toList()).forEach(e -> {\n            LineWrapper lb = LineWrapper.of(e.getLine());\n            Point2D apnt = lb.centerPoint();\n            List<Tuple<Line2D, Point2D>> possibleOtherDoubleBonds = rejBondOrderLines.stream().filter(l -> l.length() > ctab.getAverageBondLength() * 0.4).filter(l -> l.cosTheta(lb) > 0.8).map(l -> Tuple.of(l, l.centerPoint())).filter(p -> p.v().distance(apnt) < ctab.getAverageBondLength() * 0.8).map(Tuple.kmap(l -> l.getLine())).collect(Collectors.toList());\n            if (possibleOtherDoubleBonds.size() == 1) {\n                e.setOrder(2);\n                return;\n            }\n            List<Tuple<Edge, Shape>> edgeShapes = ctab.getEdges().stream().filter(e1 -> e1 != e).map(e1 -> Tuple.of(e1, GeomUtil.growLine(e1.getLine(), ctab.getAverageBondLength() * 0.5))).collect(Collectors.toList());\n            List<Tuple<Line2D, Edge>> bestEdge = possibleOtherDoubleBonds.stream().map(t -> Tuple.of(t.k(), edgeShapes.stream().filter(es -> es.v().contains(t.v())).filter(es -> GeomUtil.cosTheta(t.k(), es.k().getLine()) > 0.8).map(ee -> ee.k()).findAny().orElse(null))).collect(Collectors.toList());\n            long c = bestEdge.stream().filter(te -> {\n                if (te.v() != null) {\n                    if (te.v().getOrder() == 1) {\n                        te.v().setOrder(2);\n                    }\n                    return true;\n                }\n                return false;\n            }).count();\n            if (c == 1) {\n                e.setOrder(2);\n            } else if (c == 2) {\n                e.setOrder(1);\n            }\n        });\n        ctab.getNodes().stream().filter(n -> n.getSymbol().equals(\"C\")).filter(n -> n.getValanceTotal() >= 5).forEach(n -> {\n            if (n.getEdgeCount() <= 4) {\n                int toomuch = n.getValanceTotal() - 4;\n                if (toomuch == 1 && n.getEdgeCount() == 4) {\n                    Optional<Edge> ope = n.getEdges().stream().filter(e -> e.getOrder() == 1).filter(e -> e.getDashed()).findFirst();\n                    if (ope.isPresent()) {\n                        ope.ifPresent(e -> {\n                            ctab.removeEdge(e);\n                        });\n                    } else {\n                        n.getEdges().stream().filter(e -> e.getOrder() > 1).forEach(e -> e.setOrder(1));\n                    }\n                } else if (toomuch == 1 && n.getEdgeCount() == 3) {\n                    Optional<Edge> opEdge = n.getEdges().stream().filter(e -> e.getOrder() > 2).findFirst();\n                    if (opEdge.isPresent()) {\n                        opEdge.ifPresent(e -> {\n                            e.setOrder(2);\n                        });\n                    } else {\n                        n.getEdges().stream().filter(e -> e.getOrder() == 2).map(e -> Tuple.of(e, GeomUtil.growLine(e.getLine(), ctab.getAverageBondLength() / 3.0))).map(t -> Tuple.of(t.k(), rejBondOrderLines.stream().map(l -> Tuple.of(l, l.centerPoint())).filter(p -> t.v().contains(p.v())).mapToDouble(t1 -> t1.k().length()).findAny())).map(Tuple.vmap(v -> v.orElse(0.0))).map(t -> t.withVComparator()).min(Comparator.naturalOrder()).map(e -> e.k()).ifPresent(e -> {\n                            e.setOrder(1);\n                        });\n                        ;\n                    }\n                }\n            }\n        });\n        List<Shape> dashShapes = new ArrayList<Shape>();\n        List<List<Node>> toMergeNodes = new ArrayList<>();\n        maybeDashCollection.forEach(bshape -> {\n            dashShapes.add(GeomUtil.growShapeNPoly(bshape, 2, 12));\n            Point2D[] pts = GeomUtil.getPairOfFarthestPoints(bshape);\n            List<Node> forN1 = ctab.getNodes().stream().filter(n -> n.getPoint().distance(pts[0]) < ctab.getAverageBondLength() * 0.6).collect(Collectors.toList());\n            List<Node> forN2 = ctab.getNodes().stream().filter(n -> n.getPoint().distance(pts[1]) < ctab.getAverageBondLength() * 0.6).filter(n -> !forN1.contains(n)).collect(Collectors.toList());\n            if (forN1.size() + forN2.size() == 0)\n                return;\n            LineWrapper splitLine = GeomUtil.findLongestSplittingLine(bshape);\n            if (forN1.size() + forN2.size() > 1) {\n                if (forN1.size() > 1) {\n                    List<Node> nadd = forN1.stream().filter(nn -> !forN2.contains(nn)).collect(Collectors.toList());\n                    toMergeNodes.add(nadd);\n                } else if (forN2.size() > 1) {\n                    List<Node> nadd = forN2.stream().filter(nn -> !forN1.contains(nn)).collect(Collectors.toList());\n                    toMergeNodes.add(nadd);\n                } else {\n                    if (forN1.size() == 1 && forN2.size() == 1) {\n                        realRescueOCRCandidates.add(splitLine.getLine());\n                        Node n1 = forN1.get(0);\n                        Node n2 = forN2.get(0);\n                        Point2D np1 = splitLine.projectPointOntoLine(n1.getPoint());\n                        Point2D np2 = splitLine.projectPointOntoLine(n2.getPoint());\n                        n1.setPoint(np1);\n                        n2.setPoint(np2);\n                    }\n                }\n                return;\n            }\n            Node pnode = null;\n            Point2D newPoint = pts[0];\n            if (!forN1.isEmpty()) {\n                newPoint = pts[1];\n                pnode = forN1.get(0);\n            } else {\n                pnode = forN2.get(0);\n            }\n            double ndist = newPoint.distance(pnode.getPoint());\n            if (ndist < ctab.getAverageBondLength() * 1.3 && ndist > ctab.getAverageBondLength() * 0.6) {\n                Point2D cShape = GeomUtil.centerOfMass(bshape);\n                Line2D nline = new Line2D.Double(pnode.getPoint(), cShape);\n                double len = ctab.getAverageBondLength();\n                Point2D op = GeomUtil.resizeLine(nline, len).getP2();\n                Node otherNode = ctab.getNodes().stream().map(n -> Tuple.of(n, n.getPoint().distance(op)).withVComparator()).filter(t -> t.v() < len * 0.3).max(Comparator.reverseOrder()).map(t -> t.k()).orElse(null);\n                if (otherNode == null) {\n                    Node realNode = ctab.addNode(op);\n                    BranchNode bn = bestGuessOCR.entrySet().stream().map(Tuple::of).filter(t -> GeomUtil.growShapeNPoly(t.k(), len * 0.3, 12).contains(op)).map(Tuple.vmap(s1 -> BranchNode.interpretOCRStringAsAtom2(s1))).findFirst().map(t -> t.v()).orElse(null);\n                    if (bn != null && bn.isRealNode()) {\n                        realNode.setSymbol(bn.getSymbol());\n                    }\n                } else {\n                    otherNode.setPoint(op);\n                }\n            }\n        });\n        if (!toMergeNodes.isEmpty()) {\n            toMergeNodes.forEach(nm -> {\n                ctab.mergeNodes(nm.stream().map(n -> n.getIndex()).collect(Collectors.toList()), pl -> pl.stream().collect(GeomUtil.averagePoint()));\n            });\n            ctab.standardCleanEdges();\n        }\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        ctab.getNodes().stream().filter(n -> !n.isInvented()).collect(GeomUtil.groupThings(t -> {\n            Node n1 = t.k();\n            Node n2 = t.v();\n            if (n1.distanceTo(n2) < ctab.getAverageBondLength() * 0.2) {\n                if (n1.connectsTo(n2)) {\n                    return true;\n                }\n            }\n            return false;\n        })).stream().filter(nl -> nl.size() == 2).forEach(nlist -> {\n            Point2D p1 = nlist.stream().map(n -> n.getPoint()).collect(GeomUtil.averagePoint());\n            ctab.mergeNodes(nlist.stream().map(n -> n.getIndex()).collect(Collectors.toList()), (pl) -> p1);\n            ctab.standardCleanEdges();\n        });\n        @SuppressWarnings(\"unchecked\")\n        List<Tuple<Edge, WedgeInfo>> winfo = (List<Tuple<Edge, WedgeInfo>>) ctab.getEdges().stream().filter(e -> !e.isInventedBond()).map(e -> {\n            LineWrapper useLine = GeomUtil.getLinesNotInside(e.getLine(), growLikelyOCR).stream().map(l -> LineWrapper.of(l)).max(Comparator.naturalOrder()).orElse(null);\n            if (useLine != null) {\n                Point2D c = useLine.centerPoint();\n                Optional<Shape> isDash = dashShapes.stream().filter(d -> d.contains(c)).findFirst();\n                boolean isDotted = dottedLines.stream().map(dl -> dl.growLine(ctab.getAverageBondLength() * .3)).filter(sl -> sl.contains(c)).findAny().isPresent();\n                if (isDotted) {\n                    e.setDashed(true);\n                } else {\n                    if (isDash.isPresent()) {\n                        e.setDashed(true);\n                        if (e.getOrder() != 1) {\n                            if (e.getRealNode1().getSymbol().equals(\"C\") && e.getRealNode2().getSymbol().equals(\"C\")) {\n                            } else {\n                                e.setOrder(1);\n                            }\n                        }\n                        Point2D cmass = GeomUtil.centerOfMass(isDash.get());\n                        if (e.getRealNode1().getPoint().distance(cmass) < e.getRealNode2().getPoint().distance(cmass)) {\n                            e.switchNodes();\n                        }\n                    } else {\n                        if (e.getRealNode1().getSymbol().equals(\"C\") && e.getRealNode2().getSymbol().equals(\"C\")) {\n                            if (e.getDashed() && e.getOrder() == 1) {\n                                double grow = ctab.getAverageBondLength() * 0.1;\n                                Shape lshape = useLine.growLine(grow);\n                                List<Shape> lineShapes = lines.stream().filter(l -> !bestGuessOCR.keySet().stream().filter(ss -> ss.contains(l.centerPoint())).findAny().isPresent()).filter(l -> l.absCosTheta(useLine) > 0.8).filter(l -> lshape.contains(l.centerPoint())).map(l -> l.growLine(grow)).collect(Collectors.toList());\n                                if (lineShapes.size() < 3) {\n                                    e.setDashed(false);\n                                } else {\n                                    double tarea = lineShapes.stream().mapToDouble(s -> GeomUtil.area(s)).sum();\n                                    if (tarea > 0.7 * GeomUtil.area(lshape)) {\n                                        e.setDashed(false);\n                                    }\n                                }\n                            }\n                        }\n                        return bitmap.getconfexHullAlongLine(useLine.getLine()).map(w -> Tuple.of(e, w));\n                    }\n                }\n            }\n            return Optional.empty();\n        }).filter(t -> t.isPresent()).map(o -> o.get()).collect(Collectors.toList());\n        Predicate<Node> couldBeStereoCenter = (n1) -> n1.getEdgeCount() >= 3 && n1.getSymbol().equals(\"C\") && !n1.getEdges().stream().filter(e1 -> e1.getOrder() > 1).findAny().isPresent();\n        double averageThickness = winfo.stream().mapToDouble(t -> t.v().getAverageThickness()).average().orElse(2);\n        Set<Edge> thickEdges = new HashSet<Edge>();\n        Set<Edge> wedgeEdges = new HashSet<Edge>();\n        winfo.forEach(t -> {\n            Edge e = t.k();\n            WedgeInfo s = t.v();\n            double wl = s.getCorrel();\n            if (s.getAverageThickness() > averageThickness * 1.9) {\n                if (s.getOnPixels() > s.getArea() * 0.5) {\n                    double cutoff = WEDGE_LIKE_PEARSON_SCORE_CUTOFF;\n                    if (e.getRealNode1().getEdges().stream().filter(ed -> ed.getOrder() > 1).findAny().isPresent()) {\n                        cutoff = WEDGE_LIKE_PEARSON_SCORE_CUTOFF_DOUBLE;\n                    }\n                    if (e.getRealNode2().getEdges().stream().filter(ed -> ed.getOrder() > 1).findAny().isPresent()) {\n                        cutoff = WEDGE_LIKE_PEARSON_SCORE_CUTOFF_DOUBLE;\n                    }\n                    if (wl > cutoff) {\n                        e.setWedge(true);\n                        wedgeEdges.add(e);\n                    } else if (wl < -cutoff) {\n                        e.setWedge(true);\n                        wedgeEdges.add(e);\n                        e.switchNodes();\n                    } else if (s.getAverageThickness() > averageThickness * 2.2) {\n                        thickEdges.add(e);\n                        e.setWedge(true);\n                    }\n                    if (e.getWedge()) {\n                        if (e.getRealNode1().getEdgeCount() < 3 && e.getRealNode2().getEdgeCount() >= 3) {\n                            e.switchNodes();\n                        }\n                        if (couldBeStereoCenter.test(e.getRealNode2()) && !couldBeStereoCenter.test(e.getRealNode1())) {\n                            e.switchNodes();\n                        }\n                    }\n                }\n            }\n        });\n        if (thickEdges.size() > 0 && wedgeEdges.size() > 0) {\n            thickEdges.stream().filter(e -> {\n                if (e.getRealNode1().getEdges().stream().filter(e2 -> e2 != e).filter(e2 -> e2.getWedge() || e2.getDashed()).findAny().isPresent() || e.getRealNode2().getEdges().stream().filter(e2 -> e2 != e).filter(e2 -> e2.getWedge() || e2.getDashed()).findAny().isPresent()) {\n                    return true;\n                }\n                return false;\n            }).forEach(w -> w.setWedge(false));\n        }\n        GeomUtil.eachCombination(ctab.getNodes().stream().filter(n -> !n.isInvented()).collect(Collectors.toList())).filter(t -> t.k().distanceTo(t.v()) < 1.5 * ctab.getAverageBondLength()).filter(t -> !t.k().getBondTo(t.v()).isPresent()).forEach(t1 -> {\n            Line2D l2 = new Line2D.Double(t1.k().getPoint(), t1.v().getPoint());\n            Line2D useLine = GeomUtil.getLinesNotInside(l2, growLikelyOCR).stream().map(l -> LineWrapper.of(l)).max(Comparator.naturalOrder()).map(l -> l.getLine()).orElse(null);\n            if (useLine == null)\n                return;\n            long c = polygons.stream().filter(s -> GeomUtil.getIntersection(s, useLine).isPresent()).filter(s -> GeomUtil.findLongestSplittingLine(s).length() < ctab.getAverageBondLength()).count();\n            if (c > 2) {\n                ctab.addEdge(t1.k().getIndex(), t1.v().getIndex(), 1);\n                Edge e = ctab.getEdges().get(ctab.getEdges().size() - 1);\n                e.setDashed(true);\n            }\n        });\n        ctab.getEdges().stream().filter(e -> e.getDashed()).map(e -> Tuple.of(e, GeomUtil.findCenterOfShape(e.getLine()))).filter(e -> !dashShapes.stream().filter(d -> d.contains(e.v())).findAny().isPresent()).map(t -> t.k()).collect(Collectors.toList()).forEach(t -> {\n            Shape sl = GeomUtil.growLine(t.getLine(), ctab.getAverageBondLength() / 3);\n            boolean findLines = lines.stream().map(l -> l.centerPoint()).filter(l -> sl.contains(l)).findAny().isPresent();\n            if (!findLines) {\n                ctab.removeEdge(t);\n            }\n        });\n        ctab.getEdges().stream().filter(e -> e.getDashed()).forEach(e -> {\n            if (couldBeStereoCenter.test(e.getRealNode2()) && !couldBeStereoCenter.test(e.getRealNode1())) {\n                e.switchNodes();\n            }\n        });\n        ctab.getRings().stream().filter(r -> r.size() == 6).forEach(r -> {\n            List<Edge> doubles = r.getEdges().stream().filter(e -> e.getOrder() == 2).collect(Collectors.toList());\n            if (doubles.size() == 2) {\n                Set<Node> sp2s = doubles.stream().flatMap(e -> Stream.of(e.getRealNode1(), e.getRealNode2())).collect(Collectors.toSet());\n                Edge maybeDouble = r.getEdges().stream().filter(e -> !sp2s.contains(e.getRealNode1())).filter(e -> !sp2s.contains(e.getRealNode2())).findFirst().orElse(null);\n                if (maybeDouble != null) {\n                    if (maybeDouble.getRealNode1().getValanceTotal() == 4 || maybeDouble.getRealNode2().getValanceTotal() == 4) {\n                        return;\n                    }\n                    Shape ls = GeomUtil.growLine(maybeDouble.getLine(), ctab.getAverageBondLength() * 0.3);\n                    long c = lines.stream().map(l -> Tuple.of(l, l.centerPoint())).filter(t -> ls.contains(t.v())).filter(t -> t.k().absCosTheta(LineWrapper.of(maybeDouble.getLine())) > 0.8).count();\n                    if (c > 1) {\n                        maybeDouble.setOrder(2);\n                    }\n                }\n            }\n        });\n        ctab.getRings().stream().filter(r -> r.size() == 6).filter(r -> r.isConjugated()).forEach(r -> {\n            Point2D center = GeomUtil.centerOfMass(r.getConvexHull());\n            Shape p = GeomUtil.convexHull2(GeomUtil.makeNPolyCenteredAt(new Point2D.Double(0, 0), 6, 100));\n            Point2D anchor = r.getNodes().stream().map(n -> Tuple.of(n, n.getPoint().distance(center)).withVComparator()).max(Comparator.naturalOrder()).map(t -> t.k().getPoint()).orElse(null);\n            Line2D nline = new Line2D.Double(center, anchor);\n            AffineTransform at = GeomUtil.getTransformFromLineToLine(new Line2D.Double(new Point2D.Double(0, 0), new Point2D.Double(100, 0)), nline, false);\n            Shape ns = at.createTransformedShape(p);\n            Point2D[] verts2 = GeomUtil.vertices(ns);\n            r.getNodes().forEach(n -> {\n                Point2D np = Arrays.stream(verts2).map(v -> Tuple.of(v, v.distance(n.getPoint())).withVComparator()).min(Comparator.naturalOrder()).map(t -> t.k()).orElse(null);\n                n.setPoint(np);\n            });\n        });\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n    }\n    rescueOCRShapes = realRescueOCRCandidates;\n    if (DEBUG)\n        ctabRaw.add(ctab.cloneTab());\n    ctab.getNodes().stream().filter(n -> n.getSymbol().equals(\"S\")).filter(n -> n.getValanceTotal() == 7).forEach(n -> {\n        n.getNeighborNodes().stream().filter(t -> t.v().getOrder() == 2).filter(nn -> nn.k().getSymbol().equals(\"N\")).findFirst().ifPresent(nn -> {\n            nn.v().setOrder(1);\n        });\n    });\n    ctab.getNodes().stream().filter(n -> n.getSymbol().equals(\"N\")).filter(n -> n.getCharge() == 0).forEach(n -> {\n        int so = n.getEdges().stream().mapToInt(e -> e.getOrder()).sum();\n        if (so > 3) {\n            n.setCharge(so - 3);\n            n.getNeighborNodes().stream().filter(t -> t.v().getOrder() == 1).map(t -> t.k()).filter(nn -> nn.getSymbol().equals(\"O\")).filter(nn -> nn.getCharge() == 0).filter(nn -> nn.getEdgeCount() == 1).findFirst().ifPresent(nn -> {\n                nn.setCharge(-1);\n            });\n        }\n    });\n    ctab.getNodes().stream().filter(n -> n.getSymbol().equals(\"S\")).filter(n -> n.getCharge() == 0).forEach(n -> {\n        int so = n.getEdges().stream().mapToInt(e -> e.getOrder()).sum();\n        if (so == 3) {\n            n.getNeighborNodes().stream().map(t -> t.k()).filter(nn -> nn.getSymbol().equals(\"O\")).filter(nn -> nn.getCharge() == 0).filter(nn -> nn.getEdgeCount() == 1).findFirst().ifPresent(nn -> {\n                nn.setCharge(-1);\n                n.setCharge(1);\n            });\n        }\n    });\n    if (DEBUG)\n        ctabRaw.add(ctab.cloneTab());\n    List<Shape> mightBeNegative = polygons.stream().filter(s -> s.getBounds2D().getHeight() < ctab.getAverageBondLength() / 10).filter(s -> s.getBounds2D().getWidth() > 1).filter(s -> s.getBounds2D().getWidth() < ctab.getAverageBondLength() / 2).filter(s -> !likelyOCRAll.contains(s) || Optional.ofNullable(ocrAttempt.get(s)).map(l -> l.get(0)).filter(t -> t.k().toString().equals(\"-\")).isPresent()).collect(Collectors.toList());\n    if (!mightBeNegative.isEmpty()) {\n        Set<Shape> already = new HashSet<Shape>();\n        BiConsumer<Node, Integer> maybeCharge = (n, o) -> {\n            int so = n.getEdges().stream().mapToInt(e -> e.getOrder()).sum();\n            if (so == o) {\n                Point2D p = n.getPoint();\n                Shape neg = mightBeNegative.stream().filter(s -> !already.contains(s)).filter(s -> GeomUtil.distanceTo(s, p) < ctab.getAverageBondLength() * 0.7).findAny().orElse(null);\n                if (neg != null) {\n                    Point2D ncenter = GeomUtil.findCenterOfShape(neg);\n                    boolean inLine = n.getEdges().stream().map(e -> e.getLine()).map(l -> GeomUtil.growLine(l, averageWidthOCRFinal[0])).filter(s -> s.contains(ncenter)).findAny().isPresent();\n                    if (!inLine) {\n                        already.add(neg);\n                        n.setCharge(-1);\n                    } else {\n                        if (ctab.getSumCharge() > 0) {\n                            already.add(neg);\n                            n.setCharge(-1);\n                        }\n                    }\n                }\n            }\n        };\n        Map<String, Integer> neededValances = new HashMap<String, Integer>();\n        neededValances.put(\"O\", 1);\n        neededValances.put(\"N\", 2);\n        neededValances.put(\"Cl\", 1);\n        ctab.getNodes().stream().filter(n -> neededValances.get(n.getSymbol()) != null).filter(n -> n.getCharge() == 0).filter(n -> !n.isInvented()).forEach(n -> maybeCharge.accept(n, neededValances.get(n.getSymbol())));\n        int sc = ctab.getSumCharge();\n        if (sc > 0) {\n            ctab.getNodes().stream().filter(n -> n.getSymbol().equals(\"C\")).filter(n -> n.getCharge() == 0).filter(n -> n.getEdgeCount() == 2).filter(n -> !n.isInvented()).filter(n -> !n.getEdges().stream().filter(e -> e.getDashed()).findAny().isPresent()).forEach(n -> maybeCharge.accept(n, 2));\n        }\n        if (ctab.getSumCharge() > 0) {\n            mightBeNegative.stream().filter(s -> !already.contains(s)).map(s -> Tuple.of(s, s)).map(Tuple.vmap(s -> bestGuessOCR.keySet().stream().map(os -> Tuple.of(os, GeomUtil.distance(os, s)).withVComparator()).min(Comparator.naturalOrder()))).filter(t -> t.v().isPresent()).map(Tuple.vmap(v1 -> v1.get())).filter(t -> t.v().v() < ctab.getAverageBondLength() * 0.3).forEach(t -> {\n                Shape neg = t.k();\n                Shape ocr = t.v().k();\n                ctab.getAllNodesInsideShape(ocr, 3).stream().map(n -> Tuple.of(Stream.of(n), n.getNeighborNodes().stream().map(t1 -> t1.k()))).flatMap(nt -> Stream.concat(nt.k(), nt.v())).distinct().filter(n -> n.getSymbol().equals(\"O\")).filter(n -> n.getValanceTotal() == 1).filter(n -> n.getCharge() == 0).findFirst().ifPresent(ncharge -> {\n                    ncharge.setCharge(-1);\n                });\n            });\n        }\n    }\n    if (DEBUG)\n        ctabRaw.add(ctab.cloneTab());\n    ctab.simpleClean();\n    circles.stream().map(c -> GeomUtil.findCenterOfShape(c)).forEach(cp -> {\n        ctab.getEdgesWithCenterWithin(cp, ctab.getAverageBondLength()).stream().filter(e -> e.isRingEdge()).forEach(Edge::setToAromatic);\n    });\n    if (DEBUG)\n        ctabRaw.add(ctab.cloneTab());\n}",
        "rpfc": "private void load(Bitmap aBitMap) throws Exception {\n    ctabRaw.clear();\n    ocrAttempt.clear();\n    bitmap = aBitMap;\n    SCOCR[] socr = new SCOCR[] { OCR_DEFAULT.orElse(OCR_BACKUP, OCRcutoffCosine) };\n    double[] maxBondLength = new double[] { INITIAL_MAX_BOND_LENGTH };\n    thin = bitmap.thin();\n    {\n        List<int[]> hollow = thin.findHollowPoints();\n        if (hollow.size() > 0.002 * thin.fractionPixelsOn() * thin.width() * thin.height()) {\n            bitmap = new Bitmap.BitmapBuilder(bitmap).boxBlur(1).threshold(2).build();\n            thin = bitmap.thin();\n        }\n    }\n    polygons = bitmap.connectedComponents(Bitmap.Bbox.DoublePolygon);\n    boolean isLarge = false;\n    if (!polygons.isEmpty()) {\n        isLarge = polygons.size() > 4000;\n    }\n    if (isLarge) {\n        throw new IllegalStateException(\"Cannot support images with over 4000 polygons at this time\");\n    }\n    Set<Shape> likelyOCR = Collections.synchronizedSet(new LinkedHashSet<Shape>());\n    Set<Shape> likelyOCRNumbers = Collections.synchronizedSet(new LinkedHashSet<Shape>());\n    Set<Shape> likelyOCRNonBond = Collections.synchronizedSet(new LinkedHashSet<Shape>());\n    Set<Shape> likelyOCRAll = Collections.synchronizedSet(new LinkedHashSet<Shape>());\n    Set<Shape> likelyOCRIgnore = Collections.synchronizedSet(new LinkedHashSet<Shape>());\n    List<Shape> ocrRescues = new ArrayList<Shape>();\n    Set<Shape> verticalShapes = Collections.synchronizedSet(new LinkedHashSet<Shape>());\n    processOCR(socr[0], polygons, bitmap, thin, (s, potential) -> {\n        ocrAttempt.put(s, potential);\n        if (potential.stream().filter(e -> e.v().doubleValue() > OCRcutoffCosine).findAny().isPresent()) {\n            CharType ct = OCRIsLikely(potential.get(0));\n            if (ct.equals(CharType.ChemLikely)) {\n                likelyOCR.add(s);\n                likelyOCRNonBond.add(s);\n            } else if (ct.equals(CharType.NumericLikely)) {\n                likelyOCRNonBond.add(s);\n                likelyOCRNumbers.add(s);\n            } else if (ct.equals(CharType.VerticalBondLikely)) {\n                verticalShapes.add(s);\n            }\n            likelyOCRAll.add(s);\n        }\n    });\n    double averageHeightOCR1 = likelyOCR.stream().map(Shape::getBounds2D).filter(Objects::nonNull).mapToDouble(Rectangle2D::getHeight).filter(Objects::nonNull).average().orElse(10000);\n    verticalShapes.stream().filter(s -> s.getBounds2D().getHeight() > averageHeightOCR1 * 1.5).forEach(s -> {\n        likelyOCR.remove(s);\n        likelyOCRNumbers.remove(s);\n        likelyOCRAll.remove(s);\n    });\n    List<Shape> circles = polygons.stream().filter(p -> !likelyOCRAll.contains(p)).map(s -> Tuple.of(s, GeomUtil.getCircleLikeScore(s))).filter(t -> t.v() > 0.9).map(t -> t.k()).map(s -> GeomUtil.growShape(s, 2)).collect(Collectors.toList());\n    lines = GeomUtil.asLines(thin.segments()).stream().filter(l -> !circles.stream().filter(s -> s.contains(l.getP1()) || s.contains(l.getP1())).findFirst().isPresent()).map(l -> GeomUtil.LineWrapper.of(l)).collect(Collectors.toList());\n    ctabRaw.clear();\n    boolean[] foundNewOCR = new boolean[] { true };\n    int repeats = 0;\n    List<Shape> realRescueOCRCandidates = Collections.synchronizedList(new ArrayList<>());\n    double[] averageHeightOCRFinal = new double[] { 0 };\n    double[] averageWidthOCRFinal = new double[] { 0 };\n    List<Point2D> intersectionNodes = new ArrayList<>();\n    if (PRE_RESCUE_OCR) {\n        rescueOCR(lines, polygons, likelyOCR, socr[0], (s, potential) -> {\n            String ss = potential.get(0).k().toString();\n            if (ss.equalsIgnoreCase(\"C\")) {\n                return;\n            }\n            Point2D cent = GeomUtil.findCenterOfShape(s);\n            boolean cont = likelyOCRAll.stream().filter(s1 -> s1.contains(cent)).findAny().isPresent();\n            if (cont)\n                return;\n            ocrAttempt.put(s, potential);\n            CharType ct = OCRIsLikely(potential.get(0));\n            if (ct.equals(CharType.ChemLikely)) {\n                likelyOCR.add(s);\n                likelyOCRNonBond.add(s);\n            } else if (ct.equals(CharType.NumericLikely)) {\n                likelyOCRNonBond.add(s);\n                likelyOCRNumbers.add(s);\n            }\n            likelyOCRAll.add(s);\n        });\n    }\n    double[] ignoreTooSmall = new double[] { 0.0 };\n    while (foundNewOCR[0] && repeats < MAX_OCR_FULL_REPEATS) {\n        if (Thread.interrupted())\n            throw new InterruptedException();\n        repeats++;\n        foundNewOCR[0] = false;\n        intersectionNodes.clear();\n        double averageLargestOCR = likelyOCR.stream().map(s -> GeomUtil.getPairOfFarthestPoints(s)).filter(p -> p != null && p.length == 2).mapToDouble(p -> p[0].distance(p[1])).average().orElse(0);\n        double averageAreaOCR = likelyOCR.stream().mapToDouble(s -> GeomUtil.area(s)).average().orElse(0);\n        double averageWidthOCR = likelyOCR.stream().map(Shape::getBounds2D).filter(Objects::nonNull).mapToDouble(Rectangle2D::getWidth).filter(Objects::nonNull).average().orElse(0);\n        double averageHeightOCR = likelyOCR.stream().map(Shape::getBounds2D).filter(Objects::nonNull).mapToDouble(Rectangle2D::getHeight).filter(Objects::nonNull).average().orElse(0);\n        double averageWidthNumberOCR = likelyOCRNumbers.stream().map(Shape::getBounds2D).filter(Objects::nonNull).mapToDouble(Rectangle2D::getWidth).filter(Objects::nonNull).average().orElse(0);\n        double averageHeightNumberOCR = likelyOCRNumbers.stream().map(Shape::getBounds2D).filter(Objects::nonNull).mapToDouble(Rectangle2D::getHeight).filter(Objects::nonNull).average().orElse(0);\n        averageHeightOCRFinal[0] = averageHeightOCR;\n        averageWidthOCRFinal[0] = averageWidthOCR;\n        likelyOCRAll.retainAll(likelyOCRAll.stream().filter(Objects::nonNull).map(s -> Tuple.of(s, GeomUtil.getPairOfFarthestPoints(s))).filter(t -> t.v()[0].distance(t.v()[1]) > averageLargestOCR * MIN_LONGEST_WIDTH_RATIO_FOR_OCR_TO_AVERAGE).map(t -> t.k()).collect(Collectors.toList()));\n        likelyOCR.retainAll(likelyOCR.stream().map(s -> Tuple.of(s, GeomUtil.getPairOfFarthestPoints(s))).filter(t -> t.v()[0].distance(t.v()[1]) > averageLargestOCR * MIN_LONGEST_WIDTH_RATIO_FOR_OCR_TO_AVERAGE).map(t -> t.k()).collect(Collectors.toList()));\n        Predicate<Line2D> isInOCRShape = (l) -> {\n            if (likelyOCR.isEmpty())\n                return false;\n            Tuple<Shape, Double> shape1 = GeomUtil.findClosestShapeTo(likelyOCRNonBond, l.getP1());\n            if (shape1.v() > OCR_TO_BOND_MAX_DISTANCE) {\n                return false;\n            }\n            Tuple<Shape, Double> shape2 = GeomUtil.findClosestShapeTo(likelyOCRNonBond, l.getP2());\n            if (shape2.v() > OCR_TO_BOND_MAX_DISTANCE) {\n                return false;\n            }\n            if (shape1.k() == shape2.k()) {\n                return true;\n            }\n            boolean anyOutside = GeomUtil.getLinesNotInside(l, Arrays.asList(shape1.k(), shape2.k())).stream().filter(l1 -> l1 != null).filter(GeomUtil.longerThan(1)).findAny().isPresent();\n            if (!anyOutside)\n                return true;\n            return false;\n        };\n        Predicate<Line2D> tryToMerge = isInOCRShape.negate().and((l) -> {\n            return true;\n        });\n        List<Line2D> useLines = lines.stream().filter(t -> !likelyOCRIgnore.stream().filter(s -> s.contains(t.centerPoint())).findAny().isPresent()).map(l -> l.getLine()).collect(Collectors.toList());\n        List<LineWrapper> smallLines = useLines.stream().filter(tryToMerge).map(l -> LineWrapper.of(l)).collect(Collectors.toList());\n        List<Line2D> bigLines = useLines.stream().filter(tryToMerge.negate()).collect(Collectors.toList());\n        smallLines = bitmap.combineLines(smallLines, MAX_DISTANCE_FOR_STITCHING_SMALL_SEGMENTS, MAX_TOLERANCE_FOR_STITCHING_SMALL_SEGMENTS_FULL, MAX_POINT_DISTANCE_TO_BE_PART_OF_MULTI_NODE, MAX_ANGLE_FOR_JOINING_SEGMENTS, MIN_SIZE_FOR_ANGLE_COMPARE_JOINING_SEGMENTS);\n        smallLines = thin.combineLines(smallLines, MAX_DISTANCE_FOR_STITCHING_SMALL_SEGMENTS, MAX_TOLERANCE_FOR_STITCHING_SMALL_SEGMENTS_THIN, MAX_POINT_DISTANCE_TO_BE_PART_OF_MULTI_NODE, MAX_ANGLE_FOR_JOINING_SEGMENTS, MIN_SIZE_FOR_ANGLE_COMPARE_JOINING_SEGMENTS);\n        List<Line2D> removedTinyLines = smallLines.stream().map(l -> l.getLine()).filter(GeomUtil.longerThan(MAX_DISTANCE_FOR_STITCHING_SMALL_SEGMENTS).negate()).collect(Collectors.toList());\n        List<Point2D> removedTinyVertices = removedTinyLines.stream().flatMap(l -> Stream.of(l.getP1(), l.getP2())).collect(Collectors.toList());\n        smallLines = smallLines.stream().filter(l -> l.length() > MAX_DISTANCE_FOR_STITCHING_SMALL_SEGMENTS).collect(Collectors.toList());\n        List<Point2D> verts = smallLines.stream().flatMap(l -> l.streamPoints()).collect(Collectors.toList());\n        double[] lDistOCRToLine = likelyOCR.stream().map(s -> Tuple.of(s, GeomUtil.findCenterOfShape(s))).map(Tuple.vmap(p -> Tuple.of(p, GeomUtil.findClosestPoint(verts, p)))).map(Tuple.vmap(t -> t.k().distance(t.v()))).mapToDouble(t -> t.v()).sorted().toArray();\n        List<Shape> extendableOCR = likelyOCR.stream().map(s -> Tuple.of(s, ocrAttempt.get(s))).filter(t -> t.v() != null).filter(t -> t.v().size() > 0).map(Tuple.vmap(l -> l.get(0).k().toString())).filter(t -> !t.v().equals(\"H\")).map(t -> t.k()).collect(Collectors.toList());\n        OptionalDouble avgDistOCRToLine = Optional.of(0).filter(d -> lDistOCRToLine.length > 0).map(d -> lDistOCRToLine[lDistOCRToLine.length / 2]).map(d -> OptionalDouble.of(d)).orElse(OptionalDouble.empty());\n        linesJoined = Stream.concat(bigLines.stream().map(l -> LineWrapper.of(l)), smallLines.stream()).collect(Collectors.toList());\n        double largestBond = smallLines.stream().mapToDouble(l -> l.length()).max().orElse(0);\n        double averageLine = smallLines.stream().mapToDouble(l -> l.length()).filter(d -> d > ignoreTooSmall[0]).average().orElse(0);\n        if (largestBond > 2.0 * averageLine) {\n            largestBond = 1.4 * averageLine;\n        }\n        List<List<LineWrapper>> preprocess = GeomUtil.reduceMultiBonds(Arrays.asList(linesJoined), MAX_ANGLE_FOR_PARALLEL, MAX_DISTANCE_TO_MERGE_PARALLEL_LINES, MIN_PROJECTION_RATIO_FOR_HIGH_ORDER_BONDS, 0, MAX_DELTA_LENGTH_FOR_STITCHING_LINES_ON_BOND_ORDER_CALC, (l) -> {\n        }).stream().map(t -> t.k()).map(t -> LineWrapper.of(t)).map(t -> Tuple.of(t, likelyOCR.stream().filter(s -> s.contains(t.centerPoint())).findAny().isPresent())).collect(Collectors.groupingBy(t -> t.v())).values().stream().map(tl -> tl.stream().map(t -> t.k()).collect(Collectors.toList())).collect(Collectors.toList());\n        List<LineWrapper> rejBondOrderLines = new ArrayList<>();\n        linesOrder = GeomUtil.reduceMultiBonds(preprocess, MAX_ANGLE_FOR_PARALLEL, largestBond / 3, MIN_PROJECTION_RATIO_FOR_HIGH_ORDER_BONDS, MIN_BIGGER_PROJECTION_RATIO_FOR_HIGH_ORDER_BONDS, MAX_DELTA_LENGTH_FOR_STITCHING_LINES_ON_BOND_ORDER_CALC, (rejLine -> rejBondOrderLines.add(LineWrapper.of(rejLine))));\n        List<Shape> growLines = linesOrder.stream().map(t -> t.k()).map(l -> GeomUtil.growLine(l, 5)).collect(Collectors.toList());\n        List<Shape> rescueOCRCandidates = new ArrayList<>();\n        List<Shape> connectedComponents = polygons.stream().map(s -> GeomUtil.growShape(s, 2)).collect(Collectors.toList());\n        int reps = 0;\n        boolean tooLongBond = true;\n        double maxRatioInitial = 0.5;\n        double maxTotalRatioInitial = 1.4;\n        double maxRatio = 0.5;\n        double maxTotalRatio = 1.4;\n        List<LineWrapper> dottedLines = new ArrayList<>();\n        while (tooLongBond) {\n            if (Thread.interrupted())\n                throw new InterruptedException();\n            rescueOCRCandidates.clear();\n            List<Tuple<Line2D, Integer>> linesOrderRestricted = linesOrder.stream().filter(t -> {\n                Line2D l = t.k();\n                return isInOCRShape.negate().test(l);\n            }).collect(Collectors.toList());\n            ctab = GeomUtil.getConnectionTable(linesOrderRestricted, extendableOCR, maxRatioForIntersection, maxCandidateRatioForIntersection, maxPerLineDistanceRatioForIntersection, minPerLineDistanceRatioForIntersection, maxCandidateRatioForIntersectionWithNeighbor, GeomUtil.longerThan(maxBondLength[0]).negate()).mergeNodesCloserThan(MAX_DISTANCE_BEFORE_MERGING_NODES);\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            ctab.getEdgesWhichMightBeWiggleLines().forEach(t -> {\n                realRescueOCRCandidates.add(t.k().getLine());\n                Point2D p1 = t.k().getLine().getP1();\n                Point2D p2 = t.k().getLine().getP2();\n                List<Node> rnodes = new ArrayList<>();\n                List<Node> lnodes = new ArrayList<>();\n                t.v().stream().flatMap(e -> e.streamNodes()).distinct().forEach(n -> {\n                    if (n.getPoint().distance(p1) < n.getPoint().distance(p2)) {\n                        n.setPoint(p1);\n                        rnodes.add(n);\n                    } else {\n                        n.setPoint(p2);\n                        lnodes.add(n);\n                    }\n                });\n                if (rnodes.size() > 0 && lnodes.size() > 0) {\n                    Edge e = ctab.addEdge(rnodes.get(0).getIndex(), lnodes.get(0).getIndex(), 1);\n                    e.setDashed(true);\n                    foundNewOCR[0] = true;\n                }\n            });\n            ctab.mergeNodesCloserThan(MAX_DISTANCE_BEFORE_MERGING_NODES);\n            ctab.standardCleanEdges();\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            List<Double> allDashLengths = new ArrayList<>();\n            ctab.getEdgesWhichMightBeDottedLines().stream().map(e -> Tuple.of(e, GeomUtil.vertices(e.stream().map(e1 -> e1.getLine()).collect(Collectors.toList())))).map(Tuple.vmap(vts -> GeomUtil.getPairOfFarthestPoints(vts))).forEach(s -> {\n                List<Node> rnodes = new ArrayList<>();\n                List<Node> lnodes = new ArrayList<>();\n                s.k().stream().peek(e -> allDashLengths.add(e.getEdgeLength())).flatMap(e -> e.streamNodes()).distinct().forEach(n -> {\n                    if (n.getPoint().distance(s.v()[0]) < n.getPoint().distance(s.v()[1])) {\n                        n.setPoint(s.v()[0]);\n                        rnodes.add(n);\n                    } else {\n                        n.setPoint(s.v()[1]);\n                        lnodes.add(n);\n                    }\n                });\n                dottedLines.add(LineWrapper.of(new Line2D.Double(s.v()[0], s.v()[1])));\n                if (rnodes.size() > 0 && lnodes.size() > 0) {\n                    Edge e = ctab.addEdge(rnodes.get(0).getIndex(), lnodes.get(0).getIndex(), 1);\n                    e.setDashed(true);\n                    foundNewOCR[0] = true;\n                }\n            });\n            ctab.standardCleanEdges();\n            ctab.mergeNodesCloserThan(MAX_DISTANCE_BEFORE_MERGING_NODES);\n            double avgDot = allDashLengths.stream().mapToDouble(d -> d).average().orElse(2);\n            if (foundNewOCR[0] && avgDot > ignoreTooSmall[0]) {\n                ignoreTooSmall[0] = avgDot * 1.1;\n                break;\n            } else {\n                foundNewOCR[0] = false;\n            }\n            for (Shape s : likelyOCR) {\n                ctab.mergeAllNodesInsideCenter(s, OCR_TO_BOND_MAX_DISTANCE);\n            }\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            List<List<Node>> newNodesForMerge = new ArrayList<>();\n            Function<List<Node>, Point2D> bestIntersectionPoint = (nl) -> {\n                Point2D center = GeomUtil.findCenterOfVertices(nl.stream().map(n -> n.getPoint()).collect(Collectors.toList()));\n                double radSq = Math.pow(ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE, 2);\n                List<Edge> el = nl.stream().flatMap(n -> n.getEdges().stream()).collect(Collectors.toList());\n                List<Point2D> intersections = GeomUtil.eachCombination(el).flatMap(t -> {\n                    if (t.k() == t.v())\n                        return Stream.of(t.k().getPoint1(), t.k().getPoint2());\n                    return Stream.of(GeomUtil.intersection(t.k().getLine(), t.v().getLine()));\n                }).filter(p -> p != null).filter(p -> p.distanceSq(center) < radSq).collect(Collectors.toList());\n                if (!intersections.isEmpty()) {\n                    return GeomUtil.findCenterMostPoint(intersections);\n                } else {\n                    return center;\n                }\n            };\n            List<Point2D> mergedPoints = new ArrayList<Point2D>();\n            ctab.mergeNodesCloserThan(ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE_INITIAL, (nl) -> {\n                Point2D[] far = GeomUtil.getPairOfFarthestPoints(nl.stream().map(n -> n.getPoint()).collect(Collectors.toList()));\n                if (nl.size() == 2 && far[0].distance(far[1]) > ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE) {\n                    return null;\n                }\n                if (far[0].distance(far[1]) > 0.9 * ctab.getAverageBondLength()) {\n                    List<Node> group1 = new ArrayList<>();\n                    List<Node> group2 = new ArrayList<>();\n                    nl.forEach(n -> {\n                        if (n.getPoint().distance(far[0]) < n.getPoint().distance(far[1])) {\n                            group1.add(n);\n                        } else {\n                            group2.add(n);\n                        }\n                    });\n                    newNodesForMerge.add(group1);\n                    newNodesForMerge.add(group2);\n                    return null;\n                }\n                Point2D cpt = GeomUtil.findCenterOfVertices(nl.stream().map(n -> n.getPoint()).collect(Collectors.toList()));\n                List<Point2D> missingPoints = removedTinyVertices.stream().filter(pt -> pt.distance(cpt) < ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE).collect(Collectors.toList());\n                missingPoints.addAll(nl.stream().map(n -> n.getPoint()).collect(Collectors.toList()));\n                if (missingPoints.size() > 3) {\n                    Shape candidate = GeomUtil.convexHull2(missingPoints.stream().toArray(i -> new Point2D[i]));\n                    if (GeomUtil.area(candidate) > 0.5 * averageAreaOCR) {\n                        Point2D center = GeomUtil.findCenterOfVertices(missingPoints);\n                        ;\n                        candidate = GeomUtil.growShape(candidate, 4);\n                        rescueOCRCandidates.add(candidate);\n                        return center;\n                    }\n                }\n                mergedPoints.addAll(missingPoints);\n                return bestIntersectionPoint.apply(nl);\n            });\n            newNodesForMerge.forEach(ln -> {\n                Point2D center = bestIntersectionPoint.apply(ln);\n                ctab.mergeNodes(ln.stream().map(n -> n.getIndex()).collect(Collectors.toList()), (ll) -> {\n                    return center;\n                });\n            });\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            ctab.removeOrphanNodes();\n            ctab.standardCleanEdges();\n            ctab.mergeNodesCloserThan(ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE, (nl) -> {\n                Point2D cpt = GeomUtil.findCenterOfVertices(nl.stream().map(n -> n.getPoint()).collect(Collectors.toList()));\n                List<Point2D> missingPoints = removedTinyVertices.stream().filter(pt -> pt.distance(cpt) < ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE).collect(Collectors.toList());\n                missingPoints.addAll(nl.stream().map(n -> n.getPoint()).collect(Collectors.toList()));\n                if (missingPoints.size() > 3) {\n                    Shape candidate = GeomUtil.convexHull2(missingPoints.stream().toArray(i -> new Point2D[i]));\n                    double area = GeomUtil.area(candidate);\n                    if (area > 0.5 * averageAreaOCR) {\n                        candidate = GeomUtil.growShape(candidate, 4);\n                        rescueOCRCandidates.add(candidate);\n                        return GeomUtil.findCenterOfVertices(missingPoints);\n                    }\n                }\n                mergedPoints.addAll(missingPoints);\n                return bestIntersectionPoint.apply(nl);\n            });\n            GeomUtil.groupThings(mergedPoints, (tp) -> {\n                Point2D p1 = tp.k();\n                Point2D p2 = tp.v();\n                if (p1.distance(p2) < ctab.getAverageBondLength() * 0.6) {\n                    return connectedComponents.stream().filter(s -> s.contains(p1) && s.contains(p2)).findAny().isPresent();\n                }\n                return false;\n            }).forEach(ll -> {\n                Point2D[] pts = ll.toArray(new Point2D[0]);\n                if (pts.length > 3) {\n                    Shape candidate = GeomUtil.convexHull2(pts);\n                    if (GeomUtil.area(candidate) > 0.5 * averageAreaOCR) {\n                        candidate = GeomUtil.growShape(candidate, 4);\n                        rescueOCRCandidates.add(candidate);\n                    }\n                }\n            });\n            ctab.removeOrphanNodes();\n            ctab.standardCleanEdges();\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            Set<Edge> splitEdges = new HashSet<Edge>();\n            ctab.createNodesOnIntersectingLines(2, elist -> {\n                splitEdges.addAll(elist);\n                return true;\n            }, (nn) -> {\n                intersectionNodes.add(nn.getPoint());\n            });\n            List<Shape> expectedLineZones = Stream.concat(growLines.stream(), likelyOCR.stream()).collect(Collectors.toList());\n            ctab.getEdges().stream().filter(e -> splitEdges.contains(e)).collect(Collectors.toList()).forEach(e -> {\n                Line2D ll = e.getLine();\n                double totLen = GeomUtil.getLinesNotInside(ll, expectedLineZones).stream().mapToDouble(l -> GeomUtil.length(l)).sum();\n                if (totLen > GeomUtil.length(ll) * 0.8) {\n                    ctab.removeEdge(e);\n                }\n            });\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            ctab.mergeFilteredNodesCloserThan(ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE_AFTER_SPLIT, n -> true);\n            ctab.standardCleanEdges();\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            if (avgDistOCRToLine.isPresent()) {\n                double nmaxRatio = (avgDistOCRToLine.getAsDouble()) / ctab.getAverageBondLength();\n                if (nmaxRatio > maxRatioInitial) {\n                    maxRatio = nmaxRatio;\n                    double maxlen = ctab.getEdges().stream().mapToDouble(e -> e.getEdgeLength()).max().orElse(1);\n                    maxlen = Math.max(maxlen, averageWidthOCR);\n                    maxTotalRatio = Math.max(maxTotalRatioInitial, maxlen / ctab.getAverageBondLength());\n                }\n            }\n            ctab.mergeNodesExtendingTo(likelyOCR, maxRatio, maxTotalRatio);\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            ctab.removeOrphanNodes();\n            ctab.mergeFilteredNodesCloserThan(ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE_AFTER_SPLIT, n -> true);\n            ctab.standardCleanEdges();\n            ctab.mergeFilteredNodesCloserThan(ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE, n -> {\n                if (intersectionNodes.stream().filter(p -> p.distance(n.getPoint()) < 0.01 * ctab.getAverageBondLength()).findAny().isPresent()) {\n                    if (n.getEdgeCount() == 4)\n                        return false;\n                    return true;\n                }\n                return true;\n            });\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            for (Shape s : likelyOCR) {\n                ctab.mergeAllNodesInsideCenter(s, OCR_TO_BOND_MAX_DISTANCE);\n            }\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            ctab.makeMissingNodesForShapes(likelyOCR, MAX_BOND_TO_AVG_BOND_RATIO_FOR_NOVEL, MIN_BOND_TO_AVG_BOND_RATIO_FOR_NOVEL);\n            Set<Node> toRemove = new HashSet<Node>();\n            Set<Edge> toRemoveEdges = new HashSet<Edge>();\n            ctab.makeMissingBondsToNeighbors(bitmap, MAX_BOND_TO_AVG_BOND_RATIO_FOR_NOVEL, MAX_TOLERANCE_FOR_DASH_BONDS, likelyOCR, OCR_TO_BOND_MAX_DISTANCE, (t) -> {\n                Edge e = t.v();\n                Node n1 = e.getRealNode1();\n                Node n2 = e.getRealNode2();\n                List<Edge> existingEdges1 = n1.getEdges();\n                List<Edge> existingEdges2 = n2.getEdges();\n                Set<Node> n1Neigh = existingEdges1.stream().flatMap(ne -> Stream.of(ne.getRealNode1(), ne.getRealNode2())).filter(n -> !n.equals(n1)).collect(Collectors.toSet());\n                Set<Node> n2Neigh = existingEdges2.stream().flatMap(ne -> Stream.of(ne.getRealNode1(), ne.getRealNode2())).filter(n -> !n.equals(n2)).collect(Collectors.toSet());\n                List<Node> commonNeigh = n1Neigh.stream().filter(nn -> n2Neigh.contains(nn)).collect(Collectors.toList());\n                boolean alreadyExists = false;\n                if (!commonNeigh.isEmpty()) {\n                    for (Node cn : commonNeigh) {\n                        Point2D cp = cn.getPoint();\n                        double distance1 = n1.getPoint().distance(cp);\n                        double distance2 = n2.getPoint().distance(cp);\n                        double sumd = distance1 + distance2;\n                        double ddelta = Math.abs(sumd - e.getEdgeLength());\n                        List<Edge> edges = cn.getEdges();\n                        if (edges.size() == 2) {\n                            if (ddelta < MAX_DELTA_LENGTH_FOR_STITCHING_LINES_ON_BOND_ORDER_CALC) {\n                                if (!toRemove.contains(n1) && !toRemove.contains(n2)) {\n                                    toRemove.add(cn);\n                                }\n                                double o2 = edges.stream().map(et -> Tuple.of(et, et.getEdgeLength())).mapToDouble(e1 -> (e1.k().getOrder() * e1.v())).sum();\n                                int o = (int) Math.round(((o2 / sumd) + 0.05));\n                                t.v().setOrder(o);\n                            }\n                            if (!edges.stream().anyMatch(e2 -> e2.getDashed())) {\n                                alreadyExists = true;\n                            }\n                        } else if (edges.size() == 4) {\n                            boolean isIntersection = intersectionNodes.stream().filter(in -> in.distance(cp) < 2).findAny().isPresent();\n                            if (isIntersection && Math.abs(sumd - t.v().getEdgeLength()) < ctab.getAverageBondLength() * 0.05) {\n                                toRemoveEdges.add(t.v());\n                            }\n                        }\n                    }\n                }\n                if (t.k() > MAX_TOLERANCE_FOR_SINGLE_BONDS) {\n                    if (!alreadyExists) {\n                        t.v().setDashed(true);\n                    }\n                }\n            });\n            toRemoveEdges.forEach(e -> ctab.removeEdge(e));\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            toRemove.forEach(n -> ctab.removeNodeAndEdges(n));\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            double avgBondLength = ctab.getAverageBondLength();\n            maxBondLength[0] = avgBondLength * MAX_BOND_TO_AVG_BOND_RATIO_TO_KEEP;\n            Predicate<Line2D> longerThanMax = GeomUtil.longerThan(maxBondLength[0]);\n            tooLongBond = ctab.getEdges().stream().filter(e -> longerThanMax.test(e.getLine())).findAny().isPresent();\n            if (tooLongBond) {\n                reps++;\n            }\n            if (reps > MAX_REPS)\n                break;\n        }\n        AtomicBoolean anyOtherIntersections = new AtomicBoolean(false);\n        ctab.createNodesOnIntersectingLines(3, elist -> {\n            long longEnoughBonds = elist.stream().filter(e -> e.getEdgeLength() > MAX_BOND_TO_AVG_BOND_RATIO_FOR_INTERSECTION * ctab.getAverageBondLength()).count();\n            if (longEnoughBonds < 3)\n                return false;\n            anyOtherIntersections.set(true);\n            return true;\n        }, (nn) -> {\n            intersectionNodes.add(nn.getPoint());\n        });\n        if (anyOtherIntersections.get()) {\n            if (DEBUG)\n                ctabRaw.add(ctab.cloneTab());\n            ctab.mergeNodesCloserThan(ctab.getAverageBondLength() * MIN_BOND_TO_AVG_BOND_RATIO_FOR_MERGE);\n            ctab.standardCleanEdges();\n        }\n        double shortestRealBondRatio = .3;\n        ctab.fixBondOrders(likelyOCR, shortestRealBondRatio, e -> {\n            e.setOrder(1);\n        });\n        double avgbond = ctab.getAverageBondLength();\n        double SEED_BOND_RATIO_FOR_OCR_WIDTH = 0.0;\n        double SEED_BOND_RATIO_FOR_OCR_WIDTH_FOR_CENTROID = 0.5;\n        List<Node> unmatchedNodes = ctab.getNodesNotInShapes(likelyOCR, OCR_TO_BOND_MAX_DISTANCE + avgbond * SEED_BOND_RATIO_FOR_OCR_WIDTH);\n        List<LineWrapper> verticesJl = linesJoined.stream().collect(Collectors.toList());\n        List<Point2D> verticesJ = verticesJl.stream().flatMap(l -> l.streamPoints()).collect(Collectors.toList());\n        List<Shape> toAddAllOCR = new ArrayList<Shape>();\n        Map<Shape, List<Tuple<Character, Number>>> gotCache = new HashMap<>();\n        unmatchedNodes.forEach(n -> {\n            Point2D cpt = n.getPoint();\n            Point2D centerRescue = rescueOCRCandidates.stream().filter(sc -> sc.contains(n.getPoint())).map(sc -> Tuple.of(sc, GeomUtil.area(sc)).withVComparator()).sorted(Comparator.reverseOrder()).map(t -> t.k()).findAny().map(sc -> GeomUtil.findCenterOfShape(sc)).orElse(null);\n            int numEdges = n.getEdges().size();\n            Shape[] area = new Shape[] { null };\n            Shape nshape = null;\n            double radius = Math.max(avgbond * SEED_BOND_RATIO_FOR_OCR_WIDTH_FOR_CENTROID, averageLargestOCR / 2);\n            List<Point2D> allVertices = verticesJ;\n            if (centerRescue != null) {\n                cpt = centerRescue;\n            }\n            boolean keep = true;\n            for (int i = 0; i < 3; i++) {\n                keep = true;\n                area[0] = GeomUtil.makeShapeAround(cpt, radius);\n                List<Point2D> insideVertices = allVertices.stream().filter(v -> area[0].contains(v)).collect(Collectors.toList());\n                if (insideVertices.size() <= numEdges + 1) {\n                    keep = false;\n                }\n                List<Point2D> insideVertices2 = allVertices.stream().filter(v -> area[0].contains(v)).collect(Collectors.toList());\n                Point2D center = GeomUtil.findCenterOfVertices(insideVertices2);\n                ;\n                double distanceMean = insideVertices2.stream().mapToDouble(pt -> center.distance(pt)).average().orElse(0);\n                double distanceVar = insideVertices2.stream().mapToDouble(pt -> Math.pow(distanceMean - center.distance(pt), 2)).sum();\n                double distanceSTDEV = Math.sqrt(distanceVar / (insideVertices2.size() - 1));\n                List<Point2D> realMissing = insideVertices2.stream().filter(pt -> center.distance(pt) < distanceMean + distanceSTDEV * 2.5).collect(Collectors.toList());\n                nshape = GeomUtil.convexHull2(realMissing.toArray(new Point2D[0]));\n                Point2D[] far = GeomUtil.getPairOfFarthestPoints(nshape);\n                double arean = GeomUtil.area(nshape);\n                double r = 0;\n                if (far != null) {\n                    r = far[0].distance(far[1]);\n                }\n                if (r < averageLargestOCR * MIN_LONGEST_WIDTH_RATIO_FOR_OCR_TO_AVERAGE) {\n                    keep = false;\n                }\n                if (arean < GeomUtil.area(nshape.getBounds2D()) * MIN_AREA_RATIO_FOR_HULL_TO_BBOX_OCR) {\n                    keep = false;\n                }\n                if (GeomUtil.area(nshape.getBounds2D()) > avgbond * avgbond * 0.5) {\n                    keep = false;\n                }\n                if (GeomUtil.area(nshape.getBounds2D()) < averageAreaOCR * MIN_AREA_RATIO_FOR_OCR_TO_AVERAGE) {\n                    keep = false;\n                }\n                if (GeomUtil.area(nshape.getBounds2D()) > averageAreaOCR * MAX_AREA_RATIO_FOR_OCR_TO_AVERAGE) {\n                    keep = false;\n                }\n                radius = Math.max(averageLargestOCR / 2, r / 2);\n                cpt = GeomUtil.findCenterOfShape(nshape);\n            }\n            if (keep) {\n                Bitmap nmap = bitmap.getLazyCrop(nshape);\n                Bitmap nthinmap = thin.getLazyCrop(nshape);\n                if (nmap != null && nthinmap != null) {\n                    nshape = GeomUtil.growShape(nshape, 2);\n                    nmap = bitmap.crop(nshape);\n                    List<Shape> slist = nmap.connectedComponents(Bitmap.Bbox.DoublePolygon);\n                    Shape bshape = slist.stream().map(s -> Tuple.of(s, s.getBounds2D().getWidth() * s.getBounds2D().getHeight()).withVComparator()).max(CompareUtil.naturalOrder()).map(t -> t.k()).orElse(nshape);\n                    Rectangle2D rect1 = nshape.getBounds2D();\n                    AffineTransform at = new AffineTransform();\n                    at.translate(rect1.getX(), rect1.getY());\n                    nshape = at.createTransformedShape(bshape).getBounds2D();\n                    nmap = bitmap.getLazyCrop(nshape);\n                    nthinmap = thin.getLazyCrop(nshape);\n                    if (GeomUtil.area(nshape) < averageAreaOCR * MIN_AREA_RATIO_FOR_OCR_TO_AVERAGE) {\n                        return;\n                    }\n                    if (nmap != null && nthinmap != null) {\n                        processOCRShape(socr[0], nshape, bitmap, thin, (s, potential) -> {\n                            if (potential.get(0).v().doubleValue() > OCRcutoffCosineRescue) {\n                                String st = potential.get(0).k().toString();\n                                if (BranchNode.interpretOCRStringAsAtom2(st) != null) {\n                                    toAddAllOCR.add(s);\n                                    gotCache.put(s, potential);\n                                }\n                            }\n                        });\n                    }\n                }\n            }\n        });\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        GeomUtil.mergeOverlappingShapes(toAddAllOCR, 0.75).forEach(nshape -> {\n            boolean sigOverlap = likelyOCRAll.stream().map(s -> Tuple.of(s, GeomUtil.getIntersectionShape(nshape, s))).filter(os -> os.v().isPresent()).map(Tuple.vmap(os -> os.get())).map(Tuple.vmap(s -> GeomUtil.area(s))).map(Tuple.kmap(s -> GeomUtil.area(s))).mapToDouble(t -> t.v() / t.k()).filter(areaFraction -> areaFraction > 0.0).findAny().isPresent();\n            if (sigOverlap) {\n                return;\n            }\n            List<Tuple<Character, Number>> matches = gotCache.getOrDefault(nshape, new ArrayList<>());\n            if (matches.isEmpty()) {\n                Bitmap nmap = bitmap.getLazyCrop(nshape);\n                Bitmap nthinmap = thin.getLazyCrop(nshape);\n                if (nmap != null && nthinmap != null) {\n                    processOCRShape(socr[0], nshape, bitmap, thin, (s, potential) -> {\n                        matches.addAll(potential);\n                    });\n                }\n            }\n            ocrAttempt.put(nshape, matches);\n            if (matches.get(0).v().doubleValue() > OCRcutoffCosineRescue) {\n                CharType ct = OCRIsLikely(matches.get(0));\n                if (ct.equals(CharType.ChemLikely)) {\n                    likelyOCR.add(nshape);\n                    likelyOCRNonBond.add(nshape);\n                } else if (ct.equals(CharType.NumericLikely)) {\n                    likelyOCRNonBond.add(nshape);\n                    likelyOCRNumbers.add(nshape);\n                }\n                likelyOCRAll.add(nshape);\n                ocrRescues.add(nshape);\n                foundNewOCR[0] = true;\n            }\n        });\n        ctab.mergeNodesExtendingTo(likelyOCR, maxRatio, maxTotalRatio);\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        double cosThetaOCRShape = Math.cos(MAX_THETA_FOR_OCR_SEPERATION);\n        List<Shape> growLikelyOCRNonBond = likelyOCRNonBond.stream().map(s -> GeomUtil.growShape(s, 2)).collect(Collectors.toList());\n        List<Shape> growLikelyOCR = likelyOCR.stream().map(s -> GeomUtil.growShape(s, 2)).collect(Collectors.toList());\n        List<Shape> maybeDash = polygons.stream().filter(s -> GeomUtil.area(s) < averageAreaOCR).filter(s -> !likelyOCR.contains(s)).map(s -> Tuple.of(s, GeomUtil.findCenterOfShape(s))).filter(st -> !growLikelyOCR.stream().filter(g -> g.contains(st.v())).findFirst().isPresent()).map(t -> t.k()).map(t -> Tuple.of(t, GeomUtil.findLongestSplittingLine(t))).filter(t -> {\n            if (t.v() != null) {\n                return t.v().length() < ctab.getAverageBondLength() * 0.5;\n            }\n            return true;\n        }).map(t -> t.k()).collect(Collectors.toList());\n        List<Shape> maybeDashCollection = GeomUtil.groupThings(maybeDash, t -> {\n            Shape s1 = t.k();\n            Shape s2 = t.v();\n            Point2D p1 = GeomUtil.findCenterOfShape(s1);\n            Point2D p2 = GeomUtil.findCenterOfShape(s2);\n            return p1.distance(p2) < ctab.getAverageBondLength() / 3;\n        }).stream().filter(sl -> sl.size() >= 3).map(l -> {\n            LineWrapper splitting = GeomUtil.findLongestSplittingLine(l.stream().collect(GeomUtil.joined()));\n            return l.stream().map(l1 -> Tuple.of(l1, GeomUtil.findLongestSplittingLine(l1))).filter(l2 -> splitting.cosTheta(l2.v()) < Math.cos(45 * Math.PI / 180)).map(t -> t.k()).collect(Collectors.toList());\n        }).map(l -> l.stream().collect(GeomUtil.joined())).filter(b -> b != null).filter(bshape -> {\n            Point2D[] pts = GeomUtil.getPairOfFarthestPoints(bshape);\n            double dist = pts[0].distance(pts[1]);\n            if (dist < ctab.getAverageBondLength() * 1.3 && dist > ctab.getAverageBondLength() * 0.6) {\n                return true;\n            } else {\n                return false;\n            }\n        }).collect(Collectors.toList());\n        likelyOCRAll.stream().filter(p -> maybeDashCollection.stream().filter(ds -> ds.contains(GeomUtil.findCenterOfShape(p))).findAny().isPresent()).collect(Collectors.toList()).forEach(ocs -> {\n            likelyOCRAll.remove(ocs);\n            likelyOCR.remove(ocs);\n            likelyOCRNumbers.remove(ocs);\n            likelyOCRNonBond.remove(ocs);\n        });\n        double wid = averageWidthOCR;\n        List<List<Shape>> ocrGroupList = GeomUtil.groupShapesIfClosestPointsMatchCriteria(likelyOCRAll, t -> {\n            Point2D[] pts = t.v();\n            Shape[] shapes = t.k();\n            double dist = pts[0].distanceSq(pts[1]);\n            double cutoff = Math.max(ctab.getAverageBondLength() * MAX_BOND_RATIO_FOR_OCR_CHAR_SPACING, wid);\n            if (dist > cutoff * cutoff) {\n                return false;\n            }\n            List<Tuple<Character, Number>> attempt0 = ocrAttempt.get(shapes[0]);\n            List<Tuple<Character, Number>> attempt1 = ocrAttempt.get(shapes[1]);\n            String v1 = (attempt0 == null || attempt0.isEmpty()) ? \"\" : attempt0.get(0).k().toString();\n            String v2 = (attempt1 == null || attempt1.isEmpty()) ? \"\" : attempt1.get(0).k().toString();\n            if (v1.equals(\"\\\\\") || v1.equals(\"/\") || v2.equals(\"\\\\\") || v2.equals(\"/\")) {\n                return false;\n            }\n            double s1y1 = shapes[0].getBounds2D().getMinY();\n            double s1y2 = shapes[0].getBounds2D().getMaxY();\n            double s2y1 = shapes[1].getBounds2D().getMinY();\n            double s2y2 = shapes[1].getBounds2D().getMaxY();\n            if (!((s1y1 >= s2y1 && s1y1 <= s2y2) || (s1y2 >= s2y1 && s1y2 <= s2y2) || ((s1y2 + s1y1) / 2 >= s2y1 && (s1y2 + s1y1) / 2 <= s2y2))) {\n                return false;\n            }\n            double pvoverlapT = Math.max(s1y2, s2y2) - Math.min(s1y1, s2y1);\n            double pvoverlapI = Math.min(s1y2, s2y2) - Math.max(s1y1, s2y1);\n            if (pvoverlapI < pvoverlapT * 0.4) {\n                if (likelyOCR.contains(shapes[0]) && likelyOCR.contains(shapes[1])) {\n                    if (!v1.equalsIgnoreCase(\"S\") && !v2.equalsIgnoreCase(\"S\")) {\n                        return false;\n                    }\n                }\n            }\n            Point2D cs1 = GeomUtil.findCenterOfShape(shapes[0]);\n            Point2D cs2 = GeomUtil.findCenterOfShape(shapes[1]);\n            LineWrapper cenLine = LineWrapper.of(new Line2D.Double(cs1, cs2));\n            double[] vec = cenLine.vector();\n            double cosTheta = Math.abs(vec[0] * cenLine.recipLength());\n            if (cosTheta > cosThetaOCRShape) {\n                return true;\n            }\n            return false;\n        });\n        bestGuessOCR.clear();\n        boolean areLikelyNumbers = (likelyOCRNumbers.size() >= 4);\n        ocrGroupList.stream().forEach(g -> {\n            List<Shape> sorted = g.stream().map(s -> Tuple.of(s, s)).map(Tuple.vmap(s -> s.getBounds2D().getMinX())).map(t -> t.withVComparator()).sorted().map(t -> t.k()).collect(Collectors.toList());\n            String soFar = \"\";\n            Shape making = null;\n            List<Tuple<Shape, Tuple<List<Shape>, String>>> toAdd = new ArrayList<>();\n            List<Shape> lsofar = new ArrayList<>();\n            for (Shape s : sorted) {\n                lsofar.add(s);\n                List<Tuple<Character, Number>> list = ocrAttempt.get(s);\n                String v = (list == null || list.isEmpty()) ? \"\" : list.get(0).k().toString();\n                if (s.getBounds2D().getHeight() < averageHeightOCR * 0.8 || (s.getBounds2D().getHeight() <= averageHeightNumberOCR * 1.1 && areLikelyNumbers)) {\n                    if (v.equalsIgnoreCase(\"S\")) {\n                        Tuple<Character, Number> tc = list.stream().filter(c -> c.k().toString().equals(\"3\") || c.k().toString().equals(\"8\")).findFirst().orElse(null);\n                        if (tc != null) {\n                            if (tc.v().doubleValue() > OCRcutoffCosineRescue) {\n                                v = tc.k().toString();\n                            }\n                        }\n                    }\n                }\n                if (v.equals(\"?\")) {\n                    v = \"N+\";\n                }\n                if (v.equals(\"-\")) {\n                    if (!soFar.equals(\"t\")) {\n                        if (making != null) {\n                            toAdd.add(Tuple.of(making, Tuple.of(lsofar, soFar)));\n                            lsofar.remove(lsofar.size() - 1);\n                            lsofar = new ArrayList<>();\n                        }\n                        soFar = \"\";\n                        making = null;\n                        continue;\n                    }\n                }\n                if (making == null) {\n                    making = s;\n                } else {\n                    making = GeomUtil.add(making, s);\n                }\n                soFar += v;\n            }\n            if (making != null) {\n                toAdd.add(Tuple.of(making, Tuple.of(lsofar, soFar)));\n            }\n            Map<String, List<String>> dontMerge = new HashMap<>();\n            dontMerge.put(\"OO\", Arrays.asList(\"O\", \"O\"));\n            dontMerge.put(\"Oo\", Arrays.asList(\"O\", \"o\"));\n            dontMerge.put(\"oO\", Arrays.asList(\"o\", \"O\"));\n            dontMerge.put(\"oo\", Arrays.asList(\"o\", \"o\"));\n            dontMerge.put(\"OF\", Arrays.asList(\"O\", \"F\"));\n            dontMerge.put(\"oF\", Arrays.asList(\"o\", \"F\"));\n            dontMerge.put(\"Fo\", Arrays.asList(\"F\", \"o\"));\n            dontMerge.put(\"FO\", Arrays.asList(\"F\", \"O\"));\n            dontMerge.put(\"FF\", Arrays.asList(\"F\", \"F\"));\n            dontMerge.put(\"CH3CH3\", Arrays.asList(\"CH3\", \"CH3\"));\n            dontMerge.put(\"cH3cH3\", Arrays.asList(\"cH3\", \"cH3\"));\n            dontMerge.put(\"HOOH\", Arrays.asList(\"HO\", \"OH\"));\n            dontMerge.put(\"OHOH\", Arrays.asList(\"OH\", \"OH\"));\n            dontMerge.put(\"OHHO\", Arrays.asList(\"OH\", \"HO\"));\n            dontMerge.put(\"BrBr\", Arrays.asList(\"Br\", \"Br\"));\n            for (Tuple<Shape, Tuple<List<Shape>, String>> tt : toAdd) {\n                boolean removeBad = false;\n                String val = tt.v().v();\n                List<Shape> contains = tt.v().k();\n                Shape parent = tt.k();\n                if (val.contains(\"~\")) {\n                    if (val.equals(\"~\")) {\n                        val = \"Cl\";\n                    } else {\n                        val = val.replace(\"~\", \"O\");\n                    }\n                }\n                if (val.contains(\"$\")) {\n                    val = val.replace(\"$\", \"O2\");\n                }\n                if (val.contains(\"!\")) {\n                    val = val.replace(\"!\", \"H3\");\n                }\n                if (val.contains(\"`\")) {\n                    val = val.replace(\"`\", \"HO\");\n                }\n                if (val.contains(\"%\")) {\n                    val = val.replace(\"%\", \"OC\");\n                }\n                if (val.matches(\"[cC][h][1ilt][r][a][1ilt]\")) {\n                    removeBad = true;\n                }\n                if (val.equals(\"IN\") || val.equals(\"tN\") || val.equals(\"lN\")) {\n                    bestGuessOCR.put(contains.get(1), \"N\");\n                    continue;\n                }\n                if (dontMerge.containsKey(val)) {\n                    List<String> keepAs = dontMerge.get(val);\n                    int findex = 0;\n                    for (int i = 0; i < keepAs.size(); i++) {\n                        String keep = keepAs.get(i);\n                        String g1 = val.substring(findex, findex + keep.length());\n                        Shape parts = contains.stream().skip(findex).limit(keep.length()).collect(GeomUtil.joined());\n                        findex = findex + keep.length();\n                        if (keep.equals(g1)) {\n                            bestGuessOCR.put(parts, keep);\n                        }\n                    }\n                    continue;\n                }\n                if (val.contains(\"F8\")) {\n                    val = val.replace(\"F8\", \"F3\");\n                }\n                if (val.matches(\"[cC]H[Ss]\")) {\n                    val = val.replaceAll(\"[cC]H[Ss]\", \"CH3\");\n                }\n                if (val.matches(\".*[cC][tlI][Ss].*\")) {\n                    val = val.replaceAll(\"[cC][tlI][Ss]\", \"Cl3\");\n                }\n                if (val.matches(\".*[cC][tlI][8].*\")) {\n                    val = val.replaceAll(\"[cC][tlI][8]\", \"Cl3\");\n                }\n                BranchNode bn = BranchNode.interpretOCRStringAsAtom2(val);\n                if (val.length() > 5) {\n                    if (bn == null) {\n                        removeBad = true;\n                    }\n                }\n                if (parent.getBounds2D().getHeight() <= averageHeightNumberOCR * 1.1 && areLikelyNumbers) {\n                    if (val.matches(\"[0-9t][0-9t]*\")) {\n                        val = \"#\" + val.replace(\"t\", \"1\");\n                        removeBad = true;\n                    }\n                }\n                if (bn == null || !bn.isRealNode()) {\n                    if (REMOVE_NONSENSE_OCR_LINES) {\n                        removeBad = true;\n                    }\n                }\n                if (removeBad) {\n                    contains.stream().forEach(s2 -> {\n                        likelyOCR.remove(s2);\n                        likelyOCRNumbers.remove(s2);\n                        likelyOCRNonBond.remove(s2);\n                        likelyOCRAll.remove(s2);\n                    });\n                    foundNewOCR[0] = true;\n                    likelyOCRIgnore.add(GeomUtil.growShape(parent, 2));\n                    likelyOCR.removeAll(ocrRescues);\n                    likelyOCRNumbers.removeAll(ocrRescues);\n                    likelyOCRNonBond.removeAll(ocrRescues);\n                    likelyOCRAll.removeAll(ocrRescues);\n                }\n                bestGuessOCR.put(parent, val);\n            }\n        });\n        bestGuessOCR.entrySet().stream().map(Tuple::of).filter(t -> t.v().equals(\"H\")).collect(Collectors.toList()).stream().forEach(t -> {\n            double cutoff = Math.max(ctab.getAverageBondLength() * MAX_BOND_RATIO_FOR_OCR_CHAR_SPACING, averageWidthOCR);\n            Tuple<Shape, String> toConnect = bestGuessOCR.entrySet().stream().map(Tuple::of).filter(t1 -> t1.k() != t.k()).filter(t1 -> t1.v().equals(\"N\") || t1.v().equals(\"Nt\") || t1.v().equals(\"N+\") || t1.v().equals(\"NI\") || t1.v().equals(\"Nl\") || t1.v().equals(\"O\") || t1.v().equals(\"S\")).filter(t1 -> GeomUtil.distance(t.k(), t1.k()) < cutoff).filter(t1 -> (Math.abs(t1.k().getBounds2D().getMinX() - t.k().getBounds2D().getMinX()) < cutoff / 3.0)).filter(t1 -> (Math.abs(t1.k().getBounds2D().getMaxX() - t.k().getBounds2D().getMaxX()) < cutoff / 1.5)).findFirst().orElse(null);\n            ;\n            if (toConnect != null) {\n                Shape nshape = GeomUtil.add(t.k(), toConnect.k());\n                String old = toConnect.v();\n                if (old.contains(\"N\") && !old.equals(\"N+\"))\n                    old = \"N\";\n                String nstring = old + t.v();\n                bestGuessOCR.put(nshape, nstring);\n                bestGuessOCR.remove(t.k());\n                bestGuessOCR.remove(toConnect.k());\n            }\n        });\n        bestGuessOCR.entrySet().stream().map(Tuple::of).filter(t -> t.v().equals(\"O2\") || t.v().equalsIgnoreCase(\"Boc\")).collect(Collectors.toList()).stream().forEach(t -> {\n            double cutoff = Math.max(ctab.getAverageBondLength() * MAX_BOND_RATIO_FOR_OCR_CHAR_SPACING, averageWidthOCR);\n            Tuple<Shape, String> toConnect = bestGuessOCR.entrySet().stream().map(Tuple::of).filter(t1 -> t1.k() != t.k()).filter(t1 -> t1.v().equals(\"S\") || t1.v().equals(\"N\")).filter(t1 -> GeomUtil.distance(t.k(), t1.k()) < cutoff).filter(t1 -> (Math.abs(t1.k().getBounds2D().getMinX() - t.k().getBounds2D().getMinX()) < cutoff / 3.0)).findFirst().orElse(null);\n            ;\n            if (toConnect != null) {\n                Shape nshape = GeomUtil.add(t.k(), toConnect.k());\n                String old = toConnect.v();\n                String nstring = old + t.v();\n                bestGuessOCR.put(nshape, nstring);\n                bestGuessOCR.remove(t.k());\n                bestGuessOCR.remove(toConnect.k());\n            }\n        });\n        ctab.standardCleanEdges();\n        List<Shape> ocrMeaningful = bestGuessOCR.keySet().stream().peek(t -> System.out.println(bestGuessOCR.get(t))).filter(s -> BranchNode.interpretOCRStringAsAtom2(bestGuessOCR.get(s)) != null).collect(Collectors.toList());\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        Set<Node> alreadyFixedNodes = new HashSet<Node>();\n        bestGuessOCR.entrySet().stream().map(Tuple::of).map(Tuple.vmap(s -> Tuple.of(s, (s.equals(\"H\")) ? 1 : 0).withVComparator())).map(t -> t.withVComparator()).sorted().filter(t -> !t.v().k().startsWith(\"#\")).map(Tuple.vmap(t -> t.k())).collect(Collectors.toList()).forEach(shapeString -> {\n            Shape s = shapeString.k();\n            String sym = shapeString.v();\n            Point2D cen = GeomUtil.findCenterOfShape(s);\n            Point2D centert = cen;\n            BranchNode actual1;\n            if (sym.equals(\"I\")) {\n                List<Node> ln = ctab.getNodesInsideShape(s, 2);\n                if (ln.isEmpty())\n                    return;\n                boolean isLinker = ln.stream().filter(n -> n.getEdgeCount() > 1).findAny().isPresent();\n                if (isLinker)\n                    return;\n                boolean tooClose = ctab.getNodes().stream().filter(n -> !ln.contains(n)).filter(n -> n.getPoint().distance(cen) < ctab.getAverageBondLength() * 0.8).findAny().isPresent();\n                if (tooClose)\n                    return;\n                boolean doublePoss = rejBondOrderLines.stream().filter(lw -> lw.growLine(ctab.getAverageBondLength() * 0.1).contains(cen)).findAny().isPresent();\n                if (doublePoss)\n                    return;\n                if (s.getBounds2D().getHeight() > ctab.getAverageBondLength() * 0.7) {\n                    return;\n                }\n                actual1 = new BranchNode(\"I\");\n                bestGuessOCR.put(s, \"t\");\n            } else {\n                actual1 = BranchNode.interpretOCRStringAsAtom2(sym);\n            }\n            BranchNode actual = actual1;\n            if (actual != null && actual.isRealNode()) {\n                if (sym.length() > 1) {\n                    List<Line2D> externalLines = ctab.getAllEdgesEntering(s, MAX_BOND_RATIO_FOR_MERGING_TO_OCR * ctab.getAverageBondLength()).stream().map(t -> t.k().getLine()).collect(Collectors.toList());\n                    if (externalLines.size() == 1) {\n                        Line2D exl = externalLines.get(0);\n                        Point2D tc = centert;\n                        Point2D cnew = likelyOCR.stream().map(s1 -> GeomUtil.findCenterOfShape(s1)).filter(spt -> s.contains(spt)).map(cpt -> GeomUtil.projectPointOntoLineWithRejection(exl, cpt)).map(Tuple.vmap(d -> Math.abs(d))).map(t -> t.withVComparator()).min(Comparator.naturalOrder()).map(t -> t.k()).orElseGet(() -> {\n                            return GeomUtil.projectPointOntoLine(externalLines.get(0), tc);\n                        });\n                        if (s.contains(cnew)) {\n                            centert = cnew;\n                        }\n                    } else {\n                        List<Point2D> intersections = GeomUtil.eachCombination(externalLines).map(t -> GeomUtil.intersection(t.k(), t.v())).filter(p -> p != null).filter(p -> s.contains(p)).collect(Collectors.toList());\n                        if (intersections.size() == 1) {\n                            centert = intersections.get(0);\n                        } else if (intersections.size() > 1) {\n                            centert = GeomUtil.findCenterOfVertices(intersections);\n                        }\n                    }\n                    if (!s.contains(centert)) {\n                        centert = GeomUtil.findCenterOfShape(s);\n                    }\n                }\n                Point2D center = centert;\n                ctab.mergeAllNodesInside(s, MAX_BOND_RATIO_FOR_MERGING_TO_OCR * ctab.getAverageBondLength(), (n) -> {\n                    if (sym.equals(\"H\")) {\n                        if (GeomUtil.findClosestShapeTo(ocrMeaningful, n.getPoint()).k() != s) {\n                            return false;\n                        }\n                    }\n                    boolean contains = s.contains(n.getPoint());\n                    if (!contains) {\n                        if (actual.isTerminal() && n.getEdgeCount() > 1) {\n                            long cc = n.getNeighborNodes().stream().map(t -> t.k()).filter(nn -> GeomUtil.distanceTo(s, nn.getPoint()) < 2).count();\n                            if (cc == 0)\n                                return false;\n                        }\n                        if (n.getEdgeCount() > 1) {\n                            boolean nhas = n.getNeighborNodes().stream().map(n1 -> n1.k().getPoint()).filter(p -> s.contains(p)).findAny().isPresent();\n                            if (!nhas) {\n                                boolean edgeHas = n.getEdges().stream().map(e -> e.getLine()).map(l -> GeomUtil.getLineInside(l, s)).filter(l -> l.isPresent()).findAny().isPresent();\n                                if (!edgeHas) {\n                                    double avgNDist = GeomUtil.eachCombination(n.getNeighborNodes().stream().map(n1 -> n1.k().getPoint()).collect(Collectors.toList())).map(t -> new Line2D.Double(t.k(), t.v())).mapToDouble(l -> GeomUtil.length(l)).average().orElseGet(() -> ctab.getAverageBondLength());\n                                    if (avgNDist > ctab.getAverageBondLength()) {\n                                        if (!sym.equals(\"H\")) {\n                                            Tuple<Shape, Double> bs = GeomUtil.findClosestShapeTo(likelyOCR, n.getPoint());\n                                            if (bs != null) {\n                                                char tt = Optional.ofNullable(ocrAttempt.get(bs.k())).map(l -> l.get(0).k()).orElse(null);\n                                                if (tt == 'H') {\n                                                    return false;\n                                                }\n                                            }\n                                        }\n                                    }\n                                }\n                            }\n                        }\n                    }\n                    if (alreadyFixedNodes.contains(n))\n                        return false;\n                    if (n.getEdgeCount() == 0)\n                        return false;\n                    return true;\n                }, (l) -> {\n                    boolean matchesOthers = l.stream().map(pt -> GeomUtil.findClosestShapeTo(ocrMeaningful, pt).k()).filter(sb -> (sb != s)).findAny().isPresent();\n                    if (!matchesOthers) {\n                        return center;\n                    } else {\n                        return GeomUtil.findCenterMostPoint(l);\n                    }\n                });\n                List<Node> mergedNodes = ctab.getAllNodesInsideShape(s, MAX_BOND_RATIO_FOR_MERGING_TO_OCR * ctab.getAverageBondLength());\n                alreadyFixedNodes.addAll(mergedNodes);\n            }\n        });\n        ctab.standardCleanEdges();\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        List<Tuple<Line2D, Point2D>> lj = Stream.concat(removedTinyLines.stream(), linesJoined.stream().map(l -> l.getLine())).flatMap(l -> GeomUtil.getLinesNotInside(l, growLikelyOCRNonBond).stream()).map(l -> Tuple.of(l, GeomUtil.findCenterOfShape(l))).collect(Collectors.toList());\n        Set<Line2D> taken = new HashSet<Line2D>();\n        List<Tuple<List<Line2D>, Tuple<Node, Node>>> edgesToMake = new ArrayList<>();\n        GeomUtil.groupShapesIfClosestPointsMatchCriteria(likelyOCR, (t) -> {\n            Point2D[] pts = t.v();\n            if (pts[0].distance(pts[1]) < ctab.getAverageBondLength() * .9) {\n                return true;\n            }\n            return false;\n        }).stream().filter(ls -> ls.size() >= 2).flatMap(ls -> {\n            return GeomUtil.eachCombination(ls).filter(t -> GeomUtil.distance(t.k(), t.v()) < ctab.getAverageBondLength() * 1.2).map(t -> {\n                return Tuple.of(t, GeomUtil.add(GeomUtil.growShape(t.k(), 1), GeomUtil.growShape(t.v(), 1)));\n            }).map(Tuple.vmap(s -> Tuple.of(s, GeomUtil.area(s)).withVComparator())).map(t -> t.withVComparator()).sorted().map(Tuple.vmap(st -> st.k())).map(t -> {\n                Shape cshape = t.v();\n                Line2D makeLine = new Line2D.Double(GeomUtil.findCenterOfShape(t.k().k()), GeomUtil.findCenterOfShape(t.k().v()));\n                List<Line2D> opl = lj.stream().filter(l1 -> cshape.contains(l1.v())).map(t1 -> t1.k()).filter(l -> GeomUtil.cosTheta(l, makeLine) > Math.cos(25 * Math.PI / 180)).collect(Collectors.toList());\n                return Tuple.of(t.k(), opl);\n            });\n        }).forEach(lst -> {\n            List<Node> nodes = Stream.of(lst.k().k(), lst.k().v()).map(s -> ctab.getNodesInsideShape(s, 2)).flatMap(nds -> nds.stream()).distinct().collect(Collectors.toList());\n            if (nodes.size() == 2) {\n                Node n1 = nodes.get(0);\n                Node n2 = nodes.get(1);\n                Edge alreadyEdge = ctab.getEdgeBetweenNodes(n1, n2).orElse(null);\n                boolean haspossibleLine = !lst.v().isEmpty();\n                boolean already = (alreadyEdge != null);\n                if (!already && haspossibleLine) {\n                    edgesToMake.add(Tuple.of(lst.v(), Tuple.of(nodes.get(0), nodes.get(1))));\n                } else if (!already && !haspossibleLine) {\n                } else if (already && !haspossibleLine) {\n                    ctab.removeEdge(alreadyEdge);\n                } else {\n                    taken.addAll(lst.v());\n                    int order = GeomUtil.groupThings(lst.v(), tlines -> {\n                        Line2D l1 = tlines.k();\n                        Line2D l2 = tlines.v();\n                        if (GeomUtil.cosTheta(l1, l2) > Math.cos(10 * Math.PI / 180)) {\n                            return true;\n                        }\n                        return false;\n                    }).stream().map(l -> GeomUtil.getLineOffsetsToLongestLine(l)).map(l -> {\n                        OptionalDouble opdoff = l.stream().filter(t -> Math.abs(t.v()) > ctab.getAverageBondLength() * 0.1).mapToDouble(t -> t.v()).min();\n                        if (!opdoff.isPresent()) {\n                            return l.stream().map(l1 -> l1.k()).limit(1).collect(Collectors.toList());\n                        }\n                        double doff = opdoff.getAsDouble();\n                        List<Line2D> nlines = l.stream().map(Tuple.vmap(d -> (int) Math.round(d / doff))).map(t -> t.swap()).collect(Tuple.toGroupedMap()).entrySet().stream().map(Tuple::of).map(Tuple.vmap(v1 -> GeomUtil.getPairOfFarthestPoints(GeomUtil.vertices(v1)))).map(Tuple.vmap(v1 -> new Line2D.Double(v1[0], v1[1]))).map(t -> t.v()).collect(Collectors.toList());\n                        return nlines;\n                    }).mapToInt(ll -> ll.size()).max().getAsInt();\n                    if (order > alreadyEdge.getOrder()) {\n                        alreadyEdge.setOrder(order);\n                    }\n                }\n            }\n        });\n        edgesToMake.stream().filter(t -> {\n            return taken.addAll(t.k());\n        }).forEach(t -> {\n            List<Edge> crossingEdges = ctab.getBondsThatCross(t.v().k(), t.v().v());\n            if (!crossingEdges.isEmpty())\n                return;\n            Line2D nline = new Line2D.Double(t.v().k().getPoint(), t.v().v().getPoint());\n            List<Line2D> k = t.k().stream().filter(l -> GeomUtil.cosTheta(nline, l) > Math.cos(45 * Math.PI / 180)).collect(Collectors.toList());\n            if (k.isEmpty())\n                return;\n            int order = GeomUtil.groupThings(k, tlines -> {\n                Line2D l1 = tlines.k();\n                Line2D l2 = tlines.v();\n                if (GeomUtil.cosTheta(l1, l2) > Math.cos(10 * Math.PI / 180)) {\n                    return true;\n                }\n                return false;\n            }).stream().map(l -> GeomUtil.getLineOffsetsToLongestLine(l)).map(l -> {\n                OptionalDouble opdoff = l.stream().filter(tb -> Math.abs(tb.v()) > ctab.getAverageBondLength() * 0.1).mapToDouble(tb -> tb.v()).min();\n                if (!opdoff.isPresent()) {\n                    return l.stream().map(l1 -> l1.k()).limit(1).collect(Collectors.toList());\n                }\n                double doff = opdoff.getAsDouble();\n                List<Line2D> nlines = l.stream().map(Tuple.vmap(d -> (int) Math.round(d / doff))).map(tb -> tb.swap()).collect(Tuple.toGroupedMap()).entrySet().stream().map(Tuple::of).map(Tuple.vmap(v1 -> GeomUtil.getPairOfFarthestPoints(GeomUtil.vertices(v1)))).map(Tuple.vmap(v1 -> new Line2D.Double(v1[0], v1[1]))).map(tb -> tb.v()).collect(Collectors.toList());\n                return nlines;\n            }).mapToInt(ll -> ll.size()).max().getAsInt();\n            ctab.addEdge(t.v().k().getIndex(), t.v().v().getIndex(), order);\n        });\n        ctab.standardCleanEdges();\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        ctab.makeMissingBondsToNeighbors(bitmap, MAX_BOND_TO_AVG_BOND_RATIO_FOR_NOVEL, MAX_TOLERANCE_FOR_SINGLE_BONDS, likelyOCR, OCR_TO_BOND_MAX_DISTANCE, (t) -> {\n            if (t.k() > MAX_TOLERANCE_FOR_SINGLE_BONDS) {\n                t.v().setDashed(true);\n            }\n        });\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        {\n            double avgL = ctab.getAverageBondLength();\n            Set<Edge> skip = new HashSet<Edge>();\n            ctab.getEdges().stream().map(l -> Tuple.of(l, LineWrapper.of(l.getLine())).withVComparator()).filter(e -> e.v().length() > avgL).sorted(Comparator.reverseOrder()).map(t -> t.k()).filter(t -> !skip.contains(t)).forEach(e -> {\n                Node n1 = e.getRealNode1();\n                Node n2 = e.getRealNode2();\n                List<KEqualityTuple<Node, Edge>> neigh1 = n1.getNeighborNodes();\n                List<KEqualityTuple<Node, Edge>> neigh2 = n2.getNeighborNodes();\n                List<KEqualityTuple<Node, Edge>> things1 = neigh1.stream().filter(ne -> neigh2.contains(ne)).collect(Collectors.toList());\n                List<KEqualityTuple<Node, Edge>> things2 = neigh2.stream().filter(ne -> neigh1.contains(ne)).collect(Collectors.toList());\n                List<KEqualityTuple<Node, Edge>> things = Stream.concat(things1.stream(), things2.stream()).collect(Collectors.toList());\n                if (things.size() > 0) {\n                    Point2D p1 = n1.getPoint();\n                    Point2D p2 = n2.getPoint();\n                    Node n3 = things.get(0).k();\n                    Point2D p3 = n3.getPoint();\n                    if (n3.getEdgeCount() == 4 && intersectionNodes.stream().filter(in -> in.distance(p3) < 2).findAny().isPresent()) {\n                        things.stream().map(t -> t.v()).forEach(e2 -> {\n                            skip.add(e2);\n                        });\n                        return;\n                    }\n                    if (n1.getEdgeCount() == 4 && intersectionNodes.stream().filter(in -> in.distance(p1) < 2).findAny().isPresent()) {\n                        things.stream().map(t -> t.v()).forEach(e2 -> {\n                            skip.add(e2);\n                        });\n                        return;\n                    }\n                    if (n2.getEdgeCount() == 4 && intersectionNodes.stream().filter(in -> in.distance(p2) < 2).findAny().isPresent()) {\n                        things.stream().map(t -> t.v()).forEach(e2 -> {\n                            skip.add(e2);\n                        });\n                        return;\n                    }\n                    Edge oedge1 = things.get(0).v();\n                    Edge oedge2 = things.get(1).v();\n                    double tarea = Math.abs(GeomUtil.areaTriangle(p1, p2, p3));\n                    double expected = Math.sqrt(3) / 4 * Math.pow(e.getEdgeLength(), 2);\n                    if (tarea < expected * 0.5) {\n                        boolean removeLong = true;\n                        if ((e.getEdgeLength() < avgL * 1.8) && (!e.getDashed() && (oedge1.getDashed() && oedge2.getDashed()) || (oedge1.getDashed() || oedge2.getDashed() && e.getOrder() > 1))) {\n                            removeLong = false;\n                        } else {\n                            if (oedge1.getEdgeLength() < avgL * 0.7 && oedge2.getEdgeLength() < avgL * 0.7) {\n                                removeLong = false;\n                            } else {\n                                removeLong = true;\n                            }\n                        }\n                        if (removeLong) {\n                            ctab.removeEdge(e);\n                        } else {\n                            ctab.removeEdge(oedge1);\n                            ctab.removeEdge(oedge2);\n                        }\n                    } else {\n                        things.stream().map(t -> t.v()).forEach(e2 -> {\n                            skip.add(e2);\n                        });\n                    }\n                }\n            });\n        }\n        ctab.removeOrphanNodes();\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        {\n            ctab.getDashLikeScoreForAllEdges(bitmap, likelyOCR).forEach(t -> {\n                if (t.v() < MIN_ST_DEV_FOR_KEEPING_DASHED_LINES && t.k().getDashed()) {\n                    t.k().setDashed(false);\n                    double tol = ctab.getToleranceForEdge(t.k(), bitmap, likelyOCR);\n                    if (tol > MAX_TOLERANCE_FOR_DASH_BONDS) {\n                        ctab.removeEdge(t.k());\n                    }\n                }\n            });\n        }\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        double fbondlength = ctab.getAverageBondLength();\n        ctab.getEdges().stream().filter(e -> e.getOrder() == 3).collect(Collectors.toList()).forEach(e -> {\n            LineWrapper lb = LineWrapper.of(e.getLine());\n            double len = e.getEdgeLength();\n            int c = ctab.getEdges().size();\n            double otherBondAverage = (c * ctab.getAverageBondLength() - len) / (c - 1);\n            int n = (int) Math.round(len / otherBondAverage);\n            if (n > 1) {\n                Node n1 = e.getRealNode1();\n                Node n2 = e.getRealNode2();\n                Shape bigLineShape = lb.growLine(len / 3);\n                Point2D apt = rejBondOrderLines.stream().filter(l -> l.length() > ctab.getAverageBondLength() * 0.5).filter(p -> bigLineShape.contains(p.centerPoint())).map(p -> lb.projectPointOntoLine(p.centerPoint())).collect(GeomUtil.averagePoint());\n                List<Point2D> pts = lb.splitIntoNPieces(n);\n                Node pnode = n1;\n                Edge closestEdge = null;\n                double closestD = 999999;\n                for (int i = 1; i < pts.size(); i++) {\n                    Node nn = null;\n                    if (i < pts.size() - 1) {\n                        Point2D np = pts.get(i);\n                        nn = ctab.addNode(np);\n                    } else {\n                        nn = n2;\n                    }\n                    Edge ne = ctab.addEdge(pnode.getIndex(), nn.getIndex(), 1);\n                    Point2D cpt = Stream.of(ne.getPoint1(), ne.getPoint2()).collect(GeomUtil.averagePoint());\n                    double dpt = cpt.distance(apt);\n                    if (dpt < closestD) {\n                        closestD = dpt;\n                        closestEdge = ne;\n                    }\n                    pnode = nn;\n                }\n                if (closestEdge != null) {\n                    closestEdge.setOrder(3);\n                    ctab.removeEdge(e);\n                }\n            }\n        });\n        List<Shape> appliedOCR = new ArrayList<Shape>();\n        for (Shape s : bestGuessOCR.keySet()) {\n            String sym = bestGuessOCR.get(s);\n            BranchNode actual = BranchNode.interpretOCRStringAsAtom2(sym);\n            if (actual != null && actual.isRealNode()) {\n                if (!actual.isLinkable()) {\n                    for (Node n : ctab.getAllNodesInsideShape(s, 0.1)) {\n                        ctab.removeNodeAndEdges(n);\n                    }\n                    continue;\n                }\n                appliedOCR.add(s);\n                List<Node> nlist = ctab.getNodesInsideShape(s, 0.1).stream().filter(n -> !n.isInvented()).collect(Collectors.toList());\n                if (actual.getSymbol().equals(\"I\") && !nlist.isEmpty() && nlist.get(0).getEdgeCount() > 1) {\n                    continue;\n                }\n                if (nlist.size() > 1) {\n                    Point2D np = nlist.get(0).getPoint();\n                    ctab.mergeAllNodesInside(s, 0.1, (n) -> !n.isInvented(), (l) -> np);\n                    ctab.standardCleanEdges();\n                    nlist = ctab.getNodesInsideShape(s, 0.1).stream().filter(n -> !n.isInvented()).collect(Collectors.toList());\n                }\n                nlist.forEach(n -> {\n                    n.setSymbol(actual.getSymbol());\n                });\n                if (nlist.size() == 1) {\n                    Node pnode = nlist.get(0);\n                    pnode.setCharge(actual.getCharge());\n                    Point2D ppoint = pnode.getPoint();\n                    Node lneigh = null;\n                    Node rneigh = null;\n                    if (pnode.getEdgeCount() >= 2) {\n                        lneigh = pnode.getNeighborNodes().stream().map(n -> n.k()).map(n -> Tuple.of(n, n.getPoint().getX())).map(t -> t.withVComparator()).min(Comparator.naturalOrder()).map(t -> t.k()).orElse(null);\n                        rneigh = pnode.getNeighborNodes().stream().map(n -> n.k()).map(n -> Tuple.of(n, n.getPoint().getX())).map(t -> t.withVComparator()).max(Comparator.naturalOrder()).map(t -> t.k()).orElse(null);\n                    }\n                    if (actual.hasChildren()) {\n                        actual.generateCoordinates();\n                        AffineTransform at = new AffineTransform();\n                        at.translate(ppoint.getX(), ppoint.getY());\n                        at.scale(fbondlength, fbondlength);\n                        if (pnode.getEdges().size() > 0) {\n                            Point2D otherPoint = pnode.getEdges().stream().map(e -> e.getOtherNode(pnode)).map(n -> n.getPoint()).collect(GeomUtil.averagePoint());\n                            double ang = GeomUtil.angle(ppoint, otherPoint);\n                            at.rotate(ang + Math.PI);\n                        }\n                        actual.applyTransform(at);\n                        Map<BranchNode, Node> parentNodes = new HashMap<BranchNode, Node>();\n                        parentNodes.put(actual, pnode);\n                        actual.forEachBranchNode((parN, curN) -> {\n                            if (parN == null)\n                                return;\n                            Node mpnode = pnode;\n                            if (parN != null) {\n                                mpnode = parentNodes.get(parN);\n                            }\n                            Node n = ctab.addNode(curN.getSuggestedPoint()).setSymbol(curN.getSymbol()).setCharge(curN.getCharge()).setInvented(true);\n                            Edge e = ctab.addEdge(mpnode.getIndex(), n.getIndex(), curN.getOrderToParent());\n                            if (curN.getWedgeType() == 1) {\n                                e.setWedge(true);\n                            } else if (curN.getWedgeType() == -1) {\n                                e.setDashed(true);\n                            }\n                            curN.getRingBond().ifPresent(t -> {\n                                Node rn = parentNodes.get(t.k());\n                                ctab.addEdge(rn.getIndex(), n.getIndex(), t.v());\n                            });\n                            parentNodes.put(curN, n);\n                        });\n                        if (actual.canBeChain()) {\n                            if (lneigh != null && rneigh != null) {\n                                Node l = lneigh;\n                                Node r = rneigh;\n                                Node nl = parentNodes.get(actual.getLeftBranchNode());\n                                Node nr = parentNodes.get(actual.getRightBranchNode());\n                                pnode.getNeighborNodes().stream().filter(t -> t.k() == l || t.k() == r).collect(Collectors.toList()).forEach(t -> {\n                                    Edge e = t.v();\n                                    Node on = t.k();\n                                    Node nn = (on == l) ? nr : nl;\n                                    ctab.removeEdge(e);\n                                    ctab.addEdge(on.getIndex(), nn.getIndex(), e.getOrder());\n                                });\n                                Line2D oldLine = new Line2D.Double(nl.getPoint(), nr.getPoint());\n                                Point2D[] far = GeomUtil.getPairOfFarthestPoints(s);\n                                double minx = Math.min(far[0].getX(), far[1].getX());\n                                double maxx = Math.max(far[0].getX(), far[1].getX());\n                                Line2D newLine = new Line2D.Double(maxx - averageWidthOCR / 2, pnode.getPoint().getY(), minx + averageWidthOCR / 2, pnode.getPoint().getY());\n                                Point2D navg = Stream.of(l.getPoint(), r.getPoint()).collect(GeomUtil.averagePoint());\n                                boolean flip = false;\n                                if (navg.getY() > newLine.getY1()) {\n                                    flip = true;\n                                }\n                                AffineTransform att = GeomUtil.getTransformFromLineToLine(oldLine, newLine, flip);\n                                parentNodes.values().stream().forEach(pn -> {\n                                    pn.setPoint(att.transform(pn.getPoint(), null));\n                                });\n                            }\n                        }\n                    }\n                }\n            }\n        }\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        if (true) {\n            ConnectionTable biggestSection = ctab.getDisconnectedComponents().stream().map(ct -> Tuple.of(ct, GeomUtil.area(ct.getConvexHull())).withVComparator()).max(Comparator.naturalOrder()).map(t -> t.k()).orElse(null);\n            if (biggestSection != null) {\n                if (biggestSection.getAverageBondLength() < 20) {\n                    bitmap = new Bitmap.BitmapBuilder(bitmap).scale(2).gaussBlur(1).threshold(6).build();\n                    load(bitmap);\n                    return;\n                }\n            }\n        }\n        if (ctab.getDisconnectedComponents().size() > 1) {\n            Tuple<ConnectionTable, Shape> ctshape = ctab.getDisconnectedComponents().stream().map(ct -> Tuple.of(ct, ct.getConvexHull())).map(Tuple.vmap(h -> Tuple.of(h, -GeomUtil.area(h)).withVComparator())).map(t -> t.withVComparator()).sorted().limit(1).map(Tuple.vmap(t -> t.k())).findFirst().orElse(null);\n            if (ctshape != null) {\n                double bestbond = ctshape.k().getAverageBondLength();\n                List<ConnectionTable> ctabs = ctab.getDisconnectedComponents();\n                Shape crop = ctabs.stream().filter(ct -> ct.getAverageBondLength() < 0.7 * bestbond || ct.getAverageBondLength() > 1.33 * bestbond).map(ct -> ct.getAreaAround(bestbond)).collect(GeomUtil.union()).orElse(null);\n                Shape keep = ctabs.stream().filter(ct -> ct.getAverageBondLength() >= 0.7 * bestbond && ct.getAverageBondLength() <= 1.33 * bestbond).map(ct -> ct.getAreaAround(bestbond)).collect(GeomUtil.union()).orElse(null);\n                if (crop != null && keep != null) {\n                    polygons.stream().filter(s -> !likelyOCR.contains(s)).map(s -> Tuple.of(s, GeomUtil.findCenterOfShape(s))).filter(t -> !keep.contains(t.v())).filter(t -> crop.contains(t.v())).map(t -> t.k()).forEach(p -> {\n                        likelyOCR.remove(p);\n                        likelyOCRNumbers.remove(p);\n                        likelyOCRNonBond.remove(p);\n                        likelyOCRAll.remove(p);\n                        likelyOCRIgnore.add(GeomUtil.growShapeNPoly(p, 2, 16));\n                        foundNewOCR[0] = true;\n                    });\n                }\n            }\n        }\n        if (foundNewOCR[0] && repeats < MAX_OCR_FULL_REPEATS) {\n            System.out.println(\"Starting over...\");\n            continue;\n        }\n        Set<Node> toRemove = new HashSet<Node>();\n        do {\n            toRemove.clear();\n            ctab.getNodesNotInShapes(appliedOCR, 0).stream().map(n -> Tuple.of(n, n.getNeighborNodes())).filter(t -> t.v().size() == 2).filter(t -> t.v().get(0).v().getEdgeLength() < ctab.getAverageBondLength()).filter(t -> t.v().get(1).v().getEdgeLength() < ctab.getAverageBondLength()).filter(t -> t.v().get(0).v().getOrder() == 1).filter(t -> t.v().get(1).v().getOrder() == 1).collect(Collectors.toList()).forEach(t -> {\n                Node n1 = t.k();\n                Node n2 = t.v().get(0).k();\n                Node n3 = t.v().get(1).k();\n                if (toRemove.contains(n2) || toRemove.contains(n3))\n                    return;\n                double d1 = n1.distanceTo(n2) + n1.distanceTo(n3);\n                double d2 = n2.distanceTo(n3);\n                if (d2 / d1 > .95) {\n                    Line2D longLine = new Line2D.Double(n2.getPoint(), n3.getPoint());\n                    Point2D np = GeomUtil.projectPointOntoLine(longLine, n1.getPoint());\n                    if (np.distance(n1.getPoint()) < 3) {\n                        ctab.addEdge(n2.getIndex(), n3.getIndex(), 1);\n                        toRemove.add(n1);\n                    }\n                }\n            });\n            toRemove.forEach(n -> ctab.removeNodeAndEdges(n));\n            ctab.standardCleanEdges();\n        } while (!toRemove.isEmpty());\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        ctab.getNodes().stream().filter(n -> n.getEdgeCount() >= 2).map(n -> {\n            return Tuple.of(n, GeomUtil.eachCombination(n.getEdges()).map(t -> {\n                if (t.k().getEdgeLength() > t.v().getEdgeLength()) {\n                    t = t.swap();\n                }\n                return t;\n            }).filter(t -> t.v().getEdgeLength() >= ctab.getAverageBondLength()).collect(Collectors.toList()));\n        }).filter(ed -> !ed.v().isEmpty()).collect(Collectors.toList()).forEach(te -> {\n            Node n = te.k();\n            te.v().forEach(t -> {\n                Node tnode = t.k().getOtherNode(n);\n                Node otherNode = t.v().getOtherNode(n);\n                Point2D ppnt = GeomUtil.projectPointOntoLine(t.v().getLine(), tnode.getPoint());\n                if (ppnt.distance(tnode.getPoint()) < 0.1 * ctab.getAverageBondLength()) {\n                    double sd1 = ppnt.distance(otherNode.getPoint());\n                    if (sd1 < t.v().getEdgeLength()) {\n                        tnode.setPoint(ppnt);\n                        ctab.addEdge(tnode.getIndex(), t.v().getOtherNode(n).getIndex(), t.v().getOrder());\n                        ctab.removeEdge(t.v());\n                    }\n                }\n            });\n        });\n        ctab.standardCleanEdges();\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        ctab.getEdges().stream().filter(e -> e.getEdgeLength() < ctab.getAverageBondLength() * 0.55).collect(Collectors.toList()).forEach(e -> {\n            Node n1 = e.getRealNode1();\n            Node n2 = e.getRealNode2();\n            boolean wasIntersection = intersectionNodes.stream().filter(p -> p.distance(n1.getPoint()) < ctab.getAverageBondLength() * 0.2 || p.distance(n2.getPoint()) < ctab.getAverageBondLength() * 0.2).findAny().isPresent();\n            if (wasIntersection) {\n                return;\n            }\n            List<Node> neigh1 = n1.getEdges().stream().filter(e1 -> e1 != e).map(e1 -> e1.getOtherNode(n1)).collect(Collectors.toList());\n            List<Node> neigh2 = n2.getEdges().stream().filter(e1 -> e1 != e).map(e1 -> e1.getOtherNode(n2)).collect(Collectors.toList());\n            if (neigh1.isEmpty() || neigh2.isEmpty())\n                return;\n            double avgBondLengthIfN1Merge = neigh2.stream().mapToDouble(nn -> nn.distanceTo(n1)).average().orElse(0);\n            double avgBondLengthIfN2Merge = neigh1.stream().mapToDouble(nn -> nn.distanceTo(n2)).average().orElse(0);\n            double maxBondLengthIfN1Merge = neigh2.stream().mapToDouble(nn -> nn.distanceTo(n1)).max().orElse(0);\n            double maxBondLengthIfN2Merge = neigh1.stream().mapToDouble(nn -> nn.distanceTo(n2)).max().orElse(0);\n            double minBondLengthIfN1Merge = neigh2.stream().mapToDouble(nn -> nn.distanceTo(n1)).min().orElse(0);\n            double minBondLengthIfN2Merge = neigh1.stream().mapToDouble(nn -> nn.distanceTo(n2)).min().orElse(0);\n            Predicate<Double> isReasonable = d -> {\n                return d < ctab.getAverageBondLength() * 1.3 && d > ctab.getAverageBondLength() * 0.7;\n            };\n            boolean n1Merge = false;\n            boolean n2Merge = false;\n            if (isReasonable.test(avgBondLengthIfN1Merge) && isReasonable.test(maxBondLengthIfN1Merge) && isReasonable.test(minBondLengthIfN1Merge)) {\n                n1Merge = true;\n            }\n            if (isReasonable.test(avgBondLengthIfN2Merge) && isReasonable.test(maxBondLengthIfN2Merge) && isReasonable.test(minBondLengthIfN2Merge)) {\n                n2Merge = true;\n            }\n            if (!n1Merge && !n2Merge)\n                return;\n            if (n1Merge && !n2Merge) {\n                Point2D mp = n1.getPoint();\n                ctab.mergeNodes(Stream.of(n1.getIndex(), n2.getIndex()).collect(Collectors.toList()), (l) -> mp);\n            } else if (n1Merge && !n2Merge) {\n                Point2D mp = n2.getPoint();\n                ctab.mergeNodes(Stream.of(n1.getIndex(), n2.getIndex()).collect(Collectors.toList()), (l) -> mp);\n            } else {\n                ctab.mergeNodesAverage(n1.getIndex(), n2.getIndex());\n            }\n        });\n        ctab.standardCleanEdges();\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        List<Node> toRemoveNodesCage = new ArrayList<>();\n        ctab.getNodes().stream().filter(n -> n.getEdgeCount() == 4).filter(n -> n.getSymbol().equals(\"C\")).filter(n -> !n.isInvented()).filter(n -> intersectionNodes.stream().filter(p -> p.distance(n.getPoint()) < ctab.getAverageBondLength() * 0.1).findAny().isPresent()).forEach(n -> {\n            if (n.isInRing(7)) {\n                List<Node> neigh = n.getNeighborNodes().stream().map(t -> t.k()).collect(Collectors.toList());\n                if (neigh.stream().filter(nn -> toRemoveNodesCage.contains(nn)).findAny().isPresent()) {\n                    return;\n                }\n                List<List<Node>> pairs = GeomUtil.groupThings(neigh, (t1) -> {\n                    Line2D nline = new Line2D.Double(t1.k().getPoint(), t1.v().getPoint());\n                    Point2D pp = GeomUtil.projectPointOntoLine(nline, n.getPoint());\n                    if (pp.distance(n.getPoint()) < ctab.getAverageBondLength() * 0.02) {\n                        return true;\n                    }\n                    return false;\n                });\n                if (pairs.size() == 2) {\n                    List<Node> npair1 = pairs.get(0);\n                    List<Node> npair2 = pairs.get(1);\n                    if (npair1.size() == 2 && npair2.size() == 2) {\n                        Line2D l1p = new Line2D.Double(npair1.get(0).getPoint(), npair1.get(1).getPoint());\n                        Line2D l2p = new Line2D.Double(npair2.get(0).getPoint(), npair2.get(1).getPoint());\n                        Shape l1s = GeomUtil.growLine(l1p, ctab.getAverageBondLength() * 0.2);\n                        Shape l2s = GeomUtil.growLine(l2p, ctab.getAverageBondLength() * 0.2);\n                        boolean l1Has = lj.stream().filter(t -> l1s.contains(t.v())).filter(t -> GeomUtil.cosTheta(t.k(), l1p) > 0.6).map(l -> l.k()).map(l -> GeomUtil.growLine(l, ctab.getAverageBondLength() * 0.2)).filter(ls -> ls.contains(n.getPoint())).findFirst().isPresent();\n                        boolean l2Has = lj.stream().filter(t -> l2s.contains(t.v())).filter(t -> GeomUtil.cosTheta(t.k(), l2p) > 0.6).map(l -> l.k()).map(l -> GeomUtil.growLine(l, ctab.getAverageBondLength() * 0.2)).filter(ls -> ls.contains(n.getPoint())).findFirst().isPresent();\n                        boolean doit = false;\n                        if ((l1Has && !l2Has) || (l2Has && !l1Has)) {\n                            doit = true;\n                        } else {\n                            double averageOtherBondLength = ctab.getEdges().stream().filter(e2 -> !e2.hasNode(n)).mapToDouble(e2 -> e2.getEdgeLength()).average().orElse(ctab.getAverageBondLength());\n                            double averageCurrentBondLength = n.getEdges().stream().mapToDouble(e -> e.getEdgeLength()).average().orElse(0);\n                            double averageNewBondLength = 0.5 * (npair1.get(0).distanceTo(npair1.get(1)) + npair2.get(0).distanceTo(npair2.get(1)));\n                            if (Math.abs(averageNewBondLength - averageOtherBondLength) < Math.abs(averageCurrentBondLength - averageOtherBondLength)) {\n                                doit = true;\n                            }\n                        }\n                        if (doit) {\n                            ctab.addEdge(npair1.get(0).getIndex(), npair1.get(1).getIndex(), 1);\n                            ctab.addEdge(npair2.get(0).getIndex(), npair2.get(1).getIndex(), 1);\n                            toRemoveNodesCage.add(n);\n                        }\n                    }\n                }\n            }\n        });\n        for (Node r : toRemoveNodesCage) {\n            ctab.removeNodeAndEdges(r);\n        }\n        toRemoveNodesCage.clear();\n        ctab.getNodes().stream().filter(n -> n.getEdgeCount() == 2).filter(n -> n.getSymbol().equals(\"C\")).filter(n -> !n.isInvented()).filter(n -> n.getEdges().stream().filter(e -> e.getOrder() == 1).count() == 2).filter(n -> Optional.ofNullable(GeomUtil.findClosestShapeTo(likelyOCR, n.getPoint())).map(t -> t.v()).orElse(100.0) > ctab.getAverageBondLength() * 0.2).forEach(n -> {\n            List<Tuple<Node, Node>> tn = GeomUtil.eachCombination(n.getNeighborNodes()).filter(t -> {\n                Node n1 = t.k().k();\n                Node n2 = t.v().k();\n                Line2D l = new Line2D.Double(n1.getPoint(), n2.getPoint());\n                Point2D pp = GeomUtil.projectPointOntoLine(l, n.getPoint());\n                if (pp.distance(n.getPoint()) < ctab.getAverageBondLength() * 0.1) {\n                    return true;\n                }\n                return false;\n            }).map(t -> Tuple.of(t.k().k(), t.v().k())).collect(Collectors.toList());\n            if (tn.size() == 1) {\n                toRemoveNodesCage.add(n);\n                ctab.addEdge(tn.get(0).k().getIndex(), tn.get(0).v().getIndex(), 1);\n            }\n        });\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        toRemoveNodesCage.forEach(n -> {\n            ctab.removeNodeAndEdges(n);\n        });\n        ctab.standardCleanEdges();\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        ctab.removeOrphanNodes();\n        List<Shape> singleBondInfluenceAreas = ctab.getEdges().stream().filter(e -> e.getOrder() == 1).filter(e -> !e.isInventedBond()).map(e -> Tuple.of(e, GeomUtil.growLine(e.getLine(), e.getEdgeLength() / 5))).map(t -> t.v()).collect(Collectors.toList());\n        Set<Edge> wasDouble = new HashSet<Edge>();\n        ctab.getEdges().stream().filter(e -> e.getOrder() == 2).filter(e -> !e.isInventedBond()).filter(e -> e.getRealNode1().getSymbol().equals(\"C\") && e.getRealNode2().getSymbol().equals(\"C\")).peek(e -> wasDouble.add(e)).collect(Collectors.toList()).forEach(e -> {\n            Line2D lb = e.getLine();\n            Point2D apnt = Stream.of(lb.getP1(), lb.getP2()).collect(GeomUtil.averagePoint());\n            List<Point2D> possibleOtherDoubleBonds = rejBondOrderLines.stream().filter(l -> l.length() > ctab.getAverageBondLength() * 0.4).map(l -> l.centerPoint()).filter(p -> p.distance(apnt) < ctab.getAverageBondLength() * 0.8).collect(Collectors.toList());\n            if (possibleOtherDoubleBonds.isEmpty()) {\n                e.getNeighborEdges().stream().filter(e2 -> GeomUtil.cosTheta(e.getLine(), e2.getLine()) > Math.cos(2 * Math.PI / 180)).filter(e2 -> wasDouble.contains(e2)).findAny().ifPresent(eo -> {\n                    e.setOrder(1);\n                });\n                return;\n            }\n            boolean couldBeAnother = possibleOtherDoubleBonds.stream().flatMap(p -> singleBondInfluenceAreas.stream().filter(s -> s.contains(p))).findAny().isPresent();\n            if (couldBeAnother) {\n                e.setOrder(1);\n            }\n        });\n        ctab.getEdges().stream().filter(e -> e.getOrder() == 3).filter(e -> !e.isInventedBond()).filter(e -> e.getRealNode1().getSymbol().equals(\"C\") && e.getRealNode2().getSymbol().equals(\"C\")).collect(Collectors.toList()).forEach(e -> {\n            LineWrapper lb = LineWrapper.of(e.getLine());\n            Point2D apnt = lb.centerPoint();\n            List<Tuple<Line2D, Point2D>> possibleOtherDoubleBonds = rejBondOrderLines.stream().filter(l -> l.length() > ctab.getAverageBondLength() * 0.4).filter(l -> l.cosTheta(lb) > 0.8).map(l -> Tuple.of(l, l.centerPoint())).filter(p -> p.v().distance(apnt) < ctab.getAverageBondLength() * 0.8).map(Tuple.kmap(l -> l.getLine())).collect(Collectors.toList());\n            if (possibleOtherDoubleBonds.size() == 1) {\n                e.setOrder(2);\n                return;\n            }\n            List<Tuple<Edge, Shape>> edgeShapes = ctab.getEdges().stream().filter(e1 -> e1 != e).map(e1 -> Tuple.of(e1, GeomUtil.growLine(e1.getLine(), ctab.getAverageBondLength() * 0.5))).collect(Collectors.toList());\n            List<Tuple<Line2D, Edge>> bestEdge = possibleOtherDoubleBonds.stream().map(t -> Tuple.of(t.k(), edgeShapes.stream().filter(es -> es.v().contains(t.v())).filter(es -> GeomUtil.cosTheta(t.k(), es.k().getLine()) > 0.8).map(ee -> ee.k()).findAny().orElse(null))).collect(Collectors.toList());\n            long c = bestEdge.stream().filter(te -> {\n                if (te.v() != null) {\n                    if (te.v().getOrder() == 1) {\n                        te.v().setOrder(2);\n                    }\n                    return true;\n                }\n                return false;\n            }).count();\n            if (c == 1) {\n                e.setOrder(2);\n            } else if (c == 2) {\n                e.setOrder(1);\n            }\n        });\n        ctab.getNodes().stream().filter(n -> n.getSymbol().equals(\"C\")).filter(n -> n.getValanceTotal() >= 5).forEach(n -> {\n            if (n.getEdgeCount() <= 4) {\n                int toomuch = n.getValanceTotal() - 4;\n                if (toomuch == 1 && n.getEdgeCount() == 4) {\n                    Optional<Edge> ope = n.getEdges().stream().filter(e -> e.getOrder() == 1).filter(e -> e.getDashed()).findFirst();\n                    if (ope.isPresent()) {\n                        ope.ifPresent(e -> {\n                            ctab.removeEdge(e);\n                        });\n                    } else {\n                        n.getEdges().stream().filter(e -> e.getOrder() > 1).forEach(e -> e.setOrder(1));\n                    }\n                } else if (toomuch == 1 && n.getEdgeCount() == 3) {\n                    Optional<Edge> opEdge = n.getEdges().stream().filter(e -> e.getOrder() > 2).findFirst();\n                    if (opEdge.isPresent()) {\n                        opEdge.ifPresent(e -> {\n                            e.setOrder(2);\n                        });\n                    } else {\n                        n.getEdges().stream().filter(e -> e.getOrder() == 2).map(e -> Tuple.of(e, GeomUtil.growLine(e.getLine(), ctab.getAverageBondLength() / 3.0))).map(t -> Tuple.of(t.k(), rejBondOrderLines.stream().map(l -> Tuple.of(l, l.centerPoint())).filter(p -> t.v().contains(p.v())).mapToDouble(t1 -> t1.k().length()).findAny())).map(Tuple.vmap(v -> v.orElse(0.0))).map(t -> t.withVComparator()).min(Comparator.naturalOrder()).map(e -> e.k()).ifPresent(e -> {\n                            e.setOrder(1);\n                        });\n                        ;\n                    }\n                }\n            }\n        });\n        List<Shape> dashShapes = new ArrayList<Shape>();\n        List<List<Node>> toMergeNodes = new ArrayList<>();\n        maybeDashCollection.forEach(bshape -> {\n            dashShapes.add(GeomUtil.growShapeNPoly(bshape, 2, 12));\n            Point2D[] pts = GeomUtil.getPairOfFarthestPoints(bshape);\n            List<Node> forN1 = ctab.getNodes().stream().filter(n -> n.getPoint().distance(pts[0]) < ctab.getAverageBondLength() * 0.6).collect(Collectors.toList());\n            List<Node> forN2 = ctab.getNodes().stream().filter(n -> n.getPoint().distance(pts[1]) < ctab.getAverageBondLength() * 0.6).filter(n -> !forN1.contains(n)).collect(Collectors.toList());\n            if (forN1.size() + forN2.size() == 0)\n                return;\n            LineWrapper splitLine = GeomUtil.findLongestSplittingLine(bshape);\n            if (forN1.size() + forN2.size() > 1) {\n                if (forN1.size() > 1) {\n                    List<Node> nadd = forN1.stream().filter(nn -> !forN2.contains(nn)).collect(Collectors.toList());\n                    toMergeNodes.add(nadd);\n                } else if (forN2.size() > 1) {\n                    List<Node> nadd = forN2.stream().filter(nn -> !forN1.contains(nn)).collect(Collectors.toList());\n                    toMergeNodes.add(nadd);\n                } else {\n                    if (forN1.size() == 1 && forN2.size() == 1) {\n                        realRescueOCRCandidates.add(splitLine.getLine());\n                        Node n1 = forN1.get(0);\n                        Node n2 = forN2.get(0);\n                        Point2D np1 = splitLine.projectPointOntoLine(n1.getPoint());\n                        Point2D np2 = splitLine.projectPointOntoLine(n2.getPoint());\n                        n1.setPoint(np1);\n                        n2.setPoint(np2);\n                    }\n                }\n                return;\n            }\n            Node pnode = null;\n            Point2D newPoint = pts[0];\n            if (!forN1.isEmpty()) {\n                newPoint = pts[1];\n                pnode = forN1.get(0);\n            } else {\n                pnode = forN2.get(0);\n            }\n            double ndist = newPoint.distance(pnode.getPoint());\n            if (ndist < ctab.getAverageBondLength() * 1.3 && ndist > ctab.getAverageBondLength() * 0.6) {\n                Point2D cShape = GeomUtil.centerOfMass(bshape);\n                Line2D nline = new Line2D.Double(pnode.getPoint(), cShape);\n                double len = ctab.getAverageBondLength();\n                Point2D op = GeomUtil.resizeLine(nline, len).getP2();\n                Node otherNode = ctab.getNodes().stream().map(n -> Tuple.of(n, n.getPoint().distance(op)).withVComparator()).filter(t -> t.v() < len * 0.3).max(Comparator.reverseOrder()).map(t -> t.k()).orElse(null);\n                if (otherNode == null) {\n                    Node realNode = ctab.addNode(op);\n                    BranchNode bn = bestGuessOCR.entrySet().stream().map(Tuple::of).filter(t -> GeomUtil.growShapeNPoly(t.k(), len * 0.3, 12).contains(op)).map(Tuple.vmap(s1 -> BranchNode.interpretOCRStringAsAtom2(s1))).findFirst().map(t -> t.v()).orElse(null);\n                    if (bn != null && bn.isRealNode()) {\n                        realNode.setSymbol(bn.getSymbol());\n                    }\n                } else {\n                    otherNode.setPoint(op);\n                }\n            }\n        });\n        if (!toMergeNodes.isEmpty()) {\n            toMergeNodes.forEach(nm -> {\n                ctab.mergeNodes(nm.stream().map(n -> n.getIndex()).collect(Collectors.toList()), pl -> pl.stream().collect(GeomUtil.averagePoint()));\n            });\n            ctab.standardCleanEdges();\n        }\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n        ctab.getNodes().stream().filter(n -> !n.isInvented()).collect(GeomUtil.groupThings(t -> {\n            Node n1 = t.k();\n            Node n2 = t.v();\n            if (n1.distanceTo(n2) < ctab.getAverageBondLength() * 0.2) {\n                if (n1.connectsTo(n2)) {\n                    return true;\n                }\n            }\n            return false;\n        })).stream().filter(nl -> nl.size() == 2).forEach(nlist -> {\n            Point2D p1 = nlist.stream().map(n -> n.getPoint()).collect(GeomUtil.averagePoint());\n            ctab.mergeNodes(nlist.stream().map(n -> n.getIndex()).collect(Collectors.toList()), (pl) -> p1);\n            ctab.standardCleanEdges();\n        });\n        @SuppressWarnings(\"unchecked\")\n        List<Tuple<Edge, WedgeInfo>> winfo = (List<Tuple<Edge, WedgeInfo>>) ctab.getEdges().stream().filter(e -> !e.isInventedBond()).map(e -> {\n            LineWrapper useLine = GeomUtil.getLinesNotInside(e.getLine(), growLikelyOCR).stream().map(l -> LineWrapper.of(l)).max(Comparator.naturalOrder()).orElse(null);\n            if (useLine != null) {\n                Point2D c = useLine.centerPoint();\n                Optional<Shape> isDash = dashShapes.stream().filter(d -> d.contains(c)).findFirst();\n                boolean isDotted = dottedLines.stream().map(dl -> dl.growLine(ctab.getAverageBondLength() * .3)).filter(sl -> sl.contains(c)).findAny().isPresent();\n                if (isDotted) {\n                    e.setDashed(true);\n                } else {\n                    if (isDash.isPresent()) {\n                        e.setDashed(true);\n                        if (e.getOrder() != 1) {\n                            if (e.getRealNode1().getSymbol().equals(\"C\") && e.getRealNode2().getSymbol().equals(\"C\")) {\n                            } else {\n                                e.setOrder(1);\n                            }\n                        }\n                        Point2D cmass = GeomUtil.centerOfMass(isDash.get());\n                        if (e.getRealNode1().getPoint().distance(cmass) < e.getRealNode2().getPoint().distance(cmass)) {\n                            e.switchNodes();\n                        }\n                    } else {\n                        if (e.getRealNode1().getSymbol().equals(\"C\") && e.getRealNode2().getSymbol().equals(\"C\")) {\n                            if (e.getDashed() && e.getOrder() == 1) {\n                                double grow = ctab.getAverageBondLength() * 0.1;\n                                Shape lshape = useLine.growLine(grow);\n                                List<Shape> lineShapes = lines.stream().filter(l -> !bestGuessOCR.keySet().stream().filter(ss -> ss.contains(l.centerPoint())).findAny().isPresent()).filter(l -> l.absCosTheta(useLine) > 0.8).filter(l -> lshape.contains(l.centerPoint())).map(l -> l.growLine(grow)).collect(Collectors.toList());\n                                if (lineShapes.size() < 3) {\n                                    e.setDashed(false);\n                                } else {\n                                    double tarea = lineShapes.stream().mapToDouble(s -> GeomUtil.area(s)).sum();\n                                    if (tarea > 0.7 * GeomUtil.area(lshape)) {\n                                        e.setDashed(false);\n                                    }\n                                }\n                            }\n                        }\n                        return bitmap.getconfexHullAlongLine(useLine.getLine()).map(w -> Tuple.of(e, w));\n                    }\n                }\n            }\n            return Optional.empty();\n        }).filter(t -> t.isPresent()).map(o -> o.get()).collect(Collectors.toList());\n        Predicate<Node> couldBeStereoCenter = (n1) -> n1.getEdgeCount() >= 3 && n1.getSymbol().equals(\"C\") && !n1.getEdges().stream().filter(e1 -> e1.getOrder() > 1).findAny().isPresent();\n        double averageThickness = winfo.stream().mapToDouble(t -> t.v().getAverageThickness()).average().orElse(2);\n        Set<Edge> thickEdges = new HashSet<Edge>();\n        Set<Edge> wedgeEdges = new HashSet<Edge>();\n        winfo.forEach(t -> {\n            Edge e = t.k();\n            WedgeInfo s = t.v();\n            double wl = s.getCorrel();\n            if (s.getAverageThickness() > averageThickness * 1.9) {\n                if (s.getOnPixels() > s.getArea() * 0.5) {\n                    double cutoff = WEDGE_LIKE_PEARSON_SCORE_CUTOFF;\n                    if (e.getRealNode1().getEdges().stream().filter(ed -> ed.getOrder() > 1).findAny().isPresent()) {\n                        cutoff = WEDGE_LIKE_PEARSON_SCORE_CUTOFF_DOUBLE;\n                    }\n                    if (e.getRealNode2().getEdges().stream().filter(ed -> ed.getOrder() > 1).findAny().isPresent()) {\n                        cutoff = WEDGE_LIKE_PEARSON_SCORE_CUTOFF_DOUBLE;\n                    }\n                    if (wl > cutoff) {\n                        e.setWedge(true);\n                        wedgeEdges.add(e);\n                    } else if (wl < -cutoff) {\n                        e.setWedge(true);\n                        wedgeEdges.add(e);\n                        e.switchNodes();\n                    } else if (s.getAverageThickness() > averageThickness * 2.2) {\n                        thickEdges.add(e);\n                        e.setWedge(true);\n                    }\n                    if (e.getWedge()) {\n                        if (e.getRealNode1().getEdgeCount() < 3 && e.getRealNode2().getEdgeCount() >= 3) {\n                            e.switchNodes();\n                        }\n                        if (couldBeStereoCenter.test(e.getRealNode2()) && !couldBeStereoCenter.test(e.getRealNode1())) {\n                            e.switchNodes();\n                        }\n                    }\n                }\n            }\n        });\n        if (thickEdges.size() > 0 && wedgeEdges.size() > 0) {\n            thickEdges.stream().filter(e -> {\n                if (e.getRealNode1().getEdges().stream().filter(e2 -> e2 != e).filter(e2 -> e2.getWedge() || e2.getDashed()).findAny().isPresent() || e.getRealNode2().getEdges().stream().filter(e2 -> e2 != e).filter(e2 -> e2.getWedge() || e2.getDashed()).findAny().isPresent()) {\n                    return true;\n                }\n                return false;\n            }).forEach(w -> w.setWedge(false));\n        }\n        GeomUtil.eachCombination(ctab.getNodes().stream().filter(n -> !n.isInvented()).collect(Collectors.toList())).filter(t -> t.k().distanceTo(t.v()) < 1.5 * ctab.getAverageBondLength()).filter(t -> !t.k().getBondTo(t.v()).isPresent()).forEach(t1 -> {\n            Line2D l2 = new Line2D.Double(t1.k().getPoint(), t1.v().getPoint());\n            LineWrapper useLine = GeomUtil.getLinesNotInside(l2, growLikelyOCR).stream().map(l -> LineWrapper.of(l)).max(Comparator.naturalOrder()).orElse(null);\n            if (useLine == null)\n                return;\n            List<LineWrapper> lt = polygons.stream().filter(s -> GeomUtil.getIntersection(s, useLine.getLine()).isPresent()).map(s -> GeomUtil.findLongestSplittingLine(s)).filter(l -> l.length() < ctab.getAverageBondLength()).collect(Collectors.toList());\n            boolean found = false;\n            if (lt.size() > 2) {\n                found = true;\n            } else if (lt.size() > 0) {\n                LineWrapper lwo = LineWrapper.of(l2);\n                found = lt.stream().filter(lw -> lw.absCosTheta(lwo) > 0.8).filter(lw -> lw.length() < useLine.length()).filter(lw -> lw.centerPoint().distance(useLine.centerPoint()) < ctab.getAverageBondLength() / 4.0).findAny().isPresent();\n            }\n            if (found) {\n                ctab.addEdge(t1.k().getIndex(), t1.v().getIndex(), 1);\n                Edge e = ctab.getEdges().get(ctab.getEdges().size() - 1);\n                e.setDashed(true);\n            }\n        });\n        ctab.getEdges().stream().filter(e -> e.getDashed()).map(e -> Tuple.of(e, GeomUtil.findCenterOfShape(e.getLine()))).filter(e -> !dashShapes.stream().filter(d -> d.contains(e.v())).findAny().isPresent()).map(t -> t.k()).collect(Collectors.toList()).forEach(t -> {\n            Shape sl = GeomUtil.growLine(t.getLine(), ctab.getAverageBondLength() / 3);\n            boolean findLines = lines.stream().map(l -> l.centerPoint()).filter(l -> sl.contains(l)).findAny().isPresent();\n            if (!findLines) {\n                ctab.removeEdge(t);\n            }\n        });\n        ctab.getEdges().stream().filter(e -> e.getDashed()).forEach(e -> {\n            if (couldBeStereoCenter.test(e.getRealNode2()) && !couldBeStereoCenter.test(e.getRealNode1())) {\n                e.switchNodes();\n            }\n        });\n        ctab.getRings().stream().filter(r -> r.size() == 6).forEach(r -> {\n            List<Edge> doubles = r.getEdges().stream().filter(e -> e.getOrder() == 2).collect(Collectors.toList());\n            if (doubles.size() == 2) {\n                Set<Node> sp2s = doubles.stream().flatMap(e -> Stream.of(e.getRealNode1(), e.getRealNode2())).collect(Collectors.toSet());\n                Edge maybeDouble = r.getEdges().stream().filter(e -> !sp2s.contains(e.getRealNode1())).filter(e -> !sp2s.contains(e.getRealNode2())).findFirst().orElse(null);\n                if (maybeDouble != null) {\n                    if (maybeDouble.getRealNode1().getValanceTotal() == 4 || maybeDouble.getRealNode2().getValanceTotal() == 4) {\n                        return;\n                    }\n                    Shape ls = GeomUtil.growLine(maybeDouble.getLine(), ctab.getAverageBondLength() * 0.3);\n                    long c = lines.stream().map(l -> Tuple.of(l, l.centerPoint())).filter(t -> ls.contains(t.v())).filter(t -> t.k().absCosTheta(LineWrapper.of(maybeDouble.getLine())) > 0.8).count();\n                    if (c > 1) {\n                        maybeDouble.setOrder(2);\n                    }\n                }\n            }\n        });\n        ctab.getRings().stream().filter(r -> r.size() == 6).filter(r -> r.isConjugated()).forEach(r -> {\n            Point2D center = GeomUtil.centerOfMass(r.getConvexHull());\n            Shape p = GeomUtil.convexHull2(GeomUtil.makeNPolyCenteredAt(new Point2D.Double(0, 0), 6, 100));\n            Point2D anchor = r.getNodes().stream().map(n -> Tuple.of(n, n.getPoint().distance(center)).withVComparator()).max(Comparator.naturalOrder()).map(t -> t.k().getPoint()).orElse(null);\n            Line2D nline = new Line2D.Double(center, anchor);\n            AffineTransform at = GeomUtil.getTransformFromLineToLine(new Line2D.Double(new Point2D.Double(0, 0), new Point2D.Double(100, 0)), nline, false);\n            Shape ns = at.createTransformedShape(p);\n            Point2D[] verts2 = GeomUtil.vertices(ns);\n            r.getNodes().forEach(n -> {\n                Point2D np = Arrays.stream(verts2).map(v -> Tuple.of(v, v.distance(n.getPoint())).withVComparator()).min(Comparator.naturalOrder()).map(t -> t.k()).orElse(null);\n                n.setPoint(np);\n            });\n        });\n        if (DEBUG)\n            ctabRaw.add(ctab.cloneTab());\n    }\n    rescueOCRShapes = realRescueOCRCandidates;\n    if (DEBUG)\n        ctabRaw.add(ctab.cloneTab());\n    ctab.getNodes().stream().filter(n -> n.getSymbol().equals(\"S\")).filter(n -> n.getValanceTotal() == 7).forEach(n -> {\n        n.getNeighborNodes().stream().filter(t -> t.v().getOrder() == 2).filter(nn -> nn.k().getSymbol().equals(\"N\")).findFirst().ifPresent(nn -> {\n            nn.v().setOrder(1);\n        });\n    });\n    ctab.getNodes().stream().filter(n -> n.getSymbol().equals(\"N\")).filter(n -> n.getCharge() == 0).forEach(n -> {\n        int so = n.getEdges().stream().mapToInt(e -> e.getOrder()).sum();\n        if (so > 3) {\n            n.setCharge(so - 3);\n            n.getNeighborNodes().stream().filter(t -> t.v().getOrder() == 1).map(t -> t.k()).filter(nn -> nn.getSymbol().equals(\"O\")).filter(nn -> nn.getCharge() == 0).filter(nn -> nn.getEdgeCount() == 1).findFirst().ifPresent(nn -> {\n                nn.setCharge(-1);\n            });\n        }\n    });\n    ctab.getNodes().stream().filter(n -> n.getSymbol().equals(\"S\")).filter(n -> n.getCharge() == 0).forEach(n -> {\n        int so = n.getEdges().stream().mapToInt(e -> e.getOrder()).sum();\n        if (so == 3) {\n            n.getNeighborNodes().stream().map(t -> t.k()).filter(nn -> nn.getSymbol().equals(\"O\")).filter(nn -> nn.getCharge() == 0).filter(nn -> nn.getEdgeCount() == 1).findFirst().ifPresent(nn -> {\n                nn.setCharge(-1);\n                n.setCharge(1);\n            });\n        }\n    });\n    if (DEBUG)\n        ctabRaw.add(ctab.cloneTab());\n    List<Shape> mightBeNegative = polygons.stream().filter(s -> s.getBounds2D().getHeight() < ctab.getAverageBondLength() / 10).filter(s -> s.getBounds2D().getWidth() > 1).filter(s -> s.getBounds2D().getWidth() < ctab.getAverageBondLength() / 2).filter(s -> !likelyOCRAll.contains(s) || Optional.ofNullable(ocrAttempt.get(s)).map(l -> l.get(0)).filter(t -> t.k().toString().equals(\"-\")).isPresent()).collect(Collectors.toList());\n    if (!mightBeNegative.isEmpty()) {\n        Set<Shape> already = new HashSet<Shape>();\n        BiConsumer<Node, Integer> maybeCharge = (n, o) -> {\n            int so = n.getEdges().stream().mapToInt(e -> e.getOrder()).sum();\n            if (so == o) {\n                Point2D p = n.getPoint();\n                Shape neg = mightBeNegative.stream().filter(s -> !already.contains(s)).filter(s -> GeomUtil.distanceTo(s, p) < ctab.getAverageBondLength() * 0.7).findAny().orElse(null);\n                if (neg != null) {\n                    Point2D ncenter = GeomUtil.findCenterOfShape(neg);\n                    boolean inLine = n.getEdges().stream().map(e -> e.getLine()).map(l -> GeomUtil.growLine(l, averageWidthOCRFinal[0])).filter(s -> s.contains(ncenter)).findAny().isPresent();\n                    if (!inLine) {\n                        already.add(neg);\n                        n.setCharge(-1);\n                    } else {\n                        if (ctab.getSumCharge() > 0) {\n                            already.add(neg);\n                            n.setCharge(-1);\n                        }\n                    }\n                }\n            }\n        };\n        Map<String, Integer> neededValances = new HashMap<String, Integer>();\n        neededValances.put(\"O\", 1);\n        neededValances.put(\"N\", 2);\n        neededValances.put(\"Cl\", 1);\n        ctab.getNodes().stream().filter(n -> neededValances.get(n.getSymbol()) != null).filter(n -> n.getCharge() == 0).filter(n -> !n.isInvented()).forEach(n -> maybeCharge.accept(n, neededValances.get(n.getSymbol())));\n        int sc = ctab.getSumCharge();\n        if (sc > 0) {\n            ctab.getNodes().stream().filter(n -> n.getSymbol().equals(\"C\")).filter(n -> n.getCharge() == 0).filter(n -> n.getEdgeCount() == 2).filter(n -> !n.isInvented()).filter(n -> !n.getEdges().stream().filter(e -> e.getDashed()).findAny().isPresent()).forEach(n -> maybeCharge.accept(n, 2));\n        }\n        if (ctab.getSumCharge() > 0) {\n            mightBeNegative.stream().filter(s -> !already.contains(s)).map(s -> Tuple.of(s, s)).map(Tuple.vmap(s -> bestGuessOCR.keySet().stream().map(os -> Tuple.of(os, GeomUtil.distance(os, s)).withVComparator()).min(Comparator.naturalOrder()))).filter(t -> t.v().isPresent()).map(Tuple.vmap(v1 -> v1.get())).filter(t -> t.v().v() < ctab.getAverageBondLength() * 0.3).forEach(t -> {\n                Shape neg = t.k();\n                Shape ocr = t.v().k();\n                ctab.getAllNodesInsideShape(ocr, 3).stream().map(n -> Tuple.of(Stream.of(n), n.getNeighborNodes().stream().map(t1 -> t1.k()))).flatMap(nt -> Stream.concat(nt.k(), nt.v())).distinct().filter(n -> n.getSymbol().equals(\"O\")).filter(n -> n.getValanceTotal() == 1).filter(n -> n.getCharge() == 0).findFirst().ifPresent(ncharge -> {\n                    ncharge.setCharge(-1);\n                });\n            });\n        }\n    }\n    if (DEBUG)\n        ctabRaw.add(ctab.cloneTab());\n    ctab.simpleClean();\n    circles.stream().map(c -> GeomUtil.findCenterOfShape(c)).forEach(cp -> {\n        ctab.getEdgesWithCenterWithin(cp, ctab.getAverageBondLength()).stream().filter(e -> e.isRingEdge()).forEach(Edge::setToAromatic);\n    });\n    if (DEBUG)\n        ctabRaw.add(ctab.cloneTab());\n}",
        "tuc": "@Ignore\n@Test\npublic void test1() {\n    File dir1 = getFile(\"regressionTest/usan\");\n    try {\n        ChemicalBuilder cb = ChemicalBuilder.createFromSmiles(\"CCCC\");\n        String ii = Inchi.asStdInchi(cb.build()).getKey();\n        System.out.println(ii);\n        Thread.sleep(1000);\n    } catch (Exception e) {\n        // TODO Auto-generated catch block\n        e.printStackTrace();\n    }\n    Arrays.stream(dir1.listFiles()).filter(f -> f.getName().contains(\".\")).map(f -> Tuple.of(f.getName().split(\"[.]\")[0], f)).collect(Tuple.toGroupedMap()).values().stream().filter(l -> l.size() == 2).map(l -> {\n        if (!l.get(1).getName().toLowerCase().endsWith(\"tif\") && !l.get(1).getName().toLowerCase().endsWith(\"png\")) {\n            List<File> flist = new ArrayList<File>();\n            flist.add(l.get(1));\n            flist.add(l.get(0));\n            return flist;\n        }\n        return l;\n    }).collect(shuffler(new Random(11111140l))).limit(100).//NOTE, I THINK THIS TECHNICALLY WORKS, BUT SINCE THERE IS PARALLEL THINGS GOING ON IN EACH, IT SOMETIMES WILL STARVE A CASE FOR A LONG TIME\n    //\t\t      .parallel()\n    map(fl -> Tuple.of(fl, testMolecule(fl.get(1), fl.get(0)))).map(t -> t.swap()).peek(t -> System.out.println(t.k())).collect(Tuple.toGroupedMap()).entrySet().stream().map(Tuple::of).forEach(t -> {\n        Result r = t.k();\n        List<List<File>> fl = t.v();\n        System.out.println(\"======================================\");\n        System.out.println(r.toString() + \"\\t\" + fl.size());\n        System.out.println(\"--------------------------------------\");\n        System.out.println(fl.stream().map(f -> f.get(1).getAbsolutePath()).collect(Collectors.joining(\"\\n\")));\n    });\n}",
        "label": 0
    },
    {
        "repo_name": "killbilling___recurly-java-library",
        "commit": "1a9a70c95347494f3ff9f24233e0e536129f0667",
        "commit_message": "add get/set for ccEmails\n",
        "p_path": "src/main/java/com/ning/billing/recurly/model/Account.java",
        "t_path": "src/test/java/com/ning/billing/recurly/model/TestAccount.java",
        "p_name": "equals",
        "t_name": "testserialization",
        "lpfc": "@Override\npublic boolean equals(final Object o) {\n    if (this == o)\n        return true;\n    if (o == null || getClass() != o.getClass())\n        return false;\n    final Account account = (Account) o;\n    if (acceptLanguage != null ? !acceptLanguage.equals(account.acceptLanguage) : account.acceptLanguage != null) {\n        return false;\n    }\n    if (accountAcquisition != null ? !accountAcquisition.equals(account.accountAcquisition) : account.accountAcquisition != null) {\n        return false;\n    }\n    if (accountCode != null ? !accountCode.equals(account.accountCode) : account.accountCode != null) {\n        return false;\n    }\n    if (address != null ? !address.equals(account.address) : account.address != null) {\n        return false;\n    }\n    if (adjustments != null ? !adjustments.equals(account.adjustments) : account.adjustments != null) {\n        return false;\n    }\n    if (billingInfo != null ? !billingInfo.equals(account.billingInfo) : account.billingInfo != null) {\n        return false;\n    }\n    if (companyName != null ? !companyName.equals(account.companyName) : account.companyName != null) {\n        return false;\n    }\n    if (createdAt != null ? createdAt.compareTo(account.createdAt) != 0 : account.createdAt != null) {\n        return false;\n    }\n    if (email != null ? !email.equals(account.email) : account.email != null) {\n        return false;\n    }\n    if (firstName != null ? !firstName.equals(account.firstName) : account.firstName != null) {\n        return false;\n    }\n    if (hasLiveSubscription != null ? !hasLiveSubscription.equals(account.hasLiveSubscription) : account.hasLiveSubscription != null) {\n        return false;\n    }\n    if (hasActiveSubscription != null ? !hasActiveSubscription.equals(account.hasActiveSubscription) : account.hasActiveSubscription != null) {\n        return false;\n    }\n    if (hasFutureSubscription != null ? !hasFutureSubscription.equals(account.hasFutureSubscription) : account.hasFutureSubscription != null) {\n        return false;\n    }\n    if (hasCanceledSubscription != null ? !hasCanceledSubscription.equals(account.hasCanceledSubscription) : account.hasCanceledSubscription != null) {\n        return false;\n    }\n    if (hasPausedSubscription != null ? !hasPausedSubscription.equals(account.hasPausedSubscription) : account.hasPausedSubscription != null) {\n        return false;\n    }\n    if (href != null ? !href.equals(account.href) : account.href != null) {\n        return false;\n    }\n    if (hostedLoginToken != null ? !hostedLoginToken.equals(account.hostedLoginToken) : account.hostedLoginToken != null) {\n        return false;\n    }\n    if (invoices != null ? !invoices.equals(account.invoices) : account.invoices != null) {\n        return false;\n    }\n    if (lastName != null ? !lastName.equals(account.lastName) : account.lastName != null) {\n        return false;\n    }\n    if (preferredLocale != null ? !preferredLocale.equals(account.preferredLocale) : account.preferredLocale != null) {\n        return false;\n    }\n    if (state != null ? !state.equals(account.state) : account.state != null) {\n        return false;\n    }\n    if (subscriptions != null ? !subscriptions.equals(account.subscriptions) : account.subscriptions != null) {\n        return false;\n    }\n    if (transactions != null ? !transactions.equals(account.transactions) : account.transactions != null) {\n        return false;\n    }\n    if (updatedAt != null ? updatedAt.compareTo(account.updatedAt) != 0 : account.updatedAt != null) {\n        return false;\n    }\n    if (username != null ? !username.equals(account.username) : account.username != null) {\n        return false;\n    }\n    if (taxExempt != null ? !taxExempt.equals(account.taxExempt) : account.taxExempt != null) {\n        return false;\n    }\n    if (shippingAddresses != null ? !shippingAddresses.equals(account.shippingAddresses) : account.shippingAddresses != null) {\n        return false;\n    }\n    if (vatNumber != null ? !vatNumber.equals(account.vatNumber) : account.vatNumber != null) {\n        return false;\n    }\n    return true;\n}",
        "rpfc": "@Override\npublic boolean equals(final Object o) {\n    if (this == o)\n        return true;\n    if (o == null || getClass() != o.getClass())\n        return false;\n    final Account account = (Account) o;\n    if (acceptLanguage != null ? !acceptLanguage.equals(account.acceptLanguage) : account.acceptLanguage != null) {\n        return false;\n    }\n    if (accountAcquisition != null ? !accountAcquisition.equals(account.accountAcquisition) : account.accountAcquisition != null) {\n        return false;\n    }\n    if (accountCode != null ? !accountCode.equals(account.accountCode) : account.accountCode != null) {\n        return false;\n    }\n    if (address != null ? !address.equals(account.address) : account.address != null) {\n        return false;\n    }\n    if (adjustments != null ? !adjustments.equals(account.adjustments) : account.adjustments != null) {\n        return false;\n    }\n    if (billingInfo != null ? !billingInfo.equals(account.billingInfo) : account.billingInfo != null) {\n        return false;\n    }\n    if (companyName != null ? !companyName.equals(account.companyName) : account.companyName != null) {\n        return false;\n    }\n    if (createdAt != null ? createdAt.compareTo(account.createdAt) != 0 : account.createdAt != null) {\n        return false;\n    }\n    if (email != null ? !email.equals(account.email) : account.email != null) {\n        return false;\n    }\n    if (ccEmails != null ? !ccEmails.equals(account.ccEmails) : account.ccEmails != null) {\n        return false;\n    }\n    if (firstName != null ? !firstName.equals(account.firstName) : account.firstName != null) {\n        return false;\n    }\n    if (hasLiveSubscription != null ? !hasLiveSubscription.equals(account.hasLiveSubscription) : account.hasLiveSubscription != null) {\n        return false;\n    }\n    if (hasActiveSubscription != null ? !hasActiveSubscription.equals(account.hasActiveSubscription) : account.hasActiveSubscription != null) {\n        return false;\n    }\n    if (hasFutureSubscription != null ? !hasFutureSubscription.equals(account.hasFutureSubscription) : account.hasFutureSubscription != null) {\n        return false;\n    }\n    if (hasCanceledSubscription != null ? !hasCanceledSubscription.equals(account.hasCanceledSubscription) : account.hasCanceledSubscription != null) {\n        return false;\n    }\n    if (hasPausedSubscription != null ? !hasPausedSubscription.equals(account.hasPausedSubscription) : account.hasPausedSubscription != null) {\n        return false;\n    }\n    if (href != null ? !href.equals(account.href) : account.href != null) {\n        return false;\n    }\n    if (hostedLoginToken != null ? !hostedLoginToken.equals(account.hostedLoginToken) : account.hostedLoginToken != null) {\n        return false;\n    }\n    if (invoices != null ? !invoices.equals(account.invoices) : account.invoices != null) {\n        return false;\n    }\n    if (lastName != null ? !lastName.equals(account.lastName) : account.lastName != null) {\n        return false;\n    }\n    if (preferredLocale != null ? !preferredLocale.equals(account.preferredLocale) : account.preferredLocale != null) {\n        return false;\n    }\n    if (state != null ? !state.equals(account.state) : account.state != null) {\n        return false;\n    }\n    if (subscriptions != null ? !subscriptions.equals(account.subscriptions) : account.subscriptions != null) {\n        return false;\n    }\n    if (transactions != null ? !transactions.equals(account.transactions) : account.transactions != null) {\n        return false;\n    }\n    if (updatedAt != null ? updatedAt.compareTo(account.updatedAt) != 0 : account.updatedAt != null) {\n        return false;\n    }\n    if (username != null ? !username.equals(account.username) : account.username != null) {\n        return false;\n    }\n    if (taxExempt != null ? !taxExempt.equals(account.taxExempt) : account.taxExempt != null) {\n        return false;\n    }\n    if (shippingAddresses != null ? !shippingAddresses.equals(account.shippingAddresses) : account.shippingAddresses != null) {\n        return false;\n    }\n    if (vatNumber != null ? !vatNumber.equals(account.vatNumber) : account.vatNumber != null) {\n        return false;\n    }\n    return true;\n}",
        "tuc": "@Test(groups = \"fast\")\npublic void testSerialization() throws Exception {\n    // See https://dev.recurly.com/docs/list-accounts\n    final String accountData = \"<account href=\\\"https://api.recurly.com/v2/accounts/1\\\">\\n\" + \"  <adjustments href=\\\"https://api.recurly.com/v2/accounts/1/adjustments\\\"/>\\n\" + \"  <billing_info href=\\\"https://api.recurly.com/v2/accounts/1/billing_info\\\"/>\\n\" + \"  <invoices href=\\\"https://api.recurly.com/v2/accounts/1/invoices\\\"/>\\n\" + \"  <redemption href=\\\"https://api.recurly.com/v2/accounts/1/redemption\\\"/>\\n\" + \"  <subscriptions href=\\\"https://api.recurly.com/v2/accounts/1/subscriptions\\\"/>\\n\" + \"  <transactions href=\\\"https://api.recurly.com/v2/accounts/1/transactions\\\"/>\\n\" + \"  <account_code>1</account_code>\\n\" + \"  <state>active</state>\\n\" + \"  <username nil=\\\"nil\\\"></username>\\n\" + \"  <email>verena@example.com</email>\\n\" + \"  <first_name>Verena</first_name>\\n\" + \"  <last_name>Example</last_name>\\n\" + \"  <tax_exempt type=\\\"boolean\\\">false</tax_exempt>\\n\\n\" + \"  <accept_language nil=\\\"nil\\\"></accept_language>\\n\" + \"  <hosted_login_token>a92468579e9c4231a6c0031c4716c01d</hosted_login_token>\\n\" + \"  <created_at type=\\\"dateTime\\\">2011-10-25T12:00:00</created_at>\\n\" + \"  <updated_at type=\\\"dateTime\\\">2011-10-25T12:00:00</updated_at>\\n\" + \"  <has_live_subscription type=\\\"boolean\\\">true</has_live_subscription>\\n\" + \"  <has_active_subscription type=\\\"boolean\\\">true</has_active_subscription>\\n\" + \"  <has_future_subscription type=\\\"boolean\\\">false</has_future_subscription>\\n\" + \"  <has_canceled_subscription type=\\\"boolean\\\">false</has_canceled_subscription>\\n\" + \"  <has_past_due_invoice type=\\\"boolean\\\">false</has_past_due_invoice>\\n\" + \"  <vat_number>U12345678</vat_number>\\n\" + \"  <address>\\n\" + \"      <address1>123 Main St.</address1>\\n\" + \"      <address2 nil=\\\"nil\\\"></address2>\\n\" + \"      <city>San Francisco</city>\\n\" + \"      <state>CA</state>\\n\" + \"      <zip>94105-1804</zip>\\n\" + \"      <country>US</country>\\n\" + \"      <phone nil=\\\"nil\\\"></phone>\\n\" + \"  </address>\" + \"</account>\";\n    final Account account = xmlMapper.readValue(accountData, Account.class);\n    Assert.assertEquals(account.getHref(), \"https://api.recurly.com/v2/accounts/1\");\n    verifyAccount(account);\n    // Verify serialization\n    final String accountSerialized = xmlMapper.writeValueAsString(account);\n    final Account account2 = xmlMapper.readValue(accountSerialized, Account.class);\n    Assert.assertNull(account2.getHref());\n    verifyAccount(account2);\n}",
        "label": 0
    },
    {
        "repo_name": "jMotif___SAX",
        "commit": "4b820f9dd9be41d1debaa7ca49e29a6f70f4b99e",
        "commit_message": "Working on validation.",
        "p_path": "src/main/java/net/seninp/jmotif/sax/tinker/DiscordDiscoveryPerformanceDoubleLong.java",
        "t_path": "src/test/java/net/seninp/jmotif/sax/registry/TestVisitRegistry.java",
        "p_name": "main",
        "t_name": "testmarkintervalvisited",
        "lpfc": "public static void main(String[] args) throws Exception {\n    series = TSProcessor.readFileColumn(TEST_DATA_FNAME, 0, 0);\n    double[] tmp = new double[series.length * 2];\n    for (int i = 0; i < series.length; i++) {\n        tmp[i] = series[i];\n        tmp[i + series.length] = series[i];\n    }\n    series = tmp;\n    DiscordRecords discordsBruteForce = null;\n    DiscordRecords discordsHash = null;\n    discordsBruteForce = BruteForceDiscordImplementation.series2BruteForceDiscords(series, WIN_SIZE, DISCORDS_TO_TEST, new LargeWindowAlgorithm());\n    for (DiscordRecord d : discordsBruteForce) {\n        System.out.println(\"brute force discord \" + d.toString());\n    }\n    discordsHash = HOTSAXImplementation.series2Discords(series, DISCORDS_TO_TEST, WIN_SIZE, PAA_SIZE, ALPHABET_SIZE, NumerosityReductionStrategy.MINDIST, NORM_THRESHOLD);\n    for (DiscordRecord d : discordsHash) {\n        System.out.println(\"hotsax hash discord \" + d.toString());\n    }\n}",
        "rpfc": "public static void main(String[] args) throws Exception {\n    series = TSProcessor.readFileColumn(TEST_DATA_FNAME, 0, 0);\n    double[] tmp = new double[series.length * 2];\n    for (int i = 0; i < series.length; i++) {\n        tmp[i] = series[i];\n        tmp[i + series.length] = series[i];\n    }\n    series = tmp;\n    DiscordRecords discordsBruteForce = null;\n    DiscordRecords discordsHash = null;\n    discordsBruteForce = BruteForceDiscordImplementation.series2BruteForceDiscords(series, WIN_SIZE, DISCORDS_TO_TEST, new LargeWindowAlgorithm());\n    for (DiscordRecord d : discordsBruteForce) {\n        System.out.println(\"brute force discord \" + d.toString());\n    }\n    discordsHash = HOTSAXImplementation.series2Discords(series, DISCORDS_TO_TEST, WIN_SIZE, PAA_SIZE, ALPHABET_SIZE, NumerosityReductionStrategy.NONE, NORM_THRESHOLD);\n    for (DiscordRecord d : discordsHash) {\n        System.out.println(\"hotsax hash discord \" + d.toString());\n    }\n}",
        "tuc": "/**\n * Test the interval marker.\n */\n@Test\npublic void testMarkIntervalVisited() {\n    int mark1Start = 3;\n    int mark1End = 29;\n    int mark2Start = vr.size() / 2;\n    int mark2End = vr.size() - 17;\n    vr.markVisited(mark1Start, mark1End);\n    vr.markVisited(mark2Start, mark2End);\n    // check first marker\n    for (int i = 0; i < mark1Start; i++) {\n        assertTrue(vr.isNotVisited(i));\n        assertFalse(vr.isVisited(i));\n    }\n    for (int i = mark1Start; i < mark1End; i++) {\n        assertFalse(vr.isNotVisited(i));\n        assertTrue(vr.isVisited(i));\n    }\n    for (int i = mark1End; i < mark2Start; i++) {\n        assertTrue(vr.isNotVisited(i));\n        assertFalse(vr.isVisited(i));\n    }\n    // check second marker\n    for (int i = mark2Start; i < mark2End; i++) {\n        assertFalse(vr.isNotVisited(i));\n        assertTrue(vr.isVisited(i));\n    }\n    for (int i = mark2End; i < vr.size(); i++) {\n        assertTrue(vr.isNotVisited(i));\n        assertFalse(vr.isVisited(i));\n    }\n    // check the interval checker\n    assertTrue(vr.isVisited(mark1Start, mark1End));\n    assertTrue(vr.isVisited(mark1Start - 1, mark1End));\n    assertTrue(vr.isVisited(mark1Start, mark1End + 1));\n}",
        "label": 0
    },
    {
        "repo_name": "powsybl___pypowsybl",
        "commit": "7a64912c297f4cc05bdab3d8deb1e3dc911c39c3",
        "commit_message": "Name update (#838)\n\nSigned-off-by: Geoffroy Jamgotchian <geoffroy.jamgotchian@rte-france.com>\r\nCo-authored-by: HugoKulesza <94374655+HugoKulesza@users.noreply.github.com>",
        "p_path": "java/src/main/java/com/powsybl/dataframe/network/NetworkDataframes.java",
        "t_path": "java/src/test/java/com/powsybl/dataframe/DataframeMapperBuilderTest.java",
        "p_name": "hvdcs",
        "t_name": "test",
        "lpfc": "private static NetworkDataframeMapper hvdcs() {\n    return NetworkDataframeMapperBuilder.ofStream(Network::getHvdcLineStream, getOrThrow(Network::getHvdcLine, \"HVDC line\")).stringsIndex(\"id\", HvdcLine::getId).strings(\"name\", l -> l.getOptionalName().orElse(\"\")).enums(\"converters_mode\", HvdcLine.ConvertersMode.class, HvdcLine::getConvertersMode, HvdcLine::setConvertersMode).doubles(\"target_p\", (hvdc, context) -> perUnitPQ(context, hvdc.getActivePowerSetpoint()), (hvdc, aps, context) -> hvdc.setActivePowerSetpoint(unPerUnitPQ(context, aps))).doubles(\"max_p\", (hvdc, context) -> perUnitPQ(context, hvdc.getMaxP()), (hvdc, maxP, context) -> hvdc.setMaxP(unPerUnitPQ(context, maxP))).doubles(\"nominal_v\", (hvdc, context) -> hvdc.getNominalV(), (hvdc, nominalV, context) -> hvdc.setNominalV(nominalV)).doubles(\"r\", (hvdc, context) -> perUnitRX(context, hvdc.getR(), hvdc.getNominalV(), hvdc.getNominalV()), (hvdc, r, context) -> hvdc.setR(unPerUnitRX(context, r, hvdc.getNominalV(), hvdc.getNominalV()))).strings(\"converter_station1_id\", l -> l.getConverterStation1().getId()).strings(\"converter_station2_id\", l -> l.getConverterStation2().getId()).booleans(\"connected1\", l -> l.getConverterStation1().getTerminal().isConnected(), connectHvdcStation1()).booleans(\"connected2\", l -> l.getConverterStation2().getTerminal().isConnected(), connectHvdcStation2()).booleans(\"fictitious\", Identifiable::isFictitious, Identifiable::setFictitious, false).addProperties().build();\n}",
        "rpfc": "private static NetworkDataframeMapper hvdcs() {\n    return NetworkDataframeMapperBuilder.ofStream(Network::getHvdcLineStream, getOrThrow(Network::getHvdcLine, \"HVDC line\")).stringsIndex(\"id\", HvdcLine::getId).strings(\"name\", l -> l.getOptionalName().orElse(\"\"), Identifiable::setName).enums(\"converters_mode\", HvdcLine.ConvertersMode.class, HvdcLine::getConvertersMode, HvdcLine::setConvertersMode).doubles(\"target_p\", (hvdc, context) -> perUnitPQ(context, hvdc.getActivePowerSetpoint()), (hvdc, aps, context) -> hvdc.setActivePowerSetpoint(unPerUnitPQ(context, aps))).doubles(\"max_p\", (hvdc, context) -> perUnitPQ(context, hvdc.getMaxP()), (hvdc, maxP, context) -> hvdc.setMaxP(unPerUnitPQ(context, maxP))).doubles(\"nominal_v\", (hvdc, context) -> hvdc.getNominalV(), (hvdc, nominalV, context) -> hvdc.setNominalV(nominalV)).doubles(\"r\", (hvdc, context) -> perUnitRX(context, hvdc.getR(), hvdc.getNominalV(), hvdc.getNominalV()), (hvdc, r, context) -> hvdc.setR(unPerUnitRX(context, r, hvdc.getNominalV(), hvdc.getNominalV()))).strings(\"converter_station1_id\", l -> l.getConverterStation1().getId()).strings(\"converter_station2_id\", l -> l.getConverterStation2().getId()).booleans(\"connected1\", l -> l.getConverterStation1().getTerminal().isConnected(), connectHvdcStation1()).booleans(\"connected2\", l -> l.getConverterStation2().getTerminal().isConnected(), connectHvdcStation2()).booleans(\"fictitious\", Identifiable::isFictitious, Identifiable::setFictitious, false).addProperties().build();\n}",
        "tuc": "@Test\nvoid test() {\n    DataframeMapper<Container, Void> mapper = new DataframeMapperBuilder<Container, Element, Void>().itemsProvider(Container::getElements).stringsIndex(\"id\", Element::getId).strings(\"str\", Element::getStrValue).ints(\"int\", Element::getIntValue).doubles(\"double\", Element::getDoubleValue).enums(\"color\", Color.class, Element::getColorValue).build();\n    Container container = new Container(new Element(\"el1\", \"val1\", 1, 10, Color.RED), new Element(\"el2\", \"val2\", 2, 20, Color.BLUE));\n    List<com.powsybl.dataframe.impl.Series> series = new ArrayList<>();\n    mapper.createDataframe(container, new DefaultDataframeHandler(series::add), new DataframeFilter());\n    assertThat(series).extracting(com.powsybl.dataframe.impl.Series::getName).containsExactly(\"id\", \"str\", \"int\", \"double\", \"color\");\n}",
        "label": 0
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "ab8aece1bd6705691a6846178963ea08bcfcd38c",
        "commit_message": "sql \u6253\u5370\u8be6\u60c5\n",
        "p_path": "QiunetDatas/src/main/java/org/qiunet/data/db/interceptor/MybatisInterceptor.java",
        "t_path": "QiunetDatas/src/test/java/org/qiunet/data/db/jdbc/TestLog4Jdbc.java",
        "p_name": "intercept",
        "t_name": "testlog4jdbc",
        "lpfc": "@Override\npublic Object intercept(Invocation invocation) throws Throwable {\n    if (!logger.isInfoEnabled())\n        return invocation.proceed();\n    MappedStatement mappedStatement = (MappedStatement) invocation.getArgs()[0];\n    Object parameter = invocation.getArgs().length > 1 ? invocation.getArgs()[1] : null;\n    BoundSql boundSql = mappedStatement.getBoundSql(parameter);\n    Configuration configuration = mappedStatement.getConfiguration();\n    long start = System.currentTimeMillis();\n    try {\n        return invocation.proceed();\n    } finally {\n        long diff = System.currentTimeMillis() - start;\n        try {\n            logger.info(new StringBuilder().append(formatSql(configuration, boundSql)).append(\"\\t\u8017\u65f6:[\").append(diff).append(\"ms]\").toString());\n        } catch (Exception e) {\n            logger.error(\"SQL\u6253\u5370\u5f02\u5e38: \", e);\n        }\n    }\n}",
        "rpfc": "@Override\npublic Object intercept(Invocation invocation) throws Throwable {\n    if (!logger.isInfoEnabled())\n        return invocation.proceed();\n    MappedStatement mappedStatement = (MappedStatement) invocation.getArgs()[0];\n    Object parameter = invocation.getArgs().length > 1 ? invocation.getArgs()[1] : null;\n    BoundSql boundSql = mappedStatement.getBoundSql(parameter);\n    Configuration configuration = mappedStatement.getConfiguration();\n    long start = System.currentTimeMillis();\n    try {\n        return invocation.proceed();\n    } finally {\n        long diff = System.currentTimeMillis() - start;\n        try {\n            logger.info(new StringBuilder().append(formatSql(configuration, boundSql)).append(\"\\t\u8017\u65f6:[\").append(diff).append(\"ms]\").toString());\n        } catch (Exception e) {\n            logger.error(\"SQL[\" + boundSql.getSql() + \"] PARAM [\" + JsonUtil.toJsonString(boundSql.getParameterObject()) + \"] \u6253\u5370\u5f02\u5e38: \", e);\n        }\n    }\n}",
        "tuc": "@Test\npublic void testLog4Jdbc() {\n    String url = \"jdbc:mysql://127.0.0.1:3306/test_0?useUnicode=true&useSSL=false\";\n    String driver = \"org.mariadb.jdbc.Driver\";\n    String ursename = \"root\";\n    String password = \"qiuyang\";\n    try {\n        Class.forName(driver);\n    } catch (ClassNotFoundException e) {\n        e.printStackTrace();\n    }\n    Connection connection = null;\n    try {\n        connection = DriverManager.getConnection(url, ursename, password);\n        Statement statement = connection.createStatement();\n        statement.execute(\"SELECT * from login;\");\n    } catch (SQLException e) {\n        e.printStackTrace();\n    } finally {\n        if (connection != null) {\n            try {\n                connection.close();\n            } catch (SQLException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "genepi___imputationserver",
        "commit": "9ebfca0c310b537dcf6396c5fc6180951d386bef",
        "commit_message": "update tabix info",
        "p_path": "src/main/java/genepi/imputationserver/steps/CompressionEncryption.java",
        "t_path": "src/test/java/genepi/imputationserver/steps/ImputationMinimac3Test.java",
        "p_name": "run",
        "t_name": "testpipelinewithphased",
        "lpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    String workingDirectory = getFolder(CompressionEncryption.class);\n    String output = context.get(\"outputimputation\");\n    String localOutput = context.get(\"local\");\n    String aesEncryption = context.get(\"aesEncryption\");\n    // read config if mails should be sent\n    String folderConfig = getFolder(CompressionEncryption.class);\n    PreferenceStore store = new PreferenceStore(new File(FileUtil.path(folderConfig, \"job.config\")));\n    String notification = \"no\";\n    if (store.getString(\"minimac.sendmail\") != null && !store.getString(\"minimac.sendmail\").equals(\"\")) {\n        notification = store.getString(\"minimac.sendmail\");\n    }\n    String serverUrl = \"https://imputationserver.sph.umich.edu\";\n    if (store.getString(\"server.url\") != null && !store.getString(\"server.url\").isEmpty()) {\n        serverUrl = store.getString(\"server.url\");\n    }\n    String password = DEFAULT_PASSWORD;\n    if (notification.equals(\"yes\")) {\n        password = PasswordCreator.createPassword();\n    }\n    try {\n        context.beginTask(\"Export data...\");\n        List<String> folders = HdfsUtil.getDirectories(output);\n        // export all chromosomes\n        for (String folder : folders) {\n            String name = FileUtil.getFilename(folder);\n            context.println(\"Export and merge chromosome \" + name);\n            // create temp fir\n            String temp = FileUtil.path(localOutput, \"temp\");\n            FileUtil.createDirectory(temp);\n            // output files\n            String doseOutput = FileUtil.path(temp, \"chr\" + name + \".info.gz\");\n            String vcfOutput = FileUtil.path(temp, \"chr\" + name + \".dose.vcf.gz\");\n            // merge all info files\n            FileMerger.mergeAndGz(doseOutput, folder, true, \".info\");\n            List<String> dataFiles = findFiles(folder, \".data.dose.vcf.gz\");\n            List<String> headerFiles = findFiles(folder, \".header.dose.vcf.gz\");\n            MergedVcfFile vcfFile = new MergedVcfFile(vcfOutput);\n            // add one header\n            // TODO: check number of samples per chunk....\n            String header = headerFiles.get(0);\n            vcfFile.addFile(HdfsUtil.open(header));\n            // add data files\n            for (String file : dataFiles) {\n                context.println(\"Read file \" + file);\n                vcfFile.addFile(HdfsUtil.open(file));\n            }\n            vcfFile.close();\n            if (name.contains(\"22\")) {\n                Command tabix = new Command(FileUtil.path(workingDirectory, \"bin\", \"tabix\"));\n                tabix.setSilent(false);\n                tabix.setParams(\"-f\", vcfOutput);\n                if (tabix.execute() != 0) {\n                    context.endTask(\"Error during index creation: \" + tabix.getStdOut(), WorkflowContext.ERROR);\n                    return false;\n                }\n            }\n            ZipParameters param = new ZipParameters();\n            param.setEncryptFiles(true);\n            param.setPassword(password);\n            param.setEncryptionMethod(Zip4jConstants.ENC_METHOD_STANDARD);\n            if (aesEncryption != null && aesEncryption.equals(\"yes\")) {\n                param.setEncryptionMethod(Zip4jConstants.ENC_METHOD_AES);\n                param.setAesKeyStrength(Zip4jConstants.AES_STRENGTH_256);\n                param.setCompressionMethod(Zip4jConstants.COMP_DEFLATE);\n                param.setCompressionLevel(Zip4jConstants.DEFLATE_LEVEL_NORMAL);\n            }\n            // create zip file\n            ArrayList<File> files = new ArrayList<File>();\n            files.add(new File(vcfOutput));\n            // files.add(new File(vcfOutput + \".tbi\"));\n            files.add(new File(doseOutput));\n            ZipFile file = new ZipFile(new File(FileUtil.path(localOutput, \"chr_\" + name + \".zip\")));\n            file.createZipFile(files, param);\n            // delete temp dir\n            FileUtil.deleteDirectory(temp);\n        }\n        // delete temporary files\n        HdfsUtil.delete(output);\n        context.endTask(\"Exported data.\", WorkflowContext.OK);\n    } catch (Exception e) {\n        e.printStackTrace();\n        context.endTask(\"Data compression failed: \" + e.getMessage(), WorkflowContext.ERROR);\n        return false;\n    }\n    // submit counters!\n    context.submitCounter(\"samples\");\n    context.submitCounter(\"genotypes\");\n    context.submitCounter(\"chromosomes\");\n    context.submitCounter(\"runs\");\n    // submit panel and phasing method counters\n    String reference = context.get(\"refpanel\");\n    String phasing = context.get(\"phasing\");\n    context.submitCounter(\"refpanel_\" + reference);\n    context.submitCounter(\"phasing_\" + phasing);\n    context.submitCounter(\"23andme-input\");\n    // send email\n    if (notification.equals(\"yes\")) {\n        Object mail = context.getData(\"cloudgene.user.mail\");\n        Object name = context.getData(\"cloudgene.user.name\");\n        if (mail != null) {\n            String subject = \"Job \" + context.getJobId() + \" is complete.\";\n            String message = \"Dear \" + name + \",\\nthe password for the imputation results is: \" + password + \"\\n\\nThe results can be downloaded from \" + serverUrl + \"/start.html#!jobs/\" + context.getJobId() + \"/results\";\n            try {\n                context.sendMail(subject, message);\n                context.ok(\"We have sent an email to <b>\" + mail + \"</b> with the password.\");\n                return true;\n            } catch (Exception e) {\n                context.error(\"Data compression failed: \" + e.getMessage());\n                return false;\n            }\n        } else {\n            context.error(\"No email address found. Please enter your email address (Account -> Profile).\");\n            return false;\n        }\n    } else {\n        context.ok(\"Email notification (and therefore encryption) is disabled. All results are encrypted with password <b>\" + password + \"</b>\");\n        return true;\n    }\n}",
        "rpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    String workingDirectory = getFolder(CompressionEncryption.class);\n    String output = context.get(\"outputimputation\");\n    String localOutput = context.get(\"local\");\n    String aesEncryption = context.get(\"aesEncryption\");\n    // read config if mails should be sent\n    String folderConfig = getFolder(CompressionEncryption.class);\n    PreferenceStore store = new PreferenceStore(new File(FileUtil.path(folderConfig, \"job.config\")));\n    String notification = \"no\";\n    if (store.getString(\"minimac.sendmail\") != null && !store.getString(\"minimac.sendmail\").equals(\"\")) {\n        notification = store.getString(\"minimac.sendmail\");\n    }\n    String serverUrl = \"https://imputationserver.sph.umich.edu\";\n    if (store.getString(\"server.url\") != null && !store.getString(\"server.url\").isEmpty()) {\n        serverUrl = store.getString(\"server.url\");\n    }\n    String password = DEFAULT_PASSWORD;\n    if (notification.equals(\"yes\")) {\n        password = PasswordCreator.createPassword();\n    }\n    try {\n        context.beginTask(\"Export data...\");\n        List<String> folders = HdfsUtil.getDirectories(output);\n        // export all chromosomes\n        for (String folder : folders) {\n            String name = FileUtil.getFilename(folder);\n            context.println(\"Export and merge chromosome \" + name);\n            // create temp fir\n            String temp = FileUtil.path(localOutput, \"temp\");\n            FileUtil.createDirectory(temp);\n            // output files\n            String doseOutput = FileUtil.path(temp, \"chr\" + name + \".info.gz\");\n            String vcfOutput = FileUtil.path(temp, \"chr\" + name + \".dose.vcf.gz\");\n            // merge all info files\n            FileMerger.mergeAndGz(doseOutput, folder, true, \".info\");\n            List<String> dataFiles = findFiles(folder, \".data.dose.vcf.gz\");\n            List<String> headerFiles = findFiles(folder, \".header.dose.vcf.gz\");\n            MergedVcfFile vcfFile = new MergedVcfFile(vcfOutput);\n            // add one header\n            // TODO: check number of samples per chunk....\n            String header = headerFiles.get(0);\n            vcfFile.addFile(HdfsUtil.open(header));\n            // add data files\n            for (String file : dataFiles) {\n                context.println(\"Read file \" + file);\n                vcfFile.addFile(HdfsUtil.open(file));\n            }\n            vcfFile.close();\n            //tabix only for one file\n            if (name.contains(\"22\")) {\n                Command tabix = new Command(FileUtil.path(workingDirectory, \"bin\", \"tabix\"));\n                tabix.setSilent(false);\n                tabix.setParams(\"-f\", vcfOutput);\n                if (tabix.execute() != 0) {\n                    context.endTask(\"Error during index creation: \" + tabix.getStdOut(), WorkflowContext.ERROR);\n                    return false;\n                }\n            }\n            ZipParameters param = new ZipParameters();\n            param.setEncryptFiles(true);\n            param.setPassword(password);\n            param.setEncryptionMethod(Zip4jConstants.ENC_METHOD_STANDARD);\n            if (aesEncryption != null && aesEncryption.equals(\"yes\")) {\n                param.setEncryptionMethod(Zip4jConstants.ENC_METHOD_AES);\n                param.setAesKeyStrength(Zip4jConstants.AES_STRENGTH_256);\n                param.setCompressionMethod(Zip4jConstants.COMP_DEFLATE);\n                param.setCompressionLevel(Zip4jConstants.DEFLATE_LEVEL_NORMAL);\n            }\n            // create zip file\n            ArrayList<File> files = new ArrayList<File>();\n            files.add(new File(vcfOutput));\n            // files.add(new File(vcfOutput + \".tbi\"));\n            files.add(new File(doseOutput));\n            ZipFile file = new ZipFile(new File(FileUtil.path(localOutput, \"chr_\" + name + \".zip\")));\n            file.createZipFile(files, param);\n            // delete temp dir\n            FileUtil.deleteDirectory(temp);\n        }\n        // delete temporary files\n        HdfsUtil.delete(output);\n        context.endTask(\"Exported data.\", WorkflowContext.OK);\n    } catch (Exception e) {\n        e.printStackTrace();\n        context.endTask(\"Data compression failed: \" + e.getMessage(), WorkflowContext.ERROR);\n        return false;\n    }\n    // submit counters!\n    context.submitCounter(\"samples\");\n    context.submitCounter(\"genotypes\");\n    context.submitCounter(\"chromosomes\");\n    context.submitCounter(\"runs\");\n    // submit panel and phasing method counters\n    String reference = context.get(\"refpanel\");\n    String phasing = context.get(\"phasing\");\n    context.submitCounter(\"refpanel_\" + reference);\n    context.submitCounter(\"phasing_\" + phasing);\n    context.submitCounter(\"23andme-input\");\n    // send email\n    if (notification.equals(\"yes\")) {\n        Object mail = context.getData(\"cloudgene.user.mail\");\n        Object name = context.getData(\"cloudgene.user.name\");\n        if (mail != null) {\n            String subject = \"Job \" + context.getJobId() + \" is complete.\";\n            String message = \"Dear \" + name + \",\\nthe password for the imputation results is: \" + password + \"\\n\\nThe results can be downloaded from \" + serverUrl + \"/start.html#!jobs/\" + context.getJobId() + \"/results\";\n            try {\n                context.sendMail(subject, message);\n                context.ok(\"We have sent an email to <b>\" + mail + \"</b> with the password.\");\n                return true;\n            } catch (Exception e) {\n                context.error(\"Data compression failed: \" + e.getMessage());\n                return false;\n            }\n        } else {\n            context.error(\"No email address found. Please enter your email address (Account -> Profile).\");\n            return false;\n        }\n    } else {\n        context.ok(\"Email notification (and therefore encryption) is disabled. All results are encrypted with password <b>\" + password + \"</b>\");\n        return true;\n    }\n}",
        "tuc": "@Test\npublic void testPipelineWithPhased() throws IOException, ZipException {\n    String configFolder = \"test-data/configs/hapmap-chr20\";\n    String inputFolder = \"test-data/data/chr20-phased\";\n    // create workflow context\n    WorkflowTestContext context = buildContext(inputFolder, \"hapmap2\", \"eagle\");\n    // run qc to create chunkfile\n    QcStatisticsMock qcStats = new QcStatisticsMock(configFolder);\n    boolean result = run(context, qcStats);\n    assertTrue(result);\n    assertTrue(context.hasInMemory(\"Remaining sites in total: 7,735\"));\n    // add panel to hdfs\n    importRefPanel(FileUtil.path(configFolder, \"ref-panels\"));\n    //importMinimacMap(\"test-data/B38_MAP_FILE.map\");\n    importBinaries(\"files/minimac/bin\");\n    // run imputation\n    ImputationMinimac3Mock imputation = new ImputationMinimac3Mock(configFolder);\n    result = run(context, imputation);\n    assertTrue(result);\n    // run export\n    CompressionEncryptionMock export = new CompressionEncryptionMock(\"files/minimac\");\n    result = run(context, export);\n    assertTrue(result);\n    ZipFile zipFile = new ZipFile(\"test-data/tmp/local/chr_20.zip\");\n    if (zipFile.isEncrypted()) {\n        zipFile.setPassword(CompressionEncryption.DEFAULT_PASSWORD);\n    }\n    zipFile.extractAll(\"test-data/tmp\");\n    VcfFile file = VcfFileUtil.load(\"test-data/tmp/chr20.dose.vcf.gz\", 100000000, false);\n    assertEquals(\"20\", file.getChromosome());\n    assertEquals(51, file.getNoSamples());\n    assertEquals(true, file.isPhased());\n    assertEquals(TOTAL_REFPANEL_CHR20 - FILTER_REFPANEL + ONLY_IN_INPUT, file.getNoSnps());\n    //FileUtil.deleteDirectory(\"test-data/tmp\");\n}",
        "label": 0
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "fb6a37948cb43418d008de2dac193cbfead3b2eb",
        "commit_message": "\u5224\u65ad\u52a0\u8f7d\u662f\u5426\u6210\u529f\n",
        "p_path": "JavaAgent/src/main/java/org/qiunet/agent/JavaAgent.java",
        "t_path": "QiunetUtils/src/test/java/org/qiunet/utils/classLoader/TestHotSwap.java",
        "p_name": "agentmain",
        "t_name": "testhotswap",
        "lpfc": "public static void agentmain(String args, Instrumentation ins) {\n    ClassInfos classInfos = ClassInfos.parse(args);\n    logger(\"======\u5f00\u59cb\u70ed\u52a0\u8f7d======\");\n    List<File> files = new ArrayList<>();\n    List<ClassDefinition> classDefinitions = new ArrayList<>();\n    for (String className : classInfos.getClassNames()) {\n        className = className.replaceAll(\"\\\\/\", \".\");\n        String[] strings = className.split(\"\\\\.\");\n        String fileName = strings[strings.length - 1];\n        File file = Paths.get(classInfos.getClassPath(), fileName + \".class\").toFile();\n        try {\n            classDefinitions.add(new ClassDefinition(Class.forName(className), Files.readAllBytes(file.toPath())));\n            files.add(file);\n        } catch (Exception e) {\n            logger(\"==HotSwap Fail!\", e);\n        }\n    }\n    if (classDefinitions.isEmpty()) {\n        logger(\"\u6ca1\u6709\u9700\u8981\u52a0\u8f7d\u7684class\");\n        return;\n    }\n    try {\n        ins.redefineClasses(classDefinitions.toArray(new ClassDefinition[classDefinitions.size()]));\n        for (ClassDefinition classDefinition : classDefinitions) {\n            logger(\"======\u70ed\u52a0\u8f7d==[\" + classDefinition.getDefinitionClass().getName() + \"]=\u6210\u529f===\");\n        }\n        files.forEach(File::delete);\n        logger(\"HotSwap Success! Delete files!!! \");\n    } catch (Exception e) {\n        logger(\"HotSwap Fail!\", e);\n    } finally {\n        logger(\"======\u7ed3\u675f\u70ed\u52a0\u8f7d======\");\n    }\n}",
        "rpfc": "public static void agentmain(String args, Instrumentation ins) {\n    ClassInfos classInfos = ClassInfos.parse(args);\n    logger(\"======\u5f00\u59cb\u70ed\u52a0\u8f7d======\");\n    List<File> files = new ArrayList<>();\n    List<ClassDefinition> classDefinitions = new ArrayList<>();\n    for (String className : classInfos.getClassNames()) {\n        className = className.replaceAll(\"\\\\/\", \".\");\n        String[] strings = className.split(\"\\\\.\");\n        String fileName = strings[strings.length - 1];\n        File file = Paths.get(classInfos.getClassPath(), fileName + \".class\").toFile();\n        try {\n            classDefinitions.add(new ClassDefinition(Class.forName(className), Files.readAllBytes(file.toPath())));\n            files.add(file);\n        } catch (Exception e) {\n            logger(\"==HotSwap Fail!\", e);\n        }\n    }\n    if (classDefinitions.isEmpty()) {\n        logger(\"\u6ca1\u6709\u9700\u8981\u52a0\u8f7d\u7684class\");\n        return;\n    }\n    try {\n        ins.redefineClasses(classDefinitions.toArray(new ClassDefinition[classDefinitions.size()]));\n        for (ClassDefinition classDefinition : classDefinitions) {\n            if (ins.isModifiableClass(classDefinition.getDefinitionClass())) {\n                logger(\"======\u70ed\u52a0\u8f7d==[\" + classDefinition.getDefinitionClass().getName() + \"]=\u6210\u529f===\");\n            } else {\n                logger(\"======\u70ed\u52a0\u8f7d==[\" + classDefinition.getDefinitionClass().getName() + \"]=\u5931\u8d25===\");\n            }\n        }\n        files.forEach(File::delete);\n        logger(\"HotSwap Success! Delete files!!! \");\n    } catch (Exception e) {\n        logger(\"HotSwap Fail!\", e);\n    } finally {\n        logger(\"======\u7ed3\u675f\u70ed\u52a0\u8f7d======\");\n    }\n}",
        "tuc": "@Test\npublic void testHotSwap() {\n    String path = getClass().getResource(\"/\").getPath();\n    ChangeClass changeClass = new ChangeClass();\n    changeClass.show();\n    ClassHotSwap.hotSwap(path);\n    changeClass.show();\n    //\n    new ChangeClass().show();\n}",
        "label": 0
    },
    {
        "repo_name": "genepi___imputationserver",
        "commit": "06d1bb9765519a5ba193719a5e521f83c6777fed",
        "commit_message": "update chrX workflow. add files to minimac3 input step. ",
        "p_path": "src/main/java/genepi/imputationserver/steps/qc/QCStatistics.java",
        "t_path": "src/test/java/genepi/imputationserver/steps/QCStatisticsTest.java",
        "p_name": "start",
        "t_name": "testcountsamplesincreatedchunk",
        "lpfc": "public boolean start() throws IOException, InterruptedException {\n    String[] vcfFilenames = FileUtil.getFiles(input, \"*.vcf.gz$|*.vcf$\");\n    // MAF file for QC report\n    mafWriter = new LineWriter(outputMaf);\n    // excluded SNPS\n    logWriter = new LineWriter(FileUtil.path(excludeLog, \"snps-excluded.txt\"));\n    logWriter.write(\"#Position\" + \"\\t\" + \"FilterType\" + \"\\t\" + \" Info\");\n    // excluded chunks\n    excludedChunkWriter = new LineWriter(FileUtil.path(excludeLog, \"chunks-excluded.txt\"));\n    excludedChunkWriter.write(\"#Chunk\" + \"\\t\" + \"SNPs (#)\" + \"\\t\" + \"Reference Overlap (%)\" + \"\\t\" + \"Low Sample Call Rates (#)\");\n    Arrays.sort(vcfFilenames);\n    for (String vcfFilename : vcfFilenames) {\n        System.out.println(vcfFilename);\n        VcfFile myvcfFile = VcfFileUtil.load(vcfFilename, chunkSize, true);\n        if (VcfFileUtil.isChrX(myvcfFile.getChromosome())) {\n            VcfFileUtil.prepareChrXEagle(myvcfFile, chunks);\n            throw new IOException(\"new chrX workflow under prepartion\");\n        } else {\n            // chunkfile manifest\n            metafileWriter = new LineWriter(FileUtil.path(chunkfile, myvcfFile.getChromosome()));\n            processFile(myvcfFile);\n            metafileWriter.close();\n        }\n    }\n    mafWriter.close();\n    logWriter.close();\n    excludedChunkWriter.close();\n    return true;\n}",
        "rpfc": "public boolean start() throws IOException, InterruptedException {\n    String[] vcfFilenames = FileUtil.getFiles(input, \"*.vcf.gz$|*.vcf$\");\n    // MAF file for QC report\n    mafWriter = new LineWriter(outputMaf);\n    // excluded SNPS\n    logWriter = new LineWriter(FileUtil.path(excludeLog, \"snps-excluded.txt\"));\n    logWriter.write(\"#Position\" + \"\\t\" + \"FilterType\" + \"\\t\" + \" Info\");\n    // excluded chunks\n    excludedChunkWriter = new LineWriter(FileUtil.path(excludeLog, \"chunks-excluded.txt\"));\n    excludedChunkWriter.write(\"#Chunk\" + \"\\t\" + \"SNPs (#)\" + \"\\t\" + \"Reference Overlap (%)\" + \"\\t\" + \"Low Sample Call Rates (#)\");\n    Arrays.sort(vcfFilenames);\n    for (String vcfFilename : vcfFilenames) {\n        System.out.println(vcfFilename);\n        VcfFile myvcfFile = VcfFileUtil.load(vcfFilename, chunkSize, true);\n        if (VcfFileUtil.isChrX(myvcfFile.getChromosome())) {\n            List<String> splits = VcfFileUtil.prepareChrXEagle(myvcfFile, chunks);\n            for (String split : splits) {\n                myvcfFile = VcfFileUtil.load(split, chunkSize, true);\n                myvcfFile.setChrX(true);\n                String chr = split.contains(VcfFileUtil.X_NON_PAR) ? VcfFileUtil.X_NON_PAR : VcfFileUtil.X_PAR;\n                metafileWriter = new LineWriter(FileUtil.path(chunkfile, chr));\n                processFile(myvcfFile);\n                metafileWriter.close();\n            }\n        } else {\n            // chunkfile manifest\n            metafileWriter = new LineWriter(FileUtil.path(chunkfile, myvcfFile.getChromosome()));\n            processFile(myvcfFile);\n            metafileWriter.close();\n        }\n    }\n    mafWriter.close();\n    logWriter.close();\n    excludedChunkWriter.close();\n    return true;\n}",
        "tuc": "public void testCountSamplesInCreatedChunk() throws IOException {\n    String configFolder = \"test-data/configs/hapmap-chr1\";\n    String inputFolder = \"test-data/data/simulated-chip-1chr-imputation\";\n    // create workflow context\n    WorkflowTestContext context = buildContext(inputFolder, \"hapmap2\");\n    // get output directory\n    String out = context.getOutput(\"excludeLog\");\n    // create step instance\n    QcStatisticsMock qcStats = new QcStatisticsMock(configFolder);\n    // run and test\n    run(context, qcStats);\n    for (File file : new File(out).listFiles()) {\n        if (file.getName().endsWith(\"chunk_1_80000001_100000000.vcf.gz\")) {\n            VCFFileReader vcfReader = new VCFFileReader(file, false);\n            CloseableIterator<VariantContext> it = vcfReader.iterator();\n            if (it.hasNext()) {\n                VariantContext a = it.next();\n                assertEquals(255, a.getNSamples());\n            }\n            vcfReader.close();\n        }\n    }\n    FileUtil.deleteDirectory(new File(out));\n}",
        "label": 0
    },
    {
        "repo_name": "gluster___glusterfs-hadoop",
        "commit": "904c19f186038dab24c2b8ae07de1f3fefd2ce22",
        "commit_message": "merge with other pul reqs\n",
        "p_path": "src/main/java/org/apache/hadoop/fs/glusterfs/GlusterFileSystem.java",
        "t_path": "src/test/java/org/gluster/test/TestGluster.java",
        "p_name": "initialize",
        "t_name": "testfileio",
        "lpfc": "public void initialize(URI uri, Configuration conf) throws IOException {\n    boolean ret = false;\n    String volName = null;\n    String remoteGFSServer = null;\n    String needQuickRead = null;\n    if (this.mounted)\n        return;\n    System.out.println(\"Initializing GlusterFS\");\n    try {\n        volName = conf.get(\"fs.glusterfs.volname\", null);\n        glusterMount = conf.get(\"fs.glusterfs.mount\", null);\n        remoteGFSServer = conf.get(\"fs.glusterfs.server\", null);\n        needQuickRead = conf.get(\"quick.slave.io\", null);\n        if ((volName.length() == 0) || (remoteGFSServer.length() == 0) || (glusterMount.length() == 0))\n            throw new RuntimeException(\"Not enough info for FUSE Mount : volname=\" + volName + \",server=\" + remoteGFSServer + \",glustermount=\" + glusterMount);\n        ret = FUSEMount(volName, remoteGFSServer, glusterMount);\n        if (!ret) {\n            throw new RuntimeException(\"Failed to init Gluster FS\");\n        }\n        if ((needQuickRead.length() != 0) && (needQuickRead.equalsIgnoreCase(\"yes\") || needQuickRead.equalsIgnoreCase(\"on\") || needQuickRead.equals(\"1\")))\n            this.quickSlaveIO = true;\n        this.mounted = true;\n        this.glusterFs = FileSystem.getLocal(conf);\n        this.workingDir = new Path(glusterMount);\n        this.uri = URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n        this.xattr = new GlusterFSXattr();\n        InetAddress addr = InetAddress.getLocalHost();\n        this.hostname = addr.getHostName();\n        setConf(conf);\n    } catch (Exception e) {\n        throw new RuntimeException(e);\n    }\n}",
        "rpfc": "public void initialize(URI uri, Configuration conf) throws IOException {\n    boolean ret = false;\n    String volName = null;\n    String remoteGFSServer = null;\n    String needQuickRead = null;\n    boolean autoMount = true;\n    if (this.mounted)\n        return;\n    System.out.println(\"Initializing GlusterFS\");\n    try {\n        volName = conf.get(\"fs.glusterfs.volname\", null);\n        glusterMount = conf.get(\"fs.glusterfs.mount\", null);\n        remoteGFSServer = conf.get(\"fs.glusterfs.server\", null);\n        needQuickRead = conf.get(\"quick.slave.io\", null);\n        autoMount = conf.getBoolean(\"fs.glusterfs.automount\", true);\n        if ((volName.length() == 0) || (remoteGFSServer.length() == 0) || (glusterMount.length() == 0))\n            throw new RuntimeException(\"Not enough info for FUSE Mount : volname=\" + volName + \",server=\" + remoteGFSServer + \",glustermount=\" + glusterMount);\n        ret = FUSEMount(volName, remoteGFSServer, glusterMount);\n        if (!ret) {\n            throw new RuntimeException(\"Failed to init Gluster FS\");\n        }\n        if (autoMount) {\n            ret = FUSEMount(volName, remoteGFSServer, glusterMount);\n            if (!ret) {\n                throw new RuntimeException(\"Initialize: Failed to mount GlusterFS \");\n            }\n        }\n        if ((needQuickRead.length() != 0) && (needQuickRead.equalsIgnoreCase(\"yes\") || needQuickRead.equalsIgnoreCase(\"on\") || needQuickRead.equals(\"1\")))\n            this.quickSlaveIO = true;\n        this.mounted = true;\n        this.glusterFs = FileSystem.getLocal(conf);\n        this.workingDir = new Path(glusterMount);\n        this.uri = URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n        this.xattr = new GlusterFSXattr();\n        InetAddress addr = InetAddress.getLocalHost();\n        this.hostname = addr.getHostName();\n        setConf(conf);\n    } catch (Exception e) {\n        throw new RuntimeException(e);\n    }\n}",
        "tuc": "public void testFileIO() throws Exception {\n    Path subDir1 = new Path(\"tfio_dir.1\");\n    Path file1 = new Path(\"tfio_dir.1/foo.1\");\n    Path baseDir = new Path(\"tfio_testDirs1\");\n    gfs.mkdirs(baseDir);\n    assertTrue(gfs.isDirectory(baseDir));\n    //\t        gfs.setWorkingDirectory(baseDir);\n    gfs.mkdirs(subDir1);\n    FSDataOutputStream s1 = gfs.create(file1, true, 4096, (short) 1, (long) 4096, null);\n    int bufsz = 4096;\n    byte[] data = new byte[bufsz];\n    for (int i = 0; i < data.length; i++) data[i] = (byte) (i % 16);\n    // write 4 bytes and read them back; read API should return a byte per call\n    s1.write(32);\n    s1.write(32);\n    s1.write(32);\n    s1.write(32);\n    // write some data\n    s1.write(data, 0, data.length);\n    // flush out the changes\n    s1.close();\n    // Read the stuff back and verify it is correct\n    FSDataInputStream s2 = gfs.open(file1, 4096);\n    int v;\n    v = s2.read();\n    assertEquals(v, 32);\n    v = s2.read();\n    assertEquals(v, 32);\n    v = s2.read();\n    assertEquals(v, 32);\n    v = s2.read();\n    assertEquals(v, 32);\n    assertEquals(s2.available(), data.length);\n    byte[] buf = new byte[bufsz];\n    s2.read(buf, 0, buf.length);\n    for (int i = 0; i < data.length; i++) assertEquals(data[i], buf[i]);\n    assertEquals(s2.available(), 0);\n    s2.close();\n    gfs.delete(file1, true);\n    assertFalse(gfs.exists(file1));\n    gfs.delete(subDir1, true);\n    assertFalse(gfs.exists(subDir1));\n    gfs.delete(baseDir, true);\n    assertFalse(gfs.exists(baseDir));\n    System.out.println(\"Deleting \" + file1.toUri());\n    gfs.delete(subDir1);\n    gfs.delete(file1);\n    gfs.delete(baseDir);\n}",
        "label": 0
    },
    {
        "repo_name": "genepi___imputationserver",
        "commit": "bc0511f6ef239648725757514bd602e50fe0cf6f",
        "commit_message": "Add logging for md5",
        "p_path": "src/main/java/genepi/imputationserver/steps/CompressionEncryption.java",
        "t_path": "src/test/java/genepi/imputationserver/steps/ImputationTest.java",
        "p_name": "run",
        "t_name": "testpipelinewitheagle",
        "lpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    String workingDirectory = getFolder(CompressionEncryption.class);\n    String output = context.get(\"outputimputation\");\n    String outputScores = context.get(\"outputScores\");\n    String localOutput = context.get(\"local\");\n    String aesEncryption = context.get(\"aesEncryption\");\n    String mode = context.get(\"mode\");\n    String password = context.get(\"password\");\n    PgsPanel pgsPanel = PgsPanel.loadFromProperties(context.getData(\"pgsPanel\"));\n    boolean phasingOnly = false;\n    if (mode != null && mode.equals(\"phasing\")) {\n        phasingOnly = true;\n    }\n    // read config if mails should be sent\n    String folderConfig = getFolder(CompressionEncryption.class);\n    File jobConfig = new File(FileUtil.path(folderConfig, \"job.config\"));\n    DefaultPreferenceStore store = new DefaultPreferenceStore();\n    if (jobConfig.exists()) {\n        store.load(jobConfig);\n    } else {\n        context.log(\"Configuration file '\" + jobConfig.getAbsolutePath() + \"' not available. Use default values.\");\n    }\n    String notification = \"no\";\n    if (store.getString(\"minimac.sendmail\") != null && !store.getString(\"minimac.sendmail\").equals(\"\")) {\n        notification = store.getString(\"minimac.sendmail\");\n    }\n    String serverUrl = \"https://imputationserver.sph.umich.edu\";\n    if (store.getString(\"server.url\") != null && !store.getString(\"server.url\").isEmpty()) {\n        serverUrl = store.getString(\"server.url\");\n    }\n    String sanityCheck = \"yes\";\n    if (store.getString(\"sanitycheck\") != null && !store.getString(\"sanitycheck\").equals(\"\")) {\n        sanityCheck = store.getString(\"sanitycheck\");\n    }\n    if (password == null || (password != null && password.equals(\"auto\"))) {\n        password = PasswordCreator.createPassword();\n    }\n    try {\n        context.beginTask(\"Export data...\");\n        // get sorted directories\n        List<String> folders = HdfsUtil.getDirectories(output);\n        Map<String, ExportObject> chromosomes = new HashMap<String, ExportObject>();\n        for (String folder : folders) {\n            String name = FileUtil.getFilename(folder);\n            context.println(\"Prepare files for chromosome \" + name);\n            List<String> data = new Vector<String>();\n            List<String> header = new Vector<String>();\n            List<String> info = new Vector<String>();\n            header = findFiles(folder, \".header.dose.vcf.gz\");\n            if (phasingOnly) {\n                data = findFiles(folder, \".phased.vcf.gz\");\n            } else {\n                data = findFiles(folder, \".data.dose.vcf.gz\");\n                info = findFiles(folder, \".info\");\n            }\n            // combine all X. to one folder\n            if (name.startsWith(\"X.\")) {\n                name = \"X\";\n            }\n            ExportObject export = chromosomes.get(name);\n            if (export == null) {\n                export = new ExportObject();\n            }\n            ArrayList<String> currentDataList = export.getDataFiles();\n            currentDataList.addAll(data);\n            export.setDataFiles(currentDataList);\n            ArrayList<String> currentHeaderList = export.getHeaderFiles();\n            currentHeaderList.addAll(header);\n            export.setHeaderFiles(currentHeaderList);\n            ArrayList<String> currentInfoList = export.getInfoFiles();\n            currentInfoList.addAll(info);\n            export.setInfoFiles(currentInfoList);\n            chromosomes.put(name, export);\n        }\n        Set<String> chromosomesSet = chromosomes.keySet();\n        boolean lastChromosome = false;\n        int index = 0;\n        ZipParameters param = new ZipParameters();\n        param.setEncryptFiles(true);\n        param.setEncryptionMethod(EncryptionMethod.ZIP_STANDARD);\n        String checksumFilename = FileUtil.path(localOutput, \"results.md5\");\n        LineWriter writer = new LineWriter(checksumFilename);\n        for (String name : chromosomesSet) {\n            index++;\n            if (index == chromosomesSet.size()) {\n                lastChromosome = true;\n            }\n            ExportObject entry = chromosomes.get(name);\n            context.println(\"Export and merge chromosome \" + name);\n            // resort for chrX only\n            if (name.equals(\"X\")) {\n                Collections.sort(entry.getDataFiles(), new ChrXComparator());\n                Collections.sort(entry.getInfoFiles(), new ChrXComparator());\n            }\n            // create temp fir\n            String temp = FileUtil.path(localOutput, \"temp\");\n            FileUtil.createDirectory(temp);\n            // output files\n            String dosageOutput;\n            String infoOutput = \"\";\n            if (phasingOnly) {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".phased.vcf.gz\");\n            } else {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".dose.vcf.gz\");\n                infoOutput = FileUtil.path(temp, \"chr\" + name + \".info.gz\");\n                FileMerger.mergeAndGzInfo(entry.getInfoFiles(), infoOutput);\n            }\n            MergedVcfFile vcfFile = new MergedVcfFile(dosageOutput);\n            // simple header check\n            String headerLine = null;\n            for (String file : entry.getHeaderFiles()) {\n                context.println(\"Read header file \" + file);\n                LineReader reader = null;\n                try {\n                    reader = new LineReader(HdfsUtil.open(file));\n                    while (reader.next()) {\n                        String line = reader.get();\n                        if (line.startsWith(\"#CHROM\")) {\n                            if (headerLine != null) {\n                                if (headerLine.equals(line)) {\n                                    context.println(\"  Header is the same as header of first file.\");\n                                } else {\n                                    context.println(\"  ERROR: Header is different as header of first file.\");\n                                    context.println(headerLine);\n                                    context.println(line);\n                                    throw new Exception(\"Different sample order in chunks.\");\n                                }\n                            } else {\n                                headerLine = line;\n                                vcfFile.addFile(HdfsUtil.open(file));\n                                context.println(\"  Keep this header as first header.\");\n                            }\n                        }\n                    }\n                    if (reader != null) {\n                        reader.close();\n                    }\n                    HdfsUtil.delete(file);\n                } catch (Exception e) {\n                    if (reader != null) {\n                        reader.close();\n                    }\n                    StringWriter errors = new StringWriter();\n                    e.printStackTrace(new PrintWriter(errors));\n                    context.println(\"Error reading header file: \" + errors.toString());\n                }\n            }\n            if (headerLine == null || headerLine.trim().isEmpty()) {\n                throw new Exception(\"No valid header file found\");\n            }\n            // add data files\n            for (String file : entry.getDataFiles()) {\n                context.println(\"Read file \" + file);\n                vcfFile.addFile(HdfsUtil.open(file));\n                HdfsUtil.delete(file);\n            }\n            vcfFile.close();\n            if (sanityCheck.equals(\"yes\") && lastChromosome) {\n                context.log(\"Run tabix on chromosome \" + name + \"...\");\n                Command tabix = new Command(FileUtil.path(workingDirectory, \"bin\", \"tabix\"));\n                tabix.setSilent(false);\n                tabix.setParams(\"-f\", dosageOutput);\n                if (tabix.execute() != 0) {\n                    context.endTask(\"Error during index creation: \" + tabix.getStdOut(), WorkflowContext.ERROR);\n                    return false;\n                }\n                context.log(\"Tabix done.\");\n            }\n            if (aesEncryption != null && aesEncryption.equals(\"yes\")) {\n                param.setEncryptionMethod(EncryptionMethod.AES);\n                param.setAesKeyStrength(AesKeyStrength.KEY_STRENGTH_256);\n                param.setCompressionMethod(CompressionMethod.DEFLATE);\n                param.setCompressionLevel(CompressionLevel.NORMAL);\n            }\n            // create zip file\n            ArrayList<File> files = new ArrayList<File>();\n            files.add(new File(dosageOutput));\n            if (!phasingOnly) {\n                files.add(new File(infoOutput));\n            }\n            String fileName = \"chr_\" + name + \".zip\";\n            String filePath = FileUtil.path(localOutput, fileName);\n            ZipFile file = new ZipFile(new File(filePath), password.toCharArray());\n            file.addFiles(files, param);\n            String checksum = FileChecksum.HashFile(new File(filePath), FileChecksum.Algorithm.MD5);\n            writer.write(checksum + \" \" + fileName);\n            // delete temp dir\n            FileUtil.deleteDirectory(temp);\n            IExternalWorkspace externalWorkspace = context.getExternalWorkspace();\n            if (externalWorkspace != null) {\n                long start = System.currentTimeMillis();\n                context.log(\"External Workspace '\" + externalWorkspace.getName() + \"' found\");\n                context.log(\"Start file upload: \" + filePath);\n                String url = externalWorkspace.upload(\"local\", file.getFile());\n                long end = (System.currentTimeMillis() - start) / 1000;\n                context.log(\"Upload finished in  \" + end + \" sec. File Location: \" + url);\n                context.log(\"Add \" + localOutput + \" to custom download\");\n                String size = FileUtils.byteCountToDisplaySize(file.getFile().length());\n                context.addDownload(\"local\", fileName, size, url);\n                FileUtil.deleteFile(filePath);\n                context.log(\"File deleted: \" + filePath);\n            } else {\n                context.log(\"No external Workspace set.\");\n            }\n        }\n        writer.close();\n        // delete temporary files\n        HdfsUtil.delete(output);\n        // Export calculated risk scores\n        if (pgsPanel != null) {\n            context.println(\"Exporting PGS scores...\");\n            String temp2 = FileUtil.path(localOutput, \"temp2\");\n            FileUtil.createDirectory(temp2);\n            List<String> scoreList = HdfsUtil.getFiles(outputScores);\n            String[] chunksScores = new String[scoreList.size() / 2];\n            String[] chunksReports = new String[scoreList.size() / 2];\n            int chunksScoresCount = 0;\n            int chunksReportsCount = 0;\n            for (String score : scoreList) {\n                String filename = FileUtil.getFilename(score);\n                String localPath = FileUtil.path(temp2, filename);\n                HdfsUtil.get(score, localPath);\n                if (score.endsWith(\".json\")) {\n                    chunksReports[chunksReportsCount] = localPath;\n                    chunksReportsCount++;\n                } else {\n                    chunksScores[chunksScoresCount] = localPath;\n                    chunksScoresCount++;\n                }\n            }\n            String outputFileScores = FileUtil.path(temp2, \"scores.txt\");\n            String outputFileReports = FileUtil.path(temp2, \"report.json\");\n            String outputFileHtml = FileUtil.path(localOutput, \"report.html\");\n            //disable ansi\n            TaskService.setAnsiSupport(false);\n            MergeScoreTask mergeScore = new MergeScoreTask();\n            mergeScore.setInputs(chunksScores);\n            mergeScore.setOutput(outputFileScores);\n            TaskService.run(mergeScore);\n            MergeReportTask mergeReport = new MergeReportTask();\n            mergeReport.setInputs(chunksReports);\n            mergeReport.setOutput(outputFileReports);\n            TaskService.run(mergeReport);\n            ReportFile report = mergeReport.getResult();\n            String folder = getFolder(CompressionEncryption.class);\n            MetaFile metaFile = MetaFile.load(FileUtil.path(folder, \"pgs-catalog.json\"));\n            report.mergeWithMeta(metaFile);\n            CreateHtmlReportTask htmlReport = new CreateHtmlReportTask();\n            htmlReport.setReport(report);\n            htmlReport.setData(mergeScore.getResult());\n            htmlReport.setOutput(outputFileHtml);\n            TaskService.run(htmlReport);\n            String fileName = \"scores.zip\";\n            String filePath = FileUtil.path(localOutput, fileName);\n            ArrayList<File> files = new ArrayList<File>();\n            files.add(new File(outputFileScores));\n            ZipFile file = new ZipFile(new File(filePath), password.toCharArray());\n            file.addFiles(files, param);\n            context.println(\"Exported PGS scores to \" + outputFileScores + \".\");\n            FileUtil.deleteDirectory(temp2);\n        }\n        context.endTask(\"Exported data.\", WorkflowContext.OK);\n    } catch (Exception e) {\n        e.printStackTrace();\n        context.endTask(\"Data export failed: \" + e.getMessage(), WorkflowContext.ERROR);\n        return false;\n    }\n    // submit counters!\n    context.submitCounter(\"samples\");\n    context.submitCounter(\"genotypes\");\n    context.submitCounter(\"chromosomes\");\n    context.submitCounter(\"runs\");\n    // submit panel and phasing method counters\n    String reference = context.get(\"refpanel\");\n    String phasing = context.get(\"phasing\");\n    context.submitCounter(\"refpanel_\" + reference);\n    context.submitCounter(\"phasing_\" + phasing);\n    context.submitCounter(\"23andme-input\");\n    // send email\n    if (notification.equals(\"yes\")) {\n        Object mail = context.getData(\"cloudgene.user.mail\");\n        Object name = context.getData(\"cloudgene.user.name\");\n        if (mail != null) {\n            String subject = \"Job \" + context.getJobId() + \" is complete.\";\n            String message = \"Dear \" + name + \",\\nthe password for the imputation results is: \" + password + \"\\n\\nThe results can be downloaded from \" + serverUrl + \"/start.html#!jobs/\" + context.getJobId() + \"/results\";\n            try {\n                context.sendMail(subject, message);\n                context.ok(\"We have sent an email to <b>\" + mail + \"</b> with the password.\");\n                return true;\n            } catch (Exception e) {\n                context.error(\"Data compression failed: \" + e.getMessage());\n                return false;\n            }\n        } else {\n            context.error(\"No email address found. Please enter your email address (Account -> Profile).\");\n            return false;\n        }\n    } else {\n        context.ok(\"Email notification is disabled. All results are encrypted with password <b>\" + password + \"</b>\");\n        return true;\n    }\n}",
        "rpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    String workingDirectory = getFolder(CompressionEncryption.class);\n    String output = context.get(\"outputimputation\");\n    String outputScores = context.get(\"outputScores\");\n    String localOutput = context.get(\"local\");\n    String aesEncryption = context.get(\"aesEncryption\");\n    String mode = context.get(\"mode\");\n    String password = context.get(\"password\");\n    PgsPanel pgsPanel = PgsPanel.loadFromProperties(context.getData(\"pgsPanel\"));\n    boolean phasingOnly = false;\n    if (mode != null && mode.equals(\"phasing\")) {\n        phasingOnly = true;\n    }\n    // read config if mails should be sent\n    String folderConfig = getFolder(CompressionEncryption.class);\n    File jobConfig = new File(FileUtil.path(folderConfig, \"job.config\"));\n    DefaultPreferenceStore store = new DefaultPreferenceStore();\n    if (jobConfig.exists()) {\n        store.load(jobConfig);\n    } else {\n        context.log(\"Configuration file '\" + jobConfig.getAbsolutePath() + \"' not available. Use default values.\");\n    }\n    String notification = \"no\";\n    if (store.getString(\"minimac.sendmail\") != null && !store.getString(\"minimac.sendmail\").equals(\"\")) {\n        notification = store.getString(\"minimac.sendmail\");\n    }\n    String serverUrl = \"https://imputationserver.sph.umich.edu\";\n    if (store.getString(\"server.url\") != null && !store.getString(\"server.url\").isEmpty()) {\n        serverUrl = store.getString(\"server.url\");\n    }\n    String sanityCheck = \"yes\";\n    if (store.getString(\"sanitycheck\") != null && !store.getString(\"sanitycheck\").equals(\"\")) {\n        sanityCheck = store.getString(\"sanitycheck\");\n    }\n    if (password == null || (password != null && password.equals(\"auto\"))) {\n        password = PasswordCreator.createPassword();\n    }\n    try {\n        context.beginTask(\"Export data...\");\n        // get sorted directories\n        List<String> folders = HdfsUtil.getDirectories(output);\n        Map<String, ExportObject> chromosomes = new HashMap<String, ExportObject>();\n        for (String folder : folders) {\n            String name = FileUtil.getFilename(folder);\n            context.println(\"Prepare files for chromosome \" + name);\n            List<String> data = new Vector<String>();\n            List<String> header = new Vector<String>();\n            List<String> info = new Vector<String>();\n            header = findFiles(folder, \".header.dose.vcf.gz\");\n            if (phasingOnly) {\n                data = findFiles(folder, \".phased.vcf.gz\");\n            } else {\n                data = findFiles(folder, \".data.dose.vcf.gz\");\n                info = findFiles(folder, \".info\");\n            }\n            // combine all X. to one folder\n            if (name.startsWith(\"X.\")) {\n                name = \"X\";\n            }\n            ExportObject export = chromosomes.get(name);\n            if (export == null) {\n                export = new ExportObject();\n            }\n            ArrayList<String> currentDataList = export.getDataFiles();\n            currentDataList.addAll(data);\n            export.setDataFiles(currentDataList);\n            ArrayList<String> currentHeaderList = export.getHeaderFiles();\n            currentHeaderList.addAll(header);\n            export.setHeaderFiles(currentHeaderList);\n            ArrayList<String> currentInfoList = export.getInfoFiles();\n            currentInfoList.addAll(info);\n            export.setInfoFiles(currentInfoList);\n            chromosomes.put(name, export);\n        }\n        Set<String> chromosomesSet = chromosomes.keySet();\n        boolean lastChromosome = false;\n        int index = 0;\n        ZipParameters param = new ZipParameters();\n        param.setEncryptFiles(true);\n        param.setEncryptionMethod(EncryptionMethod.ZIP_STANDARD);\n        String checksumFilename = FileUtil.path(localOutput, \"results.md5\");\n        LineWriter writer = new LineWriter(checksumFilename);\n        for (String name : chromosomesSet) {\n            index++;\n            if (index == chromosomesSet.size()) {\n                lastChromosome = true;\n            }\n            ExportObject entry = chromosomes.get(name);\n            context.println(\"Export and merge chromosome \" + name);\n            // resort for chrX only\n            if (name.equals(\"X\")) {\n                Collections.sort(entry.getDataFiles(), new ChrXComparator());\n                Collections.sort(entry.getInfoFiles(), new ChrXComparator());\n            }\n            // create temp fir\n            String temp = FileUtil.path(localOutput, \"temp\");\n            FileUtil.createDirectory(temp);\n            // output files\n            String dosageOutput;\n            String infoOutput = \"\";\n            if (phasingOnly) {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".phased.vcf.gz\");\n            } else {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".dose.vcf.gz\");\n                infoOutput = FileUtil.path(temp, \"chr\" + name + \".info.gz\");\n                FileMerger.mergeAndGzInfo(entry.getInfoFiles(), infoOutput);\n            }\n            MergedVcfFile vcfFile = new MergedVcfFile(dosageOutput);\n            // simple header check\n            String headerLine = null;\n            for (String file : entry.getHeaderFiles()) {\n                context.println(\"Read header file \" + file);\n                LineReader reader = null;\n                try {\n                    reader = new LineReader(HdfsUtil.open(file));\n                    while (reader.next()) {\n                        String line = reader.get();\n                        if (line.startsWith(\"#CHROM\")) {\n                            if (headerLine != null) {\n                                if (headerLine.equals(line)) {\n                                    context.println(\"  Header is the same as header of first file.\");\n                                } else {\n                                    context.println(\"  ERROR: Header is different as header of first file.\");\n                                    context.println(headerLine);\n                                    context.println(line);\n                                    throw new Exception(\"Different sample order in chunks.\");\n                                }\n                            } else {\n                                headerLine = line;\n                                vcfFile.addFile(HdfsUtil.open(file));\n                                context.println(\"  Keep this header as first header.\");\n                            }\n                        }\n                    }\n                    if (reader != null) {\n                        reader.close();\n                    }\n                    HdfsUtil.delete(file);\n                } catch (Exception e) {\n                    if (reader != null) {\n                        reader.close();\n                    }\n                    StringWriter errors = new StringWriter();\n                    e.printStackTrace(new PrintWriter(errors));\n                    context.println(\"Error reading header file: \" + errors.toString());\n                }\n            }\n            if (headerLine == null || headerLine.trim().isEmpty()) {\n                throw new Exception(\"No valid header file found\");\n            }\n            // add data files\n            for (String file : entry.getDataFiles()) {\n                context.println(\"Read file \" + file);\n                vcfFile.addFile(HdfsUtil.open(file));\n                HdfsUtil.delete(file);\n            }\n            vcfFile.close();\n            if (sanityCheck.equals(\"yes\") && lastChromosome) {\n                context.log(\"Run tabix on chromosome \" + name + \"...\");\n                Command tabix = new Command(FileUtil.path(workingDirectory, \"bin\", \"tabix\"));\n                tabix.setSilent(false);\n                tabix.setParams(\"-f\", dosageOutput);\n                if (tabix.execute() != 0) {\n                    context.endTask(\"Error during index creation: \" + tabix.getStdOut(), WorkflowContext.ERROR);\n                    return false;\n                }\n                context.log(\"Tabix done.\");\n            }\n            if (aesEncryption != null && aesEncryption.equals(\"yes\")) {\n                param.setEncryptionMethod(EncryptionMethod.AES);\n                param.setAesKeyStrength(AesKeyStrength.KEY_STRENGTH_256);\n                param.setCompressionMethod(CompressionMethod.DEFLATE);\n                param.setCompressionLevel(CompressionLevel.NORMAL);\n            }\n            // create zip file\n            ArrayList<File> files = new ArrayList<File>();\n            files.add(new File(dosageOutput));\n            if (!phasingOnly) {\n                files.add(new File(infoOutput));\n            }\n            String fileName = \"chr_\" + name + \".zip\";\n            String filePath = FileUtil.path(localOutput, fileName);\n            ZipFile file = new ZipFile(new File(filePath), password.toCharArray());\n            file.addFiles(files, param);\n            context.log(\"Creating file checksum for \" + filePath);\n            long checksumStart = System.currentTimeMillis();\n            String checksum = FileChecksum.HashFile(new File(filePath), FileChecksum.Algorithm.MD5);\n            writer.write(checksum + \" \" + fileName);\n            long checksumEnd = (System.currentTimeMillis() - checksumStart) / 1000;\n            context.log(\"File checksum for \" + filePath + \" created in \" + checksumEnd + \" seconds.\");\n            // delete temp dir\n            FileUtil.deleteDirectory(temp);\n            IExternalWorkspace externalWorkspace = context.getExternalWorkspace();\n            if (externalWorkspace != null) {\n                long start = System.currentTimeMillis();\n                context.log(\"External Workspace '\" + externalWorkspace.getName() + \"' found\");\n                context.log(\"Start file upload: \" + filePath);\n                String url = externalWorkspace.upload(\"local\", file.getFile());\n                long end = (System.currentTimeMillis() - start) / 1000;\n                context.log(\"Upload finished in  \" + end + \" sec. File Location: \" + url);\n                context.log(\"Add \" + localOutput + \" to custom download\");\n                String size = FileUtils.byteCountToDisplaySize(file.getFile().length());\n                context.addDownload(\"local\", fileName, size, url);\n                FileUtil.deleteFile(filePath);\n                context.log(\"File deleted: \" + filePath);\n            } else {\n                context.log(\"No external Workspace set.\");\n            }\n        }\n        writer.close();\n        // delete temporary files\n        HdfsUtil.delete(output);\n        // Export calculated risk scores\n        if (pgsPanel != null) {\n            context.println(\"Exporting PGS scores...\");\n            String temp2 = FileUtil.path(localOutput, \"temp2\");\n            FileUtil.createDirectory(temp2);\n            List<String> scoreList = HdfsUtil.getFiles(outputScores);\n            String[] chunksScores = new String[scoreList.size() / 2];\n            String[] chunksReports = new String[scoreList.size() / 2];\n            int chunksScoresCount = 0;\n            int chunksReportsCount = 0;\n            for (String score : scoreList) {\n                String filename = FileUtil.getFilename(score);\n                String localPath = FileUtil.path(temp2, filename);\n                HdfsUtil.get(score, localPath);\n                if (score.endsWith(\".json\")) {\n                    chunksReports[chunksReportsCount] = localPath;\n                    chunksReportsCount++;\n                } else {\n                    chunksScores[chunksScoresCount] = localPath;\n                    chunksScoresCount++;\n                }\n            }\n            String outputFileScores = FileUtil.path(temp2, \"scores.txt\");\n            String outputFileReports = FileUtil.path(temp2, \"report.json\");\n            String outputFileHtml = FileUtil.path(localOutput, \"report.html\");\n            // disable ansi\n            TaskService.setAnsiSupport(false);\n            MergeScoreTask mergeScore = new MergeScoreTask();\n            mergeScore.setInputs(chunksScores);\n            mergeScore.setOutput(outputFileScores);\n            TaskService.run(mergeScore);\n            MergeReportTask mergeReport = new MergeReportTask();\n            mergeReport.setInputs(chunksReports);\n            mergeReport.setOutput(outputFileReports);\n            TaskService.run(mergeReport);\n            ReportFile report = mergeReport.getResult();\n            String folder = getFolder(CompressionEncryption.class);\n            MetaFile metaFile = MetaFile.load(FileUtil.path(folder, \"pgs-catalog.json\"));\n            report.mergeWithMeta(metaFile);\n            CreateHtmlReportTask htmlReport = new CreateHtmlReportTask();\n            htmlReport.setReport(report);\n            htmlReport.setData(mergeScore.getResult());\n            htmlReport.setOutput(outputFileHtml);\n            TaskService.run(htmlReport);\n            String fileName = \"scores.zip\";\n            String filePath = FileUtil.path(localOutput, fileName);\n            ArrayList<File> files = new ArrayList<File>();\n            files.add(new File(outputFileScores));\n            ZipFile file = new ZipFile(new File(filePath), password.toCharArray());\n            file.addFiles(files, param);\n            context.println(\"Exported PGS scores to \" + outputFileScores + \".\");\n            FileUtil.deleteDirectory(temp2);\n        }\n        context.endTask(\"Exported data.\", WorkflowContext.OK);\n    } catch (Exception e) {\n        e.printStackTrace();\n        context.endTask(\"Data export failed: \" + e.getMessage(), WorkflowContext.ERROR);\n        return false;\n    }\n    // submit counters!\n    context.submitCounter(\"samples\");\n    context.submitCounter(\"genotypes\");\n    context.submitCounter(\"chromosomes\");\n    context.submitCounter(\"runs\");\n    // submit panel and phasing method counters\n    String reference = context.get(\"refpanel\");\n    String phasing = context.get(\"phasing\");\n    context.submitCounter(\"refpanel_\" + reference);\n    context.submitCounter(\"phasing_\" + phasing);\n    context.submitCounter(\"23andme-input\");\n    // send email\n    if (notification.equals(\"yes\")) {\n        Object mail = context.getData(\"cloudgene.user.mail\");\n        Object name = context.getData(\"cloudgene.user.name\");\n        if (mail != null) {\n            String subject = \"Job \" + context.getJobId() + \" is complete.\";\n            String message = \"Dear \" + name + \",\\nthe password for the imputation results is: \" + password + \"\\n\\nThe results can be downloaded from \" + serverUrl + \"/start.html#!jobs/\" + context.getJobId() + \"/results\";\n            try {\n                context.sendMail(subject, message);\n                context.ok(\"We have sent an email to <b>\" + mail + \"</b> with the password.\");\n                return true;\n            } catch (Exception e) {\n                context.error(\"Data compression failed: \" + e.getMessage());\n                return false;\n            }\n        } else {\n            context.error(\"No email address found. Please enter your email address (Account -> Profile).\");\n            return false;\n        }\n    } else {\n        context.ok(\"Email notification is disabled. All results are encrypted with password <b>\" + password + \"</b>\");\n        return true;\n    }\n}",
        "tuc": "/*\n\t * @Test public void testPipelineWithS3() throws IOException, ZipException {\n\t * \n\t * String configFolder = \"test-data/configs/hapmap-chr1\"; String inputFolder =\n\t * \"s3://imputationserver-aws-testdata/test-s3/hapmap300.chr1.recode.vcf.gz\";\n\t * \n\t * // create workflow context WorkflowTestContext context =\n\t * buildContext(inputFolder, \"hapmap2\");\n\t * \n\t * // create step instance InputValidation inputValidation = new\n\t * InputValidationMock(configFolder);\n\t * \n\t * // run and test boolean result = run(context, inputValidation);\n\t * \n\t * // check if step is failed assertEquals(true, result);\n\t * \n\t * // run qc to create chunkfile QcStatisticsMock qcStats = new\n\t * QcStatisticsMock(configFolder); result = run(context, qcStats);\n\t * \n\t * // add panel to hdfs importRefPanel(FileUtil.path(configFolder,\n\t * \"ref-panels\")); // importMinimacMap(\"test-data/B38_MAP_FILE.map\");\n\t * importBinaries(\"files/bin\");\n\t * \n\t * // run imputation ImputationMinimac3Mock imputation = new\n\t * ImputationMinimac3Mock(configFolder); result = run(context, imputation);\n\t * assertTrue(result);\n\t * \n\t * // run export CompressionEncryptionMock export = new\n\t * CompressionEncryptionMock(\"files\"); result = run(context, export);\n\t * assertTrue(result);\n\t * \n\t * ZipFile zipFile = new ZipFile(\"test-data/tmp/local/chr_1.zip\"); if\n\t * (zipFile.isEncrypted()) { zipFile.setPassword(PASSWORD); }\n\t * zipFile.extractAll(\"test-data/tmp\");\n\t * \n\t * VcfFile file = VcfFileUtil.load(\"test-data/tmp/chr1.dose.vcf.gz\", 100000000,\n\t * false);\n\t * \n\t * assertEquals(\"1\", file.getChromosome()); assertEquals(60,\n\t * file.getNoSamples()); assertEquals(true, file.isPhased());\n\t * \n\t * FileUtil.deleteDirectory(\"test-data/tmp\");\n\t * \n\t * }\n\t */\n/*\n\t * @Test public void testPipelineWithSFTP() throws IOException, ZipException,\n\t * InterruptedException {\n\t * \n\t * TestSFTPServer server = new TestSFTPServer(\"test-data/data\");\n\t * \n\t * String configFolder = \"test-data/configs/hapmap-chr20\"; String inputFolder =\n\t * \"sftp://localhost:8001/\" + new\n\t * File(\"test-data/data/chr20-phased\").getAbsolutePath() + \";\" +\n\t * TestSFTPServer.USERNAME + \";\" + TestSFTPServer.PASSWORD;\n\t * \n\t * // create workflow context WorkflowTestContext context =\n\t * buildContext(inputFolder, \"hapmap2\");\n\t * \n\t * // create step instance InputValidation inputValidation = new\n\t * InputValidationMock(configFolder);\n\t * \n\t * // run and test boolean result = run(context, inputValidation);\n\t * \n\t * // check if step is failed assertEquals(true, result);\n\t * \n\t * // run qc to create chunkfile QcStatisticsMock qcStats = new\n\t * QcStatisticsMock(configFolder); result = run(context, qcStats);\n\t * \n\t * // add panel to hdfs importRefPanel(FileUtil.path(configFolder,\n\t * \"ref-panels\")); // importMinimacMap(\"test-data/B38_MAP_FILE.map\");\n\t * importBinaries(\"files/bin\");\n\t * \n\t * // run imputation ImputationMinimac3Mock imputation = new\n\t * ImputationMinimac3Mock(configFolder); result = run(context, imputation);\n\t * assertTrue(result);\n\t * \n\t * // run export CompressionEncryptionMock export = new\n\t * CompressionEncryptionMock(\"files\"); result = run(context, export);\n\t * assertTrue(result);\n\t * \n\t * ZipFile zipFile = new ZipFile(\"test-data/tmp/local/chr_20.zip\"); if\n\t * (zipFile.isEncrypted()) { zipFile.setPassword(PASSWORD); }\n\t * zipFile.extractAll(\"test-data/tmp\");\n\t * \n\t * VcfFile file = VcfFileUtil.load(\"test-data/tmp/chr20.dose.vcf.gz\", 100000000,\n\t * false);\n\t * \n\t * assertEquals(\"20\", file.getChromosome()); assertEquals(51,\n\t * file.getNoSamples()); assertEquals(true, file.isPhased());\n\t * assertEquals(TOTAL_REFPANEL_CHR20_B37 + ONLY_IN_INPUT, file.getNoSnps());\n\t * \n\t * FileUtil.deleteDirectory(\"test-data/tmp\");\n\t * \n\t * server.stop();\n\t * \n\t * }\n\t * \n\t * @Test public void testPipelineWithWrongSFTPCredentials() throws IOException,\n\t * ZipException, InterruptedException {\n\t * \n\t * TestSFTPServer server = new TestSFTPServer(\"test-data/data\");\n\t * \n\t * String configFolder = \"test-data/configs/hapmap-chr20\"; String inputFolder =\n\t * \"sftp://localhost:8001/\" + new File(\"data/chr20-phased\").getAbsolutePath() +\n\t * \";\" + \"WRONG_USERNAME\" + \";\" + TestSFTPServer.PASSWORD;\n\t * \n\t * // create workflow context WorkflowTestContext context =\n\t * buildContext(inputFolder, \"hapmap2\");\n\t * \n\t * // create step instance InputValidation inputValidation = new\n\t * InputValidationMock(configFolder);\n\t * \n\t * // run and test boolean result = run(context, inputValidation);\n\t * \n\t * // check if step is failed assertEquals(false, result);\n\t * \n\t * server.stop();\n\t * \n\t * }\n\t */\n@Test\npublic void testPipelineWithEagle() throws IOException, ZipException {\n    String configFolder = \"test-data/configs/hapmap-chr20\";\n    String inputFolder = \"test-data/data/chr20-unphased\";\n    // create workflow context\n    WorkflowTestContext context = buildContext(inputFolder, \"hapmap2\");\n    // run qc to create chunkfile\n    QcStatisticsMock qcStats = new QcStatisticsMock(configFolder);\n    boolean result = run(context, qcStats);\n    assertTrue(result);\n    assertTrue(context.hasInMemory(\"Remaining sites in total: 7,735\"));\n    // add panel to hdfs\n    importRefPanel(FileUtil.path(configFolder, \"ref-panels\"));\n    // importMinimacMap(\"test-data/B38_MAP_FILE.map\");\n    importBinaries(\"files/bin\");\n    // run imputation\n    ImputationMinimac3Mock imputation = new ImputationMinimac3Mock(configFolder);\n    result = run(context, imputation);\n    assertTrue(result);\n    // run export\n    CompressionEncryptionMock export = new CompressionEncryptionMock(\"files\");\n    result = run(context, export);\n    assertTrue(result);\n    ZipFile zipFile = new ZipFile(\"test-data/tmp/local/chr_20.zip\", PASSWORD.toCharArray());\n    zipFile.extractAll(\"test-data/tmp\");\n    VcfFile file = VcfFileUtil.load(\"test-data/tmp/chr20.dose.vcf.gz\", 100000000, false);\n    assertEquals(\"20\", file.getChromosome());\n    assertEquals(51, file.getNoSamples());\n    assertEquals(true, file.isPhased());\n    assertEquals(TOTAL_REFPANEL_CHR20_B37, file.getNoSnps());\n    int snpInInfo = getLineCount(\"test-data/tmp/chr20.info.gz\") - 1;\n    assertEquals(snpInInfo, file.getNoSnps());\n    FileUtil.deleteDirectory(\"test-data/tmp\");\n}",
        "label": 0
    },
    {
        "repo_name": "powsybl___pypowsybl",
        "commit": "7a64912c297f4cc05bdab3d8deb1e3dc911c39c3",
        "commit_message": "Name update (#838)\n\nSigned-off-by: Geoffroy Jamgotchian <geoffroy.jamgotchian@rte-france.com>\r\nCo-authored-by: HugoKulesza <94374655+HugoKulesza@users.noreply.github.com>",
        "p_path": "java/src/main/java/com/powsybl/dataframe/network/NetworkDataframes.java",
        "t_path": "java/src/test/java/com/powsybl/dataframe/DataframeMapperBuilderTest.java",
        "p_name": "loads",
        "t_name": "test",
        "lpfc": "static NetworkDataframeMapper loads() {\n    return NetworkDataframeMapperBuilder.ofStream(Network::getLoadStream, getOrThrow(Network::getLoad, \"Load\")).stringsIndex(\"id\", Load::getId).strings(\"name\", l -> l.getOptionalName().orElse(\"\")).enums(\"type\", LoadType.class, Load::getLoadType).doubles(\"p0\", (l, context) -> perUnitPQ(context, l.getP0()), (l, p, context) -> l.setP0(unPerUnitPQ(context, p))).doubles(\"q0\", (l, context) -> perUnitPQ(context, l.getQ0()), (l, q, context) -> l.setQ0(unPerUnitPQ(context, q))).doubles(\"p\", getPerUnitP(), setPerUnitP()).doubles(\"q\", getPerUnitQ(), setPerUnitQ()).doubles(\"i\", (l, context) -> perUnitI(context, l.getTerminal())).strings(\"voltage_level_id\", getVoltageLevelId()).strings(\"bus_id\", l -> getBusId(l.getTerminal())).strings(\"bus_breaker_bus_id\", getBusBreakerViewBusId(), NetworkDataframes::setBusBreakerViewBusId, false).ints(\"node\", l -> getNode(l.getTerminal()), false).booleans(\"connected\", l -> l.getTerminal().isConnected(), connectInjection()).booleans(\"fictitious\", Identifiable::isFictitious, Identifiable::setFictitious, false).addProperties().build();\n}",
        "rpfc": "static NetworkDataframeMapper loads() {\n    return NetworkDataframeMapperBuilder.ofStream(Network::getLoadStream, getOrThrow(Network::getLoad, \"Load\")).stringsIndex(\"id\", Load::getId).strings(\"name\", l -> l.getOptionalName().orElse(\"\"), Identifiable::setName).enums(\"type\", LoadType.class, Load::getLoadType).doubles(\"p0\", (l, context) -> perUnitPQ(context, l.getP0()), (l, p, context) -> l.setP0(unPerUnitPQ(context, p))).doubles(\"q0\", (l, context) -> perUnitPQ(context, l.getQ0()), (l, q, context) -> l.setQ0(unPerUnitPQ(context, q))).doubles(\"p\", getPerUnitP(), setPerUnitP()).doubles(\"q\", getPerUnitQ(), setPerUnitQ()).doubles(\"i\", (l, context) -> perUnitI(context, l.getTerminal())).strings(\"voltage_level_id\", getVoltageLevelId()).strings(\"bus_id\", l -> getBusId(l.getTerminal())).strings(\"bus_breaker_bus_id\", getBusBreakerViewBusId(), NetworkDataframes::setBusBreakerViewBusId, false).ints(\"node\", l -> getNode(l.getTerminal()), false).booleans(\"connected\", l -> l.getTerminal().isConnected(), connectInjection()).booleans(\"fictitious\", Identifiable::isFictitious, Identifiable::setFictitious, false).addProperties().build();\n}",
        "tuc": "@Test\nvoid test() {\n    DataframeMapper<Container, Void> mapper = new DataframeMapperBuilder<Container, Element, Void>().itemsProvider(Container::getElements).stringsIndex(\"id\", Element::getId).strings(\"str\", Element::getStrValue).ints(\"int\", Element::getIntValue).doubles(\"double\", Element::getDoubleValue).enums(\"color\", Color.class, Element::getColorValue).build();\n    Container container = new Container(new Element(\"el1\", \"val1\", 1, 10, Color.RED), new Element(\"el2\", \"val2\", 2, 20, Color.BLUE));\n    List<com.powsybl.dataframe.impl.Series> series = new ArrayList<>();\n    mapper.createDataframe(container, new DefaultDataframeHandler(series::add), new DataframeFilter());\n    assertThat(series).extracting(com.powsybl.dataframe.impl.Series::getName).containsExactly(\"id\", \"str\", \"int\", \"double\", \"color\");\n}",
        "label": 0
    },
    {
        "repo_name": "killbilling___recurly-java-library",
        "commit": "189b5b6263e04cf739718487cf4338482f723234",
        "commit_message": "Add vatNumber to ShippingAddress class\n",
        "p_path": "src/main/java/com/ning/billing/recurly/model/ShippingAddress.java",
        "t_path": "src/test/java/com/ning/billing/recurly/model/TestBillingInfo.java",
        "p_name": "equals",
        "t_name": "testserialization",
        "lpfc": "@Override\npublic boolean equals(final Object o) {\n    if (this == o)\n        return true;\n    if (o == null || getClass() != o.getClass())\n        return false;\n    final ShippingAddress that = (ShippingAddress) o;\n    if (address1 != null ? !address1.equals(that.address1) : that.address1 != null) {\n        return false;\n    }\n    if (address2 != null ? !address2.equals(that.address2) : that.address2 != null) {\n        return false;\n    }\n    if (city != null ? !city.equals(that.city) : that.city != null) {\n        return false;\n    }\n    if (company != null ? !company.equals(that.company) : that.company != null) {\n        return false;\n    }\n    if (country != null ? !country.equals(that.country) : that.country != null) {\n        return false;\n    }\n    if (email != null ? !email.equals(that.email) : that.email != null) {\n        return false;\n    }\n    if (firstName != null ? !firstName.equals(that.firstName) : that.firstName != null) {\n        return false;\n    }\n    if (id != null ? !id.equals(that.id) : that.id != null) {\n        return false;\n    }\n    if (lastName != null ? !lastName.equals(that.lastName) : that.lastName != null) {\n        return false;\n    }\n    if (nickname != null ? !nickname.equals(that.nickname) : that.nickname != null) {\n        return false;\n    }\n    if (phone != null ? !phone.equals(that.phone) : that.phone != null) {\n        return false;\n    }\n    if (state != null ? !state.equals(that.state) : that.state != null) {\n        return false;\n    }\n    if (updatedAt != null ? updatedAt.compareTo(that.updatedAt) != 0 : that.updatedAt != null) {\n        return false;\n    }\n    if (zip != null ? !zip.equals(that.zip) : that.zip != null) {\n        return false;\n    }\n    if (geoCode != null ? !geoCode.equals(that.geoCode) : that.geoCode != null) {\n        return false;\n    }\n    return true;\n}",
        "rpfc": "@Override\npublic boolean equals(final Object o) {\n    if (this == o)\n        return true;\n    if (o == null || getClass() != o.getClass())\n        return false;\n    final ShippingAddress that = (ShippingAddress) o;\n    if (address1 != null ? !address1.equals(that.address1) : that.address1 != null) {\n        return false;\n    }\n    if (address2 != null ? !address2.equals(that.address2) : that.address2 != null) {\n        return false;\n    }\n    if (city != null ? !city.equals(that.city) : that.city != null) {\n        return false;\n    }\n    if (company != null ? !company.equals(that.company) : that.company != null) {\n        return false;\n    }\n    if (country != null ? !country.equals(that.country) : that.country != null) {\n        return false;\n    }\n    if (email != null ? !email.equals(that.email) : that.email != null) {\n        return false;\n    }\n    if (firstName != null ? !firstName.equals(that.firstName) : that.firstName != null) {\n        return false;\n    }\n    if (id != null ? !id.equals(that.id) : that.id != null) {\n        return false;\n    }\n    if (lastName != null ? !lastName.equals(that.lastName) : that.lastName != null) {\n        return false;\n    }\n    if (nickname != null ? !nickname.equals(that.nickname) : that.nickname != null) {\n        return false;\n    }\n    if (phone != null ? !phone.equals(that.phone) : that.phone != null) {\n        return false;\n    }\n    if (state != null ? !state.equals(that.state) : that.state != null) {\n        return false;\n    }\n    if (updatedAt != null ? updatedAt.compareTo(that.updatedAt) != 0 : that.updatedAt != null) {\n        return false;\n    }\n    if (zip != null ? !zip.equals(that.zip) : that.zip != null) {\n        return false;\n    }\n    if (geoCode != null ? !geoCode.equals(that.geoCode) : that.geoCode != null) {\n        return false;\n    }\n    if (vatNumber != null ? !vatNumber.equals(that.vatNumber) : that.vatNumber != null) {\n        return false;\n    }\n    return true;\n}",
        "tuc": "@Test(groups = \"fast\")\npublic void testSerialization() throws Exception {\n    final BillingInfo billingInfo = new BillingInfo();\n    billingInfo.setAddress1(randomString());\n    billingInfo.setAddress2(randomString());\n    billingInfo.setCardType(randomString());\n    billingInfo.setCity(randomString());\n    billingInfo.setCompany(randomString());\n    billingInfo.setCountry(randomString());\n    billingInfo.setFirstName(randomString());\n    billingInfo.setFirstSix(randomString());\n    billingInfo.setIpAddress(randomString());\n    billingInfo.setIpAddressCountry(randomString());\n    billingInfo.setLastFour(randomString());\n    billingInfo.setLastName(randomString());\n    billingInfo.setMonth(3);\n    billingInfo.setNumber(randomString());\n    billingInfo.setPhone(randomString());\n    billingInfo.setState(randomString());\n    billingInfo.setVatNumber(randomString());\n    //CVV can have leading zeroes\n    billingInfo.setVerificationValue(\"009\");\n    billingInfo.setYear(Integer.MIN_VALUE);\n    billingInfo.setZip(randomString());\n    billingInfo.setGeoCode(randomString());\n    billingInfo.setGatewayToken(randomString());\n    billingInfo.setGatewayCode(randomString());\n    billingInfo.setAmazonBillingAgreementId(randomString());\n    billingInfo.setAmazonRegion(randomString());\n    final String xml = xmlMapper.writeValueAsString(billingInfo);\n    Assert.assertEquals(xmlMapper.readValue(xml, BillingInfo.class), billingInfo);\n}",
        "label": 0
    },
    {
        "repo_name": "checkmarx-ltd___cx-flow",
        "commit": "f3a10a89c0c86874b218d4a0d53afdb772fb9597",
        "commit_message": "Removing default JIRA bug tracker implementation from cmd execution\n",
        "p_path": "src/main/java/com/custodela/machina/MachinaApplicationCmd.java",
        "t_path": "src/test/java/com/custodela/machina/service/JiraServiceTest.java",
        "p_name": "run",
        "t_name": "createissuetest",
        "lpfc": "@Override\npublic void run(ApplicationArguments arg) {\n    String bugTracker;\n    String application;\n    String namespace;\n    String repoName;\n    String repoUrl;\n    String branch;\n    String assignee;\n    List<String> emails;\n    String file;\n    String libFile;\n    String preset = null;\n    String team;\n    String cxProject;\n    String config;\n    List<String> severity;\n    List<String> cwe;\n    List<String> category;\n    List<String> status;\n    List<String> excludeFiles;\n    List<String> excludeFolders;\n    boolean osa;\n    MachinaOverride o = null;\n    ObjectMapper mapper = new ObjectMapper();\n    if (!arg.containsOption(\"scan\") && !arg.containsOption(\"parse\") && !arg.containsOption(\"batch\") && !arg.containsOption(\"project\")) {\n        log.error(\"--scan | --parse | --batch | --project option must be specified\");\n        exit(1);\n    }\n    if (arg.containsOption(\"config\")) {\n        config = arg.getOptionValues(\"config\").get(0);\n        try {\n            o = mapper.readValue(new File(config), MachinaOverride.class);\n        } catch (IOException e) {\n            e.printStackTrace();\n            log.error(\"Error reading config file, ignoring...\");\n        }\n    }\n    bugTracker = ((ScanUtils.empty(arg.getOptionValues(\"bug-tracker\")))) ? BugTracker.Type.JIRA.getType() : arg.getOptionValues(\"bug-tracker\").get(0);\n    file = ((ScanUtils.empty(arg.getOptionValues(\"f\")))) ? null : arg.getOptionValues(\"f\").get(0);\n    libFile = ((ScanUtils.empty(arg.getOptionValues(\"lib-file\")))) ? null : arg.getOptionValues(\"lib-file\").get(0);\n    repoName = ((ScanUtils.empty(arg.getOptionValues(\"repo-name\")))) ? null : arg.getOptionValues(\"repo-name\").get(0);\n    repoUrl = ((ScanUtils.empty(arg.getOptionValues(\"repo-url\")))) ? null : arg.getOptionValues(\"repo-url\").get(0);\n    branch = ((ScanUtils.empty(arg.getOptionValues(\"branch\")))) ? null : arg.getOptionValues(\"branch\").get(0);\n    namespace = ((ScanUtils.empty(arg.getOptionValues(\"namespace\")))) ? null : arg.getOptionValues(\"namespace\").get(0);\n    team = ((ScanUtils.empty(arg.getOptionValues(\"cx-team\")))) ? null : arg.getOptionValues(\"cx-team\").get(0);\n    cxProject = ((ScanUtils.empty(arg.getOptionValues(\"cx-project\")))) ? null : arg.getOptionValues(\"cx-project\").get(0);\n    application = ((ScanUtils.empty(arg.getOptionValues(\"app\")))) ? null : arg.getOptionValues(\"app\").get(0);\n    assignee = ((ScanUtils.empty(arg.getOptionValues(\"assignee\")))) ? null : arg.getOptionValues(\"assignee\").get(0);\n    osa = arg.getOptionValues(\"osa\") != null;\n    emails = arg.getOptionValues(\"emails\");\n    severity = arg.getOptionValues(\"severity\");\n    category = arg.getOptionValues(\"category\");\n    cwe = arg.getOptionValues(\"cwe\");\n    status = arg.getOptionValues(\"status\");\n    excludeFiles = arg.getOptionValues(\"exclude-files\");\n    excludeFolders = arg.getOptionValues(\"exclude-folders\");\n    boolean bb = arg.containsOption(\"bb\");\n    boolean bbs = arg.containsOption(\"bbs\");\n    if (((ScanUtils.empty(namespace) && ScanUtils.empty(repoName) && ScanUtils.empty(branch)) && ScanUtils.empty(application)) && !arg.containsOption(\"batch\")) {\n        log.error(\"Namespace/Repo/Branch or Application (app) must be provided\");\n        exit(1);\n    }\n    List<Filter> filters;\n    if (!ScanUtils.empty(severity) || !ScanUtils.empty(cwe) || !ScanUtils.empty(category) || !ScanUtils.empty(status)) {\n        filters = ScanUtils.getFilters(severity, cwe, category, status);\n    } else {\n        filters = ScanUtils.getFilters(machinaProperties.getFilterSeverity(), machinaProperties.getFilterCwe(), machinaProperties.getFilterCategory(), machinaProperties.getFilterStatus());\n    }\n    BugTracker.Type bugType;\n    try {\n        if (ScanUtils.empty(bugTracker)) {\n            bugTracker = machinaProperties.getBugTracker();\n        }\n        bugType = BugTracker.Type.valueOf(bugTracker);\n    } catch (IllegalArgumentException e) {\n        log.debug(\"Determine if custom bean is being used {}\", bugTracker);\n        if (machinaProperties.getBugTrackerImpl() == null || !machinaProperties.getBugTrackerImpl().contains(bugTracker)) {\n            throw e;\n        }\n        bugType = BugTracker.Type.CUSTOM;\n    }\n    ScanRequest.Product p;\n    if (osa) {\n        if (libFile == null) {\n            log.error(\"Both vulnerabilities file (f) and libraries file (lib-file) must be provided for OSA\");\n            exit(1);\n        }\n        p = ScanRequest.Product.CXOSA;\n    } else {\n        p = ScanRequest.Product.CX;\n    }\n    BugTracker bt = null;\n    String gitUrlAuth = null;\n    switch(bugType) {\n        case JIRA:\n            bt = BugTracker.builder().type(bugType).projectKey(jiraProperties.getProject()).issueType(jiraProperties.getIssueType()).assignee(assignee).priorities(jiraProperties.getPriorities()).closeTransitionField(jiraProperties.getCloseTransitionField()).closeTransitionValue(jiraProperties.getCloseTransitionValue()).closedStatus(jiraProperties.getClosedStatus()).closeTransition(jiraProperties.getCloseTransition()).openStatus(jiraProperties.getOpenStatus()).openTransition(jiraProperties.getOpenTransition()).fields(jiraProperties.getFields()).build();\n            break;\n        case EMAIL:\n            break;\n        case CUSTOM:\n            log.info(\"Using custom bean implementation  for bug tracking\");\n            bt = BugTracker.builder().type(bugType).customBean(bugTracker).build();\n            break;\n        default:\n            log.warn(\"No supported bug tracking type provided\");\n    }\n    ScanRequest request = ScanRequest.builder().application(application).product(p).namespace(namespace).team(team).project(cxProject).repoName(repoName).repoUrl(repoUrl).repoUrlWithAuth(gitUrlAuth).repoType(ScanRequest.Repository.NA).branch(branch).refs(null).email(emails).incremental(false).scanPreset(preset).excludeFolders(excludeFolders).excludeFiles(excludeFiles).bugTracker(bt).filters(filters).build();\n    request = ScanUtils.overrideMap(request, o);\n    if (bb) {\n        request.setRepoType(ScanRequest.Repository.BITBUCKETSERVER);\n    } else if (bbs) {\n        request.setRepoType(ScanRequest.Repository.BITBUCKETSERVER);\n        repoUrl = repoUrl.replaceAll(\"\\\\/scm\\\\/\", \"/projects/\");\n        repoUrl = repoUrl.replaceAll(\"\\\\/[\\\\w-]+.git$\", \"/repos$0\");\n        repoUrl = repoUrl.replaceAll(\".git$\", \"\");\n        repoUrl = repoUrl.concat(\"/browse\");\n        request.putAdditionalMetadata(\"BITBUCKET_BROWSE\", repoUrl);\n    }\n    try {\n        if (arg.containsOption(\"parse\")) {\n            File f = new File(file);\n            if (!f.exists()) {\n                log.error(\"Result File not found {}\", file);\n                exit(2);\n            }\n            if (osa) {\n                File libs = new File(libFile);\n                if (!libs.exists()) {\n                    log.error(\"Library File not found {}\", file);\n                    exit(2);\n                }\n                cxOsaParse(request, f, libs);\n            } else {\n                if (arg.containsOption(\"offline\")) {\n                    cxProperties.setOffline(true);\n                }\n                log.info(\"Processing Checkmarx result file {}\", file);\n                cxParse(request, f);\n            }\n        } else if (arg.containsOption(\"batch\")) {\n            log.info(\"Executing batch process\");\n            cxBatch(request);\n        } else if (arg.containsOption(\"project\")) {\n            if (ScanUtils.empty(team) || ScanUtils.empty(cxProject)) {\n                log.error(\"team and cx-project must be provided when --project option is used\");\n                exit(2);\n            }\n            cxResults(request);\n        } else if (arg.containsOption(\"scan\")) {\n            log.info(\"Executing scan process\");\n            cxScan(request, file);\n        }\n    } catch (Exception e) {\n        log.error(\"An error occurred while processing request\");\n        e.printStackTrace();\n        exit(10);\n    }\n    log.info(\"Completed Successfully\");\n    exit(0);\n}",
        "rpfc": "@Override\npublic void run(ApplicationArguments arg) {\n    String bugTracker;\n    String application;\n    String namespace;\n    String repoName;\n    String repoUrl;\n    String branch;\n    String assignee;\n    List<String> emails;\n    String file;\n    String libFile;\n    String preset = null;\n    String team;\n    String cxProject;\n    String config;\n    List<String> severity;\n    List<String> cwe;\n    List<String> category;\n    List<String> status;\n    List<String> excludeFiles;\n    List<String> excludeFolders;\n    boolean osa;\n    MachinaOverride o = null;\n    ObjectMapper mapper = new ObjectMapper();\n    if (!arg.containsOption(\"scan\") && !arg.containsOption(\"parse\") && !arg.containsOption(\"batch\") && !arg.containsOption(\"project\")) {\n        log.error(\"--scan | --parse | --batch | --project option must be specified\");\n        exit(1);\n    }\n    if (arg.containsOption(\"config\")) {\n        config = arg.getOptionValues(\"config\").get(0);\n        try {\n            o = mapper.readValue(new File(config), MachinaOverride.class);\n        } catch (IOException e) {\n            e.printStackTrace();\n            log.error(\"Error reading config file, ignoring...\");\n        }\n    }\n    bugTracker = ((ScanUtils.empty(arg.getOptionValues(\"bug-tracker\")))) ? null : arg.getOptionValues(\"bug-tracker\").get(0);\n    file = ((ScanUtils.empty(arg.getOptionValues(\"f\")))) ? null : arg.getOptionValues(\"f\").get(0);\n    libFile = ((ScanUtils.empty(arg.getOptionValues(\"lib-file\")))) ? null : arg.getOptionValues(\"lib-file\").get(0);\n    repoName = ((ScanUtils.empty(arg.getOptionValues(\"repo-name\")))) ? null : arg.getOptionValues(\"repo-name\").get(0);\n    repoUrl = ((ScanUtils.empty(arg.getOptionValues(\"repo-url\")))) ? null : arg.getOptionValues(\"repo-url\").get(0);\n    branch = ((ScanUtils.empty(arg.getOptionValues(\"branch\")))) ? null : arg.getOptionValues(\"branch\").get(0);\n    namespace = ((ScanUtils.empty(arg.getOptionValues(\"namespace\")))) ? null : arg.getOptionValues(\"namespace\").get(0);\n    team = ((ScanUtils.empty(arg.getOptionValues(\"cx-team\")))) ? null : arg.getOptionValues(\"cx-team\").get(0);\n    cxProject = ((ScanUtils.empty(arg.getOptionValues(\"cx-project\")))) ? null : arg.getOptionValues(\"cx-project\").get(0);\n    application = ((ScanUtils.empty(arg.getOptionValues(\"app\")))) ? null : arg.getOptionValues(\"app\").get(0);\n    assignee = ((ScanUtils.empty(arg.getOptionValues(\"assignee\")))) ? null : arg.getOptionValues(\"assignee\").get(0);\n    osa = arg.getOptionValues(\"osa\") != null;\n    emails = arg.getOptionValues(\"emails\");\n    severity = arg.getOptionValues(\"severity\");\n    category = arg.getOptionValues(\"category\");\n    cwe = arg.getOptionValues(\"cwe\");\n    status = arg.getOptionValues(\"status\");\n    excludeFiles = arg.getOptionValues(\"exclude-files\");\n    excludeFolders = arg.getOptionValues(\"exclude-folders\");\n    boolean bb = arg.containsOption(\"bb\");\n    boolean bbs = arg.containsOption(\"bbs\");\n    if (((ScanUtils.empty(namespace) && ScanUtils.empty(repoName) && ScanUtils.empty(branch)) && ScanUtils.empty(application)) && !arg.containsOption(\"batch\")) {\n        log.error(\"Namespace/Repo/Branch or Application (app) must be provided\");\n        exit(1);\n    }\n    List<Filter> filters;\n    if (!ScanUtils.empty(severity) || !ScanUtils.empty(cwe) || !ScanUtils.empty(category) || !ScanUtils.empty(status)) {\n        filters = ScanUtils.getFilters(severity, cwe, category, status);\n    } else {\n        filters = ScanUtils.getFilters(machinaProperties.getFilterSeverity(), machinaProperties.getFilterCwe(), machinaProperties.getFilterCategory(), machinaProperties.getFilterStatus());\n    }\n    BugTracker.Type bugType;\n    try {\n        if (ScanUtils.empty(bugTracker)) {\n            bugTracker = machinaProperties.getBugTracker();\n        }\n        bugType = BugTracker.Type.valueOf(bugTracker);\n    } catch (IllegalArgumentException e) {\n        log.debug(\"Determine if custom bean is being used {}\", bugTracker);\n        if (machinaProperties.getBugTrackerImpl() == null || !machinaProperties.getBugTrackerImpl().contains(bugTracker)) {\n            throw e;\n        }\n        bugType = BugTracker.Type.CUSTOM;\n    }\n    ScanRequest.Product p;\n    if (osa) {\n        if (libFile == null) {\n            log.error(\"Both vulnerabilities file (f) and libraries file (lib-file) must be provided for OSA\");\n            exit(1);\n        }\n        p = ScanRequest.Product.CXOSA;\n    } else {\n        p = ScanRequest.Product.CX;\n    }\n    BugTracker bt = null;\n    String gitUrlAuth = null;\n    switch(bugType) {\n        case JIRA:\n            bt = BugTracker.builder().type(bugType).projectKey(jiraProperties.getProject()).issueType(jiraProperties.getIssueType()).assignee(assignee).priorities(jiraProperties.getPriorities()).closeTransitionField(jiraProperties.getCloseTransitionField()).closeTransitionValue(jiraProperties.getCloseTransitionValue()).closedStatus(jiraProperties.getClosedStatus()).closeTransition(jiraProperties.getCloseTransition()).openStatus(jiraProperties.getOpenStatus()).openTransition(jiraProperties.getOpenTransition()).fields(jiraProperties.getFields()).build();\n            break;\n        case EMAIL:\n            break;\n        case CUSTOM:\n            log.info(\"Using custom bean implementation  for bug tracking\");\n            bt = BugTracker.builder().type(bugType).customBean(bugTracker).build();\n            break;\n        default:\n            log.warn(\"No supported bug tracking type provided\");\n    }\n    ScanRequest request = ScanRequest.builder().application(application).product(p).namespace(namespace).team(team).project(cxProject).repoName(repoName).repoUrl(repoUrl).repoUrlWithAuth(gitUrlAuth).repoType(ScanRequest.Repository.NA).branch(branch).refs(null).email(emails).incremental(false).scanPreset(preset).excludeFolders(excludeFolders).excludeFiles(excludeFiles).bugTracker(bt).filters(filters).build();\n    request = ScanUtils.overrideMap(request, o);\n    if (bb) {\n        request.setRepoType(ScanRequest.Repository.BITBUCKETSERVER);\n    } else if (bbs) {\n        request.setRepoType(ScanRequest.Repository.BITBUCKETSERVER);\n        repoUrl = repoUrl.replaceAll(\"\\\\/scm\\\\/\", \"/projects/\");\n        repoUrl = repoUrl.replaceAll(\"\\\\/[\\\\w-]+.git$\", \"/repos$0\");\n        repoUrl = repoUrl.replaceAll(\".git$\", \"\");\n        repoUrl = repoUrl.concat(\"/browse\");\n        request.putAdditionalMetadata(\"BITBUCKET_BROWSE\", repoUrl);\n    }\n    try {\n        if (arg.containsOption(\"parse\")) {\n            File f = new File(file);\n            if (!f.exists()) {\n                log.error(\"Result File not found {}\", file);\n                exit(2);\n            }\n            if (osa) {\n                File libs = new File(libFile);\n                if (!libs.exists()) {\n                    log.error(\"Library File not found {}\", file);\n                    exit(2);\n                }\n                cxOsaParse(request, f, libs);\n            } else {\n                if (arg.containsOption(\"offline\")) {\n                    cxProperties.setOffline(true);\n                }\n                log.info(\"Processing Checkmarx result file {}\", file);\n                cxParse(request, f);\n            }\n        } else if (arg.containsOption(\"batch\")) {\n            log.info(\"Executing batch process\");\n            cxBatch(request);\n        } else if (arg.containsOption(\"project\")) {\n            if (ScanUtils.empty(team) || ScanUtils.empty(cxProject)) {\n                log.error(\"team and cx-project must be provided when --project option is used\");\n                exit(2);\n            }\n            cxResults(request);\n        } else if (arg.containsOption(\"scan\")) {\n            log.info(\"Executing scan process\");\n            cxScan(request, file);\n        }\n    } catch (Exception e) {\n        log.error(\"An error occurred while processing request\");\n        e.printStackTrace();\n        exit(10);\n    }\n    log.info(\"Completed Successfully\");\n    exit(0);\n}",
        "tuc": "@Test\npublic void createIssueTest() {\n}",
        "label": 0
    },
    {
        "repo_name": "genepi___imputationserver",
        "commit": "b34d10127a81014491217d8546b1416d8722d5b7",
        "commit_message": "Update to build infos to java 11",
        "p_path": "src/main/java/genepi/imputationserver/steps/InputValidation.java",
        "t_path": "src/test/java/genepi/imputationserver/steps/FastQualityControlTest.java",
        "p_name": "run",
        "t_name": "testchrxmixedgenotypes",
        "lpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    URLClassLoader cl = (URLClassLoader) InputValidation.class.getClassLoader();\n    try {\n        URL url = cl.findResource(\"META-INF/MANIFEST.MF\");\n        Manifest manifest = new Manifest(url.openStream());\n        Attributes attr = manifest.getMainAttributes();\n        String buildVesion = attr.getValue(\"Version\");\n        String buildTime = attr.getValue(\"Build-Time\");\n        String builtBy = attr.getValue(\"Built-By\");\n        context.println(\"Version: \" + buildVesion + \" (Built by \" + builtBy + \" on \" + buildTime + \")\");\n    } catch (IOException E) {\n        // handle\n    }\n    if (!checkParameters(context)) {\n        return false;\n    }\n    if (!importVcfFiles(context)) {\n        return false;\n    }\n    return checkVcfFiles(context);\n}",
        "rpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    try {\n        URL url = InputValidation.class.getClassLoader().getResource(\"META-INF/MANIFEST.MF\");\n        Manifest manifest = new Manifest(url.openStream());\n        Attributes attr = manifest.getMainAttributes();\n        String buildVesion = attr.getValue(\"Version\");\n        String buildTime = attr.getValue(\"Build-Time\");\n        String builtBy = attr.getValue(\"Built-By\");\n        context.println(\"Version: \" + buildVesion + \" (Built by \" + builtBy + \" on \" + buildTime + \")\");\n    } catch (IOException E) {\n        // handle\n    }\n    if (!checkParameters(context)) {\n        return false;\n    }\n    if (!importVcfFiles(context)) {\n        return false;\n    }\n    return checkVcfFiles(context);\n}",
        "tuc": "@Test\npublic void testChrXMixedGenotypes() throws IOException, ZipException {\n    String configFolder = \"test-data/configs/hapmap-chrX\";\n    String inputFolder = \"test-data/data/chrX-unphased-mixed\";\n    File file = new File(\"test-data/tmp\");\n    if (file.exists()) {\n        FileUtil.deleteDirectory(file);\n    }\n    // create workflow context\n    WorkflowTestContext context = buildContext(inputFolder, \"phase1\");\n    // run qc to create chunkfile\n    FastQualityControlMock qcStats = new FastQualityControlMock(configFolder);\n    boolean result = run(context, qcStats);\n    assertFalse(result);\n    assertTrue(context.hasInMemory(\"Chromosome X nonPAR region includes > 10 % mixed genotypes.\"));\n    FileUtil.deleteDirectory(file);\n}",
        "label": 0
    },
    {
        "repo_name": "killbilling___recurly-java-library",
        "commit": "b5a1042746ef030bf499af86f569adcf34db90bd",
        "commit_message": "Corrected typo in Invoice\n\nDescription:\nThe usedTaxService condition for returning false should have\nreferrence invoice instead of that.\n",
        "p_path": "src/main/java/com/ning/billing/recurly/model/Invoice.java",
        "t_path": "src/test/java/com/ning/billing/recurly/model/TestInvoice.java",
        "p_name": "equals",
        "t_name": "testdeserialization",
        "lpfc": "@Override\npublic boolean equals(final Object o) {\n    if (this == o)\n        return true;\n    if (o == null || getClass() != o.getClass())\n        return false;\n    final Invoice invoice = (Invoice) o;\n    if (account != null ? !account.equals(invoice.account) : invoice.account != null) {\n        return false;\n    }\n    if (address != null ? !address.equals(invoice.address) : invoice.address != null) {\n        return false;\n    }\n    if (attemptNextCollectionAt != null ? attemptNextCollectionAt.compareTo(invoice.attemptNextCollectionAt) != 0 : invoice.attemptNextCollectionAt != null) {\n        return false;\n    }\n    if (balanceInCents != null ? !balanceInCents.equals(invoice.balanceInCents) : invoice.balanceInCents != null) {\n        return false;\n    }\n    if (closedAt != null ? closedAt.compareTo(invoice.closedAt) != 0 : invoice.closedAt != null) {\n        return false;\n    }\n    if (collectionMethod != null ? !collectionMethod.equals(invoice.collectionMethod) : invoice.collectionMethod != null) {\n        return false;\n    }\n    if (createdAt != null ? createdAt.compareTo(invoice.createdAt) != 0 : invoice.createdAt != null) {\n        return false;\n    }\n    if (currency != null ? !currency.equals(invoice.currency) : invoice.currency != null) {\n        return false;\n    }\n    if (customerNotes != null ? !customerNotes.equals(invoice.customerNotes) : invoice.customerNotes != null) {\n        return false;\n    }\n    if (discountInCents != null ? !discountInCents.equals(invoice.discountInCents) : invoice.discountInCents != null) {\n        return false;\n    }\n    if (invoiceNumber != null ? !invoiceNumber.equals(invoice.invoiceNumber) : invoice.invoiceNumber != null) {\n        return false;\n    }\n    if (invoiceNumberPrefix != null ? !invoiceNumberPrefix.equals(invoice.invoiceNumberPrefix) : invoice.invoiceNumberPrefix != null) {\n        return false;\n    }\n    if (lineItems != null ? !lineItems.equals(invoice.lineItems) : invoice.lineItems != null) {\n        return false;\n    }\n    if (netTerms != null ? !netTerms.equals(invoice.netTerms) : invoice.netTerms != null) {\n        return false;\n    }\n    if (originalInvoice != null ? !originalInvoice.equals(invoice.originalInvoice) : invoice.originalInvoice != null) {\n        return false;\n    }\n    if (originalInvoices != null ? !originalInvoices.equals(invoice.originalInvoices) : invoice.originalInvoices != null) {\n        return false;\n    }\n    if (origin != null ? !origin.equals(invoice.origin) : invoice.origin != null) {\n        return false;\n    }\n    if (poNumber != null ? !poNumber.equals(invoice.poNumber) : invoice.poNumber != null) {\n        return false;\n    }\n    if (recoveryReason != null ? !recoveryReason.equals(invoice.recoveryReason) : invoice.recoveryReason != null) {\n        return false;\n    }\n    if (shippingAddress != null ? !shippingAddress.equals(invoice.shippingAddress) : invoice.shippingAddress != null) {\n        return false;\n    }\n    if (state != null ? !state.equals(invoice.state) : invoice.state != null) {\n        return false;\n    }\n    if (subtotalBeforeDiscountInCents != null ? !subtotalBeforeDiscountInCents.equals(invoice.subtotalBeforeDiscountInCents) : invoice.subtotalBeforeDiscountInCents != null) {\n        return false;\n    }\n    if (subtotalInCents != null ? !subtotalInCents.equals(invoice.subtotalInCents) : invoice.subtotalInCents != null) {\n        return false;\n    }\n    if (surchargeInCents != null ? !surchargeInCents.equals(invoice.surchargeInCents) : invoice.surchargeInCents != null) {\n        return false;\n    }\n    if (refundableTotalInCents != null ? !refundableTotalInCents.equals(invoice.refundableTotalInCents) : invoice.refundableTotalInCents != null) {\n        return false;\n    }\n    if (taxInCents != null ? !taxInCents.equals(invoice.taxInCents) : invoice.taxInCents != null) {\n        return false;\n    }\n    if (totalInCents != null ? !totalInCents.equals(invoice.totalInCents) : invoice.totalInCents != null) {\n        return false;\n    }\n    if (taxRegion != null ? !taxRegion.equals(invoice.taxRegion) : invoice.taxRegion != null) {\n        return false;\n    }\n    if (taxType != null ? !taxType.equals(invoice.taxType) : invoice.taxType != null) {\n        return false;\n    }\n    if (taxRate != null ? !taxRate.equals(invoice.taxRate) : invoice.taxRate != null) {\n        return false;\n    }\n    if (taxDetails != null ? !taxDetails.equals(invoice.taxDetails) : invoice.taxDetails != null) {\n        return false;\n    }\n    if (usedTaxService != null ? !usedTaxService.equals(that.usedTaxService) : that.usedTaxService != null) {\n        return false;\n    }\n    if (termsAndConditions != null ? !termsAndConditions.equals(invoice.termsAndConditions) : invoice.termsAndConditions != null) {\n        return false;\n    }\n    if (transactions != null ? !transactions.equals(invoice.transactions) : invoice.transactions != null) {\n        return false;\n    }\n    if (creditPayments != null ? !creditPayments.equals(invoice.creditPayments) : invoice.creditPayments != null) {\n        return false;\n    }\n    if (type != null ? !type.equals(invoice.type) : invoice.type != null) {\n        return false;\n    }\n    if (updatedAt != null ? updatedAt.compareTo(invoice.updatedAt) != 0 : invoice.updatedAt != null) {\n        return false;\n    }\n    if (uuid != null ? !uuid.equals(invoice.uuid) : invoice.uuid != null) {\n        return false;\n    }\n    if (vatNumber != null ? !vatNumber.equals(invoice.vatNumber) : invoice.vatNumber != null) {\n        return false;\n    }\n    if (billingInfoUuid != null ? !billingInfoUuid.equals(invoice.billingInfoUuid) : invoice.billingInfoUuid != null) {\n        return false;\n    }\n    if (vatReverseChargeNotes != null ? !vatReverseChargeNotes.equals(invoice.vatReverseChargeNotes) : invoice.vatReverseChargeNotes != null) {\n        return false;\n    }\n    if (gatewayCode != null ? !gatewayCode.equals(invoice.gatewayCode) : invoice.gatewayCode != null) {\n        return false;\n    }\n    if (transactionType != null ? !transactionType.equals(invoice.transactionType) : invoice.transactionType != null) {\n        return false;\n    }\n    return true;\n}",
        "rpfc": "@Override\npublic boolean equals(final Object o) {\n    if (this == o)\n        return true;\n    if (o == null || getClass() != o.getClass())\n        return false;\n    final Invoice invoice = (Invoice) o;\n    if (account != null ? !account.equals(invoice.account) : invoice.account != null) {\n        return false;\n    }\n    if (address != null ? !address.equals(invoice.address) : invoice.address != null) {\n        return false;\n    }\n    if (attemptNextCollectionAt != null ? attemptNextCollectionAt.compareTo(invoice.attemptNextCollectionAt) != 0 : invoice.attemptNextCollectionAt != null) {\n        return false;\n    }\n    if (balanceInCents != null ? !balanceInCents.equals(invoice.balanceInCents) : invoice.balanceInCents != null) {\n        return false;\n    }\n    if (closedAt != null ? closedAt.compareTo(invoice.closedAt) != 0 : invoice.closedAt != null) {\n        return false;\n    }\n    if (collectionMethod != null ? !collectionMethod.equals(invoice.collectionMethod) : invoice.collectionMethod != null) {\n        return false;\n    }\n    if (createdAt != null ? createdAt.compareTo(invoice.createdAt) != 0 : invoice.createdAt != null) {\n        return false;\n    }\n    if (currency != null ? !currency.equals(invoice.currency) : invoice.currency != null) {\n        return false;\n    }\n    if (customerNotes != null ? !customerNotes.equals(invoice.customerNotes) : invoice.customerNotes != null) {\n        return false;\n    }\n    if (discountInCents != null ? !discountInCents.equals(invoice.discountInCents) : invoice.discountInCents != null) {\n        return false;\n    }\n    if (invoiceNumber != null ? !invoiceNumber.equals(invoice.invoiceNumber) : invoice.invoiceNumber != null) {\n        return false;\n    }\n    if (invoiceNumberPrefix != null ? !invoiceNumberPrefix.equals(invoice.invoiceNumberPrefix) : invoice.invoiceNumberPrefix != null) {\n        return false;\n    }\n    if (lineItems != null ? !lineItems.equals(invoice.lineItems) : invoice.lineItems != null) {\n        return false;\n    }\n    if (netTerms != null ? !netTerms.equals(invoice.netTerms) : invoice.netTerms != null) {\n        return false;\n    }\n    if (originalInvoice != null ? !originalInvoice.equals(invoice.originalInvoice) : invoice.originalInvoice != null) {\n        return false;\n    }\n    if (originalInvoices != null ? !originalInvoices.equals(invoice.originalInvoices) : invoice.originalInvoices != null) {\n        return false;\n    }\n    if (origin != null ? !origin.equals(invoice.origin) : invoice.origin != null) {\n        return false;\n    }\n    if (poNumber != null ? !poNumber.equals(invoice.poNumber) : invoice.poNumber != null) {\n        return false;\n    }\n    if (recoveryReason != null ? !recoveryReason.equals(invoice.recoveryReason) : invoice.recoveryReason != null) {\n        return false;\n    }\n    if (shippingAddress != null ? !shippingAddress.equals(invoice.shippingAddress) : invoice.shippingAddress != null) {\n        return false;\n    }\n    if (state != null ? !state.equals(invoice.state) : invoice.state != null) {\n        return false;\n    }\n    if (subtotalBeforeDiscountInCents != null ? !subtotalBeforeDiscountInCents.equals(invoice.subtotalBeforeDiscountInCents) : invoice.subtotalBeforeDiscountInCents != null) {\n        return false;\n    }\n    if (subtotalInCents != null ? !subtotalInCents.equals(invoice.subtotalInCents) : invoice.subtotalInCents != null) {\n        return false;\n    }\n    if (surchargeInCents != null ? !surchargeInCents.equals(invoice.surchargeInCents) : invoice.surchargeInCents != null) {\n        return false;\n    }\n    if (refundableTotalInCents != null ? !refundableTotalInCents.equals(invoice.refundableTotalInCents) : invoice.refundableTotalInCents != null) {\n        return false;\n    }\n    if (taxInCents != null ? !taxInCents.equals(invoice.taxInCents) : invoice.taxInCents != null) {\n        return false;\n    }\n    if (totalInCents != null ? !totalInCents.equals(invoice.totalInCents) : invoice.totalInCents != null) {\n        return false;\n    }\n    if (taxRegion != null ? !taxRegion.equals(invoice.taxRegion) : invoice.taxRegion != null) {\n        return false;\n    }\n    if (taxType != null ? !taxType.equals(invoice.taxType) : invoice.taxType != null) {\n        return false;\n    }\n    if (taxRate != null ? !taxRate.equals(invoice.taxRate) : invoice.taxRate != null) {\n        return false;\n    }\n    if (taxDetails != null ? !taxDetails.equals(invoice.taxDetails) : invoice.taxDetails != null) {\n        return false;\n    }\n    if (usedTaxService != null ? !usedTaxService.equals(invoice.usedTaxService) : invoice.usedTaxService != null) {\n        return false;\n    }\n    if (termsAndConditions != null ? !termsAndConditions.equals(invoice.termsAndConditions) : invoice.termsAndConditions != null) {\n        return false;\n    }\n    if (transactions != null ? !transactions.equals(invoice.transactions) : invoice.transactions != null) {\n        return false;\n    }\n    if (creditPayments != null ? !creditPayments.equals(invoice.creditPayments) : invoice.creditPayments != null) {\n        return false;\n    }\n    if (type != null ? !type.equals(invoice.type) : invoice.type != null) {\n        return false;\n    }\n    if (updatedAt != null ? updatedAt.compareTo(invoice.updatedAt) != 0 : invoice.updatedAt != null) {\n        return false;\n    }\n    if (uuid != null ? !uuid.equals(invoice.uuid) : invoice.uuid != null) {\n        return false;\n    }\n    if (vatNumber != null ? !vatNumber.equals(invoice.vatNumber) : invoice.vatNumber != null) {\n        return false;\n    }\n    if (billingInfoUuid != null ? !billingInfoUuid.equals(invoice.billingInfoUuid) : invoice.billingInfoUuid != null) {\n        return false;\n    }\n    if (vatReverseChargeNotes != null ? !vatReverseChargeNotes.equals(invoice.vatReverseChargeNotes) : invoice.vatReverseChargeNotes != null) {\n        return false;\n    }\n    if (gatewayCode != null ? !gatewayCode.equals(invoice.gatewayCode) : invoice.gatewayCode != null) {\n        return false;\n    }\n    if (transactionType != null ? !transactionType.equals(invoice.transactionType) : invoice.transactionType != null) {\n        return false;\n    }\n    return true;\n}",
        "tuc": "@Test(groups = \"fast\")\npublic void testDeserialization() throws Exception {\n    // See https://dev.recurly.com/docs/list-invoices\n    final String invoiceData = \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\" + \"<invoice href=\\\"https://api.recurly.com/v2/invoices/e3f0a9e084a2468480d00ee61b090d4d\\\">\\n\" + \"  <account href=\\\"https://api.recurly.com/v2/accounts/1\\\"/>\\n\" + \"  <original_invoices href=\\\"https://api.recurly.com/v2/invoices/1192/original_invoices\\\"/>\\n\" + \"  <uuid>421f7b7d414e4c6792938e7c49d552e9</uuid>\\n\" + \"  <state>open</state>\\n\" + \"  <invoice_number type=\\\"integer\\\">1402</invoice_number>\\n\" + \"  <invoice_number_prefix>FR</invoice_number_prefix>\\n\" + \"  <po_number>abc-123</po_number>\\n\" + \"  <vat_number></vat_number>\\n\" + \"  <subtotal_in_cents type=\\\"integer\\\">9900</subtotal_in_cents>\\n\" + \"  <tax_in_cents type=\\\"integer\\\">0</tax_in_cents>\\n\" + \"  <total_in_cents type=\\\"integer\\\">9900</total_in_cents>\\n\" + \"  <vat_reverse_charge_notes>Some reverse charge notes</vat_reverse_charge_notes>\\n\" + \"  <customer_notes>Some notes</customer_notes>\\n\" + \"  <terms_and_conditions>t and c</terms_and_conditions>\\n\" + \"  <gateway_code>Some Gateway Code</gateway_code>\\n\" + \"  <net_terms type=\\\"integer\\\">0</net_terms>\\n\" + \"  <currency>USD</currency>\\n\" + \"  <tax_type>usst</tax_type>\\n\" + \"  <tax_region>CA</tax_region>\\n\" + \"  <tax_rate type=\\\"float\\\">0.0875</tax_rate>\\n\" + \"  <tax_details type=\\\"array\\\">\\n\" + \"    <tax_detail>\\n\" + \"      <tax_type>GST</tax_type>\\n\" + \"      <tax_region>CA</tax_region>\\n\" + \"      <tax_rate type=\\\"float\\\">0.05</tax_rate>\\n\" + \"      <tax_in_cents type=\\\"integer\\\">20</tax_in_cents>\\n\" + \"    </tax_detail>\\n\" + \"  </tax_details>\\n\" + \"  <surcharge_in_cents type=\\\"integer\\\">100</surcharge_in_cents>\\n\" + \"  <created_at type=\\\"dateTime\\\">2011-08-25T12:00:00Z</created_at>\\n\" + \"  <updated_at type=\\\"dateTime\\\">2011-08-25T12:00:00Z</updated_at>\\n\" + \"  <closed_at type=\\\"dateTime\\\">2011-08-25T12:00:00Z</closed_at>\\n\" + \"    <address>\\n\" + \"        <first_name>John</first_name>\\n\" + \"        <last_name>Smith</last_name>\\n\" + \"        <name_on_account>John Smith</name_on_account>\\n\" + \"        <company>East Atlantic Trading Company</company>\\n\" + \"        <address1>123 Main St.</address1>\\n\" + \"        <address2 nil=\\\"nil\\\"></address2>\\n\" + \"        <city>San Francisco</city>\\n\" + \"        <state>CA</state>\\n\" + \"        <zip>94105</zip>\\n\" + \"        <country>US</country>\\n\" + \"        <phone nil=\\\"nil\\\"></phone>\\n\" + \"    </address>\\n\" + \"    <shipping_address>\\n\" + \"        <name>Tester Number 11</name>\\n\" + \"        <address1>123 Canal St.</address1>\\n\" + \"        <address2>Suite 101</address2>\\n\" + \"        <city>San Francisco</city>\\n\" + \"        <state>CA</state>\\n\" + \"        <zip>94105</zip>\\n\" + \"        <country>US</country>\\n\" + \"        <phone>555-222-1212</phone>\\n\" + \"    </shipping_address>\" + \"  <line_items type=\\\"array\\\">\\n\" + \"    <adjustment type=\\\"credit\\\" href=\\\"https://api.recurly.com/v2/adjustments/626db120a84102b1809909071c701c60\\\">\\n\" + \"      <account href=\\\"https://api.recurly.com/v2/accounts/1\\\"/>\\n\" + \"      <uuid>626db120a84102b1809909071c701c60</uuid>\\n\" + \"      <description>Charge for extra bandwidth</description>\\n\" + \"      <accounting_code>bandwidth</accounting_code>\\n\" + \"      <origin>charge</origin>\\n\" + \"      <unit_amount_in_cents type=\\\"integer\\\">5000</unit_amount_in_cents>\\n\" + \"      <quantity type=\\\"integer\\\">1</quantity>\\n\" + \"      <discount_in_cents type=\\\"integer\\\">0</discount_in_cents>\\n\" + \"      <tax_in_cents type=\\\"integer\\\">0</tax_in_cents>\\n\" + \"      <total_in_cents type=\\\"integer\\\">5000</total_in_cents>\\n\" + \"      <currency>USD</currency>\\n\" + \"      <taxable type=\\\"boolean\\\">false</taxable>\\n\" + \"      <start_date type=\\\"dateTime\\\">2011-08-31T03:30:00Z</start_date>\\n\" + \"      <end_date nil=\\\"nil\\\"></end_date>\\n\" + \"      <created_at type=\\\"dateTime\\\">2011-08-31T03:30:00Z</created_at>\\n\" + \"    </adjustment>\\n\" + \"  </line_items>\\n\" + \"  <transactions type=\\\"array\\\">\\n\" + \"  </transactions>\\n\" + \"  <credit_payments type=\\\"array\\\">\\n\" + \"    <credit_payment href=\\\"https://api.recurly.com/v2/credit_payments/4f1dd58d3cb9af5a09ba634dcca690a6\\\">\\n\" + \"      <account href=\\\"https://api.recurly.com/v2/accounts/1\\\"></account>\\n\" + \"      <action>write_off</action>\\n\" + \"    </credit_payment>\\n\" + \"  </credit_payments>\\n\" + \"  <dunning_campaign_id>1234abcd</dunning_campaign_id>\\n\" + \"</invoice>\";\n    final Invoice invoice = xmlMapper.readValue(invoiceData, Invoice.class);\n    Assert.assertEquals(invoice.getAccount().getHref(), \"https://api.recurly.com/v2/accounts/1\");\n    Assert.assertTrue(invoice.hasOriginalInvoices());\n    Assert.assertEquals(invoice.getUuid(), \"421f7b7d414e4c6792938e7c49d552e9\");\n    Assert.assertEquals(invoice.getState(), \"open\");\n    Assert.assertEquals((int) invoice.getInvoiceNumber(), 1402);\n    Assert.assertEquals(invoice.getPoNumber(), \"abc-123\");\n    Assert.assertEquals(invoice.getVatReverseChargeNotes(), \"Some reverse charge notes\");\n    Assert.assertEquals(invoice.getCustomerNotes(), \"Some notes\");\n    Assert.assertEquals(invoice.getTermsAndConditions(), \"t and c\");\n    Assert.assertEquals((int) invoice.getNetTerms(), 0);\n    Assert.assertNull(invoice.getVatNumber());\n    Assert.assertEquals(invoice.getGatewayCode(), \"Some Gateway Code\");\n    Assert.assertEquals((int) invoice.getSubtotalInCents(), 9900);\n    Assert.assertEquals((int) invoice.getTaxInCents(), 0);\n    Assert.assertEquals((int) invoice.getTotalInCents(), 9900);\n    Assert.assertEquals(invoice.getCurrency(), \"USD\");\n    Assert.assertEquals(invoice.getTaxType(), \"usst\");\n    Assert.assertEquals(invoice.getTaxRegion(), \"CA\");\n    Assert.assertEquals(invoice.getTaxRate(), new BigDecimal(\"0.0875\"));\n    Assert.assertEquals((boolean) invoice.getUsedTaxService(), true);\n    Assert.assertEquals(invoice.getCreatedAt(), new DateTime(\"2011-08-25T12:00:00Z\"));\n    Assert.assertEquals(invoice.getUpdatedAt(), new DateTime(\"2011-08-25T12:00:00Z\"));\n    Assert.assertEquals(invoice.getClosedAt(), new DateTime(\"2011-08-25T12:00:00Z\"));\n    Assert.assertNotNull(invoice.getLineItems());\n    Assert.assertEquals(invoice.getLineItems().size(), 1);\n    Assert.assertEquals(invoice.getInvoiceNumberPrefix(), \"FR\");\n    Assert.assertEquals(invoice.getId(), \"FR1402\");\n    Assert.assertEquals(invoice.getSurchargeInCents(), new Integer(100));\n    Assert.assertEquals(invoice.getTaxDetails().size(), 1);\n    TaxDetail taxDetail = invoice.getTaxDetails().get(0);\n    Assert.assertEquals(taxDetail.getTaxRate(), new BigDecimal(\"0.05\"));\n    Assert.assertEquals((int) taxDetail.getTaxInCents(), 20);\n    Assert.assertEquals(taxDetail.getTaxRegion(), \"CA\");\n    Assert.assertEquals(taxDetail.getTaxType(), \"GST\");\n    final Adjustment adjustment = invoice.getLineItems().get(0);\n    Assert.assertEquals(adjustment.getDescription(), \"Charge for extra bandwidth\");\n    Assert.assertEquals((int) adjustment.getTotalInCents(), 5000);\n    Assert.assertEquals(adjustment.getStartDate(), new DateTime(\"2011-08-31T03:30:00Z\"));\n    Assert.assertEquals(invoice.getTransactions().size(), 0);\n    Assert.assertEquals(invoice.getCreditPayments().size(), 1);\n    Assert.assertEquals(invoice.getAddress().getAddress1(), \"123 Main St.\");\n    Assert.assertEquals(invoice.getShippingAddress().getAddress1(), \"123 Canal St.\");\n    Assert.assertEquals(invoice.getDunningCampaignId(), \"1234abcd\");\n    // test setting billing info uuid\n    invoice.setBillingInfoUuid(\"iiznlrvdt8py\");\n}",
        "label": 0
    },
    {
        "repo_name": "genepi___imputationserver",
        "commit": "b7f61eb13c19c3842fc28b8e2e6fd7d210a01a14",
        "commit_message": "Update tabix log message",
        "p_path": "src/main/java/genepi/imputationserver/steps/CompressionEncryption.java",
        "t_path": "src/test/java/genepi/imputationserver/steps/FastQualityControlTest.java",
        "p_name": "run",
        "t_name": "testchrxmixedgenotypes",
        "lpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    String workingDirectory = getFolder(CompressionEncryption.class);\n    String output = context.get(\"outputimputation\");\n    String localOutput = context.get(\"local\");\n    String aesEncryption = context.get(\"aesEncryption\");\n    String mode = context.get(\"mode\");\n    String password = context.get(\"password\");\n    boolean phasingOnly = false;\n    if (mode != null && mode.equals(\"phasing\")) {\n        phasingOnly = true;\n    }\n    // read config if mails should be sent\n    String folderConfig = getFolder(CompressionEncryption.class);\n    File jobConfig = new File(FileUtil.path(folderConfig, \"job.config\"));\n    DefaultPreferenceStore store = new DefaultPreferenceStore();\n    if (jobConfig.exists()) {\n        store.load(jobConfig);\n    } else {\n        context.log(\"Configuration file '\" + jobConfig.getAbsolutePath() + \"' not available. Use default values.\");\n    }\n    String notification = \"no\";\n    if (store.getString(\"minimac.sendmail\") != null && !store.getString(\"minimac.sendmail\").equals(\"\")) {\n        notification = store.getString(\"minimac.sendmail\");\n    }\n    String serverUrl = \"https://imputationserver.sph.umich.edu\";\n    if (store.getString(\"server.url\") != null && !store.getString(\"server.url\").isEmpty()) {\n        serverUrl = store.getString(\"server.url\");\n    }\n    if (password == null || (password != null && password.equals(\"auto\"))) {\n        password = PasswordCreator.createPassword();\n    }\n    try {\n        context.beginTask(\"Export data...\");\n        // get sorted directories\n        List<String> folders = HdfsUtil.getDirectories(output);\n        Map<String, ExportObject> chromosomes = new HashMap<String, ExportObject>();\n        for (String folder : folders) {\n            String name = FileUtil.getFilename(folder);\n            context.println(\"Prepare files for chromosome: \" + name);\n            List<String> data = new Vector<String>();\n            List<String> header = new Vector<String>();\n            List<String> info = new Vector<String>();\n            header = findFiles(folder, \".header.dose.vcf.gz\");\n            if (phasingOnly) {\n                data = findFiles(folder, \".phased.vcf.gz\");\n            } else {\n                data = findFiles(folder, \".data.dose.vcf.gz\");\n                info = findFiles(folder, \".info\");\n            }\n            // combine all X. to one folder\n            if (name.startsWith(\"X.\")) {\n                name = \"X\";\n            }\n            ExportObject export = chromosomes.get(name);\n            if (export == null) {\n                export = new ExportObject();\n            }\n            ArrayList<String> currentDataList = export.getDataFiles();\n            currentDataList.addAll(data);\n            export.setDataFiles(currentDataList);\n            ArrayList<String> currentHeaderList = export.getHeaderFiles();\n            currentHeaderList.addAll(header);\n            export.setHeaderFiles(currentHeaderList);\n            ArrayList<String> currentInfoList = export.getInfoFiles();\n            currentInfoList.addAll(info);\n            export.setInfoFiles(currentInfoList);\n            chromosomes.put(name, export);\n        }\n        Set<String> chromosomesSet = chromosomes.keySet();\n        boolean lastChromosome = false;\n        int index = 0;\n        for (String name : chromosomesSet) {\n            index++;\n            if (index == chromosomesSet.size()) {\n                lastChromosome = true;\n            }\n            ExportObject entry = chromosomes.get(name);\n            context.println(\"Export and merge chromosome \" + name);\n            // resort for chrX only\n            if (name.equals(\"X\")) {\n                Collections.sort(entry.getDataFiles(), new ChrXComparator());\n                Collections.sort(entry.getInfoFiles(), new ChrXComparator());\n            }\n            // create temp fir\n            String temp = FileUtil.path(localOutput, \"temp\");\n            FileUtil.createDirectory(temp);\n            // output files\n            String dosageOutput;\n            String infoOutput = \"\";\n            if (phasingOnly) {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".phased.vcf.gz\");\n            } else {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".dose.vcf.gz\");\n                infoOutput = FileUtil.path(temp, \"chr\" + name + \".info.gz\");\n                FileMerger.mergeAndGzInfo(entry.getInfoFiles(), infoOutput);\n            }\n            MergedVcfFile vcfFile = new MergedVcfFile(dosageOutput);\n            // simple header check\n            String headerLine = null;\n            for (String file : entry.getHeaderFiles()) {\n                context.println(\"Read header file \" + file);\n                LineReader reader = null;\n                try {\n                    reader = new LineReader(HdfsUtil.open(file));\n                    while (reader.next()) {\n                        String line = reader.get();\n                        if (line.startsWith(\"#CHROM\")) {\n                            if (headerLine != null) {\n                                if (headerLine.equals(line)) {\n                                    context.println(\"  Header is the same as header of first file.\");\n                                } else {\n                                    context.println(\"  ERROR: Header is different as header of first file.\");\n                                    context.println(headerLine);\n                                    context.println(line);\n                                    throw new Exception(\"Different sample order in chunks.\");\n                                }\n                            } else {\n                                headerLine = line;\n                                vcfFile.addFile(HdfsUtil.open(file));\n                                context.println(\"  Keep this header as first header.\");\n                            }\n                        }\n                    }\n                    if (reader != null) {\n                        reader.close();\n                    }\n                    HdfsUtil.delete(file);\n                } catch (Exception e) {\n                    if (reader != null) {\n                        reader.close();\n                    }\n                    StringWriter errors = new StringWriter();\n                    e.printStackTrace(new PrintWriter(errors));\n                    context.println(\"Error reading header file: \" + errors.toString());\n                }\n            }\n            if (headerLine == null || headerLine.trim().isEmpty()) {\n                throw new Exception(\"No valid header file found\");\n            }\n            // add data files\n            for (String file : entry.getDataFiles()) {\n                context.println(\"Read file \" + file);\n                vcfFile.addFile(HdfsUtil.open(file));\n                HdfsUtil.delete(file);\n            }\n            vcfFile.close();\n            // run tabix on last file only\n            if (lastChromosome) {\n                context.println(\"Run tabix on chromosome \" + lastChromosome);\n                Command tabix = new Command(FileUtil.path(workingDirectory, \"bin\", \"tabix\"));\n                tabix.setSilent(false);\n                tabix.setParams(\"-f\", dosageOutput);\n                if (tabix.execute() != 0) {\n                    context.endTask(\"Error during index creation: \" + tabix.getStdOut(), WorkflowContext.ERROR);\n                    return false;\n                }\n            }\n            ZipParameters param = new ZipParameters();\n            param.setEncryptFiles(true);\n            param.setPassword(password);\n            param.setEncryptionMethod(Zip4jConstants.ENC_METHOD_STANDARD);\n            if (aesEncryption != null && aesEncryption.equals(\"yes\")) {\n                param.setEncryptionMethod(Zip4jConstants.ENC_METHOD_AES);\n                param.setAesKeyStrength(Zip4jConstants.AES_STRENGTH_256);\n                param.setCompressionMethod(Zip4jConstants.COMP_DEFLATE);\n                param.setCompressionLevel(Zip4jConstants.DEFLATE_LEVEL_NORMAL);\n            }\n            // create zip file\n            ArrayList<File> files = new ArrayList<File>();\n            files.add(new File(dosageOutput));\n            // files.add(new File(vcfOutput + \".tbi\"));\n            if (!phasingOnly) {\n                files.add(new File(infoOutput));\n            }\n            ZipFile file = new ZipFile(new File(FileUtil.path(localOutput, \"chr_\" + name + \".zip\")));\n            file.createZipFile(files, param);\n            // delete temp dir\n            FileUtil.deleteDirectory(temp);\n        }\n        // delete temporary files\n        HdfsUtil.delete(output);\n        context.endTask(\"Exported data.\", WorkflowContext.OK);\n    } catch (Exception e) {\n        e.printStackTrace();\n        context.endTask(\"Data compression failed: \" + e.getMessage(), WorkflowContext.ERROR);\n        return false;\n    }\n    // submit counters!\n    context.submitCounter(\"samples\");\n    context.submitCounter(\"genotypes\");\n    context.submitCounter(\"chromosomes\");\n    context.submitCounter(\"runs\");\n    // submit panel and phasing method counters\n    String reference = context.get(\"refpanel\");\n    String phasing = context.get(\"phasing\");\n    context.submitCounter(\"refpanel_\" + reference);\n    context.submitCounter(\"phasing_\" + phasing);\n    context.submitCounter(\"23andme-input\");\n    // send email\n    if (notification.equals(\"yes\")) {\n        Object mail = context.getData(\"cloudgene.user.mail\");\n        Object name = context.getData(\"cloudgene.user.name\");\n        if (mail != null) {\n            String subject = \"Job \" + context.getJobId() + \" is complete.\";\n            String message = \"Dear \" + name + \",\\nthe password for the imputation results is: \" + password + \"\\n\\nThe results can be downloaded from \" + serverUrl + \"/start.html#!jobs/\" + context.getJobId() + \"/results\";\n            try {\n                context.sendMail(subject, message);\n                context.ok(\"We have sent an email to <b>\" + mail + \"</b> with the password.\");\n                return true;\n            } catch (Exception e) {\n                context.error(\"Data compression failed: \" + e.getMessage());\n                return false;\n            }\n        } else {\n            context.error(\"No email address found. Please enter your email address (Account -> Profile).\");\n            return false;\n        }\n    } else {\n        context.ok(\"Email notification is disabled. All results are encrypted with password <b>\" + password + \"</b>\");\n        return true;\n    }\n}",
        "rpfc": "@Override\npublic boolean run(WorkflowContext context) {\n    String workingDirectory = getFolder(CompressionEncryption.class);\n    String output = context.get(\"outputimputation\");\n    String localOutput = context.get(\"local\");\n    String aesEncryption = context.get(\"aesEncryption\");\n    String mode = context.get(\"mode\");\n    String password = context.get(\"password\");\n    boolean phasingOnly = false;\n    if (mode != null && mode.equals(\"phasing\")) {\n        phasingOnly = true;\n    }\n    // read config if mails should be sent\n    String folderConfig = getFolder(CompressionEncryption.class);\n    File jobConfig = new File(FileUtil.path(folderConfig, \"job.config\"));\n    DefaultPreferenceStore store = new DefaultPreferenceStore();\n    if (jobConfig.exists()) {\n        store.load(jobConfig);\n    } else {\n        context.log(\"Configuration file '\" + jobConfig.getAbsolutePath() + \"' not available. Use default values.\");\n    }\n    String notification = \"no\";\n    if (store.getString(\"minimac.sendmail\") != null && !store.getString(\"minimac.sendmail\").equals(\"\")) {\n        notification = store.getString(\"minimac.sendmail\");\n    }\n    String serverUrl = \"https://imputationserver.sph.umich.edu\";\n    if (store.getString(\"server.url\") != null && !store.getString(\"server.url\").isEmpty()) {\n        serverUrl = store.getString(\"server.url\");\n    }\n    if (password == null || (password != null && password.equals(\"auto\"))) {\n        password = PasswordCreator.createPassword();\n    }\n    try {\n        context.beginTask(\"Export data...\");\n        // get sorted directories\n        List<String> folders = HdfsUtil.getDirectories(output);\n        Map<String, ExportObject> chromosomes = new HashMap<String, ExportObject>();\n        for (String folder : folders) {\n            String name = FileUtil.getFilename(folder);\n            context.println(\"Prepare files for chromosome \" + name);\n            List<String> data = new Vector<String>();\n            List<String> header = new Vector<String>();\n            List<String> info = new Vector<String>();\n            header = findFiles(folder, \".header.dose.vcf.gz\");\n            if (phasingOnly) {\n                data = findFiles(folder, \".phased.vcf.gz\");\n            } else {\n                data = findFiles(folder, \".data.dose.vcf.gz\");\n                info = findFiles(folder, \".info\");\n            }\n            // combine all X. to one folder\n            if (name.startsWith(\"X.\")) {\n                name = \"X\";\n            }\n            ExportObject export = chromosomes.get(name);\n            if (export == null) {\n                export = new ExportObject();\n            }\n            ArrayList<String> currentDataList = export.getDataFiles();\n            currentDataList.addAll(data);\n            export.setDataFiles(currentDataList);\n            ArrayList<String> currentHeaderList = export.getHeaderFiles();\n            currentHeaderList.addAll(header);\n            export.setHeaderFiles(currentHeaderList);\n            ArrayList<String> currentInfoList = export.getInfoFiles();\n            currentInfoList.addAll(info);\n            export.setInfoFiles(currentInfoList);\n            chromosomes.put(name, export);\n        }\n        Set<String> chromosomesSet = chromosomes.keySet();\n        boolean lastChromosome = false;\n        int index = 0;\n        for (String name : chromosomesSet) {\n            index++;\n            if (index == chromosomesSet.size()) {\n                lastChromosome = true;\n            }\n            ExportObject entry = chromosomes.get(name);\n            context.println(\"Export and merge chromosome \" + name);\n            // resort for chrX only\n            if (name.equals(\"X\")) {\n                Collections.sort(entry.getDataFiles(), new ChrXComparator());\n                Collections.sort(entry.getInfoFiles(), new ChrXComparator());\n            }\n            // create temp fir\n            String temp = FileUtil.path(localOutput, \"temp\");\n            FileUtil.createDirectory(temp);\n            // output files\n            String dosageOutput;\n            String infoOutput = \"\";\n            if (phasingOnly) {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".phased.vcf.gz\");\n            } else {\n                dosageOutput = FileUtil.path(temp, \"chr\" + name + \".dose.vcf.gz\");\n                infoOutput = FileUtil.path(temp, \"chr\" + name + \".info.gz\");\n                FileMerger.mergeAndGzInfo(entry.getInfoFiles(), infoOutput);\n            }\n            MergedVcfFile vcfFile = new MergedVcfFile(dosageOutput);\n            // simple header check\n            String headerLine = null;\n            for (String file : entry.getHeaderFiles()) {\n                context.println(\"Read header file \" + file);\n                LineReader reader = null;\n                try {\n                    reader = new LineReader(HdfsUtil.open(file));\n                    while (reader.next()) {\n                        String line = reader.get();\n                        if (line.startsWith(\"#CHROM\")) {\n                            if (headerLine != null) {\n                                if (headerLine.equals(line)) {\n                                    context.println(\"  Header is the same as header of first file.\");\n                                } else {\n                                    context.println(\"  ERROR: Header is different as header of first file.\");\n                                    context.println(headerLine);\n                                    context.println(line);\n                                    throw new Exception(\"Different sample order in chunks.\");\n                                }\n                            } else {\n                                headerLine = line;\n                                vcfFile.addFile(HdfsUtil.open(file));\n                                context.println(\"  Keep this header as first header.\");\n                            }\n                        }\n                    }\n                    if (reader != null) {\n                        reader.close();\n                    }\n                    HdfsUtil.delete(file);\n                } catch (Exception e) {\n                    if (reader != null) {\n                        reader.close();\n                    }\n                    StringWriter errors = new StringWriter();\n                    e.printStackTrace(new PrintWriter(errors));\n                    context.println(\"Error reading header file: \" + errors.toString());\n                }\n            }\n            if (headerLine == null || headerLine.trim().isEmpty()) {\n                throw new Exception(\"No valid header file found\");\n            }\n            // add data files\n            for (String file : entry.getDataFiles()) {\n                context.println(\"Read file \" + file);\n                vcfFile.addFile(HdfsUtil.open(file));\n                HdfsUtil.delete(file);\n            }\n            vcfFile.close();\n            // run tabix on last file only\n            if (lastChromosome) {\n                context.println(\"Run tabix on chromosome \" + name + \"...\");\n                Command tabix = new Command(FileUtil.path(workingDirectory, \"bin\", \"tabix\"));\n                tabix.setSilent(false);\n                tabix.setParams(\"-f\", dosageOutput);\n                if (tabix.execute() != 0) {\n                    context.endTask(\"Error during index creation: \" + tabix.getStdOut(), WorkflowContext.ERROR);\n                    return false;\n                }\n            }\n            ZipParameters param = new ZipParameters();\n            param.setEncryptFiles(true);\n            param.setPassword(password);\n            param.setEncryptionMethod(Zip4jConstants.ENC_METHOD_STANDARD);\n            if (aesEncryption != null && aesEncryption.equals(\"yes\")) {\n                param.setEncryptionMethod(Zip4jConstants.ENC_METHOD_AES);\n                param.setAesKeyStrength(Zip4jConstants.AES_STRENGTH_256);\n                param.setCompressionMethod(Zip4jConstants.COMP_DEFLATE);\n                param.setCompressionLevel(Zip4jConstants.DEFLATE_LEVEL_NORMAL);\n            }\n            // create zip file\n            ArrayList<File> files = new ArrayList<File>();\n            files.add(new File(dosageOutput));\n            // files.add(new File(vcfOutput + \".tbi\"));\n            if (!phasingOnly) {\n                files.add(new File(infoOutput));\n            }\n            ZipFile file = new ZipFile(new File(FileUtil.path(localOutput, \"chr_\" + name + \".zip\")));\n            file.createZipFile(files, param);\n            // delete temp dir\n            FileUtil.deleteDirectory(temp);\n        }\n        // delete temporary files\n        HdfsUtil.delete(output);\n        context.endTask(\"Exported data.\", WorkflowContext.OK);\n    } catch (Exception e) {\n        e.printStackTrace();\n        context.endTask(\"Data compression failed: \" + e.getMessage(), WorkflowContext.ERROR);\n        return false;\n    }\n    // submit counters!\n    context.submitCounter(\"samples\");\n    context.submitCounter(\"genotypes\");\n    context.submitCounter(\"chromosomes\");\n    context.submitCounter(\"runs\");\n    // submit panel and phasing method counters\n    String reference = context.get(\"refpanel\");\n    String phasing = context.get(\"phasing\");\n    context.submitCounter(\"refpanel_\" + reference);\n    context.submitCounter(\"phasing_\" + phasing);\n    context.submitCounter(\"23andme-input\");\n    // send email\n    if (notification.equals(\"yes\")) {\n        Object mail = context.getData(\"cloudgene.user.mail\");\n        Object name = context.getData(\"cloudgene.user.name\");\n        if (mail != null) {\n            String subject = \"Job \" + context.getJobId() + \" is complete.\";\n            String message = \"Dear \" + name + \",\\nthe password for the imputation results is: \" + password + \"\\n\\nThe results can be downloaded from \" + serverUrl + \"/start.html#!jobs/\" + context.getJobId() + \"/results\";\n            try {\n                context.sendMail(subject, message);\n                context.ok(\"We have sent an email to <b>\" + mail + \"</b> with the password.\");\n                return true;\n            } catch (Exception e) {\n                context.error(\"Data compression failed: \" + e.getMessage());\n                return false;\n            }\n        } else {\n            context.error(\"No email address found. Please enter your email address (Account -> Profile).\");\n            return false;\n        }\n    } else {\n        context.ok(\"Email notification is disabled. All results are encrypted with password <b>\" + password + \"</b>\");\n        return true;\n    }\n}",
        "tuc": "@Test\npublic void testChrXMixedGenotypes() throws IOException, ZipException {\n    String configFolder = \"test-data/configs/hapmap-chrX\";\n    String inputFolder = \"test-data/data/chrX-unphased-mixed\";\n    // create workflow context\n    WorkflowTestContext context = buildContext(inputFolder, \"phase1\");\n    // run qc to create chunkfile\n    FastQualityControlMock qcStats = new FastQualityControlMock(configFolder);\n    boolean result = run(context, qcStats);\n    assertFalse(result);\n    assertTrue(context.hasInMemory(\"Chromosome X nonPAR region includes > 10 % mixed genotypes.\"));\n}",
        "label": 0
    },
    {
        "repo_name": "qiunet___DuoDuo",
        "commit": "f1cdf5c13ab76a72614900c5436e1eea45297a23",
        "commit_message": "feat(FlashHandler): \u4f18\u5316\u542f\u52a8\u7c7b\n",
        "p_path": "FlashHandler/src/main/java/org/qiunet/flash/handler/netty/server/BootstrapServer.java",
        "t_path": "QiunetUtils/src/test/java/org/qiunet/utils/string/TestStringUtil.java",
        "p_name": "run",
        "t_name": "testisnum",
        "lpfc": "@Override\npublic void run() {\n    logger.error(\"[HookListener]\u670d\u52a1\u7aef Hook Listener on port [{}]\", hook.getHookPort());\n    try {\n        while (RUNNING) {\n            try {\n                this.selector.select(1000);\n                Iterator<SelectionKey> itr = this.selector.selectedKeys().iterator();\n                while (itr.hasNext()) {\n                    SelectionKey key = itr.next();\n                    itr.remove();\n                    if (key.isAcceptable()) {\n                        logger.error(\"[HookListener]\u670d\u52a1\u7aef: ProcessAcceptor Msg\");\n                        ServerSocketChannel serverSocketChannel = (ServerSocketChannel) key.channel();\n                        SocketChannel channel = serverSocketChannel.accept();\n                        String ip = ((InetSocketAddress) channel.getRemoteAddress()).getHostString();\n                        if (!NetUtil.isInnerIp(ip) && !NetUtil.isLocalIp(ip)) {\n                            logger.error(\"[HookListener]\u670d\u52a1\u7aef: Remote ip [{}] is not allow !\", ip);\n                            channel.close();\n                            continue;\n                        }\n                        channel.configureBlocking(false);\n                        channel.register(this.selector, SelectionKey.OP_READ);\n                    } else if (key.isReadable()) {\n                        SocketChannel channel = (SocketChannel) key.channel();\n                        ByteBuffer byteBuffer = ByteBuffer.allocate(2048);\n                        channel.read(byteBuffer);\n                        byteBuffer.flip();\n                        try {\n                            handlerMsg(byteBuffer);\n                        } finally {\n                            channel.close();\n                        }\n                    }\n                }\n            } catch (Exception e) {\n                logger.error(\"[HookListener]\", e);\n            }\n        }\n    } finally {\n        try {\n            this.selector.close();\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n}",
        "rpfc": "@Override\npublic void run() {\n    logger.error(\"[HookListener]\u670d\u52a1\u7aef Hook Listener on port [{}]\", hook.getHookPort());\n    try {\n        while (RUNNING) {\n            try {\n                this.selector.select(1000);\n                Iterator<SelectionKey> itr = this.selector.selectedKeys().iterator();\n                while (itr.hasNext()) {\n                    SelectionKey key = itr.next();\n                    itr.remove();\n                    if (key.isAcceptable()) {\n                        logger.error(\"[HookListener]\u670d\u52a1\u7aef: ProcessAcceptor Msg\");\n                        SocketChannel channel = ((ServerSocketChannel) key.channel()).accept();\n                        String ip = ((InetSocketAddress) channel.getRemoteAddress()).getHostString();\n                        if (!NetUtil.isInnerIp(ip) && !NetUtil.isLocalIp(ip)) {\n                            logger.error(\"[HookListener]\u670d\u52a1\u7aef: Remote ip [{}] is not allow !\", ip);\n                            channel.close();\n                            continue;\n                        }\n                        channel.configureBlocking(false);\n                        channel.register(this.selector, SelectionKey.OP_READ);\n                    } else if (key.isReadable()) {\n                        SocketChannel channel = (SocketChannel) key.channel();\n                        ByteBuffer byteBuffer = ByteBuffer.allocate(2048);\n                        channel.read(byteBuffer);\n                        byteBuffer.flip();\n                        try {\n                            handlerMsg(byteBuffer);\n                        } finally {\n                            channel.close();\n                        }\n                    }\n                }\n            } catch (Exception e) {\n                logger.error(\"[HookListener]\", e);\n            }\n        }\n    } finally {\n        try {\n            this.selector.close();\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n}",
        "tuc": "@Test\npublic void testIsNum() {\n    Assert.assertTrue(StringUtil.isNum(\"123\"));\n    Assert.assertTrue(StringUtil.isNum(\"-123\"));\n    Assert.assertFalse(StringUtil.isNum(\"-1ff\"));\n    Assert.assertFalse(StringUtil.isNum(\"\"));\n}",
        "label": 0
    },
    {
        "repo_name": "acmsigsoft___submission-checker",
        "commit": "af51b4f82cd7e87228365d3775dfae593cfb1ef0",
        "commit_message": "Display usage info on argparse error\n",
        "p_path": "src/main/java/org/sigsoft/sfc/BatchChecker.java",
        "t_path": "src/test/java/org/sigsoft/sfc/MetaDataTest.java",
        "p_name": "usage",
        "t_name": "testlatexnames",
        "lpfc": "public void usage(String error) {\n    if (error != null) {\n        System.err.println(String.format(\"ERROR: %s\", error));\n    }\n    String tool = BatchChecker.class.getName();\n    String msg = String.format(\"Usage: %s [options] [folder-with-pdfs...] [pdf-file ...]\\n\", tool);\n    msg += \"  Options:\\n\";\n    msg += \"  --style <style>    Set style in ACM or IEEE, default IEEE\\n\";\n    msg += \"  --showtext <pages> Show plain text of pages on stdout. <pages> can be nr or 'all'\\n\";\n    msg += \"                     Combine with '... | grep ...' to fetch custom patterns\\n\";\n    msg += \"  --meta <csv-file>  .csv file with author meta data. One row per author. Valid columns:\\n\";\n    msg += \"                     paper,title,first,last,affiliation,email\\n\";\n    System.err.println(msg);\n}",
        "rpfc": "public void usage(String error) {\n    if (error != null) {\n        System.err.println(String.format(\"ERROR: %s\", error));\n    }\n    String tool = BatchChecker.class.getName();\n    String msg = String.format(\"Usage: %s [options] [folder-with-pdfs...] [pdf-file ...]\\n\", tool);\n    msg += \"  Options:\\n\";\n    msg += \"  -s|--style <style>    Set style in ACM or IEEE, default IEEE\\n\";\n    msg += \"  -t|--showtext <pages> Show plain text of pages on stdout. <pages> can be nr or 'all'\\n\";\n    msg += \"                        Combine with '... | grep ...' to fetch custom patterns\\n\";\n    msg += \"  -m|--meta <csv-file>  .csv file with author meta data. One row per author. Valid columns:\\n\";\n    msg += \"                        paper,title,first,last,affiliation,email\\n\";\n    msg += \"  -h|--help             Show this information\";\n    System.err.println(msg);\n}",
        "tuc": "@Test\npublic void testLatexNames() {\n    PaperMetaData paper666 = md.getPaper(\"666\");\n    assertThat(paper666).isNotNull();\n    assertThat(paper666.getAuthors()).hasSize(1);\n    assertThat(paper666.getAuthors().get(0).getName()).isEqualTo(\"Erich K{\\\\\\\"a}stner\");\n}",
        "label": 0
    },
    {
        "repo_name": "genepi___imputationserver",
        "commit": "7806197d7d266d1d3969a46336538034f13ac652",
        "commit_message": "skip monomorphic check for single sample imputation",
        "p_path": "src/main/java/genepi/imputationserver/steps/fastqc/StatisticsTask.java",
        "t_path": "src/test/java/genepi/imputationserver/steps/FastQualityControlTest.java",
        "p_name": "run",
        "t_name": "testqcstatistics",
        "lpfc": "public TaskResults run(ITaskProgressListener progressListener) throws IOException, InterruptedException {\n    TaskResults qcObject = new TaskResults();\n    qcObject.setMessage(\"\");\n    LineWriter mafWriter = new LineWriter(mafFile);\n    LineWriter excludedChunkWriter = new LineWriter(FileUtil.path(statDir, \"chunks-excluded.txt\"));\n    excludedChunkWriter.write(\"#Chunk\" + \"\\t\" + \"SNPs (#)\" + \"\\t\" + \"Reference Overlap (%)\" + \"\\t\" + \"Low Sample Call Rates (#)\");\n    HashSet<String> hapSamples = new HashSet<String>();\n    int i = 0;\n    for (String vcfFilename : vcfFilenames) {\n        i++;\n        if (progressListener != null) {\n            progressListener.progress(getName() + \" [\" + i + \"/\" + vcfFilenames.length + \"]\\n\\n\" + \"Analyze file \" + FileUtil.getFilename(vcfFilename) + \"...\");\n        }\n        VcfFile myvcfFile = VcfFileUtil.load(vcfFilename, chunkSize, true);\n        String chromosome = myvcfFile.getChromosome();\n        boolean phased = myvcfFile.isPhased();\n        if (VcfFileUtil.isChrX(chromosome)) {\n            List<String> splits = prepareChrX(vcfFilename, phased, hapSamples);\n            for (String split : splits) {\n                VcfFile _myvcfFile = VcfFileUtil.load(split, chunkSize, true);\n                _myvcfFile.setChrX(true);\n                processFile(_myvcfFile.getVcfFilename(), chromosome, chunkSize, phased, mafWriter, excludedSnpsWriter, excludedChunkWriter);\n            }\n        } else {\n            processFile(vcfFilename, chromosome, chunkSize, phased, mafWriter, excludedSnpsWriter, excludedChunkWriter);\n        }\n    }\n    mafWriter.close();\n    excludedSnpsWriter.close();\n    excludedChunkWriter.close();\n    if (hapSamples.size() > 0) {\n        LineWriter writer = new LineWriter(FileUtil.path(statDir, \"chrX-samples.txt\"));\n        writer.write(\"The following samples have been changed from haploid to diploid\");\n        for (String sample : hapSamples) {\n            writer.write(sample);\n        }\n        writer.close();\n        qcObject.setMessage(\"<b>Chromosome X Info:</b> For phasing/imputation we changed samples from haploid to diploid. Please check chrX-samples.txt\");\n    }\n    qcObject.setSuccess(true);\n    return qcObject;\n}",
        "rpfc": "public TaskResults run(ITaskProgressListener progressListener) throws IOException, InterruptedException {\n    TaskResults qcObject = new TaskResults();\n    qcObject.setMessage(\"\");\n    LineWriter mafWriter = new LineWriter(mafFile);\n    LineWriter excludedChunkWriter = new LineWriter(FileUtil.path(statDir, \"chunks-excluded.txt\"));\n    excludedChunkWriter.write(\"#Chunk\" + \"\\t\" + \"SNPs (#)\" + \"\\t\" + \"Reference Overlap (%)\" + \"\\t\" + \"Low Sample Call Rates (#)\");\n    HashSet<String> hapSamples = new HashSet<String>();\n    int i = 0;\n    for (String vcfFilename : vcfFilenames) {\n        i++;\n        if (progressListener != null) {\n            progressListener.progress(getName() + \" [\" + i + \"/\" + vcfFilenames.length + \"]\\n\\n\" + \"Analyze file \" + FileUtil.getFilename(vcfFilename) + \"...\");\n        }\n        VcfFile myvcfFile = VcfFileUtil.load(vcfFilename, chunkSize, true);\n        String chromosome = myvcfFile.getChromosome();\n        if (VcfFileUtil.isChrX(chromosome)) {\n            List<String> splits = prepareChrX(chromosome, myvcfFile.isPhased(), hapSamples);\n            for (String split : splits) {\n                VcfFile _myvcfFile = VcfFileUtil.load(split, chunkSize, true);\n                _myvcfFile.setChrX(true);\n                processFile(_myvcfFile, mafWriter, excludedSnpsWriter, excludedChunkWriter);\n            }\n        } else {\n            processFile(myvcfFile, mafWriter, excludedSnpsWriter, excludedChunkWriter);\n        }\n    }\n    mafWriter.close();\n    excludedSnpsWriter.close();\n    excludedChunkWriter.close();\n    if (hapSamples.size() > 0) {\n        LineWriter writer = new LineWriter(FileUtil.path(statDir, \"chrX-samples.txt\"));\n        writer.write(\"The following samples have been changed from haploid to diploid\");\n        for (String sample : hapSamples) {\n            writer.write(sample);\n        }\n        writer.close();\n        qcObject.setMessage(\"<b>Chromosome X Info:</b> For phasing/imputation we changed samples from haploid to diploid. Please check chrX-samples.txt\");\n    }\n    qcObject.setSuccess(true);\n    return qcObject;\n}",
        "tuc": "public void testQcStatistics() throws IOException {\n    String configFolder = \"test-data/configs/hapmap-chr1\";\n    String inputFolder = \"test-data/data/single\";\n    // create workflow context\n    WorkflowTestContext context = buildContext(inputFolder, \"hapmap2\");\n    // create step instance\n    FastQualityControlMock qcStats = new FastQualityControlMock(configFolder);\n    // run and test\n    boolean result = run(context, qcStats);\n    assertFalse(result);\n    // check statistics\n    assertTrue(context.hasInMemory(\"Alternative allele frequency > 0.5 sites: 185\"));\n    assertTrue(context.hasInMemory(\"Excluded sites in total: 336\"));\n    assertTrue(context.hasInMemory(\"Remaining sites in total: 96\"));\n    assertTrue(context.hasInMemory(\"Monomorphic sites: 331\"));\n    FileUtil.deleteDirectory(\"test-data/tmp\");\n}",
        "label": 0
    },
    {
        "repo_name": "ops4j___org.ops4j.pax.exam",
        "commit": "bf5d72a06941744061aedad165340a9aff629f08",
        "commit_message": " #1088 Put dynamic RMI port values into Karaf's management configuration file\n",
        "p_path": "containers/pax-exam-container-karaf/src/main/java/org/ops4j/pax/exam/karaf/container/internal/KarafTestContainer.java",
        "t_path": "containers/pax-exam-container-karaf/src/test/java/org/ops4j/pax/exam/karaf/container/internal/LoggingBackendTest.java",
        "p_name": "start",
        "t_name": "testlog4j2overwritesalreadysetrootloggerlevel",
        "lpfc": "@Override\npublic synchronized TestContainer start() {\n    try {\n        String name = system.createID(KARAF_TEST_CONTAINER);\n        Option invokerConfiguration = getInvokerConfiguration();\n        String host = InetAddress.getLoopbackAddress().getHostAddress();\n        System.setProperty(\"java.rmi.server.hostname\", host);\n        int port = findFreePort();\n        LOGGER.info(\"Creating RMI registry server on {}:{}\", host, port);\n        registry = LocateRegistry.createRegistry(port);\n        ExamSystem subsystem = system.fork(options(systemProperty(\"java.rmi.server.hostname\").value(host), systemProperty(RMI_HOST_PROPERTY).value(host), systemProperty(RMI_PORT_PROPERTY).value(Integer.toString(port)), systemProperty(RMI_NAME_PROPERTY).value(name), invokerConfiguration, systemProperty(EXAM_INJECT_PROPERTY).value(\"true\"), editConfigurationFileExtend(\"etc/system.properties\", \"jline.shutdownhook\", \"true\")));\n        System.setProperty(\"java.protocol.handler.pkgs\", \"org.ops4j.pax.url\");\n        if (framework.getExisting() != null) {\n            targetFolder = framework.getExisting();\n        } else {\n            URL sourceDistribution = new URL(framework.getFrameworkURL());\n            targetFolder = retrieveFinalTargetFolder(subsystem);\n            ArchiveExtractor.extract(sourceDistribution, targetFolder);\n        }\n        target = new RBCRemoteTarget(name, port, subsystem.getTimeout());\n        karafBase = searchKarafBase(targetFolder);\n        File karafHome = karafBase;\n        versionAdaptions = createVersionAdapter(karafBase);\n        DependenciesDeployer deployer = new DependenciesDeployer(subsystem, karafBase, karafHome);\n        deployer.copyBootClasspathLibraries();\n        if (framework.getExisting() != null) {\n            backupConfigFiles();\n        }\n        setupSystemProperties(karafHome, subsystem);\n        updateLogProperties(karafHome, subsystem);\n        List<KarafDistributionConfigurationFileOption> options = new ArrayList<>(Arrays.asList(subsystem.getOptions(KarafDistributionConfigurationFileOption.class)));\n        options.addAll(fromFeatureOptions(subsystem.getOptions(KarafFeaturesOption.class)));\n        String usedExamFeature = shouldInjectJUnitBundles(system) ? \"exam\" : \"exam-no-junit\";\n        options.addAll(fromFeatureOptions(KarafDistributionOption.features(EXAM_REPO_URL, usedExamFeature)));\n        if (framework.isUseDeployFolder()) {\n            deployer.copyReferencedArtifactsToDeployFolder();\n        } else {\n            options.addAll(fromFeatureOptions(deployer.getDependenciesFeature()));\n        }\n        options.addAll(configureBootDelegation(subsystem));\n        options.addAll(configureSystemPackages(subsystem));\n        updateUserSetProperties(karafHome, options);\n        startKaraf(subsystem, karafBase, karafHome);\n        started = true;\n    } catch (IOException e) {\n        throw new RuntimeException(\"Problem starting container\", e);\n    }\n    return this;\n}",
        "rpfc": "@Override\npublic synchronized TestContainer start() {\n    try {\n        String name = system.createID(KARAF_TEST_CONTAINER);\n        Option invokerConfiguration = getInvokerConfiguration();\n        String host = InetAddress.getLoopbackAddress().getHostAddress();\n        System.setProperty(\"java.rmi.server.hostname\", host);\n        final int rmiRegistryPort = findFreePort();\n        final int rmiServerPort = findFreePort();\n        LOGGER.info(\"Creating RMI registry server on {}:{}\", host, rmiRegistryPort);\n        registry = LocateRegistry.createRegistry(rmiRegistryPort);\n        ExamSystem subsystem = system.fork(options(systemProperty(\"java.rmi.server.hostname\").value(host), systemProperty(RMI_HOST_PROPERTY).value(host), systemProperty(RMI_PORT_PROPERTY).value(Integer.toString(rmiRegistryPort)), systemProperty(RMI_NAME_PROPERTY).value(name), editConfigurationFilePut(\"etc/org.apache.karaf.management.cfg\", \"rmiRegistryPort\", Integer.toString(rmiRegistryPort)), editConfigurationFilePut(\"etc/org.apache.karaf.management.cfg\", \"rmiServerPort\", Integer.toString(rmiServerPort)), invokerConfiguration, systemProperty(EXAM_INJECT_PROPERTY).value(\"true\"), editConfigurationFileExtend(\"etc/system.properties\", \"jline.shutdownhook\", \"true\")));\n        System.setProperty(\"java.protocol.handler.pkgs\", \"org.ops4j.pax.url\");\n        if (framework.getExisting() != null) {\n            targetFolder = framework.getExisting();\n        } else {\n            URL sourceDistribution = new URL(framework.getFrameworkURL());\n            targetFolder = retrieveFinalTargetFolder(subsystem);\n            ArchiveExtractor.extract(sourceDistribution, targetFolder);\n        }\n        target = new RBCRemoteTarget(name, rmiRegistryPort, subsystem.getTimeout());\n        karafBase = searchKarafBase(targetFolder);\n        File karafHome = karafBase;\n        versionAdaptions = createVersionAdapter(karafBase);\n        DependenciesDeployer deployer = new DependenciesDeployer(subsystem, karafBase, karafHome);\n        deployer.copyBootClasspathLibraries();\n        if (framework.getExisting() != null) {\n            backupConfigFiles();\n        }\n        setupSystemProperties(karafHome, subsystem);\n        updateLogProperties(karafHome, subsystem);\n        List<KarafDistributionConfigurationFileOption> options = new ArrayList<>(Arrays.asList(subsystem.getOptions(KarafDistributionConfigurationFileOption.class)));\n        options.addAll(fromFeatureOptions(subsystem.getOptions(KarafFeaturesOption.class)));\n        String usedExamFeature = shouldInjectJUnitBundles(system) ? \"exam\" : \"exam-no-junit\";\n        options.addAll(fromFeatureOptions(KarafDistributionOption.features(EXAM_REPO_URL, usedExamFeature)));\n        if (framework.isUseDeployFolder()) {\n            deployer.copyReferencedArtifactsToDeployFolder();\n        } else {\n            options.addAll(fromFeatureOptions(deployer.getDependenciesFeature()));\n        }\n        options.addAll(configureBootDelegation(subsystem));\n        options.addAll(configureSystemPackages(subsystem));\n        updateUserSetProperties(karafHome, options);\n        startKaraf(subsystem, karafBase, karafHome);\n        started = true;\n    } catch (IOException e) {\n        throw new RuntimeException(\"Problem starting container\", e);\n    }\n    return this;\n}",
        "tuc": "@Test\npublic void testLog4J2OverwritesAlreadySetRootLoggerLevel() {\n    Properties properties = makeProperties(\"log4j2.rootLogger.level\", \"originalLevel\");\n    LoggingBackend.LOG4J2.updatePaxLoggingConfiguration(properties, \"chosenLevel\");\n    Properties expectedProperties = makeProperties(\"log4j2.rootLogger.level\", \"chosenLevel\");\n    assertThat(properties, equalTo(expectedProperties));\n}",
        "label": 0
    },
    {
        "repo_name": "karussell___Jetwick",
        "commit": "736fe69e9f4e18db2f076fa35a8ce467420250c5",
        "commit_message": "minor layout adjustments\n",
        "p_path": "src/main/java/de/jetwick/ui/JetwickApp.java",
        "t_path": "src/test/java/de/jetwick/tw/TweetConsumerTest.java",
        "p_name": "init",
        "t_name": "testaddsomemore",
        "lpfc": "@Override\nprotected void init() {\n    super.init();\n    getApplicationSettings().setPageExpiredErrorPage(SessionTimeout.class);\n    getApplicationSettings().setInternalErrorPage(ErrorPage.class);\n    getMarkupSettings().setDefaultBeforeDisabledLink(null);\n    getMarkupSettings().setDefaultAfterDisabledLink(null);\n    if (\"development\".equals(cfg.getStage())) {\n        getDebugSettings().setDevelopmentUtilitiesEnabled(true);\n    }\n    mountBookmarkablePage(\"about\", About.class);\n    mountBookmarkablePage(\"imprint\", Imprint.class);\n    mountBookmarkablePage(\"mobile\", MobilePage.class);\n    mountBookmarkablePage(\"m\", MobilePage.class);\n    addComponentInstantiationListener(getGuiceInjector());\n}",
        "rpfc": "@Override\nprotected void init() {\n    super.init();\n    getApplicationSettings().setPageExpiredErrorPage(SessionTimeout.class);\n    getApplicationSettings().setInternalErrorPage(ErrorPage.class);\n    getMarkupSettings().setDefaultBeforeDisabledLink(null);\n    getMarkupSettings().setDefaultAfterDisabledLink(null);\n    if (\"development\".equals(cfg.getStage())) {\n        getDebugSettings().setDevelopmentUtilitiesEnabled(true);\n    }\n    mountBookmarkablePage(\"about\", About.class);\n    mountBookmarkablePage(\"imprint\", Imprint.class);\n    mountBookmarkablePage(\"mobile\", MobilePage.class);\n    mountBookmarkablePage(\"m\", MobilePage.class);\n    mountBookmarkablePage(\"xy\", HomePage.class);\n    addComponentInstantiationListener(getGuiceInjector());\n}",
        "tuc": "@Test\npublic void testAddSomeMore() {\n    //        dbHelper.setRemoveDays(1);\n    //        tweetConsumer.setDbHelper(dbHelper);\n    tweetConsumer.setRemoveDays(1);\n    BlockingQueue<TweetPackage> queue = new LinkedBlockingQueue<TweetPackage>();\n    JTweet tw = createTweet(4L, \"OldTweet\", \"userB\");\n    tw.setCreatedAt(new MyDate().minusDays(2).toDate());\n    JTweet tw2 = createTweet(5L, \"RT @userB: text\", \"timetabling\");\n    tw2.setCreatedAt(new Date());\n    JTweet tw3 = createTweet(6L, \"Bla bli\", \"userB\");\n    tw3.setCreatedAt(new Date());\n    queue.add(new TweetPackageList(\"\").init(0, Arrays.asList(tw, tw2, tw3)));\n    Collection<JTweet> res = tweetConsumer.updateTweets(queue, 100);\n    assertEquals(2, res.size());\n    //        assertEquals(0, res.getDeletedTweets().size());\n}",
        "label": 0
    },
    {
        "repo_name": "arcus-smart-home___arcusplatform",
        "commit": "fd2073b46a403142778d24bafdb1def37b2e5f3e",
        "commit_message": "accept \"cassandra\" as a value\n",
        "p_path": "platform/arcus-lib/src/main/java/com/iris/platform/cluster/ClusterModule.java",
        "t_path": "agent/arcus-test-agent/src/test/java/com/iris/agent/addressing/TestAddressing.java",
        "p_name": "configure",
        "t_name": "testserviceaddress",
        "lpfc": "@Override\nprotected void configure() {\n    // TODO move Clock to a more generic module\n    bind(Clock.class).toInstance(Clock.systemUTC());\n    bind(ClusterService.class).asEagerSingleton();\n    switch(clusterServiceDao) {\n        default:\n            logger.warn(\"unknown cluster dao {}: using default instead\", clusterServiceDao);\n        // fall through\n        case \"default\":\n            bind(ClusterServiceDao.class).to(CassandraClusterServiceDao.class);\n            break;\n        case \"zookeeper\":\n            logger.info(\"using zookeeper for cluster registration\");\n            bind(ClusterServiceDao.class).to(ZookeeperClusterServiceDao.class);\n            break;\n    }\n    OptionalBinder.newOptionalBinder(binder(), new TypeLiteral<Set<ClusterServiceListener>>() {\n    });\n}",
        "rpfc": "@Override\nprotected void configure() {\n    // TODO move Clock to a more generic module\n    bind(Clock.class).toInstance(Clock.systemUTC());\n    bind(ClusterService.class).asEagerSingleton();\n    switch(clusterServiceDao) {\n        default:\n            logger.warn(\"unknown cluster dao {}: using default instead\", clusterServiceDao);\n        // fall through\n        case \"default\":\n        case \"cassandra\":\n            bind(ClusterServiceDao.class).to(CassandraClusterServiceDao.class);\n            break;\n        case \"zookeeper\":\n            logger.info(\"using zookeeper for cluster registration\");\n            bind(ClusterServiceDao.class).to(ZookeeperClusterServiceDao.class);\n            break;\n    }\n    OptionalBinder.newOptionalBinder(binder(), new TypeLiteral<Set<ClusterServiceListener>>() {\n    });\n}",
        "tuc": "@Test\npublic void testServiceAddress() throws Exception {\n    HubServiceAddress addr = HubAddressUtils.service(\"test\");\n    Assert.assertFalse(addr.isPlatformBroadcast());\n    Assert.assertEquals(\"test\", addr.getServiceId());\n}",
        "label": 0
    },
    {
        "repo_name": "jgroups-extras___jgroups-kubernetes",
        "commit": "642e46074e0e92f9eb3cecdf87b70b57acc649c9",
        "commit_message": "JGRP-2238 KUBE_PING should read old env variables and should warn\n\n* KUBE_PING should support old env variables too, in case people\nforget to update their env variables when upgrading (like it\nhappened to me)\n* If both old and new env variables are defined, we should warn\nthe user.\n* If only using old env variables, we should warn the user too.\n",
        "p_path": "src/main/java/org/jgroups/protocols/kubernetes/KUBE_PING.java",
        "t_path": "src/test/java/org/jgroups/ping/kube/test/ClientTest.java",
        "p_name": "init",
        "t_name": "testpods",
        "lpfc": "public void init() throws Exception {\n    super.init();\n    TP transport = getTransport();\n    tp_bind_port = transport.getBindPort();\n    if (tp_bind_port <= 0)\n        throw new IllegalArgumentException(String.format(\"%s only works with  %s.bind_port > 0\", KUBE_PING.class.getSimpleName(), transport.getClass().getSimpleName()));\n    if (namespace == null) {\n        log.warn(\"namespace not set; clustering disabled\");\n        return;\n    }\n    log.info(\"namespace %s set; clustering enabled\", namespace);\n    Map<String, String> headers = new HashMap<>();\n    StreamProvider streamProvider;\n    if (clientCertFile != null) {\n        if (masterProtocol == null)\n            masterProtocol = \"http\";\n        streamProvider = new CertificateStreamProvider(clientCertFile, clientKeyFile, clientKeyPassword, clientKeyAlgo, caCertFile);\n    } else {\n        String saToken = readFileToString(saTokenFile);\n        if (saToken != null) {\n            headers.put(\"Authorization\", \"Bearer \" + saToken);\n        }\n        streamProvider = new InsecureStreamProvider();\n    }\n    String url = String.format(\"%s://%s:%s/api/%s\", masterProtocol, masterHost, masterPort, apiVersion);\n    client = new Client(url, headers, connectTimeout, readTimeout, operationAttempts, operationSleep, streamProvider, log);\n    log.debug(\"KubePING configuration: \" + toString());\n}",
        "rpfc": "public void init() throws Exception {\n    super.init();\n    TP transport = getTransport();\n    tp_bind_port = transport.getBindPort();\n    if (tp_bind_port <= 0)\n        throw new IllegalArgumentException(String.format(\"%s only works with  %s.bind_port > 0\", KUBE_PING.class.getSimpleName(), transport.getClass().getSimpleName()));\n    checkDeprecatedProperties();\n    if (namespace == null) {\n        log.warn(\"namespace not set; clustering disabled\");\n        return;\n    }\n    log.info(\"namespace %s set; clustering enabled\", namespace);\n    Map<String, String> headers = new HashMap<>();\n    StreamProvider streamProvider;\n    if (clientCertFile != null) {\n        if (masterProtocol == null)\n            masterProtocol = \"http\";\n        streamProvider = new CertificateStreamProvider(clientCertFile, clientKeyFile, clientKeyPassword, clientKeyAlgo, caCertFile);\n    } else {\n        String saToken = readFileToString(saTokenFile);\n        if (saToken != null) {\n            headers.put(\"Authorization\", \"Bearer \" + saToken);\n        }\n        streamProvider = new InsecureStreamProvider();\n    }\n    String url = String.format(\"%s://%s:%s/api/%s\", masterProtocol, masterHost, masterPort, apiVersion);\n    client = new Client(url, headers, connectTimeout, readTimeout, operationAttempts, operationSleep, streamProvider, log);\n    log.debug(\"KubePING configuration: \" + toString());\n}",
        "tuc": "@Test\npublic void testPods() throws Exception {\n    Client client = new TestClient();\n    List<Pod> pods = client.getPods(null, null, false);\n    Assert.assertNotNull(pods);\n    assertEquals(2, pods.size());\n    String pod = pods.get(0).getIp();\n    Assert.assertNotNull(pod);\n}",
        "label": 0
    },
    {
        "repo_name": "esensar___neovim-java",
        "commit": "77da5ced4c088d539aa918e2b257453f31086ba8",
        "commit_message": "Use constans in NeovimStreamApi implementation\n",
        "p_path": "neovim-api/src/main/java/com/ensarsarajcic/neovim/java/api/NeovimStreamApi.java",
        "t_path": "core-rpc/src/test/java/com/ensarsarajcic/neovim/java/corerpc/client/RPCClientTest.java",
        "p_name": "getCurrentLine",
        "t_name": "testdefaultbuilder",
        "lpfc": "@Override\npublic CompletableFuture<String> getCurrentLine() {\n    return sendWithResponseOfType(new RequestMessage.Builder(\"nvim_get_current_line\"), String.class);\n}",
        "rpfc": "@Override\npublic CompletableFuture<String> getCurrentLine() {\n    return sendWithResponseOfType(new RequestMessage.Builder(GET_CURRENT_LINE), String.class);\n}",
        "tuc": "@Test\npublic void testDefaultBuilder() {\n    // Just build straight away\n    RPCClient rpc1 = new RPCClient.Builder().build();\n}",
        "label": 0
    },
    {
        "repo_name": "pkiraly___metadata-qa-api",
        "commit": "e9e639d40581ac5a1ba8c70ff95888ad1c4768f5",
        "commit_message": "Get path for non arrays.\n",
        "p_path": "src/main/java/de/gwdg/metadataqa/api/json/JsonBranch.java",
        "t_path": "src/test/java/de/gwdg/metadataqa/api/JsonPathsTest.java",
        "p_name": "getAbsoluteJsonPath",
        "t_name": "testjsonpathmanual",
        "lpfc": "public String getAbsoluteJsonPath(int i) {\n    if (getParent() != null) {\n        return getParent().getJsonPath() + getJsonPath().replace(\"$.\", \"[\" + i + \"]\");\n    }\n    return getJsonPath();\n}",
        "rpfc": "public String getAbsoluteJsonPath(int i) {\n    if (getParent() != null) {\n        String parentPath = getParent().getJsonPath();\n        String currentPath = (i == -1) ? getJsonPath().replace(\"$.\", \"\") : getJsonPath().replace(\"$.\", \"[\" + i + \"]\");\n        return parentPath + currentPath;\n    }\n    return getJsonPath();\n}",
        "tuc": "@Test\npublic void testJsonPathManual() throws URISyntaxException, IOException {\n    document = Configuration.defaultConfiguration().jsonProvider().parse(FileUtils.readContent(\"general/book.json\"));\n    List<String> authors = JsonPath.read(document, \"$.store.book[*].author\");\n    assertEquals(Arrays.asList(\"Nigel Rees\", \"Evelyn Waugh\", \"Herman Melville\", \"J. R. R. Tolkien\"), authors);\n    assertEquals(\"Nigel Rees\", JsonPath.read(document, \"$.store.book[0].author\"));\n    assertEquals(\"Evelyn Waugh\", JsonPath.read(document, \"$.store.book[1].author\"));\n    List<Map<String, Object>> expensiveBooks = JsonPath.using(Configuration.defaultConfiguration()).parse(FileUtils.readContent(\"general/book.json\")).read(\"$.store.book[?(@.price > 10)]\", List.class);\n    assertEquals(2, expensiveBooks.size());\n    String json = \"{\\\"date_as_long\\\" : 1411455611975}\";\n    Date date = JsonPath.parse(json).read(\"$['date_as_long']\", Date.class);\n    assertEquals(\"23 Sep 2014 07:00:11 GMT\", date.toGMTString());\n    List<Map<String, Object>> books = JsonPath.read(document, \"$.store.book[?(@.price < 10)]\");\n    assertEquals(2, books.size());\n    Filter cheapFictionFilter = filter(where(\"category\").is(\"fiction\").and(\"price\").lte(10D));\n    books = JsonPath.read(document, \"$.store.book[?]\", cheapFictionFilter);\n    assertEquals(1, books.size());\n    Filter fooOrBar = filter(where(\"category\").exists(true)).or(where(\"price\").exists(true));\n    books = JsonPath.read(document, \"$.store.book[?]\", fooOrBar);\n    assertEquals(4, books.size());\n    Filter fooAndBar = filter(where(\"category\").exists(true)).and(where(\"price\").exists(true));\n    books = JsonPath.read(document, \"$.store.book[?]\", fooAndBar);\n    assertEquals(4, books.size());\n}",
        "label": 0
    },
    {
        "repo_name": "Doctoror___PainlessMusicPlayer",
        "commit": "fce23e407f2c394ced83467b1a05303cc7e5c924",
        "commit_message": "Remove setEmptyMessage in favor of using ObservableField directly\n",
        "p_path": "presentation/src/main/java/com/doctoror/fuckoffmusicplayer/presentation/library/LibraryListFragment.java",
        "t_path": "data/src/test/java/com/doctoror/fuckoffmusicplayer/data/media/MediaManagerFileTest.java",
        "p_name": "setEmptyMessage",
        "t_name": "testdeletemedia",
        "lpfc": "protected final void setEmptyMessage(@Nullable final CharSequence emptyMessage) {\n    mModel.setEmptyMessage(emptyMessage);\n}",
        "rpfc": "protected final void setEmptyMessage(@Nullable final CharSequence emptyMessage) {\n    mModel.getEmptyMessage().set(emptyMessage);\n}",
        "tuc": "@Test\npublic void testDeleteMedia() throws Exception {\n    if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.M) {\n        // Tests will not work with runtime permissions\n        return;\n    }\n    final File file = configurator.createTestFileMedia();\n    final long id = configurator.insertToMediaStoreAsMedia(file);\n    try {\n        getMediaManager().deleteMedia(id);\n        assertFalse(file.exists());\n    } finally {\n        configurator.cleanup(MediaStore.Audio.Media.EXTERNAL_CONTENT_URI, file, id);\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "Doctoror___PainlessMusicPlayer",
        "commit": "5346a77fe706f1e1dbbcb56a779bd437f26916b9",
        "commit_message": "Refactor to use WakeLockAcquirer\n",
        "p_path": "data/src/main/java/com/doctoror/fuckoffmusicplayer/data/playback/PlaybackServiceImpl.java",
        "t_path": "data/src/test/java/com/doctoror/fuckoffmusicplayer/data/media/MediaManagerMediaStoreTest.java",
        "p_name": "init",
        "t_name": "testdeletemedia",
        "lpfc": "private void init() {\n    acquireWakeLock();\n    mMediaPlayer = mMediaPlayerFactory.newMediaPlayer();\n    mDestroying = false;\n    mErrorMessage = null;\n    mAudioManager = (AudioManager) mContext.getSystemService(Context.AUDIO_SERVICE);\n    mMediaSessionHolder.openSession();\n    final MediaSessionCompat mediaSession = mMediaSessionHolder.getMediaSession();\n    if (mediaSession == null) {\n        throw new IllegalStateException(\"MediaSession is null\");\n    }\n    mPlaybackReporter = mPlaybackReporterFactory.newUniversalReporter(mediaSession);\n    mContext.registerReceiver(mBecomingNoisyReceiver, mBecomingNoisyReceiver.mIntentFilter);\n    mMediaPlayer.setListener(mMediaPlayerListener);\n    mMediaPlayer.init(mContext);\n    mDisposableQueue = mPlaybackData.queueObservable().subscribe(new QueueConsumer());\n}",
        "rpfc": "private void init() {\n    registerLifecycleObserver(new WakelockAcquirer(mContext));\n    onCreate();\n    mMediaPlayer = mMediaPlayerFactory.newMediaPlayer();\n    mDestroying = false;\n    mErrorMessage = null;\n    mAudioManager = (AudioManager) mContext.getSystemService(Context.AUDIO_SERVICE);\n    mMediaSessionHolder.openSession();\n    final MediaSessionCompat mediaSession = mMediaSessionHolder.getMediaSession();\n    if (mediaSession == null) {\n        throw new IllegalStateException(\"MediaSession is null\");\n    }\n    mPlaybackReporter = mPlaybackReporterFactory.newUniversalReporter(mediaSession);\n    mContext.registerReceiver(mBecomingNoisyReceiver, mBecomingNoisyReceiver.mIntentFilter);\n    mMediaPlayer.setListener(mMediaPlayerListener);\n    mMediaPlayer.init(mContext);\n    mDisposableQueue = mPlaybackData.queueObservable().subscribe(new QueueConsumer());\n}",
        "tuc": "@Test\npublic void testDeleteMedia() throws Exception {\n    if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.M) {\n        // Tests will not work with runtime permissions\n        return;\n    }\n    final File file = configurator.createTestFileMedia();\n    final long id = configurator.insertToMediaStoreAsMedia(file);\n    try {\n        getMediaManager().deleteMedia(id);\n        assertFalse(configurator.existsInMediaStore(MediaStore.Audio.Media.EXTERNAL_CONTENT_URI, id));\n    } finally {\n        configurator.cleanup(MediaStore.Audio.Media.EXTERNAL_CONTENT_URI, file, id);\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "vivo-project___Vitro",
        "commit": "054afd5de99daab2c1f0a228739d25ca63ac8edc",
        "commit_message": "Added captcha.\n",
        "p_path": "api/src/main/java/edu/cornell/mannlib/vitro/webapp/controller/authenticate/ForgotPassword.java",
        "t_path": "api/src/test/java/edu/cornell/mannlib/vitro/webapp/dao/vclassgroup/ProhibitedFromSearchTest.java",
        "p_name": "processRequest",
        "t_name": "testbuildingprohibited",
        "lpfc": "@Override\nprotected ResponseValues processRequest(VitroRequest vreq) throws Exception {\n    // Random time interval sleep so attacker can't calculate whether provided email is bound or not\n    sleepForRandomTime();\n    Map<String, Object> dataContext = new HashMap<>();\n    dataContext.put(\"forgotPasswordUrl\", getForgotPasswordUrl(vreq));\n    dataContext.put(\"contactUrl\", getContactUrl(vreq));\n    UserAccountsDao userAccountsDao = constructUserAccountsDao(vreq);\n    I18nBundle i18n = I18n.bundle(vreq);\n    if (vreq.getMethod().equalsIgnoreCase(\"GET\")) {\n        dataContext.put(\"showPasswordChangeForm\", true);\n        return new TemplateResponseValues(TEMPLATE_NAME, dataContext);\n    }\n    dataContext.put(\"showPasswordChangeForm\", false);\n    String email = vreq.getParameter(\"email\");\n    PasswordChangeRequestSpamMitigationResponse mitigationResponse = PasswordChangeRequestSpamMitigation.isPasswordResetRequestable(email);\n    if (!mitigationResponse.getCanBeRequested()) {\n        dataContext.put(\"message\", i18n.text(\"password_reset_too_many_requests\") + mitigationResponse.getNextRequestAvailableAtDate() + i18n.text(\"password_reset_too_many_requests_at_time\") + mitigationResponse.getNextRequestAvailableAtTime());\n        return new TemplateResponseValues(TEMPLATE_NAME, dataContext);\n    }\n    UserAccount userAccount = getAccountForInternalAuth(email, vreq);\n    if (userAccount != null) {\n        requestPasswordChange(userAccount, userAccountsDao);\n        notifyUser(userAccount, i18n, vreq);\n    }\n    PasswordChangeRequestSpamMitigation.requestSuccessfullyHandledAndUserIsNotified(email);\n    return emailSentMessage(dataContext, i18n, email);\n}",
        "rpfc": "@Override\nprotected ResponseValues processRequest(VitroRequest vreq) throws Exception {\n    // Random time interval sleep so attacker can't calculate whether provided email is bound or not\n    sleepForRandomTime();\n    Map<String, Object> dataContext = new HashMap<>();\n    dataContext.put(\"forgotPasswordUrl\", getForgotPasswordUrl(vreq));\n    dataContext.put(\"contactUrl\", getContactUrl(vreq));\n    dataContext.put(\"wrongCaptcha\", false);\n    UserAccountsDao userAccountsDao = constructUserAccountsDao(vreq);\n    I18nBundle i18n = I18n.bundle(vreq);\n    if (vreq.getMethod().equalsIgnoreCase(\"GET\")) {\n        return showForm(dataContext);\n    }\n    String captchaInput = vreq.getParameter(\"defaultReal\");\n    String captchaDisplay = vreq.getParameter(\"defaultRealHash\");\n    if (!captchaHash(captchaInput).equals(captchaDisplay)) {\n        dataContext.put(\"wrongCaptcha\", true);\n        return showForm(dataContext);\n    }\n    dataContext.put(\"showPasswordChangeForm\", false);\n    String email = vreq.getParameter(\"email\");\n    PasswordChangeRequestSpamMitigationResponse mitigationResponse = PasswordChangeRequestSpamMitigation.isPasswordResetRequestable(email);\n    if (!mitigationResponse.getCanBeRequested()) {\n        dataContext.put(\"message\", i18n.text(\"password_reset_too_many_requests\") + mitigationResponse.getNextRequestAvailableAtDate() + i18n.text(\"password_reset_too_many_requests_at_time\") + mitigationResponse.getNextRequestAvailableAtTime());\n        return new TemplateResponseValues(TEMPLATE_NAME, dataContext);\n    }\n    UserAccount userAccount = getAccountForInternalAuth(email, vreq);\n    if (userAccount != null) {\n        requestPasswordChange(userAccount, userAccountsDao);\n        notifyUser(userAccount, i18n, vreq);\n    }\n    PasswordChangeRequestSpamMitigation.requestSuccessfullyHandledAndUserIsNotified(email);\n    return emailSentMessage(dataContext, i18n, email);\n}",
        "tuc": "@Test\npublic void testBuildingProhibited() {\n    Model r = ModelFactory.createDefaultModel().read(new StringReader(n3), null, \"N3\");\n    OntModel m = ModelFactory.createOntologyModel(OntModelSpec.OWL_MEM);\n    m.add(r.listStatements());\n    Assert.assertTrue(m.size() > 4);\n    ProhibitedFromSearch pfs = new ProhibitedFromSearch(SEARCH_CONFIG_URI, m);\n    Assert.assertNotNull(pfs.prohibitedClasses);\n    Assert.assertTrue(pfs.prohibitedClasses.size() == 4);\n    Assert.assertTrue(pfs.isClassProhibitedFromSearch(TEST_CLASS));\n    Assert.assertTrue(!pfs.isClassProhibitedFromSearch(\"http://someOtherClass.com/test\"));\n}",
        "label": 0
    },
    {
        "repo_name": "apache___commons-rdf",
        "commit": "b6e938c54c5d45a79f5d88b0661688bd60dc7315",
        "commit_message": "RepositoryDatasetImpl constructors\n",
        "p_path": "rdf4j/src/main/java/org/apache/commons/rdf/rdf4j/impl/RepositoryDatasetImpl.java",
        "t_path": "rdf4j/src/test/java/org/apache/commons/rdf/rdf4j/MemoryRDFTermFactoryTest.java",
        "p_name": "getGraph",
        "t_name": "testinvalidliterallang",
        "lpfc": "@Override\npublic Optional<Graph> getGraph(BlankNodeOrIRI graphName) {\n    Resource context = (Resource) rdf4jTermFactory.asValue(graphName);\n    return new RepositoryGraphImpl(repository, includeInferred, context);\n}",
        "rpfc": "@Override\npublic Optional<Graph> getGraph(BlankNodeOrIRI graphName) {\n    Resource context = (Resource) rdf4jTermFactory.asValue(graphName);\n    return Optional.of(new RepositoryGraphImpl(repository, includeInferred, context));\n}",
        "tuc": "@Override\npublic void testInvalidLiteralLang() throws Exception {\n    Assume.assumeTrue(\"Sesame doesn't check Lang strings\", false);\n    super.testInvalidLiteralLang();\n}",
        "label": 0
    },
    {
        "repo_name": "killbilling___recurly-java-library",
        "commit": "b8d8c1c7aa39f47ebe51beadf19f18f1c54f4115",
        "commit_message": "Merge pull request #382 from saasquatch/issue20200225\n\nAdd URL encoding to URL path segments",
        "p_path": "src/main/java/com/ning/billing/recurly/RecurlyClient.java",
        "t_path": "src/test/java/com/ning/billing/recurly/model/TestPlan.java",
        "p_name": "getCoupon",
        "t_name": "testdeserializationwithamounts",
        "lpfc": "public Coupon getCoupon(final String couponCode) {\n    if (couponCode == null || couponCode.isEmpty())\n        throw new RuntimeException(\"couponCode cannot be empty!\");\n    return doGET(Coupon.COUPON_RESOURCE + \"/\" + couponCode, Coupon.class);\n}",
        "rpfc": "public Coupon getCoupon(final String couponCode) {\n    if (couponCode == null || couponCode.isEmpty())\n        throw new RuntimeException(\"couponCode cannot be empty!\");\n    return doGET(Coupon.COUPON_RESOURCE + \"/\" + urlEncode(couponCode), Coupon.class);\n}",
        "tuc": "@Test(groups = \"fast\")\npublic void testDeserializationWithAmounts() throws Exception {\n    // See https://dev.recurly.com/docs/list-plans\n    final String planData = \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n\" + \"<plan href=\\\"https://api.recurly.com/v2/plans/gold\\\">\\n\" + \"  <add_ons href=\\\"https://api.recurly.com/v2/plans/gold/add_ons\\\"/>\\n\" + \"  <plan_code>gold</plan_code>\\n\" + \"  <name>Gold plan</name>\\n\" + \"  <description nil=\\\"nil\\\"></description>\\n\" + \"  <success_url nil=\\\"nil\\\"></success_url>\\n\" + \"  <cancel_url nil=\\\"nil\\\"></cancel_url>\\n\" + \"  <display_donation_amounts type=\\\"boolean\\\">false</display_donation_amounts>\\n\" + \"  <display_quantity type=\\\"boolean\\\">false</display_quantity>\\n\" + \"  <display_phone_number type=\\\"boolean\\\">false</display_phone_number>\\n\" + \"  <bypass_hosted_confirmation type=\\\"boolean\\\">false</bypass_hosted_confirmation>\\n\" + \"  <unit_name>unit</unit_name>\\n\" + \"  <payment_page_tos_link nil=\\\"nil\\\"></payment_page_tos_link>\\n\" + \"  <plan_interval_length type=\\\"integer\\\">1</plan_interval_length>\\n\" + \"  <plan_interval_unit>months</plan_interval_unit>\\n\" + \"  <trial_interval_length type=\\\"integer\\\">0</trial_interval_length>\\n\" + \"  <trial_interval_unit>days</trial_interval_unit>\\n\" + \"  <total_billing_cycles type=\\\"integer\\\">24</total_billing_cycles>\\n\" + \"  <accounting_code nil=\\\"nil\\\"></accounting_code>\\n\" + \"  <setup_fee_accounting_code nil=\\\"nil\\\"></setup_fee_accounting_code>\\n\" + \"  <created_at type=\\\"dateTime\\\">2011-04-19T07:00:00Z</created_at>\\n\" + \"  <updated_at type=\\\"dateTime\\\">2011-04-19T07:00:00Z</updated_at>\\n\" + \"  <tax_exempt>false</tax_exempt>\\n\" + \"  <tax_code>digital</tax_code>\\n\" + \"  <accounting_code nil=\\\"nil\\\"></accounting_code>\\n\" + \"  <unit_amount_in_cents>\\n\" + \"    <USD type=\\\"integer\\\">1000</USD>\\n\" + \"    <EUR type=\\\"integer\\\">800</EUR>\\n\" + \"  </unit_amount_in_cents>\\n\" + \"  <setup_fee_in_cents>\\n\" + \"    <USD type=\\\"integer\\\">6000</USD>\\n\" + \"    <EUR type=\\\"integer\\\">4500</EUR>\\n\" + \"  </setup_fee_in_cents>\\n\" + \"</plan>\";\n    final Plan plan = xmlMapper.readValue(planData, Plan.class);\n    Assert.assertEquals(plan.getPlanCode(), \"gold\");\n    Assert.assertEquals(plan.getName(), \"Gold plan\");\n    Assert.assertEquals((int) plan.getPlanIntervalLength(), 1);\n    Assert.assertEquals(plan.getPlanIntervalUnit(), \"months\");\n    Assert.assertEquals((int) plan.getTrialIntervalLength(), 0);\n    Assert.assertEquals(plan.getTrialIntervalUnit(), \"days\");\n    Assert.assertFalse(plan.getDisplayDonationAmounts());\n    Assert.assertFalse(plan.getDisplayQuantity());\n    Assert.assertEquals(plan.getCreatedAt(), new DateTime(\"2011-04-19T07:00:00Z\"));\n    Assert.assertEquals(plan.getUpdatedAt(), new DateTime(\"2011-04-19T07:00:00Z\"));\n    Assert.assertEquals(plan.getUnitAmountInCents().getUnitAmountUSD(), new Integer(1000));\n    Assert.assertEquals(plan.getUnitAmountInCents().getUnitAmountEUR(), new Integer(800));\n    Assert.assertEquals(plan.getSetupFeeInCents().getUnitAmountUSD(), new Integer(6000));\n    Assert.assertEquals(plan.getSetupFeeInCents().getUnitAmountEUR(), new Integer(4500));\n    Assert.assertEquals(plan.getTaxExempt(), new Boolean(false));\n    Assert.assertEquals(plan.getTaxCode(), \"digital\");\n    Assert.assertNull(plan.getDescription());\n    Assert.assertNull(plan.getSuccessLink());\n    Assert.assertNull(plan.getCancelLink());\n    Assert.assertEquals(24, (int) plan.getTotalBillingCycles());\n    Assert.assertNull(plan.getAccountingCode());\n    Assert.assertNull(plan.getSetupFeeAccountingCode());\n}",
        "label": 0
    },
    {
        "repo_name": "futehkao___elements",
        "commit": "fcca092220ae54123000d38b4b2dff6391db24e3",
        "commit_message": "added setter/getter for marshaller. Changed checkResponseCode from private to protected.\n",
        "p_path": "network/src/main/java/net/e6tech/elements/network/restful/RestfulClient.java",
        "t_path": "security/src/test/java/net/e6tech/elements/security/vault/VaultManagerTest.java",
        "p_name": "checkResponseCode",
        "t_name": "basictest",
        "lpfc": "@SuppressWarnings(\"squid:MethodCyclomaticComplexity\")\nprivate void checkResponseCode(int code, String message) {\n    javax.ws.rs.core.Response.Status status = javax.ws.rs.core.Response.Status.fromStatusCode(code);\n    if (code == 500)\n        throw new InternalServerErrorException();\n    if (code > 500)\n        throw new ServiceUnavailableException();\n    switch(status) {\n        case OK:\n        case CREATED:\n        case ACCEPTED:\n        case NO_CONTENT:\n        case RESET_CONTENT:\n        case PARTIAL_CONTENT:\n            return;\n        case BAD_REQUEST:\n            throw new BadRequestException(message);\n        case UNAUTHORIZED:\n            throw new NotAuthorizedException(javax.ws.rs.core.Response.status(javax.ws.rs.core.Response.Status.UNAUTHORIZED).build());\n        case PAYMENT_REQUIRED:\n        case FORBIDDEN:\n            throw new ForbiddenException(message);\n        case NOT_FOUND:\n            throw new NotFoundException(message);\n        case METHOD_NOT_ALLOWED:\n            throw new NotAllowedException(message, javax.ws.rs.core.Response.status(javax.ws.rs.core.Response.Status.METHOD_NOT_ALLOWED).build());\n        case NOT_ACCEPTABLE:\n            throw new NotAcceptableException(message);\n        case UNSUPPORTED_MEDIA_TYPE:\n            throw new NotSupportedException(message);\n        default:\n            throw new ServerErrorException(message, status);\n    }\n}",
        "rpfc": "@SuppressWarnings(\"squid:MethodCyclomaticComplexity\")\nprotected void checkResponseCode(int code, String message) {\n    javax.ws.rs.core.Response.Status status = javax.ws.rs.core.Response.Status.fromStatusCode(code);\n    if (code == 500)\n        throw new InternalServerErrorException();\n    if (code > 500)\n        throw new ServiceUnavailableException();\n    switch(status) {\n        case OK:\n        case CREATED:\n        case ACCEPTED:\n        case NO_CONTENT:\n        case RESET_CONTENT:\n        case PARTIAL_CONTENT:\n            return;\n        case BAD_REQUEST:\n            throw new BadRequestException(message);\n        case UNAUTHORIZED:\n            throw new NotAuthorizedException(javax.ws.rs.core.Response.status(javax.ws.rs.core.Response.Status.UNAUTHORIZED).build());\n        case PAYMENT_REQUIRED:\n        case FORBIDDEN:\n            throw new ForbiddenException(message);\n        case NOT_FOUND:\n            throw new NotFoundException(message);\n        case METHOD_NOT_ALLOWED:\n            throw new NotAllowedException(message, javax.ws.rs.core.Response.status(javax.ws.rs.core.Response.Status.METHOD_NOT_ALLOWED).build());\n        case NOT_ACCEPTABLE:\n            throw new NotAcceptableException(message);\n        case UNSUPPORTED_MEDIA_TYPE:\n            throw new NotSupportedException(message);\n        default:\n            throw new ServerErrorException(message, status);\n    }\n}",
        "tuc": "@Test\npublic void basicTest() throws Exception {\n    ClearText clearText = new ClearText();\n    clearText.setBytes(new byte[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 });\n    clearText.setProperty(\"test\", \"test\");\n    manager.addSecretData(dualEntry, \"secret\", clearText);\n    clearText = manager.getSecretData(dualEntry.getUser1(), \"secret\");\n    manager.newMasterKey(dualEntry);\n    String str = manager.getUserLocalStore().writeString();\n    System.out.println(str);\n    // save\n    manager.save();\n    reopen();\n    clearText = manager.getSecretData(dualEntry.getUser1(), \"secret\");\n    clearText.getProperty(\"test\");\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0; i < 100; i++) builder.append((char) ('a' + (i % 26)));\n    String data = builder.toString();\n    String pubEncrypted = manager.encryptPublic(data.getBytes(StandardCharsets.UTF_8));\n    String privDecrypted = new String(manager.decryptPrivate(pubEncrypted), StandardCharsets.UTF_8);\n}",
        "label": 0
    },
    {
        "repo_name": "kongzhidea___commons-dbutil",
        "commit": "e5946f1cc89c495d4fc05966a5feb083b2d80f2b",
        "commit_message": "test decimal\n",
        "p_path": "dbaccess/src/main/java/com/kk/dbaccess/op/DataAccessMgr.java",
        "t_path": "dbaccess/src/test/java/com/kk/dbaccess/op/InsertTest.java",
        "p_name": "queryDate",
        "t_name": "testinsertreturnid",
        "lpfc": "public Date queryDate(final OpUniq op) throws SQLException {\n    PreparedStatement ps = null;\n    ResultSet rs = null;\n    Connection conn = null;\n    Date result = null;\n    try {\n        conn = op.getConnection();\n        ps = conn.prepareStatement(op.getSql());\n        op.setParam(ps);\n        rs = ps.executeQuery();\n        if (rs.next()) {\n            Object o = op.parse(rs, Date.class);\n            result = NumberUtil.parseDate(o);\n        } else\n            return null;\n        if (rs.next()) {\n            logger.error(\"Non Unique Result Error: wrong sql syntax or database not consistence!\");\n        }\n        return result;\n    } finally {\n        closeRSC(rs, ps, conn);\n    }\n}",
        "rpfc": "public Date queryDate(final OpUniq op) throws SQLException {\n    op.setClz(Date.class);\n    return (Date) queryObject(op);\n}",
        "tuc": "@Test\npublic void testInsertReturnId() throws SQLException {\n    String sql = \"insert user(username,realname,ctime) values(?,?,?)\";\n    long id = dataAccessMgr.insertReturnId(new OpUpdate(dataSource, sql, \"_op_3\", \"kkl\", new Date()));\n    logger.info(id);\n}",
        "label": 0
    },
    {
        "repo_name": "LearnLib___automatalib",
        "commit": "837e710b0b7dc90709eca024d0a51c7051f84945",
        "commit_message": "use native equals method\n",
        "p_path": "incremental/src/main/java/net/automatalib/incremental/mealy/tree/AbstractIncrementalMealyTreeBuilder.java",
        "t_path": "serialization/fsm/src/test/java/net/automatalib/serialization/fsm/parser/FSM2MealyParserIOTest.java",
        "p_name": "doFindSeparatingWord",
        "t_name": "donotcloseinputstreamtest",
        "lpfc": "@Nullable\nprivate <S, T> Word<I> doFindSeparatingWord(MealyMachine<S, I, T, O> target, Collection<? extends I> inputs, boolean omitUndefined) {\n    Deque<Record<@Nullable S, N, I>> dfsStack = new ArrayDeque<>();\n    @SuppressWarnings(\"nullness\")\n    final Record<@Nullable S, N, I> init = new Record<>(target.getInitialState(), root, null, inputs.iterator());\n    dfsStack.push(init);\n    while (!dfsStack.isEmpty()) {\n        @SuppressWarnings(\"nullness\")\n        @NonNull\n        Record<@Nullable S, N, I> rec = dfsStack.peek();\n        if (!rec.inputIt.hasNext()) {\n            dfsStack.pop();\n            continue;\n        }\n        I input = rec.inputIt.next();\n        Edge<N, O> edge = getEdge(rec.treeNode, input);\n        if (edge == null) {\n            continue;\n        }\n        S state = rec.automatonState;\n        T trans = state == null ? null : target.getTransition(state, input);\n        if (omitUndefined && trans == null) {\n            continue;\n        }\n        if (trans == null || !Objects.equal(target.getTransitionOutput(trans), edge.getOutput())) {\n            WordBuilder<I> wb = new WordBuilder<>(dfsStack.size());\n            wb.append(input);\n            dfsStack.pop();\n            while (!dfsStack.isEmpty()) {\n                wb.append(rec.incomingInput);\n                rec = dfsStack.pop();\n            }\n            return wb.reverse().toWord();\n        }\n        final Record<@Nullable S, N, I> nextRecord = new Record<>(target.getSuccessor(trans), edge.getTarget(), input, inputs.iterator());\n        dfsStack.push(nextRecord);\n    }\n    return null;\n}",
        "rpfc": "@Nullable\nprivate <S, T> Word<I> doFindSeparatingWord(MealyMachine<S, I, T, O> target, Collection<? extends I> inputs, boolean omitUndefined) {\n    Deque<Record<@Nullable S, N, I>> dfsStack = new ArrayDeque<>();\n    @SuppressWarnings(\"nullness\")\n    final Record<@Nullable S, N, I> init = new Record<>(target.getInitialState(), root, null, inputs.iterator());\n    dfsStack.push(init);\n    while (!dfsStack.isEmpty()) {\n        @SuppressWarnings(\"nullness\")\n        @NonNull\n        Record<@Nullable S, N, I> rec = dfsStack.peek();\n        if (!rec.inputIt.hasNext()) {\n            dfsStack.pop();\n            continue;\n        }\n        I input = rec.inputIt.next();\n        Edge<N, O> edge = getEdge(rec.treeNode, input);\n        if (edge == null) {\n            continue;\n        }\n        S state = rec.automatonState;\n        T trans = state == null ? null : target.getTransition(state, input);\n        if (omitUndefined && trans == null) {\n            continue;\n        }\n        if (trans == null || !Objects.equals(target.getTransitionOutput(trans), edge.getOutput())) {\n            WordBuilder<I> wb = new WordBuilder<>(dfsStack.size());\n            wb.append(input);\n            dfsStack.pop();\n            while (!dfsStack.isEmpty()) {\n                wb.append(rec.incomingInput);\n                rec = dfsStack.pop();\n            }\n            return wb.reverse().toWord();\n        }\n        final Record<@Nullable S, N, I> nextRecord = new Record<>(target.getSuccessor(trans), edge.getTarget(), input, inputs.iterator());\n        dfsStack.push(nextRecord);\n    }\n    return null;\n}",
        "tuc": "@Test\npublic void doNotCloseInputStreamTest() throws IOException {\n    try (InputStream is = FSM2MealyParserIOTest.class.getResourceAsStream(\"/MealyIO.fsm\")) {\n        FSM2MealyParserIO.getParser(s -> s.charAt(0)).readModel(new UnclosableInputStream(is));\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "Doctoror___PainlessMusicPlayer",
        "commit": "c12ffada819cc630f1b1e341fa490806b89221c7",
        "commit_message": "Fix stop timer not aborting on play\n",
        "p_path": "data/src/main/java/com/doctoror/fuckoffmusicplayer/data/playback/PlaybackServiceImpl.java",
        "t_path": "data/src/test/java/com/doctoror/fuckoffmusicplayer/data/media/MediaManagerFileTest.java",
        "p_name": "onPlaybackStarted",
        "t_name": "testdeleteplaylist",
        "lpfc": "@Override\npublic void onPlaybackStarted() {\n    errorMessage = null;\n    setState(STATE_PLAYING);\n    showNotification();\n    unitMediaPositionUpdater.initializeMediaPositionUpdater();\n}",
        "rpfc": "@Override\npublic void onPlaybackStarted() {\n    unitStopTimeout.abortStopTimer();\n    errorMessage = null;\n    setState(STATE_PLAYING);\n    showNotification();\n    unitMediaPositionUpdater.initializeMediaPositionUpdater();\n}",
        "tuc": "@Test\npublic void testDeletePlaylist() throws Exception {\n    if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.M) {\n        // Tests will not work with runtime permissions\n        return;\n    }\n    final File file = configurator.createTestFilePlaylist();\n    final long id = configurator.insertToMediaStoreAsPlaylist(file);\n    try {\n        getMediaManager().deletePlaylist(id);\n        assertFalse(file.exists());\n    } finally {\n        configurator.cleanup(MediaStore.Audio.Playlists.EXTERNAL_CONTENT_URI, file, id);\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "Doctoror___PainlessMusicPlayer",
        "commit": "e0d4da6368718c9ae78ca11cd01484f21e47e253",
        "commit_message": "Extract QueueMonitor\n",
        "p_path": "data/src/main/java/com/doctoror/fuckoffmusicplayer/data/playback/PlaybackServiceImpl.java",
        "t_path": "data/src/test/java/com/doctoror/fuckoffmusicplayer/data/util/ProtoUtilsTest.java",
        "p_name": "onPlaybackFinished",
        "t_name": "testreadfromfilewhennull",
        "lpfc": "@Override\npublic void onPlaybackFinished() {\n    mErrorMessage = null;\n    mCurrentTrack = null;\n    if (!mDestroying) {\n        playNextInner(false);\n    }\n}",
        "rpfc": "@Override\npublic void onPlaybackFinished() {\n    mErrorMessage = null;\n    if (!mDestroying) {\n        playNextInner(false);\n    }\n}",
        "tuc": "@Test\npublic void testReadFromFileWhenNull() {\n    final String fileName = \"protoUtilsReadFromFileWhenNull\";\n    final Context context = RuntimeEnvironment.application;\n    context.deleteFile(fileName);\n    final SettingsProto.Settings read = ProtoUtils.readFromFile(context, fileName, new SettingsProto.Settings());\n    assertNull(read);\n}",
        "label": 0
    },
    {
        "repo_name": "apache___empire-db",
        "commit": "ba0fa6612abe529c2e849c7e5f5a99770ff9717c",
        "commit_message": "fixing query in example-basic for postgresql\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/empire-db/trunk@1181643 13f79535-47bb-0310-9956-ffa450edef68\n",
        "p_path": "empire-db-examples/empire-db-example-basic/src/main/java/org/apache/empire/samples/db/SampleApp.java",
        "t_path": "empire-db/src/test/java/org/apache/empire/commons/ObjectUtilsTest.java",
        "p_name": "queryRecords",
        "t_name": "testisempty",
        "lpfc": "private static void queryRecords(Connection conn, QueryType queryType) {\n    DBCommand cmd = db.createCommand();\n    SampleDB.Employees EMP = db.EMPLOYEES;\n    SampleDB.Departments DEP = db.DEPARTMENTS;\n    DBColumnExpr EMPLOYEE_FULLNAME = EMP.LASTNAME.append(\", \").append(EMP.FIRSTNAME).as(\"FULL_NAME\");\n    DBColumnExpr PHONE_LAST_DASH;\n    if (db.getDriver() instanceof DBDatabaseDriverHSql || db.getDriver() instanceof DBDatabaseDriverDerby || db.getDriver() instanceof DBDatabaseDriverH2)\n        PHONE_LAST_DASH = EMP.PHONE_NUMBER.indexOf(\"-\", EMP.PHONE_NUMBER.indexOf(\"-\").plus(1)).plus(1);\n    else\n        PHONE_LAST_DASH = EMP.PHONE_NUMBER.length().minus(EMP.PHONE_NUMBER.reverse().indexOf(\"-\")).plus(2);\n    DBColumnExpr PHONE_EXT_NUMBER = EMP.PHONE_NUMBER.substring(PHONE_LAST_DASH).as(\"PHONE_EXTENSION\");\n    cmd.select(EMP.EMPLOYEE_ID, EMPLOYEE_FULLNAME);\n    if (db.getDriver() instanceof DBDatabaseDriverPostgreSQL) {\n        cmd.select(EMP.GENDER, EMP.PHONE_NUMBER);\n    } else {\n        cmd.select(EMP.GENDER, EMP.PHONE_NUMBER, PHONE_EXT_NUMBER);\n    }\n    cmd.select(DEP.NAME.as(\"DEPARTMENT\"));\n    cmd.select(DEP.BUSINESS_UNIT);\n    cmd.join(EMP.DEPARTMENT_ID, DEP.DEPARTMENT_ID);\n    cmd.where(EMP.LASTNAME.length().isGreaterThan(0));\n    cmd.orderBy(EMP.LASTNAME, EMP.FIRSTNAME);\n    DBReader reader = new DBReader();\n    try {\n        System.out.println(\"Running Query:\");\n        System.out.println(cmd.getSelect());\n        reader.open(cmd, conn);\n        System.out.println(\"---------------------------------\");\n        switch(queryType) {\n            case Reader:\n                while (reader.moveNext()) {\n                    System.out.println(reader.getString(EMP.EMPLOYEE_ID) + \"\\t\" + reader.getString(EMPLOYEE_FULLNAME) + \"\\t\" + EMP.GENDER.getOptions().get(reader.getString(EMP.GENDER)) + \"\\t\" + reader.getString(PHONE_EXT_NUMBER) + \"\\t\" + reader.getString(DEP.NAME));\n                }\n                break;\n            case BeanList:\n                List<SampleBean> beanList = reader.getBeanList(SampleBean.class);\n                System.out.println(String.valueOf(beanList.size()) + \" SampleBeans returned from Query.\");\n                for (SampleBean b : beanList) {\n                    System.out.println(b.toString());\n                }\n                break;\n            case XmlDocument:\n                Document doc = reader.getXmlDocument();\n                XMLWriter.debug(doc);\n                break;\n        }\n    } finally {\n        reader.close();\n    }\n}",
        "rpfc": "private static void queryRecords(Connection conn, QueryType queryType) {\n    DBCommand cmd = db.createCommand();\n    SampleDB.Employees EMP = db.EMPLOYEES;\n    SampleDB.Departments DEP = db.DEPARTMENTS;\n    DBColumnExpr EMPLOYEE_FULLNAME = EMP.LASTNAME.append(\", \").append(EMP.FIRSTNAME).as(\"FULL_NAME\");\n    DBColumnExpr PHONE_LAST_DASH;\n    if (db.getDriver() instanceof DBDatabaseDriverHSql || db.getDriver() instanceof DBDatabaseDriverDerby || db.getDriver() instanceof DBDatabaseDriverH2)\n        PHONE_LAST_DASH = EMP.PHONE_NUMBER.indexOf(\"-\", EMP.PHONE_NUMBER.indexOf(\"-\").plus(1)).plus(1);\n    else\n        PHONE_LAST_DASH = EMP.PHONE_NUMBER.length().minus(EMP.PHONE_NUMBER.reverse().indexOf(\"-\")).plus(2);\n    DBColumnExpr PHONE_EXT_NUMBER = EMP.PHONE_NUMBER.substring(PHONE_LAST_DASH).as(\"PHONE_EXTENSION\");\n    cmd.select(EMP.EMPLOYEE_ID, EMPLOYEE_FULLNAME);\n    cmd.select(EMP.GENDER, EMP.PHONE_NUMBER, PHONE_EXT_NUMBER);\n    cmd.select(DEP.NAME.as(\"DEPARTMENT\"));\n    cmd.select(DEP.BUSINESS_UNIT);\n    cmd.join(EMP.DEPARTMENT_ID, DEP.DEPARTMENT_ID);\n    cmd.where(EMP.LASTNAME.length().isGreaterThan(0));\n    cmd.orderBy(EMP.LASTNAME, EMP.FIRSTNAME);\n    DBReader reader = new DBReader();\n    try {\n        System.out.println(\"Running Query:\");\n        System.out.println(cmd.getSelect());\n        reader.open(cmd, conn);\n        System.out.println(\"---------------------------------\");\n        switch(queryType) {\n            case Reader:\n                while (reader.moveNext()) {\n                    System.out.println(reader.getString(EMP.EMPLOYEE_ID) + \"\\t\" + reader.getString(EMPLOYEE_FULLNAME) + \"\\t\" + EMP.GENDER.getOptions().get(reader.getString(EMP.GENDER)) + \"\\t\" + reader.getString(PHONE_EXT_NUMBER) + \"\\t\" + reader.getString(DEP.NAME));\n                }\n                break;\n            case BeanList:\n                List<SampleBean> beanList = reader.getBeanList(SampleBean.class);\n                System.out.println(String.valueOf(beanList.size()) + \" SampleBeans returned from Query.\");\n                for (SampleBean b : beanList) {\n                    System.out.println(b.toString());\n                }\n                break;\n            case XmlDocument:\n                Document doc = reader.getXmlDocument();\n                XMLWriter.debug(doc);\n                break;\n        }\n    } finally {\n        reader.close();\n    }\n}",
        "tuc": "/**\n * Test method for\n * {@link org.apache.empire.commons.ObjectUtils#isEmpty(java.lang.Object)}.\n */\n@Test\npublic void testIsEmpty() {\n    assertTrue(ObjectUtils.isEmpty(ObjectUtils.NO_VALUE));\n    assertTrue(ObjectUtils.isEmpty(\"\"));\n    assertTrue(ObjectUtils.isEmpty(null));\n    assertFalse(ObjectUtils.isEmpty(\" \"));\n    assertFalse(ObjectUtils.isEmpty(new Object()));\n}",
        "label": 0
    },
    {
        "repo_name": "koocyton___reactor-guice",
        "commit": "b9b6504cf9c6417c93e3a1eb52f4730ecbae9c56",
        "commit_message": "\u4e0d\u5c0f\u5fc3\u7559\u4e0b\u4e2a BUG\n",
        "p_path": "src/main/java/com/doopp/reactor/guice/ReactorGuiceServer.java",
        "t_path": "src/test/java/com/doopp/reactor/guice/test/service/impl/TestServiceImpl.java",
        "p_name": "httpPublisher",
        "t_name": "filtercalltest",
        "lpfc": "private Publisher<Void> httpPublisher(HttpServerRequest req, HttpServerResponse resp, Method method, Function<Object, Mono<Object>> handle) {\n    if (req.isKeepAlive()) {\n        resp.header(HttpHeaderNames.CONNECTION, HttpHeaderValues.KEEP_ALIVE);\n    }\n    resp.header(HttpHeaderNames.SERVER, \"power by reactor\");\n    if (crossOrigin) {\n        resp.header(\"Access-Control-Allow-Origin\", \"*\").header(\"Access-Control-Allow-Headers\", \"X-Requested-With, accept, origin, content-type\").header(\"Access-Control-Allow-Methods\", \"PUT,POST,GET,DELETE,OPTIONS\");\n    }\n    return doFilter(req, resp, new RequestAttribute()).flatMap(handle).onErrorMap(throwable -> {\n        if (this.printError) {\n            throwable.printStackTrace();\n        }\n        if (handlePublisher.methodProductsValue(method).contains(MediaType.APPLICATION_JSON)) {\n            if (throwable instanceof StatusMessageException) {\n                resp.status(((StatusMessageException) throwable).getCode());\n                resp.header(HttpHeaderNames.CONTENT_TYPE, MediaType.APPLICATION_JSON);\n                return throwable;\n            } else {\n                resp.status(HttpResponseStatus.INTERNAL_SERVER_ERROR);\n                resp.header(HttpHeaderNames.CONTENT_TYPE, MediaType.APPLICATION_JSON);\n                return new StatusMessageException(HttpResponseStatus.INTERNAL_SERVER_ERROR, throwable.getMessage());\n            }\n        } else {\n            if (throwable instanceof StatusMessageException) {\n                resp.status(((StatusMessageException) throwable).getCode());\n                resp.header(HttpHeaderNames.CONTENT_TYPE, MediaType.TEXT_PLAIN);\n                return new Exception(throwable.getMessage());\n            } else {\n                resp.status(HttpResponseStatus.INTERNAL_SERVER_ERROR);\n                resp.header(HttpHeaderNames.CONTENT_TYPE, MediaType.TEXT_PLAIN);\n                return throwable;\n            }\n        }\n    }).onErrorResume(throwable -> {\n        if (throwable instanceof StatusMessageException) {\n            if (handlePublisher.getHttpMessageConverter() == null) {\n                return Mono.just(\"{\\\"err_code\\\":500, \\\"err_msg\\\":\\\"A Message Converter instance is required\\\", \\\"data\\\":null}\");\n            }\n            return Mono.just(handlePublisher.getHttpMessageConverter().toJson(throwable));\n        } else {\n            return Mono.just(throwable.getMessage());\n        }\n    }).flatMap(o -> {\n        return (o instanceof String) ? resp.sendString(Mono.just((String) o)).then() : resp.sendObject(Mono.just(o)).then();\n    });\n}",
        "rpfc": "private Publisher<Void> httpPublisher(HttpServerRequest req, HttpServerResponse resp, Method method, Function<Object, Mono<Object>> handle) {\n    if (req.isKeepAlive()) {\n        resp.header(HttpHeaderNames.CONNECTION, HttpHeaderValues.KEEP_ALIVE);\n    }\n    resp.header(HttpHeaderNames.SERVER, \"power by reactor\");\n    if (crossOrigin) {\n        resp.header(\"Access-Control-Allow-Origin\", \"*\").header(\"Access-Control-Allow-Headers\", \"X-Requested-With, accept, origin, content-type\").header(\"Access-Control-Allow-Methods\", \"PUT,POST,GET,DELETE,OPTIONS\");\n    }\n    return doFilter(req, resp, new RequestAttribute()).flatMap(handle).onErrorMap(throwable -> {\n        if (this.printError) {\n            throwable.printStackTrace();\n        }\n        if (handlePublisher.methodProductsValue(method).contains(MediaType.APPLICATION_JSON)) {\n            if (throwable instanceof StatusMessageException) {\n                resp.status(((StatusMessageException) throwable).getCode());\n                resp.header(HttpHeaderNames.CONTENT_TYPE, MediaType.APPLICATION_JSON);\n                return throwable;\n            } else {\n                resp.status(HttpResponseStatus.INTERNAL_SERVER_ERROR);\n                resp.header(HttpHeaderNames.CONTENT_TYPE, MediaType.APPLICATION_JSON);\n                return new StatusMessageException(HttpResponseStatus.INTERNAL_SERVER_ERROR, throwable.getMessage());\n            }\n        } else {\n            if (throwable instanceof StatusMessageException) {\n                resp.status(((StatusMessageException) throwable).getCode());\n                resp.header(HttpHeaderNames.CONTENT_TYPE, MediaType.TEXT_PLAIN);\n                return new Exception(throwable.getMessage());\n            } else {\n                resp.status(HttpResponseStatus.INTERNAL_SERVER_ERROR);\n                resp.header(HttpHeaderNames.CONTENT_TYPE, MediaType.TEXT_PLAIN);\n                return throwable;\n            }\n        }\n    }).onErrorResume(throwable -> {\n        if (throwable instanceof StatusMessageException) {\n            if (handlePublisher.getHttpMessageConverter() == null) {\n                return Mono.just(\"{\\\"err_code\\\":500, \\\"err_msg\\\":\\\"A Message Converter instance is required\\\", \\\"data\\\":null}\");\n            }\n            return Mono.just(handlePublisher.getHttpMessageConverter().toJson(throwable));\n        } else {\n            return Mono.just(throwable.getMessage());\n        }\n    }).flatMap(o -> {\n        if (o instanceof Mono) {\n            return (Mono<Void>) o;\n        }\n        return (o instanceof String) ? resp.sendString(Mono.just((String) o)).then() : resp.sendObject(Mono.just(o)).then();\n    });\n}",
        "tuc": "@Override\npublic void filterCallTest() {\n    // System.out.println(\"filterCallTest\");\n}",
        "label": 0
    },
    {
        "repo_name": "arcus-smart-home___arcusplatform",
        "commit": "5ca2b1fe4e6ee47432f5fa2eb3630acd70ed5176",
        "commit_message": "Don't use bouncycastle\n",
        "p_path": "platform/arcus-security/src/main/java/com/iris/security/crypto/AES.java",
        "t_path": "platform/arcus-lib/src/test/java/com/iris/platform/rule/catalog/serializer/TestSelectorProcessor.java",
        "p_name": "encryptSafe",
        "t_name": "testscenetypes",
        "lpfc": "protected String encryptSafe(String key, String value) {\n    Preconditions.checkNotNull(key, \"key cannot be null\");\n    if (StringUtils.isBlank(value)) {\n        return null;\n    }\n    try {\n        SecretKeySpec keySpec = new SecretKeySpec(sha1(key, secret), KEY_ALG);\n        byte[] iv = new byte[GCM_IV_LENGTH];\n        this.random.nextBytes(iv);\n        Cipher cipher = Cipher.getInstance(\"AES/GCM/NoPadding\", \"BC\");\n        IvParameterSpec ivParamSpec = new IvParameterSpec(iv);\n        cipher.init(Cipher.ENCRYPT_MODE, keySpec, ivParamSpec);\n        byte[] data = cipher.doFinal(value.getBytes(\"UTF-8\"));\n        byte[] ciphertext = Bytes.concat(iv, data);\n        return Utils.b64Encode(ciphertext);\n    } catch (NoSuchProviderException e) {\n        logger.error(\"Couldn't find BouncyCastle Provider!\", e);\n        throw new RuntimeException(e);\n    } catch (Exception e) {\n        throw new RuntimeException(e);\n    }\n}",
        "rpfc": "protected String encryptSafe(String key, String value) {\n    Preconditions.checkNotNull(key, \"key cannot be null\");\n    if (StringUtils.isBlank(value)) {\n        return null;\n    }\n    try {\n        SecretKeySpec keySpec = new SecretKeySpec(sha1(key, secret), KEY_ALG);\n        byte[] iv = new byte[GCM_IV_LENGTH];\n        this.random.nextBytes(iv);\n        Cipher cipher = Cipher.getInstance(\"AES/GCM/NoPadding\");\n        IvParameterSpec ivParamSpec = new IvParameterSpec(iv);\n        cipher.init(Cipher.ENCRYPT_MODE, keySpec, ivParamSpec);\n        byte[] data = cipher.doFinal(value.getBytes(\"UTF-8\"));\n        byte[] ciphertext = Bytes.concat(iv, data);\n        return Utils.b64Encode(ciphertext);\n    } catch (NoSuchProviderException e) {\n        logger.error(\"Couldn't find BouncyCastle Provider!\", e);\n        throw new RuntimeException(e);\n    } catch (Exception e) {\n        throw new RuntimeException(e);\n    }\n}",
        "tuc": "@Test\npublic void testSceneTypes() throws Exception {\n    String xml = \"<selector name='name' type='\" + SelectorProcessor.TYPE_SCENE + \"' />\";\n    SAXTagHandlers.parse(xml, SelectorProcessor.TAG, processor);\n    assertEquals(\"name\", processor.getName());\n    ListSelectorGenerator generator = (ListSelectorGenerator) processor.getSelectorGenerator();\n    assertNotNull(generator);\n    // TODO make assertions about the state of the generator?\n}",
        "label": 0
    },
    {
        "repo_name": "zhongxunking___ids",
        "commit": "542ad3f40fff0c4e975cb22f31e19c01e344afe2",
        "commit_message": "IDS\u83b7\u53d6\u7684id\u5468\u671f\u7c7b\u578b\u5fc5\u987b\u662f\u65e0\n",
        "p_path": "src/main/java/org/antframework/ids/IDS.java",
        "t_path": "src/test/java/org/antframework/ids/UIDTest.java",
        "p_name": "newId",
        "t_name": "testnewidperformance",
        "lpfc": "/**\n * \u521b\u5efa\u65b0\u7684id\n *\n * @param idCode id\u7f16\u7801\n * @return \u65b0\u7684id\n * @throws IllegalStateException \u5982\u679c\u83b7\u53d6id\u5931\u8d25\n */\npublic static long newId(String idCode) {\n    IdAcquirer idAcquirer = CACHE.get(idCode).getAcquirer();\n    Id id = idAcquirer.getId();\n    if (id == null) {\n        throw new IllegalStateException(\"\u4eceid\u4e2d\u5fc3\u83b7\u53d6id\u5931\u8d25\");\n    }\n    if (id.getPeriod().getType() != PeriodType.NONE) {\n        throw new IllegalStateException(\"id\u5468\u671f\u7c7b\u578b\u5fc5\u987b\u662f\u65e0\");\n    }\n    return id.getId();\n}",
        "rpfc": "/**\n * \u521b\u5efa\u65b0\u7684id\n *\n * @param idCode id\u7f16\u7801\n * @return \u65b0\u7684id\n * @throws IllegalStateException \u5982\u679c\u83b7\u53d6id\u5931\u8d25\n */\npublic static long newId(String idCode) {\n    IdAcquirer idAcquirer = CACHE.get(idCode).getAcquirer();\n    Id id = idAcquirer.getId();\n    if (id == null) {\n        throw new IllegalStateException(\"\u4eceid\u4e2d\u5fc3\u83b7\u53d6id\u5931\u8d25\");\n    }\n    if (id.getPeriod().getType() != PeriodType.NONE) {\n        throw new IllegalStateException(idCode + \"\u7684\u5468\u671f\u7c7b\u578b\u5fc5\u987b\u662f\u65e0\");\n    }\n    return id.getId();\n}",
        "tuc": "@Test\npublic void testNewIdPerformance() {\n    UID.newId();\n    long startTime = System.currentTimeMillis();\n    int count = 10000000;\n    int nullCount = 0;\n    for (int i = 0; i < count; i++) {\n        String id = UID.newId();\n        if (id == null) {\n            nullCount++;\n        }\n    }\n    long timeCost = System.currentTimeMillis() - startTime;\n    System.out.println(String.format(\"\u5faa\u73af\uff1a%d\u6b21\uff0cid\u51fa\u73b0null\uff1a%d\u6b21\uff0c\u603b\u8017\u65f6\uff1a%d\u6beb\u79d2\uff0ctps\uff1a%d\", count, nullCount, timeCost, (count - nullCount) * 1000L / timeCost));\n}",
        "label": 0
    },
    {
        "repo_name": "vert-x3___vertx-mongo-client",
        "commit": "b670aaafc82279fc2af887200332c9fc6886376b",
        "commit_message": "Futurized docs\n",
        "p_path": "src/main/java/examples/MongoClientExamples.java",
        "t_path": "src/test/java/io/vertx/ext/mongo/MongoClientTest.java",
        "p_name": "example14_02_dl",
        "t_name": "testfindbatch",
        "lpfc": "public void example14_02_dl(MongoClient mongoService) {\n    String base64EncodedString = \"a2FpbHVhIGlzIHRoZSAjMSBiZWFjaCBpbiB0aGUgd29ybGQ=\";\n    JsonObject document = new JsonObject().put(\"name\", \"Alan Turing\").put(\"binaryStuff\", new JsonObject().put(\"$binary\", base64EncodedString));\n    mongoService.save(\"smartPeople\", document, res -> {\n        if (res.succeeded()) {\n            String id = res.result();\n            mongoService.findOne(\"smartPeople\", new JsonObject().put(\"_id\", id), null, res2 -> {\n                if (res2.succeeded()) {\n                    String reconstitutedBase64EncodedString = res2.result().getJsonObject(\"binaryStuff\").getString(\"$binary\");\n                } else {\n                    res2.cause().printStackTrace();\n                }\n            });\n        } else {\n            res.cause().printStackTrace();\n        }\n    });\n}",
        "rpfc": "public void example14_02_dl(MongoClient mongoService) {\n    String base64EncodedString = \"a2FpbHVhIGlzIHRoZSAjMSBiZWFjaCBpbiB0aGUgd29ybGQ=\";\n    JsonObject document = new JsonObject().put(\"name\", \"Alan Turing\").put(\"binaryStuff\", new JsonObject().put(\"$binary\", base64EncodedString));\n    mongoService.save(\"smartPeople\", document).compose(id -> {\n        return mongoService.findOne(\"smartPeople\", new JsonObject().put(\"_id\", id), null);\n    }).onComplete(res -> {\n        if (res.succeeded()) {\n            String reconstitutedBase64EncodedString = res.result().getJsonObject(\"binaryStuff\").getString(\"$binary\");\n        } else {\n            res.cause().printStackTrace();\n        }\n    });\n}",
        "tuc": "private void testFindBatch(int numDocs, BiFunction<CountDownLatch, ReadStream<JsonObject>, List<String>> checker) throws Exception {\n    AtomicReference<ReadStream<JsonObject>> streamReference = new AtomicReference<>();\n    String collection = randomCollection();\n    CountDownLatch latch = new CountDownLatch(1);\n    AtomicReference<List<String>> foos = new AtomicReference<>();\n    mongoClient.createCollection(collection, onSuccess(res -> {\n        insertDocs(mongoClient, collection, numDocs, onSuccess(res2 -> {\n            FindOptions findOptions = new FindOptions().setSort(new JsonObject().put(\"counter\", 1)).setBatchSize(1);\n            ReadStream<JsonObject> stream = mongoClient.findBatchWithOptions(collection, new JsonObject(), findOptions);\n            streamReference.set(stream);\n            foos.set(checker.apply(latch, stream));\n        }));\n    }));\n    awaitLatch(latch);\n    assertEquals(numDocs, foos.get().size());\n    assertEquals(\"bar0\", foos.get().get(0));\n    assertEquals(\"bar\" + (numDocs - 1), foos.get().get(numDocs - 1));\n    // Make sure stream handlers can be set to null after closing\n    streamReference.get().handler(null).exceptionHandler(null).endHandler(null);\n}",
        "label": 0
    },
    {
        "repo_name": "jenkinsci___analysis-core-plugin",
        "commit": "574ebb743f9d093f5562af4381f0e4d8e39150d8",
        "commit_message": "Fixed warning.",
        "p_path": "src/main/java/hudson/plugins/analysis/core/BuildResultEvaluator.java",
        "t_path": "src/test/java/hudson/plugins/analysis/core/HealthReportBuilderTest.java",
        "p_name": "logSuccess",
        "t_name": "testnohealthyreport",
        "lpfc": "private Result logSuccess(final StringBuilder logger) {\n    logger.append(hudson.plugins.analysis.Messages.BuildResultEvaluator_success());\n    return Result.SUCCESS;\n}",
        "rpfc": "private Result logSuccess(final StringBuilder logger) {\n    logger.append(Messages.BuildResultEvaluator_success());\n    return Result.SUCCESS;\n}",
        "tuc": "/**\n * Tests whether we don't get a healthy report if the reporting is disabled.\n */\n@Test\npublic void testNoHealthyReport() {\n    HealthReport health = createHealthReport(false, 0, 100, 75);\n    assertNull(ERROR_MESSAGE, health);\n}",
        "label": 0
    },
    {
        "repo_name": "killbilling___recurly-java-library",
        "commit": "acfb6d6ff41f8171ebb8c45dbe41f818495a2f1f",
        "commit_message": "account: add missing billingInfo in toString, equals and hashCode\n\nSigned-off-by: Pierre-Alexandre Meyer <pierre@mouraf.org>\n",
        "p_path": "src/main/java/com/ning/billing/recurly/model/Account.java",
        "t_path": "src/test/java/com/ning/billing/recurly/model/TestBillingInfo.java",
        "p_name": "hashCode",
        "t_name": "testserialization",
        "lpfc": "@Override\npublic int hashCode() {\n    int result = adjustments != null ? adjustments.hashCode() : 0;\n    result = 31 * result + (invoices != null ? invoices.hashCode() : 0);\n    result = 31 * result + (subscriptions != null ? subscriptions.hashCode() : 0);\n    result = 31 * result + (transactions != null ? transactions.hashCode() : 0);\n    result = 31 * result + (accountCode != null ? accountCode.hashCode() : 0);\n    result = 31 * result + (state != null ? state.hashCode() : 0);\n    result = 31 * result + (username != null ? username.hashCode() : 0);\n    result = 31 * result + (email != null ? email.hashCode() : 0);\n    result = 31 * result + (firstName != null ? firstName.hashCode() : 0);\n    result = 31 * result + (lastName != null ? lastName.hashCode() : 0);\n    result = 31 * result + (companyName != null ? companyName.hashCode() : 0);\n    result = 31 * result + (acceptLanguage != null ? acceptLanguage.hashCode() : 0);\n    result = 31 * result + (hostedLoginToken != null ? hostedLoginToken.hashCode() : 0);\n    result = 31 * result + (createdAt != null ? createdAt.hashCode() : 0);\n    return result;\n}",
        "rpfc": "@Override\npublic int hashCode() {\n    int result = adjustments != null ? adjustments.hashCode() : 0;\n    result = 31 * result + (invoices != null ? invoices.hashCode() : 0);\n    result = 31 * result + (subscriptions != null ? subscriptions.hashCode() : 0);\n    result = 31 * result + (transactions != null ? transactions.hashCode() : 0);\n    result = 31 * result + (accountCode != null ? accountCode.hashCode() : 0);\n    result = 31 * result + (state != null ? state.hashCode() : 0);\n    result = 31 * result + (username != null ? username.hashCode() : 0);\n    result = 31 * result + (email != null ? email.hashCode() : 0);\n    result = 31 * result + (firstName != null ? firstName.hashCode() : 0);\n    result = 31 * result + (lastName != null ? lastName.hashCode() : 0);\n    result = 31 * result + (companyName != null ? companyName.hashCode() : 0);\n    result = 31 * result + (acceptLanguage != null ? acceptLanguage.hashCode() : 0);\n    result = 31 * result + (hostedLoginToken != null ? hostedLoginToken.hashCode() : 0);\n    result = 31 * result + (createdAt != null ? createdAt.hashCode() : 0);\n    result = 31 * result + (billingInfo != null ? billingInfo.hashCode() : 0);\n    return result;\n}",
        "tuc": "@Test(groups = \"fast\")\npublic void testSerialization() throws Exception {\n    final BillingInfo billingInfo = new BillingInfo();\n    billingInfo.setAddress1(randomString());\n    billingInfo.setAddress2(randomString());\n    billingInfo.setCardType(randomString());\n    billingInfo.setCity(randomString());\n    billingInfo.setCompany(randomString());\n    billingInfo.setCountry(randomString());\n    billingInfo.setFirstName(randomString());\n    billingInfo.setFirstSix(randomString());\n    billingInfo.setIpAddress(randomString());\n    billingInfo.setIpAddressCountry(randomString());\n    billingInfo.setLastFour(randomString());\n    billingInfo.setLastName(randomString());\n    billingInfo.setMonth(3);\n    billingInfo.setNumber(randomString());\n    billingInfo.setPhone(randomString());\n    billingInfo.setState(randomString());\n    billingInfo.setVatNumber(randomString());\n    billingInfo.setVerificationValue(Integer.MAX_VALUE);\n    billingInfo.setYear(Integer.MIN_VALUE);\n    billingInfo.setZip(randomString());\n    final String xml = xmlMapper.writeValueAsString(billingInfo);\n    Assert.assertEquals(xmlMapper.readValue(xml, BillingInfo.class), billingInfo);\n}",
        "label": 0
    },
    {
        "repo_name": "nefeed___WebSocket-SpringBoot",
        "commit": "588b455412bb587138268afdb4a525680be8fbbc",
        "commit_message": "fix Bug\uff1a\u89e3\u51b3jedis.close\u524d\u6ca1\u6709\u505a\u7a7a\u6307\u9488\u6821\u9a8c\n",
        "p_path": "src/main/java/com/xbongbong/component/RedisClient.java",
        "t_path": "src/test/java/com/xbongbong/XbbWebSocketApplicationTests.java",
        "p_name": "rpoplpush",
        "t_name": "afterjunittest",
        "lpfc": "public String rpoplpush(String popKey, String pushKey) throws Exception {\n    Jedis jedis = null;\n    String subUserHashPopKey = localConstant.getProjectName() + \":\" + popKey;\n    String subUserHashPushKey = localConstant.getProjectName() + \":\" + pushKey;\n    try {\n        jedis = jedisPool.getResource();\n        return jedis.rpoplpush(subUserHashPopKey, subUserHashPushKey);\n    } finally {\n        jedis.close();\n    }\n}",
        "rpfc": "public String rpoplpush(String popKey, String pushKey) throws Exception {\n    Jedis jedis = null;\n    String subUserHashPopKey = localConstant.getProjectName() + \":\" + popKey;\n    String subUserHashPushKey = localConstant.getProjectName() + \":\" + pushKey;\n    try {\n        jedis = jedisPool.getResource();\n        return jedis.rpoplpush(subUserHashPopKey, subUserHashPushKey);\n    } finally {\n        if (jedis != null) {\n            jedis.close();\n        }\n    }\n}",
        "tuc": "@After\npublic void afterJunitTest() {\n    int endTime = DateUtil.getInt();\n    LOG.info(\"========================= \u5355\u5143\u6d4b\u8bd5\u5f00\u59cb\u65f6\u95f4\uff1a\" + endTime + \" ==========================\");\n    LOG.info(\"*********************** \u7d2f\u8ba1\u7528\u65f6\uff1a\" + (endTime - mStartTime) + \"s ************************\");\n}",
        "label": 0
    },
    {
        "repo_name": "apache___oodt",
        "commit": "82b5225162f78ee092e473218769bea61e3e510f",
        "commit_message": "close connection to release client\n",
        "p_path": "filemgr/src/main/java/org/apache/oodt/cas/filemgr/system/AvroFileManagerClient.java",
        "t_path": "pge/src/test/java/org/apache/oodt/cas/pge/util/TestXmlHelper.java",
        "p_name": "retrieveFile",
        "t_name": "testgetnamespace",
        "lpfc": "@Override\npublic byte[] retrieveFile(String filePath, int offset, int numBytes) throws DataTransferException {\n    try {\n        return this.proxy.retrieveFile(filePath, offset, numBytes).array();\n    } catch (AvroRemoteException e) {\n        throw new DataTransferException(e.getMessage());\n    }\n}",
        "rpfc": "@Override\npublic byte[] retrieveFile(String filePath, int offset, int numBytes) throws DataTransferException {\n    try {\n        return this.proxy.retrieveFile(filePath, offset, numBytes).array();\n    } catch (AvroRemoteException e) {\n        throw new DataTransferException(e.getMessage());\n    } finally {\n        try {\n            this.client.close();\n        } catch (IOException e) {\n            throw new DataTransferException(e.getMessage());\n        }\n    }\n}",
        "tuc": "public void testGetNamespace() throws Exception {\n    Element elem = getRootElement(IMPORTS_ONLY_PGE_CONFIG);\n    Metadata metadata = new Metadata();\n    NodeList importTags = elem.getElementsByTagName(IMPORT_TAG);\n    assertEquals(\"blank\", getNamespace((Element) importTags.item(0), metadata));\n    assertEquals(\"blank-pge-config.xml\", getFile((Element) importTags.item(0), metadata));\n    assertNull(getNamespace((Element) importTags.item(1), metadata));\n    assertEquals(\"pge-config.xml\", getFile((Element) importTags.item(1), metadata));\n}",
        "label": 0
    },
    {
        "repo_name": "killbilling___recurly-java-library",
        "commit": "f4b0d28dedefbd89a55b934bdeb9fe13ffe6b15e",
        "commit_message": "Added url encoding\n",
        "p_path": "src/main/java/com/ning/billing/recurly/RecurlyClient.java",
        "t_path": "src/test/java/com/ning/billing/recurly/model/TestSubscriptionUpdate.java",
        "p_name": "deleteAddOn",
        "t_name": "testserializationwithoutaddons",
        "lpfc": "public void deleteAddOn(final String planCode, final String addOnCode) {\n    doDELETE(Plan.PLANS_RESOURCE + \"/\" + planCode + AddOn.ADDONS_RESOURCE + \"/\" + addOnCode);\n}",
        "rpfc": "public void deleteAddOn(final String planCode, final String addOnCode) {\n    doDELETE(Plan.PLANS_RESOURCE + \"/\" + urlEncode(planCode) + AddOn.ADDONS_RESOURCE + \"/\" + urlEncode(addOnCode));\n}",
        "tuc": "@Test(groups = \"fast\")\npublic void testSerializationWithoutAddOns() throws Exception {\n    final SubscriptionUpdate subscription = new SubscriptionUpdate();\n    subscription.setPlanCode(\"gold\");\n    subscription.setTimeframe(SubscriptionUpdate.Timeframe.now);\n    subscription.setUnitAmountInCents(800);\n    subscription.setQuantity(1);\n    final String xml = xmlMapper.writeValueAsString(subscription);\n    Assert.assertEquals(xml, \"<subscription xmlns=\\\"\\\">\" + \"<timeframe>now</timeframe>\" + \"<unit_amount_in_cents>800</unit_amount_in_cents>\" + \"<quantity>1</quantity>\" + \"<plan_code>gold</plan_code>\" + \"</subscription>\");\n}",
        "label": 0
    },
    {
        "repo_name": "jetlang___remoting",
        "commit": "b3e900e1d104028d770a27aced6d7be65f8098a6",
        "commit_message": "assign state before sending subscriptions.\n",
        "p_path": "src/main/java/org/jetlang/remote/client/JetlangTcpNioClient.java",
        "t_path": "src/test/java/org/jetlang/web/HttpRequestTest.java",
        "p_name": "createClientOnConnect",
        "t_name": "testgettingbodywithbadencoding",
        "lpfc": "@Override\npublic TcpClientNioFiber.ConnectedClient createClientOnConnect(SocketChannel chan, NioFiber nioFiber, TcpClientNioFiber.Writer writer) {\n    NioJetlangProtocolReader<R> reader = new NioJetlangProtocolReader<R>(chan, msgHandler, ser.getReader(), topicReader, () -> {\n        timeout.publish(new ReadTimeoutEvent());\n    });\n    NioJetlangRemotingClientFactory.Id chanId = new NioJetlangRemotingClientFactory.Id(chan);\n    NioJetlangSendFiber.ChannelState newState = new NioJetlangSendFiber.ChannelState(chan, chanId, nioFiber);\n    this.sendFiber.onNewSession(newState);\n    this.remoteSubscriptions.onConnect();\n    this.connectEventChannel.publish(new ConnectEvent());\n    Disposable hbSched = sendFiber.scheduleHeartbeat(newState, config.getHeartbeatIntervalInMs(), config.getHeartbeatIntervalInMs(), TimeUnit.MILLISECONDS);\n    Disposable readTimeout = nioFiber.scheduleAtFixedRate(() -> {\n        reader.checkForReadTimeout(readTimeoutInMs);\n    }, readTimeoutInMs, readTimeoutInMs, TimeUnit.MILLISECONDS);\n    this.channel = newState;\n    return new TcpClientNioFiber.ConnectedClient() {\n        @Override\n        public boolean read(SocketChannel chan) {\n            return reader.read();\n        }\n        @Override\n        public void onDisconnect() {\n            readTimeout.dispose();\n            hbSched.dispose();\n            sendFiber.handleClose(newState);\n            JetlangClientFactory.this.channel = null;\n            closed.publish(new CloseEvent.GracefulDisconnect());\n        }\n    };\n}",
        "rpfc": "@Override\npublic TcpClientNioFiber.ConnectedClient createClientOnConnect(SocketChannel chan, NioFiber nioFiber, TcpClientNioFiber.Writer writer) {\n    NioJetlangProtocolReader<R> reader = new NioJetlangProtocolReader<R>(chan, msgHandler, ser.getReader(), topicReader, () -> {\n        timeout.publish(new ReadTimeoutEvent());\n    });\n    NioJetlangRemotingClientFactory.Id chanId = new NioJetlangRemotingClientFactory.Id(chan);\n    NioJetlangSendFiber.ChannelState newState = new NioJetlangSendFiber.ChannelState(chan, chanId, nioFiber);\n    this.sendFiber.onNewSession(newState);\n    this.channel = newState;\n    this.remoteSubscriptions.onConnect();\n    this.connectEventChannel.publish(new ConnectEvent());\n    Disposable hbSched = sendFiber.scheduleHeartbeat(newState, config.getHeartbeatIntervalInMs(), config.getHeartbeatIntervalInMs(), TimeUnit.MILLISECONDS);\n    Disposable readTimeout = nioFiber.scheduleAtFixedRate(() -> {\n        reader.checkForReadTimeout(readTimeoutInMs);\n    }, readTimeoutInMs, readTimeoutInMs, TimeUnit.MILLISECONDS);\n    return new TcpClientNioFiber.ConnectedClient() {\n        @Override\n        public boolean read(SocketChannel chan) {\n            return reader.read();\n        }\n        @Override\n        public void onDisconnect() {\n            readTimeout.dispose();\n            hbSched.dispose();\n            sendFiber.handleClose(newState);\n            JetlangClientFactory.this.channel = null;\n            closed.publish(new CloseEvent.GracefulDisconnect());\n        }\n    };\n}",
        "tuc": "@Test\npublic void testGettingBodyWithBadEncoding() {\n    HttpRequest req = new HttpRequest(\"POST\", \"/path\", \"HTTP/1.1\", null);\n    req.add(\"Content-Type\", \"application/x-www-form-urlencoded ; charset=UTF-99\");\n    try {\n        req.getContentCharset(true);\n        fail(\"Should fail\");\n    } catch (UnsupportedCharsetException expected) {\n    }\n}",
        "label": 0
    },
    {
        "repo_name": "Doctoror___PainlessMusicPlayer",
        "commit": "0d822f214b93e6f82732a9842ada2ff9973db50a",
        "commit_message": "TrackableDisposable: remove m prefixes.\n",
        "p_path": "app/src/androidTest/java/com/doctoror/fuckoffmusicplayer/base/TrackableDisposable.java",
        "t_path": "data/src/test/java/com/doctoror/fuckoffmusicplayer/data/util/RandomHolderTest.java",
        "p_name": "dispose",
        "t_name": "testrandomnotnull",
        "lpfc": "@Override\npublic void dispose() {\n    mDisposed = true;\n}",
        "rpfc": "@Override\npublic void dispose() {\n    disposed = true;\n}",
        "tuc": "@Test\npublic void testRandomNotNull() {\n    assertNotNull(RandomHolder.getInstance().getRandom());\n}",
        "label": 0
    },
    {
        "repo_name": "doov-org___doov",
        "commit": "0d21be6601564b3fda0256109e7ebf75d023a366",
        "commit_message": "renamed ResultAssert#hasFailedNode to hasInvalidated\n",
        "p_path": "assertions/src/main/java/io/doov/assertions/ResultAssert.java",
        "t_path": "sample/generated/src/test/java/io/doov/sample/model/SampleModelIteratorTest.java",
        "p_name": "isTrue",
        "t_name": "iterator_test",
        "lpfc": "public ResultAssert isTrue() {\n    if (!actual.isTrue()) {\n        failWithMessage(\"Expected result to be true (failed nodes: \" + getInvalidatedMetadata() + \")\");\n    }\n    return this;\n}",
        "rpfc": "public ResultAssert isTrue() {\n    if (!actual.isTrue()) {\n        failWithMessage(\"Expected result to be true (invalidated nodes: \" + getInvalidatedMetadata() + \")\");\n    }\n    return this;\n}",
        "tuc": "@Test\npublic void iterator_test() {\n    for (Entry<FieldId, Object> entry : source) {\n        assertThat(entry.getValue()).isEqualTo(source.get(entry.getKey()));\n    }\n}",
        "label": 0
    }
]